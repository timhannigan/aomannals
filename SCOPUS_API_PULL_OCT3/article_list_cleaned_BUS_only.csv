,abstract,date,id,publicationName,title,abstract_cleaned
0,"Well-being as an intangible, philosophical, and multi-faceted phenomenon is hard to measure. By taking a psycholinguistic expression of well-being, we measure how tourists' experiencing holiday destinations affects their well-being states. The well-being state includes increases or decreases in Hedonia and Eudaimonia as a result of destination experiences. We apply text topic modelling to analyse big Web 2.0 datasets. These include tourists’ self-reports of their experiences during their visit to destination countries collected from Travelblog.org weblog. The findings from a Global sample, New Zealand, and France are compared to generalise Hedonia and Eudaimonia conceptualisations across these destinations. The outputs also characterise experiences with maximum well-being and ill-being that include both Hedonia and Eudaimonia. Managerial implications include how a destination image may be managed according to desired well-being states. This also helps tourists make informed decisions about their holiday destinations.",2018-12-01,2-s2.0-85048543703,Tourism Management,Hedonic and eudaimonic well-being: A psycholinguistic view,"Well-being as an intangible, philosophical, and multi-faceted phenomenon is hard to measure. By taking a psycholinguistic expression of well-being, we measure how tourists' experiencing holiday destinations affects their well-being states. The well-being state includes increases or decreases in Hedonia and Eudaimonia as a result of destination experiences. We apply text topic modelling to analyse big Web 2.0 datasets. These include tourists’ self-reports of their experiences during their visit to destination countries collected from Travelblog.org weblog. The findings from a Global sample, New Zealand, and France are compared to generalise Hedonia and Eudaimonia conceptualisations across these destinations. The outputs also characterise experiences with maximum well-being and ill-being that include both Hedonia and Eudaimonia. Managerial implications include how a destination image may be managed according to desired well-being states. This also helps tourists make informed decisions about their holiday destinations."
1,"The rapid growth of the Chinese economy has resulted in great pressure on the environment: technological innovation is the fundamental pathway for improvement the efficiency in the process of energy saving and emission reduction. Based on large-scale technical text data in 31 Chinese provinces from 1985 to 2017, the Latent Dirichlet Allocation (LDA) topic model is introduced to technology content analysis. Then the LDA provincial-topic model is constructed, the subject and object of energy technology are jointly modelled, and the relationship between technology subject and technology have been region studied. The energy saving and emission reduction technology research direction in 31 provinces of China in the past 30 years has been examined: the status and level of technical reserves in each province have been evaluated, and the heterogeneity of provincial patent subject content has been compared. The research found that, despite the cumulative patent record (No. 1 world ranking), similarities in research directions in different provinces of China caused by poor regional technology mobility are a growing problem for Chinese energy technology, indicating much repeated research and development in different regions of China. Targeted suggestions on inter-provincial technology transfer and cooperation on energy technology development have been put forward.",2018-11-20,2-s2.0-85053080182,Journal of Cleaner Production,Heterogeneity evaluation of China's provincial energy technology based on large-scale technical text data mining,"The rapid growth of the Chinese economy has resulted in great pressure on the environment: technological innovation is the fundamental pathway for improvement the efficiency in the process of energy saving and emission reduction. Based on large-scale technical text data in 31 Chinese provinces from 1985 to 2017, the Latent Dirichlet Allocation (LDA) topic model is introduced to technology content analysis. Then the LDA provincial-topic model is constructed, the subject and object of energy technology are jointly modelled, and the relationship between technology subject and technology have been region studied. The energy saving and emission reduction technology research direction in 31 provinces of China in the past 30 years has been examined: the status and level of technical reserves in each province have been evaluated, and the heterogeneity of provincial patent subject content has been compared. The research found that, despite the cumulative patent record (No. 1 world ranking), similarities in research directions in different provinces of China caused by poor regional technology mobility are a growing problem for Chinese energy technology, indicating much repeated research and development in different regions of China. Targeted suggestions on inter-provincial technology transfer and cooperation on energy technology development have been put forward."
2,"The aim of this study is to introduce a decision framework that integrates Quality Function Deployment (QFD) methodology and topic modelling to facilitate analysing online reviews, that is, a digital source of voice of the customer (VoC), to extract the true customer needs for developing a product/service. Within the framework, after collecting and preprocessing customer reviews, Latent Dirichlet Allocation is used for grouping them as topics. Most representative reviews are determined based on corresponding topic distributions, and QFD methodology is employed to clarify these reviews and reveal true customer needs. For demonstration, this framework was used to analyse 83,545 reviews on Italian restaurants gathered from Yelp.com via topic modelling with a more objective perspective. The number of documents to be examined was decreased by approximately 95%. Thereupon the completion time of customer voice table, one of the most important tools in QFD studies, reduced dramatically. Besides, text mining process is enhanced for retrieving meaningful information from customer reviews by using QFD tools and their tailored approach. This study is the first attempt that integrates QFD and topic modelling in order to increase the efficiency of the VoC clarification process.",2018-11-10,2-s2.0-85007493460,Total Quality Management and Business Excellence,Topic modelling-based decision framework for analysing digital voice of the customer,"The aim of this study is to introduce a decision framework that integrates Quality Function Deployment (QFD) methodology and topic modelling to facilitate analysing online reviews, that is, a digital source of voice of the customer (VoC), to extract the true customer needs for developing a product/service. Within the framework, after collecting and preprocessing customer reviews, Latent Dirichlet Allocation is used for grouping them as topics. Most representative reviews are determined based on corresponding topic distributions, and QFD methodology is employed to clarify these reviews and reveal true customer needs. For demonstration, this framework was used to analyse 83,545 reviews on Italian restaurants gathered from Yelp.com via topic modelling with a more objective perspective. The number of documents to be examined was decreased by approximately 95%. Thereupon the completion time of customer voice table, one of the most important tools in QFD studies, reduced dramatically. Besides, text mining process is enhanced for retrieving meaningful information from customer reviews by using QFD tools and their tailored approach. This study is the first attempt that integrates QFD and topic modelling in order to increase the efficiency of the VoC clarification process."
3,"The new reality of networked publics on social media calls for crisis communication practitioners and researchers to understand the narratives generated by publics on social media during organizational crises. As social media publics possess diverse, unique characteristics and communicative needs during a crisis, they form interpretative communities and co-create various symbolic interpretations of the crisis. Extending the public-centric and narrative perspective to the context of social media crises, we examined what crisis narratives were constructed by social media publics (i.e., multiplicity) and how these narratives changed by crisis stages (i.e., dynamics). Using topic modelling based on large-scale Twitter data of the Chipotle E. coli crisis (N = 40,610), we identified ten narratives subsumed under two themes (i.e., sharing-based and conversation-based) based on publics’ social constructions of their perceived risks and crisis experience. On the one hand, sharing-based narratives, heavily impacted by publics’ shared media coverage, reflected media crisis narratives and salient risk perceptions aligning with the news agenda. On the other hand, conversation-based narratives, fueled by publics’ opinion expression and emotional venting, demonstrated publics’ interpretations of their experience with the organization in the crisis with less salient but more diversified risk perceptions. Crisis managers are recommended to produce and deliver compelling narratives resonating with different groups of social media publics during crises.",2018-11-01,2-s2.0-85050884991,Public Relations Review,Examining multiplicity and dynamics of publics’ crisis narratives with large-scale Twitter data,"The new reality of networked publics on social media calls for crisis communication practitioners and researchers to understand the narratives generated by publics on social media during organizational crises. As social media publics possess diverse, unique characteristics and communicative needs during a crisis, they form interpretative communities and co-create various symbolic interpretations of the crisis. Extending the public-centric and narrative perspective to the context of social media crises, we examined what crisis narratives were constructed by social media publics (i.e., multiplicity) and how these narratives changed by crisis stages (i.e., dynamics). Using topic modelling based on large-scale Twitter data of the Chipotle E. coli crisis (N = 40,610), we identified ten narratives subsumed under two themes (i.e., sharing-based and conversation-based) based on publics’ social constructions of their perceived risks and crisis experience. On the one hand, sharing-based narratives, heavily impacted by publics’ shared media coverage, reflected media crisis narratives and salient risk perceptions aligning with the news agenda. On the other hand, conversation-based narratives, fueled by publics’ opinion expression and emotional venting, demonstrated publics’ interpretations of their experience with the organization in the crisis with less salient but more diversified risk perceptions. Crisis managers are recommended to produce and deliver compelling narratives resonating with different groups of social media publics during crises."
4,"The study of technology and innovation management (TIM) has continued to evolve and expand with great speed over the last three decades. This research aims to identify core topics in TIM studies and explore their dynamic changes. The conventional approach, based on discrete assignments by subjective judgment with predetermined categories, cannot effectively capture latent topics from large volumes of scholarly data. Hence, this study adopts the topic model approach, which automatically discovers topics that pervade a large and unstructured collection of documents, to uncover research topics in TIM research. The 50 topics of TIM research are identified through the Latent Dirichlet Allocation model from 11,693 articles published from 1997 to 2016 in 11 TIM journals, and top 10 most popular topics in TIM research are briefly reviewed. We then explore topic trends by examining the changes in topics rankings over different time periods and identifying hot and cold topics of TIM research over the last two decades. For each of the 11 TIM journals, the areas of subspecialty and the effects of editor changes on topic portfolios are also investigated. The findings of this study are expected to provide implications for researchers, journal editors, and policy makers in the field of TIM.",2018-10-01,2-s2.0-85012236978,Journal of Technology Transfer,Identifying core topics in technology and innovation management studies: a topic model approach,"The study of technology and innovation management (TIM) has continued to evolve and expand with great speed over the last three decades. This research aims to identify core topics in TIM studies and explore their dynamic changes. The conventional approach, based on discrete assignments by subjective judgment with predetermined categories, cannot effectively capture latent topics from large volumes of scholarly data. Hence, this study adopts the topic model approach, which automatically discovers topics that pervade a large and unstructured collection of documents, to uncover research topics in TIM research. The 50 topics of TIM research are identified through the Latent Dirichlet Allocation model from 11,693 articles published from 1997 to 2016 in 11 TIM journals, and top 10 most popular topics in TIM research are briefly reviewed. We then explore topic trends by examining the changes in topics rankings over different time periods and identifying hot and cold topics of TIM research over the last two decades. For each of the 11 TIM journals, the areas of subspecialty and the effects of editor changes on topic portfolios are also investigated. The findings of this study are expected to provide implications for researchers, journal editors, and policy makers in the field of TIM."
5,"The current debate on corporate social responsibility was triggered by the understanding that companies should be responsible for mitigating the impacts of their activities on the society and the environment. In this paper, drawing on the structural power of business, we examine the rule-setting power in which sugarcane ethanol companies engage, and then discuss why these companies develop and use institutions to promote sustainability. We use a machine learning algorithm, latent Dirichlet allocation, to identify companies’ commitment to sustainability and business-led governance by analyzing a large volume of data from public corporate documents. The results reveal 36 main themes that demonstrate the rule-setting power of the sugarcane ethanol industry through voluntary standards, codes of conduct, and corporate social responsibility these companies use as indicators of superior social and environmental performance and to show sustainability is embedded in companies’ priorities. However, the results also show critical issues of unsustainable production practices can affect industry image and stress socio-environmental relationships. These issues can be ameliorated with integrative governance that considers the independence of the sectors involved in ethanol production.",2018-10-01,2-s2.0-85049884289,Journal of Cleaner Production,Sustainability and governance of sugarcane ethanol companies in Brazil: Topic modeling analysis of CSR reporting,"The current debate on corporate social responsibility was triggered by the understanding that companies should be responsible for mitigating the impacts of their activities on the society and the environment. In this paper, drawing on the structural power of business, we examine the rule-setting power in which sugarcane ethanol companies engage, and then discuss why these companies develop and use institutions to promote sustainability. We use a machine learning algorithm, latent Dirichlet allocation, to identify companies’ commitment to sustainability and business-led governance by analyzing a large volume of data from public corporate documents. The results reveal 36 main themes that demonstrate the rule-setting power of the sugarcane ethanol industry through voluntary standards, codes of conduct, and corporate social responsibility these companies use as indicators of superior social and environmental performance and to show sustainability is embedded in companies’ priorities. However, the results also show critical issues of unsustainable production practices can affect industry image and stress socio-environmental relationships. These issues can be ameliorated with integrative governance that considers the independence of the sectors involved in ethanol production."
6,"Large-scale infrastructure projects have important strategic positions in economy and social development, especially in developing countries, and are growing constantly larger and more complex. During the life cycle of a large-scale infrastructure project, technological problems and environmental and socio-economic impacts drive relevant scientific research. However, few empirical studies have been conducted to examine the interaction between project and research. In this paper, we use the Three Gorges Project (TGP), which is the world's largest hydropower project, as a case study to find the patterns of scientific research on specific large-scale infrastructure projects. Text mining and other quantitative methods have been used to reveal: (1) the growth of the publication outputs of the TGP relevant research, (2) major research topics and their temporal trends and (3) the effects of language and corresponding author's nationality on topic proportions in papers. We find that topics of engineering issues were discussed more in domestic journals and in earlier stage of the project's life cycle. Meanwhile, topics of environmental issues were discussed more in international journals and became increasingly popular during the life cycle of the project. These findings are useful for policy-makers and project managers to better establish collaborations with the academia and to more properly allocate resources in future project management practices.",2018-09-01,2-s2.0-85047395068,Technological Forecasting and Social Change,Scientific research driven by large-scale infrastructure projects: A case study of the Three Gorges Project in China,"Large-scale infrastructure projects have important strategic positions in economy and social development, especially in developing countries, and are growing constantly larger and more complex. During the life cycle of a large-scale infrastructure project, technological problems and environmental and socio-economic impacts drive relevant scientific research. However, few empirical studies have been conducted to examine the interaction between project and research. In this paper, we use the Three Gorges Project (TGP), which is the world's largest hydropower project, as a case study to find the patterns of scientific research on specific large-scale infrastructure projects. Text mining and other quantitative methods have been used to reveal: (1) the growth of the publication outputs of the TGP relevant research, (2) major research topics and their temporal trends and (3) the effects of language and corresponding author's nationality on topic proportions in papers. We find that topics of engineering issues were discussed more in domestic journals and in earlier stage of the project's life cycle. Meanwhile, topics of environmental issues were discussed more in international journals and became increasingly popular during the life cycle of the project. These findings are useful for policy-makers and project managers to better establish collaborations with the academia and to more properly allocate resources in future project management practices."
7,"Entrepreneurial ecosystems are vital sources of innovation and critical engines for economic growth. In this study, we use text-based analysis, network visualizations, and topic modeling of nearly 60&#x2009;000 venture business descriptions&#x2014;i.e., how ventures present technologies and products to key stakeholders including customers, employees, and investors&#x2014;to examine the structure of 35 global entrepreneurial ecosystems. Rather than using predefined industry classifications, we allow the structural configurations to emerge endogenously revealing a more variegated perspective of venture strategic positioning in entrepreneurial ecosystems. Our study makes several important contributions. First, by examining strategic positioning statements of geographically defined ventures, we contribute and advance our understanding of the geography of innovation and structure of entrepreneurial ecosystems. Our results indicate that there are wide differences in entrepreneurial ecosystem size, structure, composition, and venture strategic positioning. Second, methodologically, we use novel computational approaches and introduce visualization as a powerful means to understand entrepreneurial ecosystems. Third, our results show that ventures from widely different industries often use similar position statements, thus highlighting that ecosystems are indeed not just defined by industries, but also strategic positioning. We conclude with theoretical and managerial implications.",2018-08-03,2-s2.0-85051011185,IEEE Transactions on Engineering Management,Visual Analysis of Venture Similarity in Entrepreneurial Ecosystems,"Entrepreneurial ecosystems are vital sources of innovation and critical engines for economic growth. In this study, we use text-based analysis, network visualizations, and topic modeling of nearly 60&#x2009;000 venture business descriptions&#x2014;i.e., how ventures present technologies and products to key stakeholders including customers, employees, and investors&#x2014;to examine the structure of 35 global entrepreneurial ecosystems. Rather than using predefined industry classifications, we allow the structural configurations to emerge endogenously revealing a more variegated perspective of venture strategic positioning in entrepreneurial ecosystems. Our study makes several important contributions. First, by examining strategic positioning statements of geographically defined ventures, we contribute and advance our understanding of the geography of innovation and structure of entrepreneurial ecosystems. Our results indicate that there are wide differences in entrepreneurial ecosystem size, structure, composition, and venture strategic positioning. Second, methodologically, we use novel computational approaches and introduce visualization as a powerful means to understand entrepreneurial ecosystems. Third, our results show that ventures from widely different industries often use similar position statements, thus highlighting that ecosystems are indeed not just defined by industries, but also strategic positioning. We conclude with theoretical and managerial implications."
8,"In recent decades, the amount of text available for organizational science research has grown tremendously. Despite the availability of text and advances in text analysis methods, many of these techniques remain largely segmented by discipline. Moreover, there is an increasing number of open-source tools (R, Python) for text analysis, yet these tools are not easily taken advantage of by social science researchers who likely have limited programming knowledge and exposure to computational methods. In this article, we compare quantitative and qualitative text analysis methods used across social sciences. We describe basic terminology and the overlooked, but critically important, steps in pre-processing raw text (e.g., selection of stop words; stemming). Next, we provide an exploratory analysis of open-ended responses from a prototypical survey dataset using topic modeling with R. We provide a list of best practice recommendations for text analysis focused on (1) hypothesis and question formation, (2) design and data collection, (3) data pre-processing, and (4) topic modeling. We also discuss the creation of scale scores for more traditional correlation and regression analyses. All the data are available in an online repository for the interested reader to practice with, along with a reference list for additional reading, an R markdown file, and an open source interactive topic model tool (topicApp; see https://github.com/wesslen/topicApp, https://github.com/wesslen/text-analysis-org-science, https://dataverse.unc.edu/dataset.xhtml?persistentId=doi:10.15139/S3/R4W7ZS).",2018-08-01,2-s2.0-85040335587,Journal of Business and Psychology,A Review of Best Practice Recommendations for Text Analysis in R (and a User-Friendly App),"In recent decades, the amount of text available for organizational science research has grown tremendously. Despite the availability of text and advances in text analysis methods, many of these techniques remain largely segmented by discipline. Moreover, there is an increasing number of open-source tools (R, Python) for text analysis, yet these tools are not easily taken advantage of by social science researchers who likely have limited programming knowledge and exposure to computational methods. In this article, we compare quantitative and qualitative text analysis methods used across social sciences. We describe basic terminology and the overlooked, but critically important, steps in pre-processing raw text (e.g., selection of stop words; stemming). Next, we provide an exploratory analysis of open-ended responses from a prototypical survey dataset using topic modeling with R. We provide a list of best practice recommendations for text analysis focused on (1) hypothesis and question formation, (2) design and data collection, (3) data pre-processing, and (4) topic modeling. We also discuss the creation of scale scores for more traditional correlation and regression analyses. All the data are available in an online repository for the interested reader to practice with, along with a reference list for additional reading, an R markdown file, and an open source interactive topic model tool (topicApp; see https://github.com/wesslen/topicApp, https://github.com/wesslen/text-analysis-org-science, https://dataverse.unc.edu/dataset.xhtml?persistentId=doi:10.15139/S3/R4W7ZS)."
9,"The purpose of this paper is to demonstrate that user-generated online contents can be used as an alternative data source for assessing airport service quality, which effectively complements and cross-validates the conventional service quality surveys. We apply sentiment analysis and topic modeling technique to 42,137 reviews collected from Google Maps. The results are compared to the well-publicized ASQ ratings conducted by Airport Council International. The sentiment scores computed from the textual Google reviews are very good predictors of the associated Google star ratings, with r",2018-08-01,2-s2.0-85048149191,Journal of Air Transport Management,Assessment of airport service quality: A complementary approach to measure perceived service quality based on Google reviews,"The purpose of this paper is to demonstrate that user-generated online contents can be used as an alternative data source for assessing airport service quality, which effectively complements and cross-validates the conventional service quality surveys. We apply sentiment analysis and topic modeling technique to 42,137 reviews collected from Google Maps. The results are compared to the well-publicized ASQ ratings conducted by Airport Council International. The sentiment scores computed from the textual Google reviews are very good predictors of the associated Google star ratings, with r"
10,"Despite the ubiquity of textual data, so far few researchers have applied text mining to answer organizational research questions. Text mining, which essentially entails a quantitative approach to the analysis of (usually) voluminous textual data, helps accelerate knowledge discovery by radically increasing the amount data that can be analyzed. This article aims to acquaint organizational researchers with the fundamental logic underpinning text mining, the analytical stages involved, and contemporary techniques that may be used to achieve different types of objectives. The specific analytical techniques reviewed are (a) dimensionality reduction, (b) distance and similarity computing, (c) clustering, (d) topic modeling, and (e) classification. We describe how text mining may extend contemporary organizational research by allowing the testing of existing or new research questions with data that are likely to be rich, contextualized, and ecologically valid. After an exploration of how evidence for the validity of text mining output may be generated, we conclude the article by illustrating the text mining process in a job analysis setting using a dataset composed of job vacancies.",2018-07-01,2-s2.0-85046617359,Organizational Research Methods,Text Mining in Organizational Research,"Despite the ubiquity of textual data, so far few researchers have applied text mining to answer organizational research questions. Text mining, which essentially entails a quantitative approach to the analysis of (usually) voluminous textual data, helps accelerate knowledge discovery by radically increasing the amount data that can be analyzed. This article aims to acquaint organizational researchers with the fundamental logic underpinning text mining, the analytical stages involved, and contemporary techniques that may be used to achieve different types of objectives. The specific analytical techniques reviewed are (a) dimensionality reduction, (b) distance and similarity computing, (c) clustering, (d) topic modeling, and (e) classification. We describe how text mining may extend contemporary organizational research by allowing the testing of existing or new research questions with data that are likely to be rich, contextualized, and ecologically valid. After an exploration of how evidence for the validity of text mining output may be generated, we conclude the article by illustrating the text mining process in a job analysis setting using a dataset composed of job vacancies."
11,"A topic model is a statistical model for modeling high dimensional count data. Many different parameters (solutions) of a topic model can be obtained through a learning algorithm due to different initial conditions. This paper focuses on diversity of solutions. To utilize diversity of solutions, it is necessary to acquire distribution structure of them. Therefore, this paper proposes a novel method to define similarity (inner product) of solutions using normalized mutual information to analyze distribution of solutions. Experimental results for text data are presented to show the usefulness of the proposed method.",2018-06-20,2-s2.0-85050125812,"Proceedings of 2018 5th International Conference on Business and Industrial Research: Smart Technology for Next Generation of Information, Engineering, Business and Social Science, ICBIR 2018",A method for analyzing solution diversity in topic models,"A topic model is a statistical model for modeling high dimensional count data. Many different parameters (solutions) of a topic model can be obtained through a learning algorithm due to different initial conditions. This paper focuses on diversity of solutions. To utilize diversity of solutions, it is necessary to acquire distribution structure of them. Therefore, this paper proposes a novel method to define similarity (inner product) of solutions using normalized mutual information to analyze distribution of solutions. Experimental results for text data are presented to show the usefulness of the proposed method."
12,"Natural language processing (NLP) in Thai language is notoriously complicated. One major problem is the lack of word boundary in a sentence, introducing ambiguity in word tokenization. For topic extraction, semantic ambiguity adds another layer of complexity to the problem. Topic model that disregards word order, such as Latent Dirichlet Allocation (LDA), performs poorly in Thai Language. In this paper, we experimented and tested a probabilistic language model equipped with word location information, the so-called Topic N-grams model (TNG). We deployed several testing tasks to assess TNG's capabilities of modeling the generative process of Thai text and established benchmarks that compare the performance of LDA and TNG for various NLP tasks in Thai language. To our knowledge, this paper is the first to explore word-order model in Thai language topic extraction. We concluded that TNG can help boosting performance of Thai language processing in word cutting, semantic checking, word prediction, and document generation task. We also explored how we can measure performance of LDA and TNG on such tasks using perplexity.",2018-06-20,2-s2.0-85050154385,"Proceedings of 2018 5th International Conference on Business and Industrial Research: Smart Technology for Next Generation of Information, Engineering, Business and Social Science, ICBIR 2018",Probabilistic learning models for topic extraction i Thai language,"Natural language processing (NLP) in Thai language is notoriously complicated. One major problem is the lack of word boundary in a sentence, introducing ambiguity in word tokenization. For topic extraction, semantic ambiguity adds another layer of complexity to the problem. Topic model that disregards word order, such as Latent Dirichlet Allocation (LDA), performs poorly in Thai Language. In this paper, we experimented and tested a probabilistic language model equipped with word location information, the so-called Topic N-grams model (TNG). We deployed several testing tasks to assess TNG's capabilities of modeling the generative process of Thai text and established benchmarks that compare the performance of LDA and TNG for various NLP tasks in Thai language. To our knowledge, this paper is the first to explore word-order model in Thai language topic extraction. We concluded that TNG can help boosting performance of Thai language processing in word cutting, semantic checking, word prediction, and document generation task. We also explored how we can measure performance of LDA and TNG on such tasks using perplexity."
13,"This study examines analyst information intermediary roles using a textual analysis of analyst reports and corporate disclosures. We employ a topic modeling methodology from computational linguistic research to compare the thematic content of a large sample of analyst reports issued promptly after earnings conference calls with the content of the calls themselves. We show that analysts discuss exclusive topics beyond those from conference calls and interpret topics from conference calls. In addition, we find that investors place a greater value on new information in analyst reports when managers face greater incentives to withhold value-relevant information. Analyst interpretation is particularly valuable when the processing costs of conference call information increase. Finally, we document that investors react to analyst report content that simply confirms managers’ conference call discussions. Overall, our study shows that analysts play the information intermediary roles by discovering information beyond corporate disclosures and by clarifying and confirming corporate disclosures.",2018-06-01,2-s2.0-85048106526,Management Science,Analyst information discovery and interpretation roles: A topic modeling approach,"This study examines analyst information intermediary roles using a textual analysis of analyst reports and corporate disclosures. We employ a topic modeling methodology from computational linguistic research to compare the thematic content of a large sample of analyst reports issued promptly after earnings conference calls with the content of the calls themselves. We show that analysts discuss exclusive topics beyond those from conference calls and interpret topics from conference calls. In addition, we find that investors place a greater value on new information in analyst reports when managers face greater incentives to withhold value-relevant information. Analyst interpretation is particularly valuable when the processing costs of conference call information increase. Finally, we document that investors react to analyst report content that simply confirms managers’ conference call discussions. Overall, our study shows that analysts play the information intermediary roles by discovering information beyond corporate disclosures and by clarifying and confirming corporate disclosures."
14,"With the ever increasing number of Web services, discovering an appropriate Web service requested by users has become a vital yet challenging task. We need a scalable and efficient search engine to deal with the large volume of Web services. The aim of this approach is to provide an efficient search engine that can retrieve the most relevant Web services in a short time. The proposed Web service search engine (WSSE) is based on the probabilistic topic modeling and clustering techniques that are integrated to support each other by discovering the semantic meaning of Web services and reducing the search space. The latent Dirichlet allocation (LDA) is used to extract topics from Web service descriptions. These topics are used to group similar Web services together. Each Web service description is represented as a topic vector, so the topic model is an efficient technique to reduce the dimensionality of word vectors and to discover the semantic meaning that is hidden in Web service descriptions. Also, the Web service description is represented as a word vector to address the drawbacks of the keyword-based search system. The accuracy of the proposed WSSE is compared with the keyword-based search system. Also, the precision and recall metrics are used to evaluate the performance of the proposed approach and the keyword-based search system. The results show that the proposed WSSE based on LDA and clustering outperforms the keyword-based search system.",2018-06-01,2-s2.0-85044388351,Service Oriented Computing and Applications,A Web service search engine for large-scale Web service discovery based on the probabilistic topic modeling and clustering,"With the ever increasing number of Web services, discovering an appropriate Web service requested by users has become a vital yet challenging task. We need a scalable and efficient search engine to deal with the large volume of Web services. The aim of this approach is to provide an efficient search engine that can retrieve the most relevant Web services in a short time. The proposed Web service search engine (WSSE) is based on the probabilistic topic modeling and clustering techniques that are integrated to support each other by discovering the semantic meaning of Web services and reducing the search space. The latent Dirichlet allocation (LDA) is used to extract topics from Web service descriptions. These topics are used to group similar Web services together. Each Web service description is represented as a topic vector, so the topic model is an efficient technique to reduce the dimensionality of word vectors and to discover the semantic meaning that is hidden in Web service descriptions. Also, the Web service description is represented as a word vector to address the drawbacks of the keyword-based search system. The accuracy of the proposed WSSE is compared with the keyword-based search system. Also, the precision and recall metrics are used to evaluate the performance of the proposed approach and the keyword-based search system. The results show that the proposed WSSE based on LDA and clustering outperforms the keyword-based search system."
15,"Despite the popularity of online food and grocery shopping, little research has been conducted to understand the factors that influence consumers’ online food purchases. Using a topic modeling approach, our results show four interpretable factors have significant impacts on the helpfulness of customer reviews: Amazon Service, Physical Feature, Flavor Feature, and Subjective Expression. Readers of customer reviews perceive objective reviews as more helpful than subjective reviews. In addition, customer review helpfulness has a concave relationship with the length of the reviews. Our results provide important business implications on how to encourage more helpful reviews to assist potential shoppers in making better purchase decisions.",2018-05-01,2-s2.0-85044872428,Journal of Retailing and Consumer Services,Exploring hidden factors behind online food shopping from Amazon reviews: A topic mining approach,"Despite the popularity of online food and grocery shopping, little research has been conducted to understand the factors that influence consumers’ online food purchases. Using a topic modeling approach, our results show four interpretable factors have significant impacts on the helpfulness of customer reviews: Amazon Service, Physical Feature, Flavor Feature, and Subjective Expression. Readers of customer reviews perceive objective reviews as more helpful than subjective reviews. In addition, customer review helpfulness has a concave relationship with the length of the reviews. Our results provide important business implications on how to encourage more helpful reviews to assist potential shoppers in making better purchase decisions."
16,"Topic sentiment joint model aims to deal with the problem about the mixture of topics and sentiment simultaneously from online reviews. Most of existing topic sentiment modeling algorithms are mainly based on the state-of-art latent Dirichlet allocation (LDA) and probabilistic latent semantic analysis (PLSA), which infer sentiment and topic distributions from the co-occurrence of words. These methods have been proposed and successfully used for topic and sentiment analysis. However, when the training corpus is small or when the documents are short, the textual features become sparse, so that the results of the sentiment and topic distributions might be not very satisfied. In this paper, we propose a novel topic sentiment joint model called weakly supervised topic sentiment joint model with word embeddings (WS-TSWE), which incorporates word embeddings and HowNet lexicon simultaneously to improve the topic identification and sentiment recognition. The main contributions of WS-TSWE include the following two aspects. (1) Existing models generate the words only from the sentiment-topic-to-word Dirichlet multinomial component, but the WS-TSWE model replaces it with a mixture of two components, a Dirichlet multinomial component and a word embeddings component. Since the word embeddings are trained on a very large corpora and can be used to extend the semantic information of the words, they can provide a certain solution for the problem of the textual sparse. (2) Most of previous models incorporate sentiment knowledge in the β priors. And the priors are usually set from a dictionary and completely rely on previous domain knowledge to identify positive and negative words. In contrast, the WS-TSWE model calculates the sentiment orientation of each word with the HowNet lexicon and automatically infers sentiment-based β priors for sentiment analysis and opinion mining. Furthermore, we implement WS-TSWE with Gibbs sampling algorithms. The experimental results on Chinese and English data sets show that WS-TSWE achieved significant performance in the task of detecting sentiment and topics simultaneously.",2018-05-01,2-s2.0-85041922050,Knowledge-Based Systems,Weakly supervised topic sentiment joint model with word embeddings,"Topic sentiment joint model aims to deal with the problem about the mixture of topics and sentiment simultaneously from online reviews. Most of existing topic sentiment modeling algorithms are mainly based on the state-of-art latent Dirichlet allocation (LDA) and probabilistic latent semantic analysis (PLSA), which infer sentiment and topic distributions from the co-occurrence of words. These methods have been proposed and successfully used for topic and sentiment analysis. However, when the training corpus is small or when the documents are short, the textual features become sparse, so that the results of the sentiment and topic distributions might be not very satisfied. In this paper, we propose a novel topic sentiment joint model called weakly supervised topic sentiment joint model with word embeddings (WS-TSWE), which incorporates word embeddings and HowNet lexicon simultaneously to improve the topic identification and sentiment recognition. The main contributions of WS-TSWE include the following two aspects. (1) Existing models generate the words only from the sentiment-topic-to-word Dirichlet multinomial component, but the WS-TSWE model replaces it with a mixture of two components, a Dirichlet multinomial component and a word embeddings component. Since the word embeddings are trained on a very large corpora and can be used to extend the semantic information of the words, they can provide a certain solution for the problem of the textual sparse. (2) Most of previous models incorporate sentiment knowledge in the β priors. And the priors are usually set from a dictionary and completely rely on previous domain knowledge to identify positive and negative words. In contrast, the WS-TSWE model calculates the sentiment orientation of each word with the HowNet lexicon and automatically infers sentiment-based β priors for sentiment analysis and opinion mining. Furthermore, we implement WS-TSWE with Gibbs sampling algorithms. The experimental results on Chinese and English data sets show that WS-TSWE achieved significant performance in the task of detecting sentiment and topics simultaneously."
17,"Web 2.0 offers manifold ways in order to integrate community members via online communities (OCs) for innovation processes. OCs prove to be a valuable and dynamic source of information. External information sources are also important for foresight in order to be able to identify and monitor all relevant changes. However, traditional foresight methods are rather static in comparison with dynamic OCs. Thus, this study gives first insights into the use of OCs for foresight. First, based on literature, it is conceptually shown that OCs can contribute to foresight. Second, the question of how to assess the potential of OCs for foresight is considered. Renewable energies OCs are identified using a netnographic approach. One selected OC is analyzed in-depth by applying a prior developed criteria catalog which is based on Popper's (2008) foresight diamond. Each of its four dimensions – creativity, expertise, interaction, and evidence – is operationalized with measurement items taken from literature. In particular, the evidence dimension is supported by a text mining approach. Lastly, a focus group interview proves the usefulness of OCs for foresight. The findings show that OCs can contribute to each dimension of the foresight diamond and serve as an additional source of information for foresight.",2018-04-01,2-s2.0-85044349949,Technological Forecasting and Social Change,Foresight by online communities – The case of renewable energies,"Web 2.0 offers manifold ways in order to integrate community members via online communities (OCs) for innovation processes. OCs prove to be a valuable and dynamic source of information. External information sources are also important for foresight in order to be able to identify and monitor all relevant changes. However, traditional foresight methods are rather static in comparison with dynamic OCs. Thus, this study gives first insights into the use of OCs for foresight. First, based on literature, it is conceptually shown that OCs can contribute to foresight. Second, the question of how to assess the potential of OCs for foresight is considered. Renewable energies OCs are identified using a netnographic approach. One selected OC is analyzed in-depth by applying a prior developed criteria catalog which is based on Popper's (2008) foresight diamond. Each of its four dimensions – creativity, expertise, interaction, and evidence – is operationalized with measurement items taken from literature. In particular, the evidence dimension is supported by a text mining approach. Lastly, a focus group interview proves the usefulness of OCs for foresight. The findings show that OCs can contribute to each dimension of the foresight diamond and serve as an additional source of information for foresight."
18,"Nanotechnology is an emerging and promising field of research. Creating sufficient technological diversity among its alternatives is important for the long-term success of nanotechnologies, as well as for other emerging technologies. Diversity prevents early lock-in, facilitates recombinant innovation, increases resilience, and allows market growth. Creation of new technological alternatives usually takes place in innovation projects in which public and private partners often collaborate. Currently, there is little empirical evidence about which characteristics of innovation projects influence diversity. In this paper we study the influence of characteristics of EU-funded nanotechnology projects on the creation of technological diversity. In addition to actor diversity and the network of the project, we also include novel variables that have a plausible influence on diversity creation: the degree of multi-disciplinarity of the project and the size of the joint knowledge base of project partners. We apply topic modelling (Latent Dirichlet allocation) as a novel method to categorize technological alternatives. Using an ordinal logistic regression model, our results show that the largest contribution to diversity comes from the multi-disciplinary nature of a project. The joint knowledge base of project partners and the geographical distance between them were positively associated with technological diversity creation. In contrast, the number and diversity of actors and the degree of clustering showed a negative association with technological diversity creation. These results extend current micro-level explanations of how the diversity of an emerging technology is created. The contribution of this study could also be helpful for policy makers to influence the level of diversity in a technological field, and hence to contribute to survival of emerging technologies.",2018-04-01,2-s2.0-85011281203,Journal of Technology Transfer,Multi-disciplinarity breeds diversity: the influence of innovation project characteristics on diversity creation in nanotechnology,"Nanotechnology is an emerging and promising field of research. Creating sufficient technological diversity among its alternatives is important for the long-term success of nanotechnologies, as well as for other emerging technologies. Diversity prevents early lock-in, facilitates recombinant innovation, increases resilience, and allows market growth. Creation of new technological alternatives usually takes place in innovation projects in which public and private partners often collaborate. Currently, there is little empirical evidence about which characteristics of innovation projects influence diversity. In this paper we study the influence of characteristics of EU-funded nanotechnology projects on the creation of technological diversity. In addition to actor diversity and the network of the project, we also include novel variables that have a plausible influence on diversity creation: the degree of multi-disciplinarity of the project and the size of the joint knowledge base of project partners. We apply topic modelling (Latent Dirichlet allocation) as a novel method to categorize technological alternatives. Using an ordinal logistic regression model, our results show that the largest contribution to diversity comes from the multi-disciplinary nature of a project. The joint knowledge base of project partners and the geographical distance between them were positively associated with technological diversity creation. In contrast, the number and diversity of actors and the degree of clustering showed a negative association with technological diversity creation. These results extend current micro-level explanations of how the diversity of an emerging technology is created. The contribution of this study could also be helpful for policy makers to influence the level of diversity in a technological field, and hence to contribute to survival of emerging technologies."
19,"With the rapid development of e-commerce, a new type of secondhand e-commerce website has appeared in recent years. Any user can have his or her own shop and list superfluous items for sale online without much supervision. These secondhand e-commerce platforms maximize the economic value of secondhand markets online, but buyers risk conducting unpleasant transactions with low-reputation sellers. The main contribution of our research is the design of a text analytics framework to assess secondhand sellers' reputation. In addition, we develop a new aspect-extraction method that combines the results of domain ontology and topic modeling to extract topical features from product descriptions. We conduct our experiments based on a real-word dataset crawled from XianYu. The experimental results reveal that our ontology-based topic model method outperforms a traditional topic model method. Furthermore, the proposed framework performs well in different item categories. The managerial implication of our research is that potential buyers can prejudge the reputation of secondhand sellers when making purchase decisions. The results can support a more effective development of online secondhand markets.",2018-04-01,2-s2.0-85042661166,Decision Support Systems,Secondhand seller reputation in online markets: A text analytics framework,"With the rapid development of e-commerce, a new type of secondhand e-commerce website has appeared in recent years. Any user can have his or her own shop and list superfluous items for sale online without much supervision. These secondhand e-commerce platforms maximize the economic value of secondhand markets online, but buyers risk conducting unpleasant transactions with low-reputation sellers. The main contribution of our research is the design of a text analytics framework to assess secondhand sellers' reputation. In addition, we develop a new aspect-extraction method that combines the results of domain ontology and topic modeling to extract topical features from product descriptions. We conduct our experiments based on a real-word dataset crawled from XianYu. The experimental results reveal that our ontology-based topic model method outperforms a traditional topic model method. Furthermore, the proposed framework performs well in different item categories. The managerial implication of our research is that potential buyers can prejudge the reputation of secondhand sellers when making purchase decisions. The results can support a more effective development of online secondhand markets."
20,"Online platforms are prone to abuse and manipulations from strategic parties. For example, social media and review websites suffer from sentiment manipulations, manifested in the form of opinion spam and fake reviews. The consequence of such manipulations is the deterioration of information quality as well as loss in consumer welfare. We study one of movie studios' operation activities, sentiment manipulation, in the context of movie tweets. Using the movie release and movie studios' earning announcement dates as sources of exogenous shocks, we find that both the average Twitter sentiment and the proportion of highly positive tweets exhibit a significant drop on the movie's release day or movie studios' earnings announcement day. In addition, independent productions and low budget movies tend to experience a larger drop than major studio productions and high budget movies. To examine the effect of competition on firm manipulation, we construct a movie competition measure based on both the time and theme dimensions through topic modeling, and we find that a higher level of competition leads to a larger drop in Twitter sentiment. Overall, these observations suggest that firms might be actively managing online sentiment in a strategic manner. Our study sheds light on the reliability of sentiment analysis and contributes to our understanding of potential strategic manipulation in the operation of movie studios.",2018-03-01,2-s2.0-85043713983,Production and Operations Management,Sentiment Manipulation in Online Platforms: An Analysis of Movie Tweets,"Online platforms are prone to abuse and manipulations from strategic parties. For example, social media and review websites suffer from sentiment manipulations, manifested in the form of opinion spam and fake reviews. The consequence of such manipulations is the deterioration of information quality as well as loss in consumer welfare. We study one of movie studios' operation activities, sentiment manipulation, in the context of movie tweets. Using the movie release and movie studios' earning announcement dates as sources of exogenous shocks, we find that both the average Twitter sentiment and the proportion of highly positive tweets exhibit a significant drop on the movie's release day or movie studios' earnings announcement day. In addition, independent productions and low budget movies tend to experience a larger drop than major studio productions and high budget movies. To examine the effect of competition on firm manipulation, we construct a movie competition measure based on both the time and theme dimensions through topic modeling, and we find that a higher level of competition leads to a larger drop in Twitter sentiment. Overall, these observations suggest that firms might be actively managing online sentiment in a strategic manner. Our study sheds light on the reliability of sentiment analysis and contributes to our understanding of potential strategic manipulation in the operation of movie studios."
21,"This study focuses on examining the thematic landscape of the history of scholarly publication in business ethics. We analyze the titles, abstracts, full texts, and citation information of all research papers published in the field’s leading journal, the Journal of Business Ethics, from its inaugural issue in February 1982 until December 2016—a dataset that comprises 6308 articles and 42 million words. Our key method is a computational algorithm known as probabilistic topic modeling, which we use to examine objectively the field’s latent thematic landscape based on the vast volume of scholarly texts. This “big-data” approach allows us not only to provide time-specific snapshots of various research topics, but also to track the dynamic evolution of each topic over time. We further examine the pattern of individual papers’ topic diversity and the influence of individual papers’ topic diversity on their impact over time. We conclude this study with our recommendation for future studies in business ethics research.",2018-02-26,2-s2.0-85042535268,Journal of Business Ethics,"A Big-Data Approach to Understanding the Thematic Landscape of the Field of Business Ethics, 1982–2016","This study focuses on examining the thematic landscape of the history of scholarly publication in business ethics. We analyze the titles, abstracts, full texts, and citation information of all research papers published in the field’s leading journal, the Journal of Business Ethics, from its inaugural issue in February 1982 until December 2016—a dataset that comprises 6308 articles and 42 million words. Our key method is a computational algorithm known as probabilistic topic modeling, which we use to examine objectively the field’s latent thematic landscape based on the vast volume of scholarly texts. This “big-data” approach allows us not only to provide time-specific snapshots of various research topics, but also to track the dynamic evolution of each topic over time. We further examine the pattern of individual papers’ topic diversity and the influence of individual papers’ topic diversity on their impact over time. We conclude this study with our recommendation for future studies in business ethics research."
22,"This article analyses all articles published in Accounting History using a topic modeling technique. Previous studies focus on the content of accounting history, but not how the field has evolved. The article complements prior assessments of the research published in Accounting History by providing measures of the relative prevalence of research areas and their evolution over time. The analysis offers insights into accounting history by refining previous categorisations, uncovering overlooked topic areas and substantiating trends, such as the demise of interest in the technical core of accounting in favour of more variegated and fragmented approaches. The findings are discussed in light of the claimed pluralisation of methodological and theoretical approaches in this field.",2018-02-01,2-s2.0-85044145991,Accounting History,Accounting for Accounting History: A topic modeling approach (1996–2015),"This article analyses all articles published in Accounting History using a topic modeling technique. Previous studies focus on the content of accounting history, but not how the field has evolved. The article complements prior assessments of the research published in Accounting History by providing measures of the relative prevalence of research areas and their evolution over time. The analysis offers insights into accounting history by refining previous categorisations, uncovering overlooked topic areas and substantiating trends, such as the demise of interest in the technical core of accounting in favour of more variegated and fragmented approaches. The findings are discussed in light of the claimed pluralisation of methodological and theoretical approaches in this field."
23,"Service innovation is intertwined with service design, and knowledge from both fields should be integrated to advance theoretical and normative insights. However, studies bridging service innovation and service design are in their infancy. This is because the body of service innovation and service design research is large and heterogeneous, which makes it difficult, if not impossible, for any human to read and understand its entire content and to delineate appropriate guidelines on how to broaden the scope of either field. Our work addresses this challenge by presenting the first application of topic modeling, a type of machine learning, to review and analyze currently available service innovation and service design research (n = 641 articles with 10,543 pages of written text or 4,119,747 words). We provide an empirical contribution to service research by identifying and analyzing 69 distinct research topics in the published text corpus, a theoretical contribution by delineating an extensive research agenda consisting of four research directions and 12 operationalizable guidelines to facilitate cross-fertilization between the two fields, and a methodological contribution by introducing and demonstrating the applicability of topic modeling and machine learning as a novel type of big data analytics to our discipline.",2018-02-01,2-s2.0-85040034771,Journal of Service Research,"Big Data, Big Insights? Advancing Service Innovation and Design With Machine Learning","Service innovation is intertwined with service design, and knowledge from both fields should be integrated to advance theoretical and normative insights. However, studies bridging service innovation and service design are in their infancy. This is because the body of service innovation and service design research is large and heterogeneous, which makes it difficult, if not impossible, for any human to read and understand its entire content and to delineate appropriate guidelines on how to broaden the scope of either field. Our work addresses this challenge by presenting the first application of topic modeling, a type of machine learning, to review and analyze currently available service innovation and service design research (n = 641 articles with 10,543 pages of written text or 4,119,747 words). We provide an empirical contribution to service research by identifying and analyzing 69 distinct research topics in the published text corpus, a theoretical contribution by delineating an extensive research agenda consisting of four research directions and 12 operationalizable guidelines to facilitate cross-fertilization between the two fields, and a methodological contribution by introducing and demonstrating the applicability of topic modeling and machine learning as a novel type of big data analytics to our discipline."
24,"Nowadays plenty of user-generated posts, e.g., sina weibos, are published on the social media. The posts contain the public's sentiments (i.e., positive or negative) towards various topics. Bursty sentiment-aware topics from these posts reveal sentiment-aware events which have attracted much attention. To detect sentiment-aware topics, we attempt to utilize Joint Sentiment/Topic models, these models are achieved with Latent Dirichlet Allocation (LDA) based models. However, most of the existing sentiment/topic models cannot be directly utilized to detect sentiment-aware topics on the posts, since applying the models to the posts directly suffers from the context sparsity problem. In this paper, we propose a Time-User Sentiment/Topic Latent Dirichlet Allocation (TUS-LDA) which simultaneously models sentiments and topics for posts. Thereinto, TUS-LDA aggregates posts in the same timeslices or from the same users as pseudo-documents to alleviate the context sparsity problem. Based on TUS-LDA, we further design an approach to detect bursty sentiment-aware topics and these sentiment-ware topics can reflect bursty real-world events. Experiments on the Chinese sina weibos show that TUS-LDA outperforms previous models in the tasks of sentiment classification and burst detection in sentiment-aware topics. Finally, we visualize the bursty sentiment-aware topics discovered by TUS-LDA.",2018-02-01,2-s2.0-85034631338,Knowledge-Based Systems,Detecting bursts in sentiment-aware topics from social media,"Nowadays plenty of user-generated posts, e.g., sina weibos, are published on the social media. The posts contain the public's sentiments (i.e., positive or negative) towards various topics. Bursty sentiment-aware topics from these posts reveal sentiment-aware events which have attracted much attention. To detect sentiment-aware topics, we attempt to utilize Joint Sentiment/Topic models, these models are achieved with Latent Dirichlet Allocation (LDA) based models. However, most of the existing sentiment/topic models cannot be directly utilized to detect sentiment-aware topics on the posts, since applying the models to the posts directly suffers from the context sparsity problem. In this paper, we propose a Time-User Sentiment/Topic Latent Dirichlet Allocation (TUS-LDA) which simultaneously models sentiments and topics for posts. Thereinto, TUS-LDA aggregates posts in the same timeslices or from the same users as pseudo-documents to alleviate the context sparsity problem. Based on TUS-LDA, we further design an approach to detect bursty sentiment-aware topics and these sentiment-ware topics can reflect bursty real-world events. Experiments on the Chinese sina weibos show that TUS-LDA outperforms previous models in the tasks of sentiment classification and burst detection in sentiment-aware topics. Finally, we visualize the bursty sentiment-aware topics discovered by TUS-LDA."
25,"Social media web sites have become major media platforms to share personal information, news, photos, videos and more. Users can even share live streams whenever they want to reach out to many other. This prevalent usage of social media attracted companies, data scientists, and researchers who are trying to infer meaningful information from this vast amount of data. Information diffusion and maximizing the spread of words is one of the most important focus for researchers working on social media. This information can serve many purposes such as; user or content recommendation, viral marketing, and user modeling. In this research, finding topical influential/authority users on Twitter is addressed. Since Twitter is a good platform to spread knowledge as a word of mouth approach and it has many more public profiles than protected ones, it is a target media for marketers. In this paper, we introduce a novel methodology, called Personalized PageRank, that integrates both the information obtained from network topology and the information obtained from user actions and activities in Twitter. The proposed approach aims to determine the topical influencers who are experts on a specific topic. Experimental results on a large dataset consisting of Turkish tweets show that using user specific features like topical focus rate, activeness, authenticity and speed of getting reaction on specific topics positively affects identifying influencers and lead to higher information diffusion. Algorithms are implemented on a distributed computing environment which makes high-cost graph processing more efficient.",2018-02-01,2-s2.0-85035234575,Knowledge-Based Systems,Identifying topical influencers on twitter based on user behavior and network topology,"Social media web sites have become major media platforms to share personal information, news, photos, videos and more. Users can even share live streams whenever they want to reach out to many other. This prevalent usage of social media attracted companies, data scientists, and researchers who are trying to infer meaningful information from this vast amount of data. Information diffusion and maximizing the spread of words is one of the most important focus for researchers working on social media. This information can serve many purposes such as; user or content recommendation, viral marketing, and user modeling. In this research, finding topical influential/authority users on Twitter is addressed. Since Twitter is a good platform to spread knowledge as a word of mouth approach and it has many more public profiles than protected ones, it is a target media for marketers. In this paper, we introduce a novel methodology, called Personalized PageRank, that integrates both the information obtained from network topology and the information obtained from user actions and activities in Twitter. The proposed approach aims to determine the topical influencers who are experts on a specific topic. Experimental results on a large dataset consisting of Turkish tweets show that using user specific features like topical focus rate, activeness, authenticity and speed of getting reaction on specific topics positively affects identifying influencers and lead to higher information diffusion. Algorithms are implemented on a distributed computing environment which makes high-cost graph processing more efficient."
26,"With the rapid growth of population on social networks, people are confronted with information overload problem. This clearly makes filtering the targeted users a demanding and key research task. Uni-directional social networks are the scenarios where users provide limited follow or not binary features. Related works prefer to utilize these follower-followee relations for recommendation. However, a major problem of these methods is that they assume every follower-followee user pairs are equally likely, and this leads to the coarse user following preferences inferring. Intuitively, a user's adoption of others as followees may be motivated by her interests as well as social connections, hence a good recommender should be able to separate the two situations and take both factors into account for better recommendation results. In this regard, we propose a new user recommendation framework namely UIS-MF in this work. UIS-MF can well capture user preferences by involving both interest and social factors in prediction, and targeted to recommend Top-N followees who have similar interest and close social connection relevant to a target user. Specifically, we first present a unified probabilistic topic model on follower-followee relations, namely UIS-LDA, and it employs Generalized Pólya Urn (GPU) models on mutual-following relations for discovering interest topics and social topics of users. Next we propose a community-based method for user recommendation, it organizes social communities and interest communities based on the estimation of topics obtained from UIS-LDA, and then performs Matrix Factorization (MF) method on each community to generate N most likely followees for individual user. Systematic experiments on Twitter, Sina Weibo and Epinions datasets have not only revealed the significant effect of our UIS-LDA model for the extraction of interest and social topics of users in improving recommending accuracy, but also demonstrated the advantage of our proposed recommendation framework over competitive baselines by large margins.",2018-01-15,2-s2.0-85034955336,Knowledge-Based Systems,Improving user recommendation by extracting social topics and interest topics of users in uni-directional social networks,"With the rapid growth of population on social networks, people are confronted with information overload problem. This clearly makes filtering the targeted users a demanding and key research task. Uni-directional social networks are the scenarios where users provide limited follow or not binary features. Related works prefer to utilize these follower-followee relations for recommendation. However, a major problem of these methods is that they assume every follower-followee user pairs are equally likely, and this leads to the coarse user following preferences inferring. Intuitively, a user's adoption of others as followees may be motivated by her interests as well as social connections, hence a good recommender should be able to separate the two situations and take both factors into account for better recommendation results. In this regard, we propose a new user recommendation framework namely UIS-MF in this work. UIS-MF can well capture user preferences by involving both interest and social factors in prediction, and targeted to recommend Top-N followees who have similar interest and close social connection relevant to a target user. Specifically, we first present a unified probabilistic topic model on follower-followee relations, namely UIS-LDA, and it employs Generalized Pólya Urn (GPU) models on mutual-following relations for discovering interest topics and social topics of users. Next we propose a community-based method for user recommendation, it organizes social communities and interest communities based on the estimation of topics obtained from UIS-LDA, and then performs Matrix Factorization (MF) method on each community to generate N most likely followees for individual user. Systematic experiments on Twitter, Sina Weibo and Epinions datasets have not only revealed the significant effect of our UIS-LDA model for the extraction of interest and social topics of users in improving recommending accuracy, but also demonstrated the advantage of our proposed recommendation framework over competitive baselines by large margins."
28,"Adynamics. research the current landscape High-quality state of a is certain a high-level research scientific landscapes description field and are its of an important tools that allow for more effective research management. This paper presents a novel framework for the mapping of research. It relies on full-text mining and topic modeling to pool data from many sources without relying on any specific taxonomy of scientific fields and areas. The framework is especially useful for scientific fields that are poorly represented in scientometric databases, i.e., Scopus or Web of Science. The high-level algorithm consists of (1) full-text collection from reliable sources; (2) the automatic extraction of research fields using topic modeling; (3) semi-automatic linking to scientometric databases; and (4) a statistical analysis of metrics for the extracted scientific areas. Full-text mining is crucial due to (a) the poor representation of many Russian research areas in systems like Scopus or Web of Science; (b) the poor quality of Russian Science Index data; and (c) the differences between taxonomies used in different data sources. Major advantages of the proposed framework include its data-driven approach, its independence from scientific subjects' taxonomies, and its ability to integrate data from multiple heterogeneous data sources. Furthermore, this framework complements traditional approaches to research mapping using scientometric software like Scopus or Web of Science rather than replacing them. We experimentally evaluated the framework using agricultural science as an example, but the framework is not limited to any particular domain. As a result, we created the first research landscape covering young researchers in agricultural science. Topic modeling yielded six major scientific areas within the field of agriculture. We found that statistically significant differences between these areas exist. This means that a differentiated approach to research management is critical. Further research on this subject includes the application of the framework to other scientific fields and the integration of other collections of research and technical documentation (especially patents).",2018-01-01,2-s2.0-85046454831,Foresight and STI Governance,Mapping the research landscape of agricultural sciences,"Adynamics. research the current landscape High-quality state of a is certain a high-level research scientific landscapes description field and are its of an important tools that allow for more effective research management. This paper presents a novel framework for the mapping of research. It relies on full-text mining and topic modeling to pool data from many sources without relying on any specific taxonomy of scientific fields and areas. The framework is especially useful for scientific fields that are poorly represented in scientometric databases, i.e., Scopus or Web of Science. The high-level algorithm consists of (1) full-text collection from reliable sources; (2) the automatic extraction of research fields using topic modeling; (3) semi-automatic linking to scientometric databases; and (4) a statistical analysis of metrics for the extracted scientific areas. Full-text mining is crucial due to (a) the poor representation of many Russian research areas in systems like Scopus or Web of Science; (b) the poor quality of Russian Science Index data; and (c) the differences between taxonomies used in different data sources. Major advantages of the proposed framework include its data-driven approach, its independence from scientific subjects' taxonomies, and its ability to integrate data from multiple heterogeneous data sources. Furthermore, this framework complements traditional approaches to research mapping using scientometric software like Scopus or Web of Science rather than replacing them. We experimentally evaluated the framework using agricultural science as an example, but the framework is not limited to any particular domain. As a result, we created the first research landscape covering young researchers in agricultural science. Topic modeling yielded six major scientific areas within the field of agriculture. We found that statistically significant differences between these areas exist. This means that a differentiated approach to research management is critical. Further research on this subject includes the application of the framework to other scientific fields and the integration of other collections of research and technical documentation (especially patents)."
29,"Using a case-study based approach, this research contributes to the standardisation versus adaptation debate in global marketing. It analyses the influence of the local culture dimension reflected in consumers' comments in the Facebook platform regarding a new global technological product, the Samsung Galaxy S8/S8+, launched worldwide in 2017. Consumers' comments about this new smartphone were gathered and analysed for three cultural distinct English-speaking countries: Australia, India, and South Africa. The analysis' procedure consisted of a text mining and topic modelling approach, including sentiment classification analysis, to discern and understand consumers' responses to global brand communications. The findings indicate that cultural aspects still play a key role in consumers' reactions to the product in each country, justifying the continued need for marketing strategies that conflate pursuing economies of scale with accounting for the cultural sensitivities of demand at country level. Evidence of consumers attitudes' and behaviours' homogenisation across countries is still limited.",2018-01-01,2-s2.0-85051477258,Journal of Business Research,A cross-cultural case study of consumers' communications about a new technological product,"Using a case-study based approach, this research contributes to the standardisation versus adaptation debate in global marketing. It analyses the influence of the local culture dimension reflected in consumers' comments in the Facebook platform regarding a new global technological product, the Samsung Galaxy S8/S8+, launched worldwide in 2017. Consumers' comments about this new smartphone were gathered and analysed for three cultural distinct English-speaking countries: Australia, India, and South Africa. The analysis' procedure consisted of a text mining and topic modelling approach, including sentiment classification analysis, to discern and understand consumers' responses to global brand communications. The findings indicate that cultural aspects still play a key role in consumers' reactions to the product in each country, justifying the continued need for marketing strategies that conflate pursuing economies of scale with accounting for the cultural sensitivities of demand at country level. Evidence of consumers attitudes' and behaviours' homogenisation across countries is still limited."
30,"Purpose: This paper aims to identify the intellectual structure of four leading hospitality journals over 40 years by applying mixed-method approach, using both machine learning and traditional statistical analyses. Design/methodology/approach: Abstracts from all 4,139 articles published in four top hospitality journals were analyzed using the structured topic modeling and inferential statistics. Topic correlation and community detection were applied to identify strengths of correlations and sub-groups of topics. Trend visualization and regression analysis were used to quantify the effects of the metadata (i.e. year of publication and journal) on topic proportions. Findings: The authors found 50 topics and eight subgroups in the hospitality journals. Different evolutionary patterns in topic popularity were demonstrated, thereby providing the insights for popular research topics over time. The significant differences in topical proportions were found across the four leading hospitality journals, suggesting different foci in research topics in each journal. Research limitations/implications: Combining machine learning techniques with traditional statistics demonstrated potential for discovering valuable insights from big text data in hospitality and tourism research contexts. The findings of this study may serve as a guide to understand the trends in the research field as well as the progress of specific areas or subfields. Originality/value: It is the first attempt to apply topic modeling to academic publications and explore the effects of article metadata with the hospitality literature.",2018-01-01,2-s2.0-85052567701,International Journal of Contemporary Hospitality Management,Toward understanding the topical structure of hospitality literature: Applying machine learning and traditional statistics,"Purpose: This paper aims to identify the intellectual structure of four leading hospitality journals over 40 years by applying mixed-method approach, using both machine learning and traditional statistical analyses. Design/methodology/approach: Abstracts from all 4,139 articles published in four top hospitality journals were analyzed using the structured topic modeling and inferential statistics. Topic correlation and community detection were applied to identify strengths of correlations and sub-groups of topics. Trend visualization and regression analysis were used to quantify the effects of the metadata (i.e. year of publication and journal) on topic proportions. Findings: The authors found 50 topics and eight subgroups in the hospitality journals. Different evolutionary patterns in topic popularity were demonstrated, thereby providing the insights for popular research topics over time. The significant differences in topical proportions were found across the four leading hospitality journals, suggesting different foci in research topics in each journal. Research limitations/implications: Combining machine learning techniques with traditional statistics demonstrated potential for discovering valuable insights from big text data in hospitality and tourism research contexts. The findings of this study may serve as a guide to understand the trends in the research field as well as the progress of specific areas or subfields. Originality/value: It is the first attempt to apply topic modeling to academic publications and explore the effects of article metadata with the hospitality literature."
31,"Purpose: The purpose of this paper is to explore and describe how research on quality management (QM) has evolved historically. The study includes the complete digital archive of three academic journals in the field of QM. Thereby, a unique depiction of how the general outlines of the field as well as trends in research topics have evolved through the years is presented. Design/methodology/approach: The study applies cluster and probabilistic topic modeling to unstructured data from The International Journal of Quality & Reliability Management, The TQM Journal and Total Quality Management & Business Excellence. In addition, trend analysis using support vector machine is performed. Findings: The study identifies six central, perpetual themes of QM research: control, costs, reliability and failure; service quality; TQM – implementation and performance; ISO – certification, standards and systems; Innovation, practices and learning and customers – research and product design. Additionally, historical surges and shifts in research focus are recognized in the study. From these trends, a decrease in interest in TQM and control of quality, costs and processes in favor of service quality, customer satisfaction, Six Sigma, Lean and innovation can be noted during the past decade. The results validate previous findings. Originality/value: Of the identified central themes, innovation, practices and learning appears not to have been documented as a fundamental part of QM research in previous studies. Thus, this theme can be regarded as a new perspective on QM research and thereby on QM.",2018-01-01,2-s2.0-85039788432,International Journal of Quality and Reliability Management,25 years of quality management research – outlines and trends,"Purpose: The purpose of this paper is to explore and describe how research on quality management (QM) has evolved historically. The study includes the complete digital archive of three academic journals in the field of QM. Thereby, a unique depiction of how the general outlines of the field as well as trends in research topics have evolved through the years is presented. Design/methodology/approach: The study applies cluster and probabilistic topic modeling to unstructured data from The International Journal of Quality & Reliability Management, The TQM Journal and Total Quality Management & Business Excellence. In addition, trend analysis using support vector machine is performed. Findings: The study identifies six central, perpetual themes of QM research: control, costs, reliability and failure; service quality; TQM – implementation and performance; ISO – certification, standards and systems; Innovation, practices and learning and customers – research and product design. Additionally, historical surges and shifts in research focus are recognized in the study. From these trends, a decrease in interest in TQM and control of quality, costs and processes in favor of service quality, customer satisfaction, Six Sigma, Lean and innovation can be noted during the past decade. The results validate previous findings. Originality/value: Of the identified central themes, innovation, practices and learning appears not to have been documented as a fundamental part of QM research in previous studies. Thus, this theme can be regarded as a new perspective on QM research and thereby on QM."
32,"Purpose: This paper aims to present an automated literature analysis to unveil the drivers for incorporating social media in tourism and hospitality brand strategies. Design/methodology/approach: To gather relevant literature, Google Scholar was queried with “brand”/“branding” and “social media” for articles in ten top-ranked tourism and hospitality journals, resulting in a total of 479 collected articles. The methodology adopted for the analysis is based on text mining and topic modeling procedures. The topics discovered are characterized by terms belonging to a dictionary previously compiled and provide a segmentation of the articles in coherent sets of the literature. Findings: Most of the 213 articles that encompass a strong relation between social media and branding are mentioning mainly brand building stages. A large research gap was found in hospitality and tourism considering that, besides advertising, no topic was discovered related to known brand strategies such as co-branding or franchising. Practical implications: The present analysis concludes that specialized tourism and hospitality literature needs to keep pace with research that is being conducted on a wide range of industries to assess the influence of social media. Originality/value: The automated analysis approach used has no precedent in tourism and hospitality research. By including an innovative topical concept map, it led to identifying and summarizing the topics, providing a clear picture on the findings. This study calls for research by specialized tourism and hospitality publications, eventually leading to special issues on this vibrant subject.",2018-01-01,2-s2.0-85041715546,International Journal of Contemporary Hospitality Management,Brand strategies in social media in hospitality and tourism,"Purpose: This paper aims to present an automated literature analysis to unveil the drivers for incorporating social media in tourism and hospitality brand strategies. Design/methodology/approach: To gather relevant literature, Google Scholar was queried with “brand”/“branding” and “social media” for articles in ten top-ranked tourism and hospitality journals, resulting in a total of 479 collected articles. The methodology adopted for the analysis is based on text mining and topic modeling procedures. The topics discovered are characterized by terms belonging to a dictionary previously compiled and provide a segmentation of the articles in coherent sets of the literature. Findings: Most of the 213 articles that encompass a strong relation between social media and branding are mentioning mainly brand building stages. A large research gap was found in hospitality and tourism considering that, besides advertising, no topic was discovered related to known brand strategies such as co-branding or franchising. Practical implications: The present analysis concludes that specialized tourism and hospitality literature needs to keep pace with research that is being conducted on a wide range of industries to assess the influence of social media. Originality/value: The automated analysis approach used has no precedent in tourism and hospitality research. By including an innovative topical concept map, it led to identifying and summarizing the topics, providing a clear picture on the findings. This study calls for research by specialized tourism and hospitality publications, eventually leading to special issues on this vibrant subject."
33,"Research has emphasized the limitations of qualitative and quantitative approaches to studying organizational phenomena. For example, in-depth interviews are resource-intensive, while questionnaires with closed-ended questions can only measure predefined constructs. With the recent availability of large textual data sets and increased computational power, text mining has become an attractive method that has the potential to mitigate some of these limitations. Thus, we suggest applying topic modeling, a specific text mining technique, as a new and complementary strategy of inquiry to study organizational phenomena. In particular, we outline the potentials of structural topic modeling for organizational research and provide a step-by-step tutorial on how to apply it. Our application example builds on 428,492 reviews of Fortune 500 companies from the online platform Glassdoor, on which employees can evaluate organizations. We demonstrate how structural topic models allow to inductively identify topics that matter to employees and quantify their relationship with employees’ perception of organizational culture. We discuss the advantages and limitations of topic modeling as a research method and outline how future research can apply the technique to study organizational phenomena.",2018-01-01,2-s2.0-85046719447,Organizational Research Methods,Topic Modeling as a Strategy of Inquiry in Organizational Research: A Tutorial With an Application Example on Organizational Culture,"Research has emphasized the limitations of qualitative and quantitative approaches to studying organizational phenomena. For example, in-depth interviews are resource-intensive, while questionnaires with closed-ended questions can only measure predefined constructs. With the recent availability of large textual data sets and increased computational power, text mining has become an attractive method that has the potential to mitigate some of these limitations. Thus, we suggest applying topic modeling, a specific text mining technique, as a new and complementary strategy of inquiry to study organizational phenomena. In particular, we outline the potentials of structural topic modeling for organizational research and provide a step-by-step tutorial on how to apply it. Our application example builds on 428,492 reviews of Fortune 500 companies from the online platform Glassdoor, on which employees can evaluate organizations. We demonstrate how structural topic models allow to inductively identify topics that matter to employees and quantify their relationship with employees’ perception of organizational culture. We discuss the advantages and limitations of topic modeling as a research method and outline how future research can apply the technique to study organizational phenomena."
34,"Purpose: The purpose of this study was to explore influences of review-related information on topical proportions and the pattern of word appearances in each topic (topical content) using structural topic model (STM). Design/methodology/approach: For 173,607 Yelp.com reviews written in 2005-2016, STM-based topic modeling was applied with inclusion of covariates in addition to traditional statistical analyses. Findings: Differences in topic prevalence and topical contents were found between certified green and non-certified restaurants. Customers’ recognition in sustainable food topics were changed over time. Research limitations/implications: This study demonstrates the application of STM for the systematic analysis of a large amount of text data. Originality/value: Limited study in the hospitality literature examined the influence of review-level metadata on topic and term estimation. Through topic modeling, customers’ natural responses toward green practices were identified.",2018-01-01,2-s2.0-85053030459,Journal of Hospitality and Tourism Technology,The structural topic model for online review analysis: Comparison between green and non-green restaurants,"Purpose: The purpose of this study was to explore influences of review-related information on topical proportions and the pattern of word appearances in each topic (topical content) using structural topic model (STM). Design/methodology/approach: For 173,607 Yelp.com reviews written in 2005-2016, STM-based topic modeling was applied with inclusion of covariates in addition to traditional statistical analyses. Findings: Differences in topic prevalence and topical contents were found between certified green and non-certified restaurants. Customers’ recognition in sustainable food topics were changed over time. Research limitations/implications: This study demonstrates the application of STM for the systematic analysis of a large amount of text data. Originality/value: Limited study in the hospitality literature examined the influence of review-level metadata on topic and term estimation. Through topic modeling, customers’ natural responses toward green practices were identified."
35,"This article reviews Decision Sciences journal articles and metadata to analyze its intellectual tradition. Text analytics is used with probabilistic topic modeling. The topical structure of the journal is reviewed by topic definition and popularity, with correlations. Fifty research topics or themes involving a wide range of quantitative methods and decision-making practice were selected. Functional areas were also examined. The evolution of topics since 1975 is noted. There is clear indication that journal coverage has evolved. In early years, emphasis was on quantitative modeling methods and relevant methodologies. More recently new research areas to include supply chain management, marketing, service management, and health care are more noted. Some topics are highly correlated. We find that this evolution reflects the changes occurring in business and decision-making environment. The article discusses external and internal factors important in shaping the journal's topical trajectory. Unique challenges in analyzing text data are discussed. Latent Dirichlet Allocation, an unsupervised Bayesian approach for statistical topic modeling, is applied to 1,698 research articles from Decision Sciences over the period from 1975 to 2016. This approach is found to be useful to discover journal topical trends. Potential for other applications in decision sciences is discussed.",2018-01-01,2-s2.0-85050506351,Decision Sciences,A Topical Exploration of the Intellectual Development of Decision Sciences 1975-2016,"This article reviews Decision Sciences journal articles and metadata to analyze its intellectual tradition. Text analytics is used with probabilistic topic modeling. The topical structure of the journal is reviewed by topic definition and popularity, with correlations. Fifty research topics or themes involving a wide range of quantitative methods and decision-making practice were selected. Functional areas were also examined. The evolution of topics since 1975 is noted. There is clear indication that journal coverage has evolved. In early years, emphasis was on quantitative modeling methods and relevant methodologies. More recently new research areas to include supply chain management, marketing, service management, and health care are more noted. Some topics are highly correlated. We find that this evolution reflects the changes occurring in business and decision-making environment. The article discusses external and internal factors important in shaping the journal's topical trajectory. Unique challenges in analyzing text data are discussed. Latent Dirichlet Allocation, an unsupervised Bayesian approach for statistical topic modeling, is applied to 1,698 research articles from Decision Sciences over the period from 1975 to 2016. This approach is found to be useful to discover journal topical trends. Potential for other applications in decision sciences is discussed."
36,"Strategic management requires an assessment of a firm's internal and external environments. Our work extends the body of management tools (e.g., SWOT analysis or growth-share matrix) by proposing an automated text mining framework. Here we draw on narrative materials from firms (e.g., financial disclosures) and perform topic modeling so as to identify the key issues faced by an organization. We then quantify the use of language along two dimensions: risk and optimism. This reveals a firm's strengths and weaknesses by identifying business units, activities, and processes subject to risk, while also comparing it with competitors or the market.",2018-01-01,2-s2.0-85047428654,Information and Management,Business analytics for strategic management: Identifying and assessing corporate challenges via topic modeling,"Strategic management requires an assessment of a firm's internal and external environments. Our work extends the body of management tools (e.g., SWOT analysis or growth-share matrix) by proposing an automated text mining framework. Here we draw on narrative materials from firms (e.g., financial disclosures) and perform topic modeling so as to identify the key issues faced by an organization. We then quantify the use of language along two dimensions: risk and optimism. This reveals a firm's strengths and weaknesses by identifying business units, activities, and processes subject to risk, while also comparing it with competitors or the market."
37,"Although investments of R&D by government and firms have enlarged and the amount of patents has increased rapidly, R&D almost fails to commercialize for various reasons. For the purpose of decreasing failure rate of technology commercialization, it is important to identify emerging business based on technology in advance and establish appropriate strategy, leading to surviving at the market. Therefore, this paper aims to explore emerging Research and Business Development (R&BD) areas, and establish a business strategy based on valuable patents by comprehensively analyzing IPRs - patent as well as design and trademark. First, unrevealed but potential R&BD areas are explored by analyzing the relation between patent and trademark through topic modeling and network analysis, which aims to preferentially find potential business opportunities that can be implemented by new technology. Potential R&BD areas are recognized as the hidden link in the network of patents and trademarks. Second, emerging R&BD areas are selected by considering the status of the competition and markets through trademark analysis based on generative topographic mapping (GTM) after finding potential R&BD areas with network analysis from the viewpoint of the applicant for a trademark. Finally, new opportunities and strategies for successful R&BD are suggested by analyzing design patents that are representative of the appearance of a product in detail. The result of this study provides more concrete R&BD strategies within the framework of product and business development, based on relations between IPRs, which can be regarded as an initial study that comprehensively utilizes diverse kinds of IPRs.",2018-01-01,2-s2.0-85047060072,Technological Forecasting and Social Change,Identifying emerging Research and Business Development (R&BD) areas based on topic modeling and visualization with intellectual property right data,"Although investments of R&D by government and firms have enlarged and the amount of patents has increased rapidly, R&D almost fails to commercialize for various reasons. For the purpose of decreasing failure rate of technology commercialization, it is important to identify emerging business based on technology in advance and establish appropriate strategy, leading to surviving at the market. Therefore, this paper aims to explore emerging Research and Business Development (R&BD) areas, and establish a business strategy based on valuable patents by comprehensively analyzing IPRs - patent as well as design and trademark. First, unrevealed but potential R&BD areas are explored by analyzing the relation between patent and trademark through topic modeling and network analysis, which aims to preferentially find potential business opportunities that can be implemented by new technology. Potential R&BD areas are recognized as the hidden link in the network of patents and trademarks. Second, emerging R&BD areas are selected by considering the status of the competition and markets through trademark analysis based on generative topographic mapping (GTM) after finding potential R&BD areas with network analysis from the viewpoint of the applicant for a trademark. Finally, new opportunities and strategies for successful R&BD are suggested by analyzing design patents that are representative of the appearance of a product in detail. The result of this study provides more concrete R&BD strategies within the framework of product and business development, based on relations between IPRs, which can be regarded as an initial study that comprehensively utilizes diverse kinds of IPRs."
38,"Using a probabilistic approach for exploring latent patterns in high-dimensional co-occurrence data, topic models offer researchers a flexible and open framework for soft-clustering large data sets. In recent years, there has been a growing interest among marketing scholars and practitioners to adopt topic models in various marketing application domains. However, to this date, there is no comprehensive overview of this rapidly evolving field. By analyzing a set of 61 published papers along with conceptual contributions, we systematically review this highly heterogeneous area of research. In doing so, we characterize extant contributions employing topic models in marketing along the dimensions data structures and retrieval of input data, implementation and extensions of basic topic models, and model performance evaluation. Our findings confirm that there is considerable progress done in various marketing sub-areas. However, there is still scope for promising future research, in particular with respect to integrating multiple, dynamic data sources, including time-varying covariates and the combination of exploratory topic models with powerful predictive marketing models.",2018-01-01,2-s2.0-85053608300,Journal of Business Economics,Topic modeling in marketing: recent advances and research opportunities,"Using a probabilistic approach for exploring latent patterns in high-dimensional co-occurrence data, topic models offer researchers a flexible and open framework for soft-clustering large data sets. In recent years, there has been a growing interest among marketing scholars and practitioners to adopt topic models in various marketing application domains. However, to this date, there is no comprehensive overview of this rapidly evolving field. By analyzing a set of 61 published papers along with conceptual contributions, we systematically review this highly heterogeneous area of research. In doing so, we characterize extant contributions employing topic models in marketing along the dimensions data structures and retrieval of input data, implementation and extensions of basic topic models, and model performance evaluation. Our findings confirm that there is considerable progress done in various marketing sub-areas. However, there is still scope for promising future research, in particular with respect to integrating multiple, dynamic data sources, including time-varying covariates and the combination of exploratory topic models with powerful predictive marketing models."
39,"Topic modeling algorithms, such as LDA, find topics, hidden structures, in document corpora in an unsupervised manner. Traditionally, applications of topic modeling over textual data use the bag-of-words model, i.e. only consider words in the documents. In our previous work we developed a framework for mining enriched topic models. We proposed a bag-of-features approach, where a document consists not only of words but also of linked named entities and their related information, such as types or categories. In this work we focused on the feature engineering and selection aspects of enriched topic modeling and evaluated the results based on two measures for assessing the understandability of estimated topics for humans: model precision and topic log odds. In our 10-model experimental setup with 7 pure resource-, 2 hybrid words/resource- and one word-based model, the traditional bag-of-words models were outperformed by 5 pure resource-based models in both measures. These results show that incorporating background knowledge into topic models makes them more understandable for humans.",2018-01-01,2-s2.0-85050623971,Lecture Notes in Business Information Processing,Human perception of enriched topic models,"Topic modeling algorithms, such as LDA, find topics, hidden structures, in document corpora in an unsupervised manner. Traditionally, applications of topic modeling over textual data use the bag-of-words model, i.e. only consider words in the documents. In our previous work we developed a framework for mining enriched topic models. We proposed a bag-of-features approach, where a document consists not only of words but also of linked named entities and their related information, such as types or categories. In this work we focused on the feature engineering and selection aspects of enriched topic modeling and evaluated the results based on two measures for assessing the understandability of estimated topics for humans: model precision and topic log odds. In our 10-model experimental setup with 7 pure resource-, 2 hybrid words/resource- and one word-based model, the traditional bag-of-words models were outperformed by 5 pure resource-based models in both measures. These results show that incorporating background knowledge into topic models makes them more understandable for humans."
40,"National governments take advantage of collective intelligence when conducting foresight processes. They grasp emerging issues through expert reviews as well as public opinions. It raises national agendas and affects policy-making process. Therefore, by examining policy papers which contain societal issues, we can perceive past, current, and future environments. In this study, we exploit policy research database of Republic of Korea, which is a unique source that automatically collects all policy papers written by national research institutes, to extract latent topics and their trends over 10 years through a probabilistic topic model. Detected topics fairly correspond to expert-selected future drivers in national foresight report, implying that public discourse and policy agenda are coupled. We suggest to utilize open government data and text mining methods for building open foresight framework that various actors exchange their opinions on societal issues.",2018-01-01,2-s2.0-85042008530,Technological Forecasting and Social Change,Horizon scanning in policy research database with a probabilistic topic model,"National governments take advantage of collective intelligence when conducting foresight processes. They grasp emerging issues through expert reviews as well as public opinions. It raises national agendas and affects policy-making process. Therefore, by examining policy papers which contain societal issues, we can perceive past, current, and future environments. In this study, we exploit policy research database of Republic of Korea, which is a unique source that automatically collects all policy papers written by national research institutes, to extract latent topics and their trends over 10 years through a probabilistic topic model. Detected topics fairly correspond to expert-selected future drivers in national foresight report, implying that public discourse and policy agenda are coupled. We suggest to utilize open government data and text mining methods for building open foresight framework that various actors exchange their opinions on societal issues."
41,"Given the research interest on Big Data in Marketing, we present a research literature analysis based on a text mining semi-automated approach with the goal of identifying the main trends in this domain. In particular, the analysis focuses on relevant terms and topics related with five dimensions: Big Data, Marketing, Geographic location of authors’ affiliation (countries and continents), Products, and Sectors. A total of 1560 articles published from 2010 to 2015 were scrutinized. The findings revealed that research is bipartite between technological and research domains, with Big Data publications not clearly aligning cutting edge techniques toward Marketing benefits. Also, few inter-continental co-authored publications were found. Moreover, findings show that research in Big Data applications to Marketing is still in an embryonic stage, thus making it essential to develop more direct efforts toward business for Big Data to thrive in the Marketing arena.",2018-01-01,2-s2.0-85021832314,European Research on Management and Business Economics,Research trends on Big Data in Marketing: A text mining and topic modeling based literature analysis,"Given the research interest on Big Data in Marketing, we present a research literature analysis based on a text mining semi-automated approach with the goal of identifying the main trends in this domain. In particular, the analysis focuses on relevant terms and topics related with five dimensions: Big Data, Marketing, Geographic location of authors’ affiliation (countries and continents), Products, and Sectors. A total of 1560 articles published from 2010 to 2015 were scrutinized. The findings revealed that research is bipartite between technological and research domains, with Big Data publications not clearly aligning cutting edge techniques toward Marketing benefits. Also, few inter-continental co-authored publications were found. Moreover, findings show that research in Big Data applications to Marketing is still in an embryonic stage, thus making it essential to develop more direct efforts toward business for Big Data to thrive in the Marketing arena."
42,"Assessing the impact of events on the evolution of online public discourse is challenging due to the lack of data prior to the event and appropriate methodologies for capturing the progression of tenor of public discourse, both in terms of their tone and topic. In this article, we introduce a geovisual analytics framework, CarSenToGram, which integrates topic modeling and sentiment analysis with cartograms to identify the changing dynamics of public discourse on a particular topic across space and time. The main novelty of CarSenToGram is coupling comprehensible spatiotemporal overviews of the overall distribution, topical and sentiment patterns with increasing levels of information supported by zoom and filter, and details-on-demand interactions. To demonstrate the utility of CarSenToGram, in this article, we analyze tweets related to immigration the month before and after the 27 January 2017 travel ban in order to reveal insights into one of the defining moments of President Trump’s first year in office. Not only do we find that the travel ban influenced online public discourse and sentiment on immigration, but it also highlighted important partisan divisions within the US.",2018-01-01,2-s2.0-85053527100,Cartography and Geographic Information Science,CarSenToGram: geovisual text analytics for exploring spatiotemporal variation in public discourse on Twitter,"Assessing the impact of events on the evolution of online public discourse is challenging due to the lack of data prior to the event and appropriate methodologies for capturing the progression of tenor of public discourse, both in terms of their tone and topic. In this article, we introduce a geovisual analytics framework, CarSenToGram, which integrates topic modeling and sentiment analysis with cartograms to identify the changing dynamics of public discourse on a particular topic across space and time. The main novelty of CarSenToGram is coupling comprehensible spatiotemporal overviews of the overall distribution, topical and sentiment patterns with increasing levels of information supported by zoom and filter, and details-on-demand interactions. To demonstrate the utility of CarSenToGram, in this article, we analyze tweets related to immigration the month before and after the 27 January 2017 travel ban in order to reveal insights into one of the defining moments of President Trump’s first year in office. Not only do we find that the travel ban influenced online public discourse and sentiment on immigration, but it also highlighted important partisan divisions within the US."
43,"Purpose: The increasing competition among higher education institutions (HEI) has led students to conduct a more in-depth analysis to choose where to study abroad. Since students are usually unable to visit each HEIs before making their decision, they are strongly influenced by what is written by former international students (IS) on the internet. HEIs also benefit from such information online. The purpose of this paper is to provide an understanding of the drivers of HEIs success online. Design/methodology/approach: Due to the increasing amount of information published online, HEIs have to use automatic techniques to search for patterns instead of analysing such information manually. The present paper uses text mining (TM) and sentiment analysis (SA) to study online reviews of IS about their HEIs. The paper studied 1938 reviews from 65 different business schools with Association to Advance Collegiate Schools of Business accreditation. Findings: Results show that HEIs may become more attractive online if they financially support students cost of living, provide courses in English, and promote an international environment. Research limitations/implications: Despite the use of a major platform with a broad number of reviews from students around the world, other sources focussed on other types of HEIs may have been used to reinforce the findings in the current paper. Originality/value: The study pioneers the use of TM and SA to highlight topics and sentiments mentioned in online reviews by students attending HEIs, clarifying how such opinions are correlated with satisfaction. Using such information, HEIs’ managers may focus their efforts on promoting international attractiveness of their institutions.",2018-01-01,2-s2.0-85045695822,International Journal of Educational Management,Improving international attractiveness of higher education institutions based on text mining and sentiment analysis,"Purpose: The increasing competition among higher education institutions (HEI) has led students to conduct a more in-depth analysis to choose where to study abroad. Since students are usually unable to visit each HEIs before making their decision, they are strongly influenced by what is written by former international students (IS) on the internet. HEIs also benefit from such information online. The purpose of this paper is to provide an understanding of the drivers of HEIs success online. Design/methodology/approach: Due to the increasing amount of information published online, HEIs have to use automatic techniques to search for patterns instead of analysing such information manually. The present paper uses text mining (TM) and sentiment analysis (SA) to study online reviews of IS about their HEIs. The paper studied 1938 reviews from 65 different business schools with Association to Advance Collegiate Schools of Business accreditation. Findings: Results show that HEIs may become more attractive online if they financially support students cost of living, provide courses in English, and promote an international environment. Research limitations/implications: Despite the use of a major platform with a broad number of reviews from students around the world, other sources focussed on other types of HEIs may have been used to reinforce the findings in the current paper. Originality/value: The study pioneers the use of TM and SA to highlight topics and sentiments mentioned in online reviews by students attending HEIs, clarifying how such opinions are correlated with satisfaction. Using such information, HEIs’ managers may focus their efforts on promoting international attractiveness of their institutions."
44,"Emotional conversation generation has elicited a wide interest in both academia and industry. However, existing emotional neural conversation systems tend to ignore the necessity to combine topic and emotion in generating responses, possibly leading to a decline in the quality of responses. This paper proposes a topic-enhanced emotional conversation generation model that incorporates emotional factors and topic information into the conversation system, by using two mechanisms. First, we use a Twitter latent Dirichlet allocation (LDA) model to obtain topic words of the input sequences as extra prior information, ensuring the consistency of content between posts and responses for emotional conversation generation. Second, the system uses a dynamic emotional attention mechanism to adaptively acquire content-related and affective information of the input texts and extra topics. The advantage of this study lies in the fact that the presented model can generate abundant emotional responses, with the contents being related and diverse. To demonstrate the effectiveness of our method, we conduct extensive experiments on large-scale Weibo post–response pairs. Experimental results show that our method achieves good performance, even outperforming some existing models.",2018-01-01,2-s2.0-85053828339,Knowledge-Based Systems,Topic-enhanced emotional conversation generation with attention mechanism,"Emotional conversation generation has elicited a wide interest in both academia and industry. However, existing emotional neural conversation systems tend to ignore the necessity to combine topic and emotion in generating responses, possibly leading to a decline in the quality of responses. This paper proposes a topic-enhanced emotional conversation generation model that incorporates emotional factors and topic information into the conversation system, by using two mechanisms. First, we use a Twitter latent Dirichlet allocation (LDA) model to obtain topic words of the input sequences as extra prior information, ensuring the consistency of content between posts and responses for emotional conversation generation. Second, the system uses a dynamic emotional attention mechanism to adaptively acquire content-related and affective information of the input texts and extra topics. The advantage of this study lies in the fact that the presented model can generate abundant emotional responses, with the contents being related and diverse. To demonstrate the effectiveness of our method, we conduct extensive experiments on large-scale Weibo post–response pairs. Experimental results show that our method achieves good performance, even outperforming some existing models."
45,"Learning topics from short texts has become a critical and fundamental task for understanding the widely-spread streaming social messages, e.g., tweets, snippets and questions/answers. Up to date, there are two distinctive topic learning schemes: generative probabilistic graphical models and geometrically linear algebra approaches, with LDA and NMF being the representative works, respectively. Since these two methods both could uncover the latent topics hidden in the unstructured short texts, some interesting doubts are coming to our minds that which one is better and why? Are there any other more effective extensions? In order to explore valuable insights between LDA and NMF based learning schemes, we comprehensively conduct a series of experiments into two parts. Specifically, the basic LDA and NMF are compared with different experimental settings on several public short text datasets in the first part which would exhibit that NMF tends to perform better than LDA; in the second part, we propose a novel model called “Knowledge-guided Non-negative Matrix Factorization for Better Short Text Topic Mining” (abbreviated as KGNMF), which leverages external knowledge as a semantic regulator with low-rank formalizations, yielding up a time-efficient algorithm. Extensive experiments are conducted on three representative corpora with currently typical short text topic models to demonstrate the effectiveness of our proposed KGNMF. Overall, learning with NMF-based schemes is another effective manner in short text topic mining in addition to the popular LDA-based paradigms.",2018-01-01,2-s2.0-85052838005,Knowledge-Based Systems,Experimental explorations on short text topic mining between LDA and NMF based Schemes,"Learning topics from short texts has become a critical and fundamental task for understanding the widely-spread streaming social messages, e.g., tweets, snippets and questions/answers. Up to date, there are two distinctive topic learning schemes: generative probabilistic graphical models and geometrically linear algebra approaches, with LDA and NMF being the representative works, respectively. Since these two methods both could uncover the latent topics hidden in the unstructured short texts, some interesting doubts are coming to our minds that which one is better and why? Are there any other more effective extensions? In order to explore valuable insights between LDA and NMF based learning schemes, we comprehensively conduct a series of experiments into two parts. Specifically, the basic LDA and NMF are compared with different experimental settings on several public short text datasets in the first part which would exhibit that NMF tends to perform better than LDA; in the second part, we propose a novel model called “Knowledge-guided Non-negative Matrix Factorization for Better Short Text Topic Mining” (abbreviated as KGNMF), which leverages external knowledge as a semantic regulator with low-rank formalizations, yielding up a time-efficient algorithm. Extensive experiments are conducted on three representative corpora with currently typical short text topic models to demonstrate the effectiveness of our proposed KGNMF. Overall, learning with NMF-based schemes is another effective manner in short text topic mining in addition to the popular LDA-based paradigms."
46,"Purpose: Delivering messages and information to potentially interested users is one of the distinguishing applications of online enterprise social network (ESN). The purpose of this paper is to provide insights to better understand the repost preferences of users and provide personalized information service in enterprise social media marketing. Design/methodology/approach: It is accomplished by constructing a target audience identification framework. Repost preference latent Dirichlet allocation (RPLDA) topic model topic model is proposed to understand the mass user online repost preferences toward different contents. A topic-oriented preference metric is proposed to measure the preference degree of individual users. And the function of reposting forecasting is formulated to identify target audience. Findings: The empirical research shows the following: a total of 20 percent of the repost users in ESN represent the key active users who are particularly interested in the latent topic of messages in ESN and fits Pareto distribution; and the target audience identification framework can successfully identify different target key users for messages with different latent topics. Practical implications: The findings should motivate marketing managers to improve enterprise brand by identifying key target audience in ESN and marketing in a way that truthfully reflects personalized preferences. Originality/value: This study runs counter to most current business practices, which tend to use simple popularity to seek important users. Adaptively and dynamically identifying target audience appears to have considerable potential, especially in the rapidly growing area of enterprise social media information service.",2018-01-01,2-s2.0-85053036198,Industrial Management and Data Systems,Identifying target audience on enterprise social network,"Purpose: Delivering messages and information to potentially interested users is one of the distinguishing applications of online enterprise social network (ESN). The purpose of this paper is to provide insights to better understand the repost preferences of users and provide personalized information service in enterprise social media marketing. Design/methodology/approach: It is accomplished by constructing a target audience identification framework. Repost preference latent Dirichlet allocation (RPLDA) topic model topic model is proposed to understand the mass user online repost preferences toward different contents. A topic-oriented preference metric is proposed to measure the preference degree of individual users. And the function of reposting forecasting is formulated to identify target audience. Findings: The empirical research shows the following: a total of 20 percent of the repost users in ESN represent the key active users who are particularly interested in the latent topic of messages in ESN and fits Pareto distribution; and the target audience identification framework can successfully identify different target key users for messages with different latent topics. Practical implications: The findings should motivate marketing managers to improve enterprise brand by identifying key target audience in ESN and marketing in a way that truthfully reflects personalized preferences. Originality/value: This study runs counter to most current business practices, which tend to use simple popularity to seek important users. Adaptively and dynamically identifying target audience appears to have considerable potential, especially in the rapidly growing area of enterprise social media information service."
47,"Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection.",2018-01-01,2-s2.0-85034428503,Decision Support Systems,Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud,"Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection."
48,"WeChat is a new generation of mobile instant messaging product, which has over 800 million active accounts and support voice, video, pictures and text messages. China's Ministry of Science and Technology has applied social media to science and technology management. Rui Dong Ruan is one of the official accounts in the WeChat matrix of China's Ministry of Science and Technology. This research proposes an approach which uses Chinese word segmentation and topic model to processing texts of Rui Dong Ruan. The experimental result shows that there is a certain relationship between the content of the articles and the view counts on WeChat, which can make the science and technology publicity more efficient and effective.",2017-11-29,2-s2.0-85043505907,"PICMET 2017 - Portland International Conference on Management of Engineering and Technology: Technology Management for the Interconnected World, Proceedings",Text and data mining of social media in science and technology publicity,"WeChat is a new generation of mobile instant messaging product, which has over 800 million active accounts and support voice, video, pictures and text messages. China's Ministry of Science and Technology has applied social media to science and technology management. Rui Dong Ruan is one of the official accounts in the WeChat matrix of China's Ministry of Science and Technology. This research proposes an approach which uses Chinese word segmentation and topic model to processing texts of Rui Dong Ruan. The experimental result shows that there is a certain relationship between the content of the articles and the view counts on WeChat, which can make the science and technology publicity more efficient and effective."
49,"Bibliometric studies have long used simple search strings, publications count, and word counts to track the emergence of technologies. Novel machine learning methods open new possibilities to study bibliometric data and use algorithmic approaches to uncover emergence of a technology. This study looks at the large and complex dataset of vehicle related patents to uncover emergence indicators. By using machine learning methods this study focuses on if, and to what extent different methods can produce patterns of emergence from the data directly. The data extracted from PATSTAT contains 711296 granted US patent abstracts between the years 1980 and 2014 resulting from a search for ""vehicle"" creating a complex dataset of technologies from automotive to medical applications. Using Latent Dirichlet Allocation and Dynamic Topic Modeling we show different emergence patterns. Finally, we discuss in detail the possibilities of using machine learning approaches to draw emergence dynamics of technologies.",2017-11-29,2-s2.0-85043474004,"PICMET 2017 - Portland International Conference on Management of Engineering and Technology: Technology Management for the Interconnected World, Proceedings",Using machine learning approaches to identify emergence: Case of vehicle related patent data,"Bibliometric studies have long used simple search strings, publications count, and word counts to track the emergence of technologies. Novel machine learning methods open new possibilities to study bibliometric data and use algorithmic approaches to uncover emergence of a technology. This study looks at the large and complex dataset of vehicle related patents to uncover emergence indicators. By using machine learning methods this study focuses on if, and to what extent different methods can produce patterns of emergence from the data directly. The data extracted from PATSTAT contains 711296 granted US patent abstracts between the years 1980 and 2014 resulting from a search for ""vehicle"" creating a complex dataset of technologies from automotive to medical applications. Using Latent Dirichlet Allocation and Dynamic Topic Modeling we show different emergence patterns. Finally, we discuss in detail the possibilities of using machine learning approaches to draw emergence dynamics of technologies."
50,"With the rapid development of e-commerce, recommender systems have been widely studied. Many recommendation algorithms utilize ratings and reviews information. However, as the number of users and items grows, these algorithms face the problems of sparsity and scalability. Those problems make it hard to extract useful information from a highly sparse rating matrix and to apply a trained model to larger datasets. In this paper, we aim at solving the sparsity and scalability problems using rating and review information. Three possible solutions for sparsity and scalability problems are concluded and a novel recommendation model called TCR which combines those three ideas are proposed. Experiments on real-world datasets show that our proposed method has better performance on top-N recommendation and has better scalability compared to the state-of-the-art models.",2017-11-22,2-s2.0-85041663753,"Proceedings - 14th IEEE International Conference on E-Business Engineering, ICEBE 2017 - Including 13th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 207",Optimize Recommendation System with Topic Modeling and Clustering,"With the rapid development of e-commerce, recommender systems have been widely studied. Many recommendation algorithms utilize ratings and reviews information. However, as the number of users and items grows, these algorithms face the problems of sparsity and scalability. Those problems make it hard to extract useful information from a highly sparse rating matrix and to apply a trained model to larger datasets. In this paper, we aim at solving the sparsity and scalability problems using rating and review information. Three possible solutions for sparsity and scalability problems are concluded and a novel recommendation model called TCR which combines those three ideas are proposed. Experiments on real-world datasets show that our proposed method has better performance on top-N recommendation and has better scalability compared to the state-of-the-art models."
51,"The proceedings contain 47 papers. The topics discussed include: pricing mechanism research in duopoly online bicycle-sharing market: a game theory approach; online retailer assortment planning and managing under customer and supplier uncertainty effects using internal and external data; optimize recommendation system with topic modeling and clustering; a multi-view clustering method for community discovery integrating links and tags; development of bulk material management system and research on material balance applications based on business intelligence; LODQL: a language for creation, query, and service generation of linked open data; graph analysis of fog computing systems for Industry 4.0; the effects of consumer perceived different service of trusted third party on trust intention: an empirical study in Australia; role identification to discover potential opportunity information in business process; research on government data publishing based on differential privacy model; an analysis of energy-efficient approaches used for virtual machines and data centres; a parallel FP-growth algorithm based on GPU; indexing for large scale data querying based on spark SQL; a development framework for customer experience management applications: principles and case study; methods of introducing continuous process mining to service management for mobile apps; investment recommendation with total capital value maximization in online P2P lending; a study on the influence of engagement marketing strategy on customer perceived support and willingness to customer engagement; and a blockchain-based supply chain quality management framework.",2017-11-22,2-s2.0-85041646801,"Proceedings - 14th IEEE International Conference on E-Business Engineering, ICEBE 2017 - Including 13th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 207","Proceedings - 14th IEEE International Conference on E-Business Engineering, ICEBE 2017 - Including 13th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 207","The proceedings contain 47 papers. The topics discussed include: pricing mechanism research in duopoly online bicycle-sharing market: a game theory approach; online retailer assortment planning and managing under customer and supplier uncertainty effects using internal and external data; optimize recommendation system with topic modeling and clustering; a multi-view clustering method for community discovery integrating links and tags; development of bulk material management system and research on material balance applications based on business intelligence; LODQL: a language for creation, query, and service generation of linked open data; graph analysis of fog computing systems for Industry 4.0; the effects of consumer perceived different service of trusted third party on trust intention: an empirical study in Australia; role identification to discover potential opportunity information in business process; research on government data publishing based on differential privacy model; an analysis of energy-efficient approaches used for virtual machines and data centres; a parallel FP-growth algorithm based on GPU; indexing for large scale data querying based on spark SQL; a development framework for customer experience management applications: principles and case study; methods of introducing continuous process mining to service management for mobile apps; investment recommendation with total capital value maximization in online P2P lending; a study on the influence of engagement marketing strategy on customer perceived support and willingness to customer engagement; and a blockchain-based supply chain quality management framework."
52,"Topic modeling is a widely used technique in knowledge discovery and data mining. However, finding the right number of topics in a given text source has remained a challenging issue. In this paper, we study the concept of conceptual stability via nonnegative matrix factorization. Based on this finding, we propose a method to identify the correct number of topics and offer empirical evidence in its favor in terms of classification accuracy and the number of topics that are naturally present in the text sources. Experiments on real-world text corpora demonstrate that the proposed method has outperformed state-of-the-art latent Dirichlet allocation and nonnegative matrix factorization models.",2017-11-06,2-s2.0-85037351048,"International Conference on Information and Knowledge Management, Proceedings",On discovering the number of document topics via conceptual latent space,"Topic modeling is a widely used technique in knowledge discovery and data mining. However, finding the right number of topics in a given text source has remained a challenging issue. In this paper, we study the concept of conceptual stability via nonnegative matrix factorization. Based on this finding, we propose a method to identify the correct number of topics and offer empirical evidence in its favor in terms of classification accuracy and the number of topics that are naturally present in the text sources. Experiments on real-world text corpora demonstrate that the proposed method has outperformed state-of-the-art latent Dirichlet allocation and nonnegative matrix factorization models."
53,"The soaring of social media services has greatly propelled the prevalence of document networks. Rather than a set of plain texts, documents are nodes in graphs. An observable link connects the documents at its two ends, thus it implicitly reflects the semantic association between the document pair. Previous work assumes that only similar documents tend to be connected, which neglects the rich connective patterns in the topological structure. In this paper, we introduce a latent correlation factor to categorize the links into several categories, and each category corresponds to a unique kind of association. By fitting the data, the relational information (e.g., homophily and heterophily) can be comprehensively captured. By resorting to Canonical Correlation Analysis (CCA), we maximize the correlation between all pairs of linked documents. We propose a pure generative model and derive efficient learning algorithms based on the variational EMmethods. Experiments on three different datasets demonstrate that the proposed model is competitive and usually better than the state-of-the-art baselines on both topic modeling and link prediction.",2017-11-06,2-s2.0-85037332333,"International Conference on Information and Knowledge Management, Proceedings",Incorporating the latent link categories in relational topic modeling,"The soaring of social media services has greatly propelled the prevalence of document networks. Rather than a set of plain texts, documents are nodes in graphs. An observable link connects the documents at its two ends, thus it implicitly reflects the semantic association between the document pair. Previous work assumes that only similar documents tend to be connected, which neglects the rich connective patterns in the topological structure. In this paper, we introduce a latent correlation factor to categorize the links into several categories, and each category corresponds to a unique kind of association. By fitting the data, the relational information (e.g., homophily and heterophily) can be comprehensively captured. By resorting to Canonical Correlation Analysis (CCA), we maximize the correlation between all pairs of linked documents. We propose a pure generative model and derive efficient learning algorithms based on the variational EMmethods. Experiments on three different datasets demonstrate that the proposed model is competitive and usually better than the state-of-the-art baselines on both topic modeling and link prediction."
54,"People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification.",2017-11-06,2-s2.0-85037361799,"International Conference on Information and Knowledge Management, Proceedings",An enhanced topic modeling approach to multiple stance identification,"People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification."
55,"Linking multiple news streams based on the reported events and analyzing the streams' temporal publishing patterns are two very important tasks for information analysis, discovering newsworthy stories, studying the event evolution, and detecting untrustworthy sources of information. In this paper, we propose techniques for cross-linking news streams based on the reported events with the purpose of analyzing the temporal dependencies among streams. Our research tackles two main issues: (1) how news streams are connected as reporting an event or the evolution of the same event and (2) how timely the newswires report related events using different publishing platforms. Our approach is based on dynamic topic modeling for detecting and tracking events over the timeline and on clustering news according to the events. We leverage the event-based clustering to link news across different streams and present two scoring functions for ranking the streams based on their timeliness in publishing news about a specific event.",2017-11-06,2-s2.0-85037356525,"International Conference on Information and Knowledge Management, Proceedings",Linking news across multiple streams for timeliness analysis,"Linking multiple news streams based on the reported events and analyzing the streams' temporal publishing patterns are two very important tasks for information analysis, discovering newsworthy stories, studying the event evolution, and detecting untrustworthy sources of information. In this paper, we propose techniques for cross-linking news streams based on the reported events with the purpose of analyzing the temporal dependencies among streams. Our research tackles two main issues: (1) how news streams are connected as reporting an event or the evolution of the same event and (2) how timely the newswires report related events using different publishing platforms. Our approach is based on dynamic topic modeling for detecting and tracking events over the timeline and on clustering news according to the events. We leverage the event-based clustering to link news across different streams and present two scoring functions for ranking the streams based on their timeliness in publishing news about a specific event."
56,"We propose Topic Anchoring-based Review Summarization (TARS), a two-step extractive summarization method, which creates review summaries from the sentences that represent the most important aspects of a review. In the first step, the proposed method utilizes Topic Aspect Sentiment Model (TASM), a novel sentiment-topic model, to identify aspects of sentiment-specific topics in a collection of reviews. The output of TASM is utilized in the second step of TARS to rank review sentences based on how representative of the most important review aspects their words are. Qualitative and quantitative evaluation of review summaries using two collections indicate the effectiveness of structuring review summaries around aspects of sentiment-specific topics.",2017-11-06,2-s2.0-85037356952,"International Conference on Information and Knowledge Management, Proceedings",Sentence retrieval with sentiment-specific topical anchoring for review summarization,"We propose Topic Anchoring-based Review Summarization (TARS), a two-step extractive summarization method, which creates review summaries from the sentences that represent the most important aspects of a review. In the first step, the proposed method utilizes Topic Aspect Sentiment Model (TASM), a novel sentiment-topic model, to identify aspects of sentiment-specific topics in a collection of reviews. The output of TASM is utilized in the second step of TARS to rank review sentences based on how representative of the most important review aspects their words are. Qualitative and quantitative evaluation of review summaries using two collections indicate the effectiveness of structuring review summaries around aspects of sentiment-specific topics."
57,"Exploratory analysis of a text corpus is an important task that can be aided by informative visualization. One spatially-oriented form of document visualization is a scatterplot, whereby every document is associated with a coordinate, and relationships among documents can be perceived through their spatial distances. Semantic visualization further infuses the visualization space with latent semantics, by incorporating a topic model that has a representation in the visualization space, allowing users to also perceive relationships between documents and topics spatially.We illustrate how a semantic visualization system called SemVis could be used to navigate a text corpus interactively and topically via browsing and searching.",2017-11-06,2-s2.0-85037377634,"International Conference on Information and Knowledge Management, Proceedings",SemVis: Semantic visualization for interactive topical analysis,"Exploratory analysis of a text corpus is an important task that can be aided by informative visualization. One spatially-oriented form of document visualization is a scatterplot, whereby every document is associated with a coordinate, and relationships among documents can be perceived through their spatial distances. Semantic visualization further infuses the visualization space with latent semantics, by incorporating a topic model that has a representation in the visualization space, allowing users to also perceive relationships between documents and topics spatially.We illustrate how a semantic visualization system called SemVis could be used to navigate a text corpus interactively and topically via browsing and searching."
58,"Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics.",2017-11-06,2-s2.0-85037334875,"International Conference on Information and Knowledge Management, Proceedings",A study of feature construction for text-based forecasting of time series variables,"Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics."
59,"Internet information services have been greatly improved profiting from the growing performance of interest mining technology. Visual perceptual behaviours, a newhotspot of mining user's interests, have resulted in great gains in some typical Internet information services, e.g., information retrieval and recommendation. It is validated that combining the subjective visual perceptual behaviours with the objective contents can significantly improve these services' performance. However, the existing methods usually treat the contents and visual perceptual behaviours as two independent parts in the calculating process. The gain of visual perceptual behaviours has not been fully exploited. In this paper, we mainly aim at improving the gain of visual perceptual behaviour for text recommendation, by integrating the objective contents with subjective visual perceptual behaviours. We investigate the correlation between user's reading interests and records of real-time interaction on texts, and then design a real-time visual perceptual behaviour based method for text recommendation, which is able to: (1) build a joint interest model, called ViP-LDA (Visual Perceptual LDA), by integrating the user's visual perceptual behaviours into topic model; (2) make more accurate text recommendation based on ViP-LDA with feedback adjustment. Several experiments on a real data set are implemented to demonstrate the effectiveness of our method.",2017-11-06,2-s2.0-85037372352,"International Conference on Information and Knowledge Management, Proceedings",Improving the gain of visual perceptual behaviour on topic modeling for text recommendation,"Internet information services have been greatly improved profiting from the growing performance of interest mining technology. Visual perceptual behaviours, a newhotspot of mining user's interests, have resulted in great gains in some typical Internet information services, e.g., information retrieval and recommendation. It is validated that combining the subjective visual perceptual behaviours with the objective contents can significantly improve these services' performance. However, the existing methods usually treat the contents and visual perceptual behaviours as two independent parts in the calculating process. The gain of visual perceptual behaviours has not been fully exploited. In this paper, we mainly aim at improving the gain of visual perceptual behaviour for text recommendation, by integrating the objective contents with subjective visual perceptual behaviours. We investigate the correlation between user's reading interests and records of real-time interaction on texts, and then design a real-time visual perceptual behaviour based method for text recommendation, which is able to: (1) build a joint interest model, called ViP-LDA (Visual Perceptual LDA), by integrating the user's visual perceptual behaviours into topic model; (2) make more accurate text recommendation based on ViP-LDA with feedback adjustment. Several experiments on a real data set are implemented to demonstrate the effectiveness of our method."
60,"Personalized recommendation of items frequently faces scenarios where we have sparse observations on users' adoption of items. In the literature, there are two promising directions. One is to connect sparse items through similarity in content. The other is to connect sparse users through similarity in social relations. We seek to integrate both types of information, in addition to the adoption information, within a single integrated model. Our proposed method models item content via a topic model, and user communities via an autoencoder model, while bridging a user's community-based preference to her topic-based preference. Experiments on public real-life data showcase the utility of the model, particularly when there is significant compatibility between communities and topics.",2017-11-06,2-s2.0-85037356079,"International Conference on Information and Knowledge Management, Proceedings",Collaborative topic regression with denoising autoencoder for content and community co-representation,"Personalized recommendation of items frequently faces scenarios where we have sparse observations on users' adoption of items. In the literature, there are two promising directions. One is to connect sparse items through similarity in content. The other is to connect sparse users through similarity in social relations. We seek to integrate both types of information, in addition to the adoption information, within a single integrated model. Our proposed method models item content via a topic model, and user communities via an autoencoder model, while bridging a user's community-based preference to her topic-based preference. Experiments on public real-life data showcase the utility of the model, particularly when there is significant compatibility between communities and topics."
61,"Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines.",2017-11-06,2-s2.0-85037352373,"International Conference on Information and Knowledge Management, Proceedings",Joint topic-semantic-aware social recommendation for online voting,"Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines."
62,"Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA.",2017-11-06,2-s2.0-85037341070,"International Conference on Information and Knowledge Management, Proceedings",A topic model based on poisson decomposition,"Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA."
63,"Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that ""one-hot"" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.",2017-11-06,2-s2.0-85037353960,"International Conference on Information and Knowledge Management, Proceedings",RATE: Overcoming noise and sparsity of textual features in real-time location estimation,"Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that ""one-hot"" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression."
64,"Social media platforms such as weblogs and social networking sites provide Internet users with an unprecedented means to express their opinions and debate on a wide range of issues. Concurrently with their growing importance in public communication, social media platforms may foster echo chambers and filter bubbles: homophily and content personalization lead users to be increasingly exposed to conforming opinions.There is therefore a need for unbiased systems able to identify and provide access to varied viewpoints. To address this task, we propose in this paper a novel unsupervised topic model, the Social Network Viewpoint Discovery Model (SNVDM). Given a specific issue (e.g., U.S. policy) as well as the text and social interactions from the users discussing this issue on a social networking site, SNVDM jointly identifies the issue's topics, the users' viewpoints, and the discourse pertaining to the different topics and viewpoints. In order to overcome the potential sparsity of the social network (i.e., some users interact with only a few other users), we propose an extension to SNVDM based on the Generalized pólya Urn sampling scheme (SNVDM-GPU) to leverage ""acquaintances of acquaintances"" relationships. We benchmark the different proposed models against three baselines, namely TAM, SN-LDA, and VODUM, on a viewpoint clustering task using two real-world datasets. We thereby provide evidence that our model SNVDM and its extension SNVDM-GPU significantly outperform state-of-the-art baselines, and we show that utilizing social interactions greatly improves viewpoint clustering performance.",2017-11-06,2-s2.0-85037336749,"International Conference on Information and Knowledge Management, Proceedings",Users are known by the company they keep: Topic models for viewpoint discovery in social networks,"Social media platforms such as weblogs and social networking sites provide Internet users with an unprecedented means to express their opinions and debate on a wide range of issues. Concurrently with their growing importance in public communication, social media platforms may foster echo chambers and filter bubbles: homophily and content personalization lead users to be increasingly exposed to conforming opinions.There is therefore a need for unbiased systems able to identify and provide access to varied viewpoints. To address this task, we propose in this paper a novel unsupervised topic model, the Social Network Viewpoint Discovery Model (SNVDM). Given a specific issue (e.g., U.S. policy) as well as the text and social interactions from the users discussing this issue on a social networking site, SNVDM jointly identifies the issue's topics, the users' viewpoints, and the discourse pertaining to the different topics and viewpoints. In order to overcome the potential sparsity of the social network (i.e., some users interact with only a few other users), we propose an extension to SNVDM based on the Generalized pólya Urn sampling scheme (SNVDM-GPU) to leverage ""acquaintances of acquaintances"" relationships. We benchmark the different proposed models against three baselines, namely TAM, SN-LDA, and VODUM, on a viewpoint clustering task using two real-world datasets. We thereby provide evidence that our model SNVDM and its extension SNVDM-GPU significantly outperform state-of-the-art baselines, and we show that utilizing social interactions greatly improves viewpoint clustering performance."
65,"This study aims to identify both where technology transfer research originated and where it is going. A quantitative approach was adopted in this study to observe the trends from an objective perspective. To do this, longitudinal bibliographic data of journal papers describing technology transfer from 1980 to 2015 are collected. Topic modeling and co-authorship network analyses are then applied to classify topics and identify an evolution of research groups. First, the principal transfer agent is changed from governmental organizations to universities, as technology donors, while industry plays the role of technology recipients. Second, major technology fields that researchers have focused on follow socially attractive interests. Third, the scope of focus gradually moves from national level research or international transfers to organizational level research. In addition, technology transfer research seems to change from a technology transfer application to a dynamic technology transfer process. In addition, six topics are identified and further discussed to understand future research directions. The research findings are expected to help us understand research trends in technology transfer and, thus, are expected to provide valuable insights to researchers in this field and policy makers who are in charge of developing policies to support technology transfer.",2017-11-03,2-s2.0-85032817031,Journal of Technology Transfer,Where technology transfer research originated and where it is going: a quantitative analysis of literature published between 1980 and 2015,"This study aims to identify both where technology transfer research originated and where it is going. A quantitative approach was adopted in this study to observe the trends from an objective perspective. To do this, longitudinal bibliographic data of journal papers describing technology transfer from 1980 to 2015 are collected. Topic modeling and co-authorship network analyses are then applied to classify topics and identify an evolution of research groups. First, the principal transfer agent is changed from governmental organizations to universities, as technology donors, while industry plays the role of technology recipients. Second, major technology fields that researchers have focused on follow socially attractive interests. Third, the scope of focus gradually moves from national level research or international transfers to organizational level research. In addition, technology transfer research seems to change from a technology transfer application to a dynamic technology transfer process. In addition, six topics are identified and further discussed to understand future research directions. The research findings are expected to help us understand research trends in technology transfer and, thus, are expected to provide valuable insights to researchers in this field and policy makers who are in charge of developing policies to support technology transfer."
66,"During the past two decades, the focus of marketing has moved from the tactics of persuasion to the strategies of value cocreation. After moving toward cognitive science and corporate strategies in the early 2000s, marketing research returned to its traditional domains of consumer psychologies and customer management. While conscientious consumers are gradually restraining themselves from selfish indulgence, marketers have refocused on a new set of values that encompass mental, experiential, and societal well-being. In this regard, we adopt an unprecedented approach by incorporating topic modeling with social network analysis. The results show that, in terms of topic heterogeneity, the most impactful journals are the most diverse, whereas each runner-up has a unique focus. Among the journals, we detect two major co-authorship communities, and among the topics, we detect three. Further, we find that the communities of the most cited papers are composed of heterogeneous clusters of similar topics. The pivots within, and the bridges between, these communities are also reported. In the spirit of collaborative research, our topic model and network analysis are shared via online collaboration and visualization platforms that readers can use to explore our models interactively and to download the dataset for further studies.",2017-11-01,2-s2.0-85030455391,Journal of Interactive Marketing,"Popular Research Topics in Marketing Journals, 1995–2014","During the past two decades, the focus of marketing has moved from the tactics of persuasion to the strategies of value cocreation. After moving toward cognitive science and corporate strategies in the early 2000s, marketing research returned to its traditional domains of consumer psychologies and customer management. While conscientious consumers are gradually restraining themselves from selfish indulgence, marketers have refocused on a new set of values that encompass mental, experiential, and societal well-being. In this regard, we adopt an unprecedented approach by incorporating topic modeling with social network analysis. The results show that, in terms of topic heterogeneity, the most impactful journals are the most diverse, whereas each runner-up has a unique focus. Among the journals, we detect two major co-authorship communities, and among the topics, we detect three. Further, we find that the communities of the most cited papers are composed of heterogeneous clusters of similar topics. The pivots within, and the bridges between, these communities are also reported. In the spirit of collaborative research, our topic model and network analysis are shared via online collaboration and visualization platforms that readers can use to explore our models interactively and to download the dataset for further studies."
67,"In recent decades, analyzing the sentiments in online customer reviews has become important to many businesses and researchers. However, insufficient amount of labeled training corpus is a bottleneck for machine learning approaches. Self-training is one of the promising semi-supervised techniques which does not require large amounts of labeled data. However, self-training also suffers from an incorrect labeling problem along with insufficient amount of labeled data. This study proposed a semi-supervised learning framework that adds only confidently predicted data to the training corpus in order to enrich the initial classifier in self-training. The experimental results indicate that the proposed method performed better than self-training.",2017-11-01,2-s2.0-85030475861,Electronic Commerce Research and Applications,Sentiment labeling for extending initial labeled data to improve semi-supervised sentiment classification,"In recent decades, analyzing the sentiments in online customer reviews has become important to many businesses and researchers. However, insufficient amount of labeled training corpus is a bottleneck for machine learning approaches. Self-training is one of the promising semi-supervised techniques which does not require large amounts of labeled data. However, self-training also suffers from an incorrect labeling problem along with insufficient amount of labeled data. This study proposed a semi-supervised learning framework that adds only confidently predicted data to the training corpus in order to enrich the initial classifier in self-training. The experimental results indicate that the proposed method performed better than self-training."
68,"The increasing use of the Internet for many purposes is creating big data, many of which are generated from social media. These big data potentially could assist in obtaining valuable administrative information and even explore new social phenomena. Traditional ways of collecting data, such as questionnaire surveys, are time-consuming and costly. Therefore, the use of social media affords the opportunity to extract information that might be of benefit to the construction industry in a responsive and inexpensive manner. To this end, this paper explores whether information and knowledge that would be valuable in the construction domain can be generated by analyzing social media data. Twitter was selected for an initial trial analysis because of its wide usage in the United States. Because they represent a majority of the construction users in Twitter, the following four user clusters were selected and analyzed: construction workers, construction companies, construction unions, and construction media. For each user identified in the four clusters, the 3,200 most recent Twitter messages were collected, which were analyzed from the following aspects: sentiment analysis, topic modeling, link analysis, geolocation analysis, and timeline analysis. Different data-analysis methods were used for the specific themes, such as Stanford Natural Language Processing (StanfordNLP) for sentiment analysis. The detailed findings, benefits, and barriers to incorporating social media data analytics in the construction industry, as well as future research directions, are discussed in this paper. For example, the sentiment analysis results indicated that construction workers tend to have a higher proportion of negative messages compared to the other clusters, which may prompt more attention to emotional guidance and understanding by construction companies and the public. This paper benefits academia by testing an alternative way of studying the construction population, which could help decision makers gain a better understanding of real-world situations in the construction industry.",2017-11-01,2-s2.0-85029504535,Journal of Management in Engineering,Social Media Data Analytics for the U.S. Construction Industry: Preliminary Study on Twitter,"The increasing use of the Internet for many purposes is creating big data, many of which are generated from social media. These big data potentially could assist in obtaining valuable administrative information and even explore new social phenomena. Traditional ways of collecting data, such as questionnaire surveys, are time-consuming and costly. Therefore, the use of social media affords the opportunity to extract information that might be of benefit to the construction industry in a responsive and inexpensive manner. To this end, this paper explores whether information and knowledge that would be valuable in the construction domain can be generated by analyzing social media data. Twitter was selected for an initial trial analysis because of its wide usage in the United States. Because they represent a majority of the construction users in Twitter, the following four user clusters were selected and analyzed: construction workers, construction companies, construction unions, and construction media. For each user identified in the four clusters, the 3,200 most recent Twitter messages were collected, which were analyzed from the following aspects: sentiment analysis, topic modeling, link analysis, geolocation analysis, and timeline analysis. Different data-analysis methods were used for the specific themes, such as Stanford Natural Language Processing (StanfordNLP) for sentiment analysis. The detailed findings, benefits, and barriers to incorporating social media data analytics in the construction industry, as well as future research directions, are discussed in this paper. For example, the sentiment analysis results indicated that construction workers tend to have a higher proportion of negative messages compared to the other clusters, which may prompt more attention to emotional guidance and understanding by construction companies and the public. This paper benefits academia by testing an alternative way of studying the construction population, which could help decision makers gain a better understanding of real-world situations in the construction industry."
69,"The development of the Internet and mobile devices enabled the emergence of travel and hospitality review sites, leading to a large number of customer opinion posts. While such comments may influence future demand of the targeted hotels, they can also be used by hotel managers to improve customer experience. In this article, sentiment classification of an eco-hotel is assessed through a text mining approach using several different sources of customer reviews. The latent Dirichlet allocation modeling algorithm is applied to gather relevant topics that characterize a given hospitality issue by a sentiment. Several findings were unveiled including that hotel food generates ordinary positive sentiments, while hospitality generates both ordinary and strong positive feelings. Such results are valuable for hospitality management, validating the proposed approach.",2017-10-03,2-s2.0-85018173060,Journal of Hospitality Marketing and Management,Sentiment Classification of Consumer-Generated Online Reviews Using Topic Modeling,"The development of the Internet and mobile devices enabled the emergence of travel and hospitality review sites, leading to a large number of customer opinion posts. While such comments may influence future demand of the targeted hotels, they can also be used by hotel managers to improve customer experience. In this article, sentiment classification of an eco-hotel is assessed through a text mining approach using several different sources of customer reviews. The latent Dirichlet allocation modeling algorithm is applied to gather relevant topics that characterize a given hospitality issue by a sentiment. Several findings were unveiled including that hotel food generates ordinary positive sentiments, while hospitality generates both ordinary and strong positive feelings. Such results are valuable for hospitality management, validating the proposed approach."
70,"Classifying requirements into functional requirements (FR) and non-functional ones (NFR) is an important task in requirements engineering. However, automated classification of requirements written in natural language is not straightforward, due to the variability of natural language and the absence of a controlled vocabulary. This paper investigates how automated classification of requirements into FR and NFR can be improved and how well several machine learning approaches work in this context. We contribute an approach for preprocessing requirements that standardizes and normalizes requirements before applying classification algorithms. Further, we report on how well several existing machine learning methods perform for automated classification of NFRs into sub-categories such as usability, availability, or performance. Our study is performed on 625 requirements provided by the OpenScience tera-PROMISE repository. We found that our preprocessing improved the performance of an existing classification method. We further found significant differences in the performance of approaches such as Latent Dirichlet Allocation, Biterm Topic Modeling, or Naïve Bayes for the sub-classification of NFRs.",2017-09-22,2-s2.0-85032825139,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",What Works Better? A Study of Classifying Requirements,"Classifying requirements into functional requirements (FR) and non-functional ones (NFR) is an important task in requirements engineering. However, automated classification of requirements written in natural language is not straightforward, due to the variability of natural language and the absence of a controlled vocabulary. This paper investigates how automated classification of requirements into FR and NFR can be improved and how well several machine learning approaches work in this context. We contribute an approach for preprocessing requirements that standardizes and normalizes requirements before applying classification algorithms. Further, we report on how well several existing machine learning methods perform for automated classification of NFRs into sub-categories such as usability, availability, or performance. Our study is performed on 625 requirements provided by the OpenScience tera-PROMISE repository. We found that our preprocessing improved the performance of an existing classification method. We further found significant differences in the performance of approaches such as Latent Dirichlet Allocation, Biterm Topic Modeling, or Naïve Bayes for the sub-classification of NFRs."
71,"Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number-in the range of thousands per day for popular applications-a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements.",2017-09-22,2-s2.0-85032809108,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution,"Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number-in the range of thousands per day for popular applications-a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements."
72,"Due to the popularity of social networks, such as microblogs and Twitter, a vast amount of short text data is created every day. Much recent research in short text becomes increasingly significant, such as topic inference for short text. Biterm topic model (BTM) benefits from the word co-occurrence patterns of the corpus, which makes it perform better than conventional topic models in uncovering latent semantic relevance for short text. However, BTM resorts to Gibbs sampling to infer topics, which is very time consuming, especially for large-scale datasets or when the number of topics is extremely large. It requires O(K) operations per sample for K topics, where K denotes the number of topics in the corpus. In this paper, we propose an acceleration algorithm of BTM, FastBTM, using an efficient sampling method for BTM, which converges much faster than BTM without degrading topic quality. FastBTM is based on Metropolis-Hastings and alias method, both of which have been widely adopted in Latent Dirichlet Allocation (LDA) model and achieved outstanding speedup. Our FastBTM can effectively reduce the sampling complexity of biterm topic model from O(K) to O(1) amortized time. We carry out a number of experiments on three datasets including two short text datasets, Tweets2011 Collection dataset and Yahoo! Answers dataset, and one long document dataset, Enron dataset. Our experimental results show that when the number of topics K increases, the gap in running time speed between FastBTM and BTM gets especially larger. In addition, our FastBTM is effective for both short text datasets and long document datasets.",2017-09-15,2-s2.0-85020799830,Knowledge-Based Systems,FastBTM: Reducing the sampling time for biterm topic model,"Due to the popularity of social networks, such as microblogs and Twitter, a vast amount of short text data is created every day. Much recent research in short text becomes increasingly significant, such as topic inference for short text. Biterm topic model (BTM) benefits from the word co-occurrence patterns of the corpus, which makes it perform better than conventional topic models in uncovering latent semantic relevance for short text. However, BTM resorts to Gibbs sampling to infer topics, which is very time consuming, especially for large-scale datasets or when the number of topics is extremely large. It requires O(K) operations per sample for K topics, where K denotes the number of topics in the corpus. In this paper, we propose an acceleration algorithm of BTM, FastBTM, using an efficient sampling method for BTM, which converges much faster than BTM without degrading topic quality. FastBTM is based on Metropolis-Hastings and alias method, both of which have been widely adopted in Latent Dirichlet Allocation (LDA) model and achieved outstanding speedup. Our FastBTM can effectively reduce the sampling complexity of biterm topic model from O(K) to O(1) amortized time. We carry out a number of experiments on three datasets including two short text datasets, Tweets2011 Collection dataset and Yahoo! Answers dataset, and one long document dataset, Enron dataset. Our experimental results show that when the number of topics K increases, the gap in running time speed between FastBTM and BTM gets especially larger. In addition, our FastBTM is effective for both short text datasets and long document datasets."
73,"Various researchers have already engaged in using auxiliary side information within recommender applications to improve the quality and accuracy of recommendations. This side information has either been in the form of structured information such as product specifications and user demographic information or unstructured information such as product reviews. The abundance of unstructured information compared to structured information entices the use of such unstructured information in the recommendation process. Existing works that employ unstructured content have been confined to standard text modeling technique such as the use of frequency measures or topic modeling techniques. In this paper, we propose to model unstructured content about both products and users through the exploitation of word embedding techniques. More specifically, we propose to learn both user and product representations from any type of unstructured textual contents available in different external information sources using recurrent neural networks. We then apply our learnt product and user representations on two recommendation frameworks based on matrix factorization and link prediction to enhance the recommendation task. Experimental results on four datasets constructed from the Rotten Tomatoes website (movie review aggregator database) have shown the effectiveness of our proposed approach in different real-world situations compared to the state of the art.",2017-09-01,2-s2.0-85028733908,Electronic Commerce Research and Applications,Embedding unstructured side information in product recommendation,"Various researchers have already engaged in using auxiliary side information within recommender applications to improve the quality and accuracy of recommendations. This side information has either been in the form of structured information such as product specifications and user demographic information or unstructured information such as product reviews. The abundance of unstructured information compared to structured information entices the use of such unstructured information in the recommendation process. Existing works that employ unstructured content have been confined to standard text modeling technique such as the use of frequency measures or topic modeling techniques. In this paper, we propose to model unstructured content about both products and users through the exploitation of word embedding techniques. More specifically, we propose to learn both user and product representations from any type of unstructured textual contents available in different external information sources using recurrent neural networks. We then apply our learnt product and user representations on two recommendation frameworks based on matrix factorization and link prediction to enhance the recommendation task. Experimental results on four datasets constructed from the Rotten Tomatoes website (movie review aggregator database) have shown the effectiveness of our proposed approach in different real-world situations compared to the state of the art."
74,"Many applications require semantic understanding of short texts, and inferring discriminative and coherent latent topics is a critical and fundamental task in these applications. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Recent studies show that the Dirichlet Multinomial Mixture (DMM) model is effective for topic inference over short texts by assuming that each piece of short text is generated by a single topic. However, DMM has two main limitations. First, even though it seems reasonable to assume that each short text has only one topic because of its shortness, the definition of ""shortness"" is subjective and the length of the short texts is dataset dependent. That is, the single-topic assumption may be too strong for some datasets. To address this limitation, we propose to model the topic number as a Poisson distribution, allowing each short text to be associated with a small number of topics (e.g., one to three topics). This model is named PDMM. Second, DMM (and also PDMM) does not have access to background knowledge (e.g., semantic relations between words) when modeling short texts.When a human being interprets a piece of short text, the understanding is not solely based on its content words, but also their semantic relations. Recent advances in word embeddings offer effective learning of word semantic relations from a large corpus. Such auxiliary word embeddings enable us to address the second limitation. To this end, we propose to promote the semantically related words under the same topic during the sampling process, by using the generalized Polya urn (GPU) model. Through the GPU model, background knowledge about word semantic relations learned from millions of external documents can be easily exploited to improve topic modeling for short texts. By directly extending the PDMM model with the GPU model, we propose two more effective topic models for short texts, named GPU-DMM and GPU-PDMM. Through extensive experiments on two real-world short text collections in two languages, we demonstrate that PDMM achieves better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to better accuracy in a text classification task, as an indirect evaluation. Both GPU-DMM and GPU-PDMM further improve topic coherence and text classification accuracy. GPUPDMM outperforms GPU-DMM at the price of higher computational costs.",2017-08-01,2-s2.0-85028533702,ACM Transactions on Information Systems,Enhancing topic modeling for short texts with auxiliary word embeddings,"Many applications require semantic understanding of short texts, and inferring discriminative and coherent latent topics is a critical and fundamental task in these applications. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Recent studies show that the Dirichlet Multinomial Mixture (DMM) model is effective for topic inference over short texts by assuming that each piece of short text is generated by a single topic. However, DMM has two main limitations. First, even though it seems reasonable to assume that each short text has only one topic because of its shortness, the definition of ""shortness"" is subjective and the length of the short texts is dataset dependent. That is, the single-topic assumption may be too strong for some datasets. To address this limitation, we propose to model the topic number as a Poisson distribution, allowing each short text to be associated with a small number of topics (e.g., one to three topics). This model is named PDMM. Second, DMM (and also PDMM) does not have access to background knowledge (e.g., semantic relations between words) when modeling short texts.When a human being interprets a piece of short text, the understanding is not solely based on its content words, but also their semantic relations. Recent advances in word embeddings offer effective learning of word semantic relations from a large corpus. Such auxiliary word embeddings enable us to address the second limitation. To this end, we propose to promote the semantically related words under the same topic during the sampling process, by using the generalized Polya urn (GPU) model. Through the GPU model, background knowledge about word semantic relations learned from millions of external documents can be easily exploited to improve topic modeling for short texts. By directly extending the PDMM model with the GPU model, we propose two more effective topic models for short texts, named GPU-DMM and GPU-PDMM. Through extensive experiments on two real-world short text collections in two languages, we demonstrate that PDMM achieves better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to better accuracy in a text classification task, as an indirect evaluation. Both GPU-DMM and GPU-PDMM further improve topic coherence and text classification accuracy. GPUPDMM outperforms GPU-DMM at the price of higher computational costs."
75,"The dynamics of knowledge transfer is an important topic for engineering managers. In this paper, we study knowledge boundaries - barriers to knowledge transfer - in groups of experts, using topic modeling, a natural language processing technique, applied to transcript data from the U.S. Food and Drug Administration's Circulatory Systems Advisory Panel. As predicted by prior theory, we find that knowledge boundaries emerge as the group faces increasingly challenging problems. Beyond this theory, we find that knowledge boundaries cease to structure communications between communities of practice when the group's expert ability is insufficient to solve its task, such as in the presence of high novelty. We conjecture that the amount of expert knowledge that the group can collectively bring to bear is a determining factor in boundary formation. This implies that some of the factors underlying knowledge boundary formation may aid - rather than hinder - knowledge aggregation. We briefly explore this conjecture using qualitative exploration of several relevant meetings. Finally, we discuss the implications of these results for organizations attempting to leverage their expertise given the state of their collective knowledge.",2017-08-01,2-s2.0-85018501372,IEEE Transactions on Engineering Management,The Emergence and Collapse of Knowledge Boundaries,"The dynamics of knowledge transfer is an important topic for engineering managers. In this paper, we study knowledge boundaries - barriers to knowledge transfer - in groups of experts, using topic modeling, a natural language processing technique, applied to transcript data from the U.S. Food and Drug Administration's Circulatory Systems Advisory Panel. As predicted by prior theory, we find that knowledge boundaries emerge as the group faces increasingly challenging problems. Beyond this theory, we find that knowledge boundaries cease to structure communications between communities of practice when the group's expert ability is insufficient to solve its task, such as in the presence of high novelty. We conjecture that the amount of expert knowledge that the group can collectively bring to bear is a determining factor in boundary formation. This implies that some of the factors underlying knowledge boundary formation may aid - rather than hinder - knowledge aggregation. We briefly explore this conjecture using qualitative exploration of several relevant meetings. Finally, we discuss the implications of these results for organizations attempting to leverage their expertise given the state of their collective knowledge."
76,"With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals.",2017-08-01,2-s2.0-85028542544,ACM Transactions on Information Systems,Mining exploratory behavior to improve mobile app recommendations,"With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals."
77,"Science and technology (S&T) linkages have been studied extensively using patent and scientific publication databases. Existing methods used to track S&T linkages, such as analysis of non-patent literature (NPL) or author-inventor matching offer a narrow window for industry level analysis of the data. This paper examines the application of a machine learning algorithm, namely Latent Dirichlet Allocation, to detect the semantic relationship between patent and scientific publication corpus. The case of 'Taxol', a cancer drug, is used to illustrate the performance of the unsupervised algorithm in clustering documents with similar topics. In total 26 475 documents retrieved from the Europe PMC database was used a sample for the analysis. Qualitative analysis of the clusters shows that the topic clustering algorithm is valuable approach in detection of patent and publication linkage.",2017-07-31,2-s2.0-85028575629,"2017 IEEE Technology and Engineering Management Society Conference, TEMSCON 2017",A topic model analysis of science and technology linkages: A case study in pharmaceutical industry,"Science and technology (S&T) linkages have been studied extensively using patent and scientific publication databases. Existing methods used to track S&T linkages, such as analysis of non-patent literature (NPL) or author-inventor matching offer a narrow window for industry level analysis of the data. This paper examines the application of a machine learning algorithm, namely Latent Dirichlet Allocation, to detect the semantic relationship between patent and scientific publication corpus. The case of 'Taxol', a cancer drug, is used to illustrate the performance of the unsupervised algorithm in clustering documents with similar topics. In total 26 475 documents retrieved from the Europe PMC database was used a sample for the analysis. Qualitative analysis of the clusters shows that the topic clustering algorithm is valuable approach in detection of patent and publication linkage."
78,"Technology assessment and planning requires that we can reliably, but indirectly, measure knowledge embedded in the organization. Operationalizing knowledge embedded into companies is increasingly challenging but also more and more relevant in the current cross-disciplinary and complex technological environment. Existing approaches for operationalizing company knowledge are based on patent data and analyzing patent classifications. These approaches have, however, significant limitations. In this study, knowledge depth and breadth is studied using full-text patent data from seven large telecommunication companies totaling 157,718 patents. The data was analyzed with Latent Dirichlet Allocation, an unsupervised learning method. The results are quantified using a technological diversity metric, showing temporal changes in companies knowledge. The result show how the operationalization of company knowledge is independent of patent count and that companies have their specific trajectory of knowledge development. The approach offers a novel method of analyzing the knowledge trajectory of a company, compared to existing patent classification based methods.",2017-07-31,2-s2.0-85028575610,"2017 IEEE Technology and Engineering Management Society Conference, TEMSCON 2017",Topic modelling approach to knowledge depth and breadth: Analyzing trajectories of technological knowledge,"Technology assessment and planning requires that we can reliably, but indirectly, measure knowledge embedded in the organization. Operationalizing knowledge embedded into companies is increasingly challenging but also more and more relevant in the current cross-disciplinary and complex technological environment. Existing approaches for operationalizing company knowledge are based on patent data and analyzing patent classifications. These approaches have, however, significant limitations. In this study, knowledge depth and breadth is studied using full-text patent data from seven large telecommunication companies totaling 157,718 patents. The data was analyzed with Latent Dirichlet Allocation, an unsupervised learning method. The results are quantified using a technological diversity metric, showing temporal changes in companies knowledge. The result show how the operationalization of company knowledge is independent of patent count and that companies have their specific trajectory of knowledge development. The approach offers a novel method of analyzing the knowledge trajectory of a company, compared to existing patent classification based methods."
79,"This paper examines the social media strategies of candidates seeking their party’s nomination for the 2016 U.S. presidential election. We use textual analysis to understand what candidates focused on. We assess eight themes covered in Twitter posts. For example, Clinton focused on GUN CONTROL, while Sanders focused on climate change. Using Facebook data, we introduce a topic modeling approach, latent Dirichlet allocation, to the political marketing literature. This allows us to uncover what topics the candidates focus on without researcher intervention and, using a dynamic model, show how this changes over time. We note that Clinton’s focus on Trump increases toward the end of the primary campaign.",2017-07-29,2-s2.0-85026518584,Journal of Political Marketing,Understanding the social media strategies of U.S. primary candidates,"This paper examines the social media strategies of candidates seeking their party’s nomination for the 2016 U.S. presidential election. We use textual analysis to understand what candidates focused on. We assess eight themes covered in Twitter posts. For example, Clinton focused on GUN CONTROL, while Sanders focused on climate change. Using Facebook data, we introduce a topic modeling approach, latent Dirichlet allocation, to the political marketing literature. This allows us to uncover what topics the candidates focus on without researcher intervention and, using a dynamic model, show how this changes over time. We note that Clinton’s focus on Trump increases toward the end of the primary campaign."
80,"As more and more companies become aware of the benefits of collecting and analyzing data, hiring employee with data analytics expertise is a key issue faced by HR practitioners. Although previous research empirically highlighted the differences of knowledge and skill requirements between big data (BD) and business intelligence (BI) in English-speaking countries, limited similar study is conducted in China. By analyzing and interpreting the topic modeling results with dataset extracted from online job recruitment website Zhaopin.com, this exploratory study reveals that (1) the demand for BD competencies is far bigger (nearly six times) than the demand for BI competencies in China job market; (2) for Chinese employer, hard skills, especially those advanced analytic and programming skills, are much emphasized when HR searching for qualified BD applicants; (3) given the large volume and unstructured nature of big data, BD investments are currently much more human-capital-intensive than BI projects are. Our findings not only enhance the understanding of similarities and differences in BI and BD areas of competency but also provide guidance for organizations to choose suitable HR decisions.",2017-07-28,2-s2.0-85028619831,"14th International Conference on Services Systems and Services Management, ICSSSM 2017 - Proceedings",Are big data talents different from business intelligence expertise?: Evidence from text mining using job recruitment advertisements,"As more and more companies become aware of the benefits of collecting and analyzing data, hiring employee with data analytics expertise is a key issue faced by HR practitioners. Although previous research empirically highlighted the differences of knowledge and skill requirements between big data (BD) and business intelligence (BI) in English-speaking countries, limited similar study is conducted in China. By analyzing and interpreting the topic modeling results with dataset extracted from online job recruitment website Zhaopin.com, this exploratory study reveals that (1) the demand for BD competencies is far bigger (nearly six times) than the demand for BI competencies in China job market; (2) for Chinese employer, hard skills, especially those advanced analytic and programming skills, are much emphasized when HR searching for qualified BD applicants; (3) given the large volume and unstructured nature of big data, BD investments are currently much more human-capital-intensive than BI projects are. Our findings not only enhance the understanding of similarities and differences in BI and BD areas of competency but also provide guidance for organizations to choose suitable HR decisions."
81,"Social tags are user-defined keywords associated with online content that reflect consumers' perceptions of various objects, including products and brands. This research presents a new approach for harvesting rich, qualitative information on brands from user-generated social tags. The authors first compare their proposed approach with conventional techniques such as brand concept maps and text mining. They highlight the added value of their approach that results from the unconstrained, open-ended, and synoptic nature of consumer-generated content contained within social tags. The authors then apply existing text-mining and data-reduction methods to analyze disaggregate-level social tagging data for marketing research and demonstrate how marketers can utilize the information in social tags by extracting key representative topics, monitoring common dynamic trends, and understanding heterogeneous perceptions of a brand.",2017-07-01,2-s2.0-85023206466,Journal of Marketing,Harvesting brand information from social Tags,"Social tags are user-defined keywords associated with online content that reflect consumers' perceptions of various objects, including products and brands. This research presents a new approach for harvesting rich, qualitative information on brands from user-generated social tags. The authors first compare their proposed approach with conventional techniques such as brand concept maps and text mining. They highlight the added value of their approach that results from the unconstrained, open-ended, and synoptic nature of consumer-generated content contained within social tags. The authors then apply existing text-mining and data-reduction methods to analyze disaggregate-level social tagging data for marketing research and demonstrate how marketers can utilize the information in social tags by extracting key representative topics, monitoring common dynamic trends, and understanding heterogeneous perceptions of a brand."
82,"With the overflowing of Short Message Service (SMS) spam nowadays, many traditional text classification algorithms are used for SMS spam filtering. Nevertheless, because the content of SMS spam messages are miscellaneous and distinct from general text files, such as more shorter, usually including mass of abbreviations, symbols, variant words and distort or deform sentences, the traditional classifiers aren't fit for the task of SMS spam filtering. In this paper, the authors propose a Short Message Biterm Topic Model (SM-BTM) which can be used to automatically learn latent semantic features from SMS spam corpus for the task of SMS spam filtering. The SM-BTM is based on the probability of topic model theory and Biterm Topic Model (BTM). The experiments in this work show the proposed model SM-BTM can acquire higher quality of topic features than the original BTM, and is more suitable for identifying the miscellaneous SMS spam.",2017-07-01,2-s2.0-85019101927,International Journal of Business Data Communications and Networking,Bi-term topic model for SMS classification,"With the overflowing of Short Message Service (SMS) spam nowadays, many traditional text classification algorithms are used for SMS spam filtering. Nevertheless, because the content of SMS spam messages are miscellaneous and distinct from general text files, such as more shorter, usually including mass of abbreviations, symbols, variant words and distort or deform sentences, the traditional classifiers aren't fit for the task of SMS spam filtering. In this paper, the authors propose a Short Message Biterm Topic Model (SM-BTM) which can be used to automatically learn latent semantic features from SMS spam corpus for the task of SMS spam filtering. The SM-BTM is based on the probability of topic model theory and Biterm Topic Model (BTM). The experiments in this work show the proposed model SM-BTM can acquire higher quality of topic features than the original BTM, and is more suitable for identifying the miscellaneous SMS spam."
83,"Tracking how discussion topics evolve in social media and where these topics are discussed geographically over time has the potential to provide useful information for many different purposes. In crisis management, knowing a specific topic's current geographical location could provide vital information to where, or even which, resources should be allocated. This paper describes an attempt to track online discussions geographically over time. A distributed geo-aware streaming latent Dirichlet allocation model was developed for the purpose of recognizing topics' locations in unstructured text. To evaluate the model it has been implemented and used for automatic discovery and geographical tracking of election topics during parts of the 2016 American presidential primary elections. It was shown that the locations correlated with the actual election locations, and that the model provides a better geolocation classification compared to using a keyword-based approach.",2017-07-01,2-s2.0-85020801622,Decision Support Systems,Tracking geographical locations using a geo-aware topic model for analyzing social media data,"Tracking how discussion topics evolve in social media and where these topics are discussed geographically over time has the potential to provide useful information for many different purposes. In crisis management, knowing a specific topic's current geographical location could provide vital information to where, or even which, resources should be allocated. This paper describes an attempt to track online discussions geographically over time. A distributed geo-aware streaming latent Dirichlet allocation model was developed for the purpose of recognizing topics' locations in unstructured text. To evaluate the model it has been implemented and used for automatic discovery and geographical tracking of election topics during parts of the 2016 American presidential primary elections. It was shown that the locations correlated with the actual election locations, and that the model provides a better geolocation classification compared to using a keyword-based approach."
84,"In order to generate user's information needs from a collection of documents, many term-based and pattern-based approaches have been used in Information Filtering. In these approaches, the documents in the collection are all about one topic. However, user's interests can be diverse and the documents in the collection often involve multiple topics. Topic modeling is useful for the area of machine learning and text mining. It generates models to discover the hidden multiple topics in a collection of documents and each of these topics are presented by distribution of words. But its effectiveness in information filtering has not been so well explored. Patterns are always thought to be more discriminative than single terms for describing documents. The major challenge found in frequent pattern mining is a large number of result patterns. As the minimum threshold becomes lower, an exponentially large number of patterns are generated. To deal with the above mentioned limitations and problems, in this paper, a novel information filtering model, EFITM (Enhanced Frequent Itemsets based on Topic Model) model is proposed. Experimental results using the CRANFIELD dataset for the task of information filtering show that the proposed model outperforms over state-of-the-art models.",2017-06-27,2-s2.0-85030652848,"Proceedings - 16th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2017",Enhanced frequent itemsets based on topic modeling in information filtering,"In order to generate user's information needs from a collection of documents, many term-based and pattern-based approaches have been used in Information Filtering. In these approaches, the documents in the collection are all about one topic. However, user's interests can be diverse and the documents in the collection often involve multiple topics. Topic modeling is useful for the area of machine learning and text mining. It generates models to discover the hidden multiple topics in a collection of documents and each of these topics are presented by distribution of words. But its effectiveness in information filtering has not been so well explored. Patterns are always thought to be more discriminative than single terms for describing documents. The major challenge found in frequent pattern mining is a large number of result patterns. As the minimum threshold becomes lower, an exponentially large number of patterns are generated. To deal with the above mentioned limitations and problems, in this paper, a novel information filtering model, EFITM (Enhanced Frequent Itemsets based on Topic Model) model is proposed. Experimental results using the CRANFIELD dataset for the task of information filtering show that the proposed model outperforms over state-of-the-art models."
85,"PM2.5 is one of the major indicators of ambient air quality which has become a focus of public attention. Urban PM2.5 can be measured by air quality monitoring stations which are costly and not sufficiently installed in a city. In this paper, we aim to infer the PM2.5 information at the place where there is no air quality monitoring station. As PM2.5 concentration varies over time and space domains, we propose a joint topic model to jointly model the spatial and temporal patterns of PM2.5. Numerical results suggest that the proposed model achieves better inference based on five related datasets compared with traditional methods.",2017-06-27,2-s2.0-85030647222,"Proceedings - 16th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2017",A spatial-temporal model to improve PM2.5 inference,"PM2.5 is one of the major indicators of ambient air quality which has become a focus of public attention. Urban PM2.5 can be measured by air quality monitoring stations which are costly and not sufficiently installed in a city. In this paper, we aim to infer the PM2.5 information at the place where there is no air quality monitoring station. As PM2.5 concentration varies over time and space domains, we propose a joint topic model to jointly model the spatial and temporal patterns of PM2.5. Numerical results suggest that the proposed model achieves better inference based on five related datasets compared with traditional methods."
86,"Named entity disambiguation (NED) refers to the task of mapping entity mentions in running texts to the correct entries in a specific knowledge base (e.g., Wikipedia). Although there has been a lot of work on NED for long and formal texts like Wikipedia and news, the task is not well studied for questions in community question answering (CQA). The challenges of the task include little context for mentions in questions, lack of ground truth for learning, and language gaps between CQA and knowledge bases. To overcome these problems, we propose a topic modelling approach to NED for questions. Our model performs learning in an unsupervised manner, but can take advantage of weak supervision signals estimated from the metadata of CQA and knowledge bases. The signals can enrich the context of mentions in questions, and bridge the language gaps between CQA and knowledge bases. Besides these advantages, our model simulates people's behavior in CQA and thus is intuitively interpretable. We conduct experiments on both Chinese and English CQA data. The experimental results show that our method can significantly outperform state-of-the-art methods when we apply them to questions in CQA.",2017-06-15,2-s2.0-85018918452,Knowledge-Based Systems,Named entity disambiguation for questions in community question answering,"Named entity disambiguation (NED) refers to the task of mapping entity mentions in running texts to the correct entries in a specific knowledge base (e.g., Wikipedia). Although there has been a lot of work on NED for long and formal texts like Wikipedia and news, the task is not well studied for questions in community question answering (CQA). The challenges of the task include little context for mentions in questions, lack of ground truth for learning, and language gaps between CQA and knowledge bases. To overcome these problems, we propose a topic modelling approach to NED for questions. Our model performs learning in an unsupervised manner, but can take advantage of weak supervision signals estimated from the metadata of CQA and knowledge bases. The signals can enrich the context of mentions in questions, and bridge the language gaps between CQA and knowledge bases. Besides these advantages, our model simulates people's behavior in CQA and thus is intuitively interpretable. We conduct experiments on both Chinese and English CQA data. The experimental results show that our method can significantly outperform state-of-the-art methods when we apply them to questions in CQA."
87,"The popularity of online forums provides a good opportunity to learn user interests which can be used in many business scenarios, such as product or news recommendation. There exist many approaches to infer forum topics and users’ interests. Among them, Author-Topic (AT) like models are most popular. But a thread in online forum is composed of a root post and some response posts which may be relevant or irrelevant to the root post. So the assumption of AT that response posts are generated from user's interest topics is not comprehensive. In this paper, we distinguish user's serious and unserious interest topics and argue that the topic of a relevant response post is jointly determined by its author's serious interest topics and the topics of its root post, while the topic of irrelevant response post is only determined by its author's unserious interest topics. Based on these assumptions, we propose Forum-LDA to model the generative process of root post, relevant and irrelevant response posts jointly. Therefore, our model can not only learn more coherent topics and serious interests, but also identify unserious users who publish many irrelevant posts. Extensive experiments on real forum dataset demonstrate the advantages of our model in tasks such as user interest and unserious user discovery.",2017-06-15,2-s2.0-85017548366,Knowledge-Based Systems,Forum latent Dirichlet allocation for user interest discovery,"The popularity of online forums provides a good opportunity to learn user interests which can be used in many business scenarios, such as product or news recommendation. There exist many approaches to infer forum topics and users’ interests. Among them, Author-Topic (AT) like models are most popular. But a thread in online forum is composed of a root post and some response posts which may be relevant or irrelevant to the root post. So the assumption of AT that response posts are generated from user's interest topics is not comprehensive. In this paper, we distinguish user's serious and unserious interest topics and argue that the topic of a relevant response post is jointly determined by its author's serious interest topics and the topics of its root post, while the topic of irrelevant response post is only determined by its author's unserious interest topics. Based on these assumptions, we propose Forum-LDA to model the generative process of root post, relevant and irrelevant response posts jointly. Therefore, our model can not only learn more coherent topics and serious interests, but also identify unserious users who publish many irrelevant posts. Extensive experiments on real forum dataset demonstrate the advantages of our model in tasks such as user interest and unserious user discovery."
88,"The study of technological forecasting is an important part of patent analysis. Although fitting models can provide a rough tendency of a technical area, the trend of the detailed content within the area remains hidden. It is also difficult to reveal the trend of specific topics using keyword-based text mining techniques, since it is very hard to track the temporal patterns of a single keyword that generally represents a technological concept. To overcome these limitations, this research proposes a topic-based technological forecasting approach, to uncover the trends of specific topics underlying massive patent claims using topic modelling. A topic annual weight matrix and a sequence of topic-based trend coefficients are generated to quantitatively estimate the developing trends of the discovered topics, and evaluate to what degree various topics have contributed to the patenting activities of the whole area. To demonstrate the effectiveness of the approach, we present a case study using 13,910 utility patents that were published during the years 2000 to 2014, owned by Australian assignees, in the United States Patent and Trademark Office (USPTO). The results indicate that the proposed approach is effective for estimating the temporal patterns and forecast the future trends of the latent topics underlying massive claims. The topic-based knowledge and the corresponding trend analysis provided by the approach can be used to facilitate further technological decisions or opportunity discovery.",2017-06-01,2-s2.0-85015677572,Technological Forecasting and Social Change,Topic-based technological forecasting based on patent data: A case study of Australian patents from 2000 to 2014,"The study of technological forecasting is an important part of patent analysis. Although fitting models can provide a rough tendency of a technical area, the trend of the detailed content within the area remains hidden. It is also difficult to reveal the trend of specific topics using keyword-based text mining techniques, since it is very hard to track the temporal patterns of a single keyword that generally represents a technological concept. To overcome these limitations, this research proposes a topic-based technological forecasting approach, to uncover the trends of specific topics underlying massive patent claims using topic modelling. A topic annual weight matrix and a sequence of topic-based trend coefficients are generated to quantitatively estimate the developing trends of the discovered topics, and evaluate to what degree various topics have contributed to the patenting activities of the whole area. To demonstrate the effectiveness of the approach, we present a case study using 13,910 utility patents that were published during the years 2000 to 2014, owned by Australian assignees, in the United States Patent and Trademark Office (USPTO). The results indicate that the proposed approach is effective for estimating the temporal patterns and forecast the future trends of the latent topics underlying massive claims. The topic-based knowledge and the corresponding trend analysis provided by the approach can be used to facilitate further technological decisions or opportunity discovery."
89,"This study has proposed a topic based competitive keywords suggestion method called TCK to enhance search engine advertising. On the basis of query logs, the method explores the indirect associations between keywords and extracts the hidden topic information to identify competitive keywords. It can help advertisers not only broaden the choices of keywords but also carry out a competitive strategy for search engine advertising. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed method. Results prove that the proposed method performs better than existing keyword suggestion methods, contributing greatly to the keyword suggestion advertising market.",2017-06-01,2-s2.0-85008235246,Information and Management,Finding competitive keywords from query logs to enhance search engine advertising,"This study has proposed a topic based competitive keywords suggestion method called TCK to enhance search engine advertising. On the basis of query logs, the method explores the indirect associations between keywords and extracts the hidden topic information to identify competitive keywords. It can help advertisers not only broaden the choices of keywords but also carry out a competitive strategy for search engine advertising. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed method. Results prove that the proposed method performs better than existing keyword suggestion methods, contributing greatly to the keyword suggestion advertising market."
90,"Topic model is a hot research topic which is attracting attentions from many fields. Recently, several studies have applied topic model to ASR (audio scene recognition). Among these studies, most of them use the document-word co-occurrence matrix for topic analysis. In this work, we propose a new ASR algorithm based on audio events and topic model, which uses the document-event co-occurrence matrix for topic analysis. Our work is based on the hypothesis that: for an audio document, compared with its word distribution, its event distribution is more in line with humans’ way of thinking, and then the topic distribution obtained based on the document-event co-occurrence matrix can represent the audio document better. The contribution of this work lies in that: (1) we propose an ASR algorithm which uses document-event co-occurrence matrix for topic analysis. Compared with the current studies which use document-word co-occurrence matrix for topic analysis, the proposed algorithm can extract the topic distribution which can express the audio documents better, and then can get better recognition results; (2) we propose a much easier method to obtain the document-event co-occurrence matrix; (3) we propose a method to weight the event distribution of audio documents; this weighting method can emphasize the audio events that are important in reflecting the unique topics of the audio documents, and can suppress the audio events that are common to many topics. Experimental results on two public datasets verify the effectiveness of the proposed ASR algorithm, and also verify the necessity and effectiveness of the proposed weighting method. The innovative ideas in this work are not limited to ASR, but can be extended to many other fields, such as the video classification etc.",2017-06-01,2-s2.0-85017444988,Knowledge-Based Systems,Audio scene recognition based on audio events and topic model,"Topic model is a hot research topic which is attracting attentions from many fields. Recently, several studies have applied topic model to ASR (audio scene recognition). Among these studies, most of them use the document-word co-occurrence matrix for topic analysis. In this work, we propose a new ASR algorithm based on audio events and topic model, which uses the document-event co-occurrence matrix for topic analysis. Our work is based on the hypothesis that: for an audio document, compared with its word distribution, its event distribution is more in line with humans’ way of thinking, and then the topic distribution obtained based on the document-event co-occurrence matrix can represent the audio document better. The contribution of this work lies in that: (1) we propose an ASR algorithm which uses document-event co-occurrence matrix for topic analysis. Compared with the current studies which use document-word co-occurrence matrix for topic analysis, the proposed algorithm can extract the topic distribution which can express the audio documents better, and then can get better recognition results; (2) we propose a much easier method to obtain the document-event co-occurrence matrix; (3) we propose a method to weight the event distribution of audio documents; this weighting method can emphasize the audio events that are important in reflecting the unique topics of the audio documents, and can suppress the audio events that are common to many topics. Experimental results on two public datasets verify the effectiveness of the proposed ASR algorithm, and also verify the necessity and effectiveness of the proposed weighting method. The innovative ideas in this work are not limited to ASR, but can be extended to many other fields, such as the video classification etc."
91,"Rapid advancements in internet and social media technologies have made “information overload” a rampant and widespread problem. Complex subjects, histories, or issues break down into branches, side stories, and intertwining narratives; a “topic evolution map” can assist in joining together and clarifying these disparate parts of an unfamiliar territory. This paper reviews the extant research on topic evolution map based on text and cross-media corpora over the past decade. We first define a series of necessary terms, then go on to describe the traditional topic evolution map per 1) topic evolution over time, based on the probabilistic generative model, and 2) topic evolution from a non-probabilistic perspective. Next, we discuss the current state of research on topic evolution map based on the cross-media corpus, including some open questions and possible future research directions. The main contribution of this review is in its construction of an evolution map that can be used to visualize and integrate the extant studies on topic modeling – specifically in regards to cross-media research.",2017-05-15,2-s2.0-85015644584,Knowledge-Based Systems,A survey on trends of cross-media topic evolution map,"Rapid advancements in internet and social media technologies have made “information overload” a rampant and widespread problem. Complex subjects, histories, or issues break down into branches, side stories, and intertwining narratives; a “topic evolution map” can assist in joining together and clarifying these disparate parts of an unfamiliar territory. This paper reviews the extant research on topic evolution map based on text and cross-media corpora over the past decade. We first define a series of necessary terms, then go on to describe the traditional topic evolution map per 1) topic evolution over time, based on the probabilistic generative model, and 2) topic evolution from a non-probabilistic perspective. Next, we discuss the current state of research on topic evolution map based on the cross-media corpus, including some open questions and possible future research directions. The main contribution of this review is in its construction of an evolution map that can be used to visualize and integrate the extant studies on topic modeling – specifically in regards to cross-media research."
92,"With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers' news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods.",2017-04-01,2-s2.0-85026465032,ACM Transactions on Information Systems,Learning to align comments to news topics,"With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers' news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods."
93,"We consider the problem of search result diversification for streams of short texts. Diversifying search results in short text streams is more challenging than in the case of long documents, as it is difficult to capture the latent topics of short documents. To capture the changes of topics and the probabilities of documents for a given query at a specific time in a short text stream, we propose a dynamic Dirichlet multinomial mixture topic model, called D2M3, as well as a Gibbs sampling algorithm for the inference. We also propose a streaming diversification algorithm, SDA, that integrates the information captured by D2M3 with our proposed modified version of the PM-2 (Proportionality-based diversification Method - second version) diversification algorithm. We conduct experiments on a Twitter dataset and find that SDA statistically significantly outperforms state-of-the-art non-streaming retrieval methods, plain streaming retrieval methods, as well as streaming diversification methods that use other dynamic topic models.",2017-04-01,2-s2.0-85026456548,ACM Transactions on Information Systems,Search result diversification in short text streams,"We consider the problem of search result diversification for streams of short texts. Diversifying search results in short text streams is more challenging than in the case of long documents, as it is difficult to capture the latent topics of short documents. To capture the changes of topics and the probabilities of documents for a given query at a specific time in a short text stream, we propose a dynamic Dirichlet multinomial mixture topic model, called D2M3, as well as a Gibbs sampling algorithm for the inference. We also propose a streaming diversification algorithm, SDA, that integrates the information captured by D2M3 with our proposed modified version of the PM-2 (Proportionality-based diversification Method - second version) diversification algorithm. We conduct experiments on a Twitter dataset and find that SDA statistically significantly outperforms state-of-the-art non-streaming retrieval methods, plain streaming retrieval methods, as well as streaming diversification methods that use other dynamic topic models."
94,"Topic models, such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA), have shown impressive success in many fields. Recently, multi-view learning via probabilistic latent semantic analysis (MVPLSA), is also designed for multi-view topic modeling. These approaches are instances of generative model, whereas they all ignore the manifold structure of data distribution, which is generally useful for preserving the nonlinear information. In this paper, we propose a novel multiple graph regularized generative model to exploit the manifold structure in multiple views. Specifically, we construct a nearest neighbor graph for each view to encode its corresponding manifold information. A multiple graph ensemble regularization framework is proposed to learn the optimal intrinsic manifold. Then, the manifold regularization term is incorporated into a multi-view topic model, resulting in a unified objective function. The solutions are derived based on the Expectation Maximization optimization framework. Experimental results on real-world multi-view data sets demonstrate the effectiveness of our approach.",2017-04-01,2-s2.0-85011601755,Knowledge-Based Systems,Multi-view learning via multiple graph regularized generative model,"Topic models, such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA), have shown impressive success in many fields. Recently, multi-view learning via probabilistic latent semantic analysis (MVPLSA), is also designed for multi-view topic modeling. These approaches are instances of generative model, whereas they all ignore the manifold structure of data distribution, which is generally useful for preserving the nonlinear information. In this paper, we propose a novel multiple graph regularized generative model to exploit the manifold structure in multiple views. Specifically, we construct a nearest neighbor graph for each view to encode its corresponding manifold information. A multiple graph ensemble regularization framework is proposed to learn the optimal intrinsic manifold. Then, the manifold regularization term is incorporated into a multi-view topic model, resulting in a unified objective function. The solutions are derived based on the Expectation Maximization optimization framework. Experimental results on real-world multi-view data sets demonstrate the effectiveness of our approach."
95,"Autonomous vehicles (AVs) have emerged as a transformative technology with the potential to both fundamentally improve lives in cities but also to exacerbate suburban sprawl, vehicle miles traveled and the associated greenhouse gas emissions. Are communities willing to adopt best practices that can lead to early adoption of more sustainable outcomes? This paper presents innovative means to analyze social preferences, demand for AVs, and the potential to resolve community concerns with integrated solutions. We discuss our comprehensive analysis of unstructured and structured data from a survey on AVs that was conducted by the Atlanta Regional Commission in 2015. We used topic modeling to synthesize the “topics” from 1540 comments. The topics captured Atlanta residents' concerns and suggestions about implementing AVs. Further, sentiment analysis revealed people's attitudes on the topics. Accordingly, we proposed an integration of AVs and transit-oriented development (TOD: the development of compact and mixed-use communities around high quality mass transit services within a 10-min walking distance). The second type of data is people's responses to multiple-choice questions about AVs and TOD, which we call structured data. Using latent-class analysis, we identified heterogeneity in preferences for AVs and TOD. More Atlanta residents are willing to live in transit-oriented communities than traditional automobile-dependent ones if AVs save time and improve productivity. This finding portends the future success of combining AVs with TOD and reaping the sustainable benefits of this transformative technology.",2017-03-01,2-s2.0-85010310683,Cities,Data-enabled public preferences inform integration of autonomous vehicles with transit-oriented development in Atlanta,"Autonomous vehicles (AVs) have emerged as a transformative technology with the potential to both fundamentally improve lives in cities but also to exacerbate suburban sprawl, vehicle miles traveled and the associated greenhouse gas emissions. Are communities willing to adopt best practices that can lead to early adoption of more sustainable outcomes? This paper presents innovative means to analyze social preferences, demand for AVs, and the potential to resolve community concerns with integrated solutions. We discuss our comprehensive analysis of unstructured and structured data from a survey on AVs that was conducted by the Atlanta Regional Commission in 2015. We used topic modeling to synthesize the “topics” from 1540 comments. The topics captured Atlanta residents' concerns and suggestions about implementing AVs. Further, sentiment analysis revealed people's attitudes on the topics. Accordingly, we proposed an integration of AVs and transit-oriented development (TOD: the development of compact and mixed-use communities around high quality mass transit services within a 10-min walking distance). The second type of data is people's responses to multiple-choice questions about AVs and TOD, which we call structured data. Using latent-class analysis, we identified heterogeneity in preferences for AVs and TOD. More Atlanta residents are willing to live in transit-oriented communities than traditional automobile-dependent ones if AVs save time and improve productivity. This finding portends the future success of combining AVs with TOD and reaping the sustainable benefits of this transformative technology."
96,"Patent data has been an obvious choice for analysis leading to strategic technology intelligence, yet, the recent proliferation of machine learning text analysis methods is changing the status of traditional patent data analysis methods and approaches. This article discusses the benefits and constraints of machine learning approaches in industry level patent analysis, and to this end offers a demonstration of unsupervised learning based analysis of the leading telecommunication firms between 2001 and 2014 based on about 160,000 USPTO full-text patents. Data were classified using full-text descriptions with Latent Dirichlet Allocation, and latent patterns emerging through the unsupervised learning process were modelled by company and year to create an overall view of patenting within the industry, and to forecast future trends. Our results demonstrate company-specific differences in their knowledge profiles, as well as show the evolution of the knowledge profiles of industry leaders from hardware to software focussed technology strategies. The results cast also light on the dynamics of emerging and declining knowledge areas in the telecommunication industry. Our results prompt a consideration of the current status of established approaches to patent landscaping, such as key-word or technology classifications and other approaches relying on semantic labelling, in the context of novel machine learning approaches. Finally, we discuss implications for policy makers, and, in particular, for strategic management in firms.",2017-02-01,2-s2.0-84992118302,Technological Forecasting and Social Change,Firms' knowledge profiles: Mapping patent data with unsupervised learning,"Patent data has been an obvious choice for analysis leading to strategic technology intelligence, yet, the recent proliferation of machine learning text analysis methods is changing the status of traditional patent data analysis methods and approaches. This article discusses the benefits and constraints of machine learning approaches in industry level patent analysis, and to this end offers a demonstration of unsupervised learning based analysis of the leading telecommunication firms between 2001 and 2014 based on about 160,000 USPTO full-text patents. Data were classified using full-text descriptions with Latent Dirichlet Allocation, and latent patterns emerging through the unsupervised learning process were modelled by company and year to create an overall view of patenting within the industry, and to forecast future trends. Our results demonstrate company-specific differences in their knowledge profiles, as well as show the evolution of the knowledge profiles of industry leaders from hardware to software focussed technology strategies. The results cast also light on the dynamics of emerging and declining knowledge areas in the telecommunication industry. Our results prompt a consideration of the current status of established approaches to patent landscaping, such as key-word or technology classifications and other approaches relying on semantic labelling, in the context of novel machine learning approaches. Finally, we discuss implications for policy makers, and, in particular, for strategic management in firms."
97,"We present the key features of topic modeling based on Latent Dirichlet Allocation (LDA), and demonstrate its application by analyzing Organization Research Methods articles since its inception. Our analysis, based on 421 ORM articles reveals 15 topics, which are quite similar to other, more human intensive review exercises.",2017-01-01,2-s2.0-85041782746,"2017 Annual Meeting of the Academy of Management, AOM 2017",Topic models as a novel approach to identify themes in content analysis: The example of organizational research methods,"We present the key features of topic modeling based on Latent Dirichlet Allocation (LDA), and demonstrate its application by analyzing Organization Research Methods articles since its inception. Our analysis, based on 421 ORM articles reveals 15 topics, which are quite similar to other, more human intensive review exercises."
98,"Purpose: The purpose of this paper is to explore and describe research presented in the International Journal of Quality & Reliability Management (IJQRM), thereby creating an increased understanding of how the areas of research have evolved through the years. An additional purpose is to show how text mining methodology can be used as a tool for exploration and description of research publications. Design/methodology/approach: The study applies text mining methodologies to explore and describe the digital library of IJQRM from 1984 up to 2014. To structure and condense the data, k-means clustering and probabilistic topic modeling with latent Dirichlet allocation is applied. The data set consists of research paper abstracts. Findings: The results support the suggestion of the occurrence of trends, fads and fashion in research publications. Research on quality function deployment (QFD) and reliability management are noted to be on the downturn whereas research on Six Sigma with a focus on lean, innovation, performance and improvement on the rise. Furthermore, the study confirms IJQRM as a scientific journal with quality and reliability management as primary areas of coverage, accompanied by specific topics such as total quality management, service quality, process management, ISO, QFD and Six Sigma. The study also gives an insight into how text mining can be used as a way to efficiently explore and describe large quantities of research paper abstracts. Research limitations/implications: The study focuses on abstracts of research papers, thus topics and categories that could be identified via other journal publications, such as book reviews; general reviews; secondary articles; editorials; guest editorials; awards for excellence (notifications); introductions or summaries from conferences; notes from the publisher; and articles without an abstract, are excluded. Originality/value: There do not seem to be any prior text mining studies that apply cluster modeling and probabilistic topic modeling to research article abstracts in the IJQRM. This study therefore offers a unique perspective on the journal’s content.",2017-01-01,2-s2.0-85027337496,International Journal of Quality and Reliability Management,Exploring research on quality and reliability management through text mining methodology,"Purpose: The purpose of this paper is to explore and describe research presented in the International Journal of Quality & Reliability Management (IJQRM), thereby creating an increased understanding of how the areas of research have evolved through the years. An additional purpose is to show how text mining methodology can be used as a tool for exploration and description of research publications. Design/methodology/approach: The study applies text mining methodologies to explore and describe the digital library of IJQRM from 1984 up to 2014. To structure and condense the data, k-means clustering and probabilistic topic modeling with latent Dirichlet allocation is applied. The data set consists of research paper abstracts. Findings: The results support the suggestion of the occurrence of trends, fads and fashion in research publications. Research on quality function deployment (QFD) and reliability management are noted to be on the downturn whereas research on Six Sigma with a focus on lean, innovation, performance and improvement on the rise. Furthermore, the study confirms IJQRM as a scientific journal with quality and reliability management as primary areas of coverage, accompanied by specific topics such as total quality management, service quality, process management, ISO, QFD and Six Sigma. The study also gives an insight into how text mining can be used as a way to efficiently explore and describe large quantities of research paper abstracts. Research limitations/implications: The study focuses on abstracts of research papers, thus topics and categories that could be identified via other journal publications, such as book reviews; general reviews; secondary articles; editorials; guest editorials; awards for excellence (notifications); introductions or summaries from conferences; notes from the publisher; and articles without an abstract, are excluded. Originality/value: There do not seem to be any prior text mining studies that apply cluster modeling and probabilistic topic modeling to research article abstracts in the IJQRM. This study therefore offers a unique perspective on the journal’s content."
99,"Purpose: Competitor analysis is a key component in operations management. Most business decisions are rooted in the analysis of rival products inferred from market structure. Relative to more traditional competitor analysis methods, the purpose of this paper is to provide operations managers with an innovative tool to monitor a firm’s market position and competitors in real time at higher resolution and lower cost than more traditional competitor analysis methods. Design/methodology/approach: The authors combine the techniques of Web Crawler, Natural Language Processing and Machine Learning algorithms with data visualization to develop a big data competitor-analysis system that informs operations managers about competitors and meaningful relationships among them. The authors illustrate the approach using the fitness mobile app business. Findings: The study shows that the system supports operational decision making both descriptively and prescriptively. In particular, the innovative probabilistic topic modeling algorithm combined with conventional multidimensional scaling, product feature comparison and market structure analyses reveal an app’s position in relation to its peers. The authors also develop a user segment overlapping index based on user’s social media data. The authors combine this new index with the product functionality similarity index to map indirect and direct competitors with and without user lock-in. Originality/value: The approach improves on previous approaches by fully automating information extraction from multiple online sources. The authors believe this is the first system of its kind. With limited human intervention, the methodology can easily be adapted to different settings, giving quicker, more reliable real-time results. The approach is also cost effective for market analysis projects covering different data sources.",2017-01-01,2-s2.0-85021349194,Business Process Management Journal,Automated competitor analysis using big data analytics: Evidence from the fitness mobile app business,"Purpose: Competitor analysis is a key component in operations management. Most business decisions are rooted in the analysis of rival products inferred from market structure. Relative to more traditional competitor analysis methods, the purpose of this paper is to provide operations managers with an innovative tool to monitor a firm’s market position and competitors in real time at higher resolution and lower cost than more traditional competitor analysis methods. Design/methodology/approach: The authors combine the techniques of Web Crawler, Natural Language Processing and Machine Learning algorithms with data visualization to develop a big data competitor-analysis system that informs operations managers about competitors and meaningful relationships among them. The authors illustrate the approach using the fitness mobile app business. Findings: The study shows that the system supports operational decision making both descriptively and prescriptively. In particular, the innovative probabilistic topic modeling algorithm combined with conventional multidimensional scaling, product feature comparison and market structure analyses reveal an app’s position in relation to its peers. The authors also develop a user segment overlapping index based on user’s social media data. The authors combine this new index with the product functionality similarity index to map indirect and direct competitors with and without user lock-in. Originality/value: The approach improves on previous approaches by fully automating information extraction from multiple online sources. The authors believe this is the first system of its kind. With limited human intervention, the methodology can easily be adapted to different settings, giving quicker, more reliable real-time results. The approach is also cost effective for market analysis projects covering different data sources."
100,"We examine the emergence of an organizational form, charter schools, in Oakland, California. We link field-level logics to organizational founding identities using topic modeling. We find corporate and community founding actors create distinct and consistent identities, whereas more peripheral founders indulge in more unique identity construction. We see the settlement of the form into a stable ecosystem with multiple identity codes rather than driving toward a single organizational identity. The variety of identities that emerge do not always map onto field-level logics. This has implications for the conditions under which organizational innovation and experimentation within a new form may develop.",2017-01-01,2-s2.0-85016287001,Research in the Sociology of Organizations,A patchwork of identities: Emergence of charter schools as a new organizational form,"We examine the emergence of an organizational form, charter schools, in Oakland, California. We link field-level logics to organizational founding identities using topic modeling. We find corporate and community founding actors create distinct and consistent identities, whereas more peripheral founders indulge in more unique identity construction. We see the settlement of the form into a stable ecosystem with multiple identity codes rather than driving toward a single organizational identity. The variety of identities that emerge do not always map onto field-level logics. This has implications for the conditions under which organizational innovation and experimentation within a new form may develop."
101,"This paper presents HBIN-LBD, a novel literature-based discovery (LBD) method that exploits the lexico-citation structures within the heterogeneous bibliographic information network (HBIN) graphs. Unlike other existing LBD methods, HBIN-LBD harnesses the metapath features found in HBIN graphs for discovering the latent associations between scientific papers published in otherwise disconnected research areas. Further, this paper investigates the effects of incorporating semantic and topic modeling components into the proposed models. Using time-sliced historical bibliographic data, we demonstrate the performance of our method by reconstructing two LBD hypotheses: the Fish Oil and Raynaud's Syndrome hypothesis and the Migraine and Magnesium hypothesis. The proposed method is capable of predicting the future co-citation links between research papers of these previously disconnected research areas with up to 88.86% accuracy and 0.89 F-measure.",2017-01-01,2-s2.0-84995910778,Knowledge-Based Systems,Learning the heterogeneous bibliographic information network for literature-based discovery,"This paper presents HBIN-LBD, a novel literature-based discovery (LBD) method that exploits the lexico-citation structures within the heterogeneous bibliographic information network (HBIN) graphs. Unlike other existing LBD methods, HBIN-LBD harnesses the metapath features found in HBIN graphs for discovering the latent associations between scientific papers published in otherwise disconnected research areas. Further, this paper investigates the effects of incorporating semantic and topic modeling components into the proposed models. Using time-sliced historical bibliographic data, we demonstrate the performance of our method by reconstructing two LBD hypotheses: the Fish Oil and Raynaud's Syndrome hypothesis and the Migraine and Magnesium hypothesis. The proposed method is capable of predicting the future co-citation links between research papers of these previously disconnected research areas with up to 88.86% accuracy and 0.89 F-measure."
102,"Due to the construction of infrastructure of wired and wireless networks and raid development of the speed, digital based data, which is no longer manageable with general technology, is increasing explosively and its form and quantity are tremendous. In accordance with this, the application plan of previously unused data and the area of value creation through this are gradually widened. Especially, the importance of text data analysis, which represents the public's opinion such as social network services (social media) and online product reviews, is magnified. Like this, the development of comprehension and prediction solutions of customer needs that utilize the reviews is estimated to optimize the values of all the future industries and technologies and is expected to become the base of upcoming economic effect creation. In other words, seeing the hidden value through data can suggest pending issues that enhance the competitiveness of the company. Especially, as extensive amount of data is created in games, it is deemed as a promising business that can expect high growth through future data analysis. However, the research on the opinion analysis based on the importance of game reviews and texts is unsatisfactory. Therefore, in this research, we would like to look into utilization of review data within a game and examine text data analysis technique and application plan based on the existing preceding research. In addition, through opinion mining, we have tried to investigate what are some major keywords per topic within a game and compare what are the characteristics of each game that can be inferred through comparison of two analytical techniques. Moreover, we wanted to suggest practical implications for creating economic value such as game sales and system improvement through utilization of review data in the game industry in the future.",2017-01-01,2-s2.0-85032572196,International Journal of Applied Business and Economic Research,A study of analyzing STEAM game review data using text mining,"Due to the construction of infrastructure of wired and wireless networks and raid development of the speed, digital based data, which is no longer manageable with general technology, is increasing explosively and its form and quantity are tremendous. In accordance with this, the application plan of previously unused data and the area of value creation through this are gradually widened. Especially, the importance of text data analysis, which represents the public's opinion such as social network services (social media) and online product reviews, is magnified. Like this, the development of comprehension and prediction solutions of customer needs that utilize the reviews is estimated to optimize the values of all the future industries and technologies and is expected to become the base of upcoming economic effect creation. In other words, seeing the hidden value through data can suggest pending issues that enhance the competitiveness of the company. Especially, as extensive amount of data is created in games, it is deemed as a promising business that can expect high growth through future data analysis. However, the research on the opinion analysis based on the importance of game reviews and texts is unsatisfactory. Therefore, in this research, we would like to look into utilization of review data within a game and examine text data analysis technique and application plan based on the existing preceding research. In addition, through opinion mining, we have tried to investigate what are some major keywords per topic within a game and compare what are the characteristics of each game that can be inferred through comparison of two analytical techniques. Moreover, we wanted to suggest practical implications for creating economic value such as game sales and system improvement through utilization of review data in the game industry in the future."
103,"The growing popularity of social media networks such as Twitter, Facebook, Instagram and so forth is taking the Internet sphere to a higher level, creating a huge volume of social network-generated data, including tweets. As a result, all these data and information can be easily or exclusively found on the Internet by the public. However, little is known yet about how to turn this data into useful knowledge and the collection of the data has not been sufficiently researched, especially in the online retail industry. In an effort to help companies in the online retail industry to utilise the data, this paper explores social network-generated contents on Twitter sites through topics identification that can subsequently facilitate companies to improve their services and performances. This paper describes a study based on data gathered on the Twitter sites of the leading UK retailers: Amazon, Argos and Tesco, during the periods of Black Friday, Boxing Day and Christmas in the UK. We chose Twitter as a platform because, whilst other previous works addressed online reviews and website contents in their studies, microblogs are just starting to be explored due to their characters' limitations and informal content making it harder to interpret. In this paper, we infer topics from short-Text tweets posted by customers mentioning brand names by employing a machine learning approach. We then analyse the tweets to determine the primary issues or topics focused by customers regarding these online retail brands by interpreting the themes of the specific keywords set represented. The topics which emerge based on the collection of tweet posts contribute to various aspects of companies' operations, such as delivery, customer service and product performance. We believe that this study derives some implications and insights for online retail brand companies in improving their service provisions, especially in customer service management. Insights on such topics can be beneficial in numerous ways, such as marketing, innovation and public image.",2017-01-01,2-s2.0-85039840879,"Proceedings of the 11th European Conference on Information Systems Management, ECISM 2017",Mining social network content of online retail brands: A machine learning approach,"The growing popularity of social media networks such as Twitter, Facebook, Instagram and so forth is taking the Internet sphere to a higher level, creating a huge volume of social network-generated data, including tweets. As a result, all these data and information can be easily or exclusively found on the Internet by the public. However, little is known yet about how to turn this data into useful knowledge and the collection of the data has not been sufficiently researched, especially in the online retail industry. In an effort to help companies in the online retail industry to utilise the data, this paper explores social network-generated contents on Twitter sites through topics identification that can subsequently facilitate companies to improve their services and performances. This paper describes a study based on data gathered on the Twitter sites of the leading UK retailers: Amazon, Argos and Tesco, during the periods of Black Friday, Boxing Day and Christmas in the UK. We chose Twitter as a platform because, whilst other previous works addressed online reviews and website contents in their studies, microblogs are just starting to be explored due to their characters' limitations and informal content making it harder to interpret. In this paper, we infer topics from short-Text tweets posted by customers mentioning brand names by employing a machine learning approach. We then analyse the tweets to determine the primary issues or topics focused by customers regarding these online retail brands by interpreting the themes of the specific keywords set represented. The topics which emerge based on the collection of tweet posts contribute to various aspects of companies' operations, such as delivery, customer service and product performance. We believe that this study derives some implications and insights for online retail brand companies in improving their service provisions, especially in customer service management. Insights on such topics can be beneficial in numerous ways, such as marketing, innovation and public image."
104,"During the three decades since its inception in 1984, the JPIM has shaped the evolution of innovation research as a scientific field. It helped create a topic landscape that is not only more diverse and rich in insights, but also more complex and fragmented in structure than ever before. We seek to map this landscape and identify salient development trajectories over time. In contrast to prior citation-based studies covering the first two decades of JPIM research, we benefit from recent advances in natural language processing and rely on a topic modeling algorithm to extract 57 distinct topics and the corresponding most common words, terms, and phrases from the entire full-text corpus of 1008 JPIM articles published between 1984 and 2013. Estimating the development trajectory of each topic based on yearly publication counts in JPIM allows us to identify “hot,” “cold,” “revival,” “evergreen,” and “wall-flower” topics. We map these topics onto the Product Development and Management Association (PDMA) Body of Knowledge categories and discover that these categories differ significantly not only in terms of their internal topic diversity and relative prevalence, but also—and arguably more importantly—in terms of their publication and citation trajectories over time. For instance, the PDMA category “Codevelopment and Alliances” exhibits only moderate topic diversity (7 out of 57 topics) and prevalence in JPIM (161 out of 1008 articles). That said, it is among the most dynamic categories featuring two evergreen topic (“Users and Innovation” and “Tools and Systems for Technology Transfer”) and three hot topics (“Open Innovation,” “Alliances and Cooperation,” and “Networks and Network Structure”) as well as a sharply growing annual number of citations received. Our findings are likely to be of interest to all those who are keen to (re)discover JPIM's topic landscape in search of hidden structures and development trajectories.",2016-11-01,2-s2.0-84949255004,Journal of Product Innovation Management,"Mapping the Topic Landscape of JPIM, 1984–2013: In Search of Hidden Structures and Development Trajectories","During the three decades since its inception in 1984, the JPIM has shaped the evolution of innovation research as a scientific field. It helped create a topic landscape that is not only more diverse and rich in insights, but also more complex and fragmented in structure than ever before. We seek to map this landscape and identify salient development trajectories over time. In contrast to prior citation-based studies covering the first two decades of JPIM research, we benefit from recent advances in natural language processing and rely on a topic modeling algorithm to extract 57 distinct topics and the corresponding most common words, terms, and phrases from the entire full-text corpus of 1008 JPIM articles published between 1984 and 2013. Estimating the development trajectory of each topic based on yearly publication counts in JPIM allows us to identify “hot,” “cold,” “revival,” “evergreen,” and “wall-flower” topics. We map these topics onto the Product Development and Management Association (PDMA) Body of Knowledge categories and discover that these categories differ significantly not only in terms of their internal topic diversity and relative prevalence, but also—and arguably more importantly—in terms of their publication and citation trajectories over time. For instance, the PDMA category “Codevelopment and Alliances” exhibits only moderate topic diversity (7 out of 57 topics) and prevalence in JPIM (161 out of 1008 articles). That said, it is among the most dynamic categories featuring two evergreen topic (“Users and Innovation” and “Tools and Systems for Technology Transfer”) and three hot topics (“Open Innovation,” “Alliances and Cooperation,” and “Networks and Network Structure”) as well as a sharply growing annual number of citations received. Our findings are likely to be of interest to all those who are keen to (re)discover JPIM's topic landscape in search of hidden structures and development trajectories."
105,"Creating links among online encyclopedia articles in different languages is crucial in the construction and integration of large multilingual knowledge bases. Most research to date has focused on linking among different language versions of Wikipedia, yet other large online encyclopedias in a variety of languages exist. In this work, we present a cross-language article-linking method using a bilingual topic model and translation features based on an SVM model to link articles in English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China. To evaluate our approach, we compile data sets from Baidu Baike articles and their corresponding English Wikipedia articles. The evaluation results show that our approach achieves at most 0.8158 in MRR, outperforming the baseline system by 0.1328 (+19.44%) in MRR. Our method does not heavily depend on linguistic characteristics, and it can be easily extended to generate cross-language article links among different online encyclopedias in other languages.",2016-11-01,2-s2.0-84990943170,Knowledge-Based Systems,Cross-language article linking with different knowledge bases using bilingual topic model and translation features,"Creating links among online encyclopedia articles in different languages is crucial in the construction and integration of large multilingual knowledge bases. Most research to date has focused on linking among different language versions of Wikipedia, yet other large online encyclopedias in a variety of languages exist. In this work, we present a cross-language article-linking method using a bilingual topic model and translation features based on an SVM model to link articles in English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China. To evaluate our approach, we compile data sets from Baidu Baike articles and their corresponding English Wikipedia articles. The evaluation results show that our approach achieves at most 0.8158 in MRR, outperforming the baseline system by 0.1328 (+19.44%) in MRR. Our method does not heavily depend on linguistic characteristics, and it can be easily extended to generate cross-language article links among different online encyclopedias in other languages."
106,"In the era of the Social Web, crowdfunding has become an increasingly more important channel for entrepreneurs to raise funds from the crowd to support their startup projects. Previous studies examined various factors such as project goals, project durations, and categories of projects that might influence the outcomes of the fund raising campaigns. However, textual information of projects has rarely been studied for analyzing crowdfunding successes. The main contribution of our research work is the design of a novel text analytics-based framework that can extract latent semantics from the textual descriptions of projects to predict the fund raising outcomes of these projects. More specifically, we develop the Domain-Constraint Latent Dirichlet Allocation (DC-LDA) topic model for effective extraction of topical features from texts. Based on two real-world crowdfunding datasets, our experimental results reveal that the proposed framework outperforms a classical LDA-based method in predicting fund raising success by an average of 11% in terms of F",2016-11-01,2-s2.0-84993965442,Decision Support Systems,The determinants of crowdfunding success: A semantic text analytics approach,"In the era of the Social Web, crowdfunding has become an increasingly more important channel for entrepreneurs to raise funds from the crowd to support their startup projects. Previous studies examined various factors such as project goals, project durations, and categories of projects that might influence the outcomes of the fund raising campaigns. However, textual information of projects has rarely been studied for analyzing crowdfunding successes. The main contribution of our research work is the design of a novel text analytics-based framework that can extract latent semantics from the textual descriptions of projects to predict the fund raising outcomes of these projects. More specifically, we develop the Domain-Constraint Latent Dirichlet Allocation (DC-LDA) topic model for effective extraction of topical features from texts. Based on two real-world crowdfunding datasets, our experimental results reveal that the proposed framework outperforms a classical LDA-based method in predicting fund raising success by an average of 11% in terms of F"
107,"Tracking public opinion in social media provides important information to enterprises or governments during a decision making process. In addition, identifying and extracting the causes of sentiment spikes allows interested parties to redesign and adjust strategies with the aim to attract more positive sentiments. In this paper, we focus on the problem of tracking sentiment towards different entities, detecting sentiment spikes and on the problem of extracting and ranking the causes of a sentiment spike. Our approach combines LDA topic model with Relative Entropy. The former is used for extracting the topics discussed in the time window before the sentiment spike. The latter allows to rank the detected topics based on their contribution to the sentiment spike.",2016-10-24,2-s2.0-84996490661,"International Conference on Information and Knowledge Management, Proceedings",Explaining sentiment spikes in twitter,"Tracking public opinion in social media provides important information to enterprises or governments during a decision making process. In addition, identifying and extracting the causes of sentiment spikes allows interested parties to redesign and adjust strategies with the aim to attract more positive sentiments. In this paper, we focus on the problem of tracking sentiment towards different entities, detecting sentiment spikes and on the problem of extracting and ranking the causes of a sentiment spike. Our approach combines LDA topic model with Relative Entropy. The former is used for extracting the topics discussed in the time window before the sentiment spike. The latter allows to rank the detected topics based on their contribution to the sentiment spike."
108,"While social data is being widely used in various applications such as sentiment analysis and trend prediction, its sheer size also presents great challenges for storing, sharing and processing such data. These challenges can be addressed by data summarization which transforms the original dataset into a smaller, yet still useful, subset. Existing methods find such subsets with objective functions based on data properties such as representativeness or informativeness but do not exploit social contexts, which are distinct characteristics of social data. Further, till date very little work has focused on topic preserving data summarization, despite the abundant work on topic modeling. This is a challenging task for two reasons. First, since topic model is based on latent variables, existing methods are not well-suited to capture latent topics. Second, it is difficult to find such social contexts that provide valuable information for building effective topic-preserving summarization model. To tackle these challenges, in this paper, we focus on exploiting social contexts to summarize social data while preserving topics in the original dataset. We take Twitter data as a case study. Through analyzing Twitter data, we discover two social contexts which are important for topic generation and dissemination, namely (i) CrowdExp topic score that captures the influence of both the crowd and the expert users in Twitter and (ii) Retweet topic score that captures the influence of Twitter users' actions. We conduct extensive experiments on two real-world Twitter datasets using two applications. The experimental results show that, by leveraging social contexts, our proposed solution can enhance topic-preserving data summarization and improve application performance by up to 18%.",2016-10-24,2-s2.0-84996567089,"International Conference on Information and Knowledge Management, Proceedings",Data summarization with social contexts,"While social data is being widely used in various applications such as sentiment analysis and trend prediction, its sheer size also presents great challenges for storing, sharing and processing such data. These challenges can be addressed by data summarization which transforms the original dataset into a smaller, yet still useful, subset. Existing methods find such subsets with objective functions based on data properties such as representativeness or informativeness but do not exploit social contexts, which are distinct characteristics of social data. Further, till date very little work has focused on topic preserving data summarization, despite the abundant work on topic modeling. This is a challenging task for two reasons. First, since topic model is based on latent variables, existing methods are not well-suited to capture latent topics. Second, it is difficult to find such social contexts that provide valuable information for building effective topic-preserving summarization model. To tackle these challenges, in this paper, we focus on exploiting social contexts to summarize social data while preserving topics in the original dataset. We take Twitter data as a case study. Through analyzing Twitter data, we discover two social contexts which are important for topic generation and dissemination, namely (i) CrowdExp topic score that captures the influence of both the crowd and the expert users in Twitter and (ii) Retweet topic score that captures the influence of Twitter users' actions. We conduct extensive experiments on two real-world Twitter datasets using two applications. The experimental results show that, by leveraging social contexts, our proposed solution can enhance topic-preserving data summarization and improve application performance by up to 18%."
109,"Spatial event detection is an important and challenging problem. Unlike traditional event detection that focuses on the timing of global urgent event, the task of spatial event detection is to detect the spatial regions (e.g. clusters of neighboring cities) where urgent events occur. In this paper, we focus on the problem of spatial event detection using textual information in social media. We observe that, when a spatial event occurs, the topics relevant to the event are often discussed more coherently in cities near the event location than those far away. In order to capture this pattern, we propose a new method called Graph Topic Scan Statistic (Graph-TSS) that corresponds to a generalized log-likelihood ratio test based on topic modeling. We first demonstrate that the detection of spatial event regions under Graph-TSS is NP-hard due to a reduction from classical node-weighted prize-collecting Steiner tree problem (NW-PCST). We then design an efficient algorithm that approximately maximizes the graph topic scan statistic over spatial regions of arbitrary form. As a case study, we consider three applications using Twitter data, including Argentina civil unrest event detection, Chile earthquake detection, and United States influenza disease outbreak detection. Empirical evidence demonstrates that the proposed Graph-TSS performs superior over state-of-the-art methods on both running time and accuracy.",2016-10-24,2-s2.0-84996588067,"International Conference on Information and Knowledge Management, Proceedings",Graph topic scan statistic for spatial event detection,"Spatial event detection is an important and challenging problem. Unlike traditional event detection that focuses on the timing of global urgent event, the task of spatial event detection is to detect the spatial regions (e.g. clusters of neighboring cities) where urgent events occur. In this paper, we focus on the problem of spatial event detection using textual information in social media. We observe that, when a spatial event occurs, the topics relevant to the event are often discussed more coherently in cities near the event location than those far away. In order to capture this pattern, we propose a new method called Graph Topic Scan Statistic (Graph-TSS) that corresponds to a generalized log-likelihood ratio test based on topic modeling. We first demonstrate that the detection of spatial event regions under Graph-TSS is NP-hard due to a reduction from classical node-weighted prize-collecting Steiner tree problem (NW-PCST). We then design an efficient algorithm that approximately maximizes the graph topic scan statistic over spatial regions of arbitrary form. As a case study, we consider three applications using Twitter data, including Argentina civil unrest event detection, Chile earthquake detection, and United States influenza disease outbreak detection. Empirical evidence demonstrates that the proposed Graph-TSS performs superior over state-of-the-art methods on both running time and accuracy."
110,"Developing text classifiers often requires a large number of labeled documents as training examples. However, manually labeling documents is costly and time-consuming. Recently, a few methods have been proposed to label documents by using a small set of relevant keywords for each category, known as dataless text classification. In this paper, we propose a Seed-Guided Topic Model (named STM) for the dataless text classification task. Given a collection of unla-beled documents, and for each category a small set of seed words that are relevant to the semantic meaning of the category, the STM predicts the category labels of the documents through topic influence. STM models two kinds of topics: category-topics and general-topics. Each category-topic is associated with one specific category, representing its semantic meaning. The general-topics capture the global semantic information underlying the whole document collection. STM assumes that each document is associated with a single category-topic and a mixture of general-topics. A novelty of the model is that STM learns the topics by exploiting the explicit word co-occurrence patterns between the seed words and regular words (i.e., non-seed words) in the document collection. A document is then labeled, or classified, based on its posterior category-topic assignment. Experiments on two widely used datasets show that STM consistently outperforms the state-of-the-art dataless text classifiers. In some tasks, STM can also achieve comparable or even better classification accuracy than the state-of-the-art supervised learning solutions. Our experimental results further show that STM is insensitive to the tuning parameters. Stable performance with little variation can be achieved in a broad range of parameter settings, making it a desired choice for real applications.",2016-10-24,2-s2.0-84996598610,"International Conference on Information and Knowledge Management, Proceedings",Effective document labeling with very few seed words: A topic modeling approach,"Developing text classifiers often requires a large number of labeled documents as training examples. However, manually labeling documents is costly and time-consuming. Recently, a few methods have been proposed to label documents by using a small set of relevant keywords for each category, known as dataless text classification. In this paper, we propose a Seed-Guided Topic Model (named STM) for the dataless text classification task. Given a collection of unla-beled documents, and for each category a small set of seed words that are relevant to the semantic meaning of the category, the STM predicts the category labels of the documents through topic influence. STM models two kinds of topics: category-topics and general-topics. Each category-topic is associated with one specific category, representing its semantic meaning. The general-topics capture the global semantic information underlying the whole document collection. STM assumes that each document is associated with a single category-topic and a mixture of general-topics. A novelty of the model is that STM learns the topics by exploiting the explicit word co-occurrence patterns between the seed words and regular words (i.e., non-seed words) in the document collection. A document is then labeled, or classified, based on its posterior category-topic assignment. Experiments on two widely used datasets show that STM consistently outperforms the state-of-the-art dataless text classifiers. In some tasks, STM can also achieve comparable or even better classification accuracy than the state-of-the-art supervised learning solutions. Our experimental results further show that STM is insensitive to the tuning parameters. Stable performance with little variation can be achieved in a broad range of parameter settings, making it a desired choice for real applications."
111,"A viewpoint is a triple consisting of an entity, a topic related to this entity and sentiment towards this topic. In time-aware multi-viewpoint summarization one monitors viewpoints for a running topic and selects a small set of informative documents. In this paper, we focus on time-aware multi-viewpoint summarization of multilingual social text streams. Viewpoint drift, ambiguous entities and multilingual text make this a challenging task. Our approach includes three core ingredients: dynamic viewpoint modeling, cross-language viewpoint alignment, and, finally, multi-viewpoint summarization. Specifically, we propose a dynamic latent factor model to explicitly characterize a set of viewpoints through which entities, topics and sentiment labels during a time interval are derived jointly; we connect viewpoints in different languages by using an entity-based semantic similarity measure; and we employ an update viewpoint summarization strategy to generate a time-aware summary to reflect viewpoints. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method for time-aware multi-viewpoint summarization of multilingual social text streams.",2016-10-24,2-s2.0-84996524181,"International Conference on Information and Knowledge Management, Proceedings",Time-aware multi-viewpoint summarization of multilingual social text streams,"A viewpoint is a triple consisting of an entity, a topic related to this entity and sentiment towards this topic. In time-aware multi-viewpoint summarization one monitors viewpoints for a running topic and selects a small set of informative documents. In this paper, we focus on time-aware multi-viewpoint summarization of multilingual social text streams. Viewpoint drift, ambiguous entities and multilingual text make this a challenging task. Our approach includes three core ingredients: dynamic viewpoint modeling, cross-language viewpoint alignment, and, finally, multi-viewpoint summarization. Specifically, we propose a dynamic latent factor model to explicitly characterize a set of viewpoints through which entities, topics and sentiment labels during a time interval are derived jointly; we connect viewpoints in different languages by using an entity-based semantic similarity measure; and we employ an update viewpoint summarization strategy to generate a time-aware summary to reflect viewpoints. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method for time-aware multi-viewpoint summarization of multilingual social text streams."
112,"Recommendation for user generated content sites has gained significant attention. To satisfy the niche tastes of users, product recommendation poses more challenges due to the data sparsity issue. This work is motivated by a real world online video recommendation problem, where the click records database suffers from s-parseness of video inventory and video tags. Targeting the long tail phenomena of user behavior and sparsity of item features, we propose a personalized compound recommendation framework for online video recommendation called Dirichlet mixture probit model for information scarcity (DPIS). Assuming that each record is generated from a representation of user preferences, DPIS is a probit classifier utilizing record topical clustering on the user part for recommendation. As demonstrated by the real-world application, the proposed DPIS achieves better performance than traditional methods.",2016-10-24,2-s2.0-84996548923,"International Conference on Information and Knowledge Management, Proceedings",Scarce feature topic mining for video recommendation,"Recommendation for user generated content sites has gained significant attention. To satisfy the niche tastes of users, product recommendation poses more challenges due to the data sparsity issue. This work is motivated by a real world online video recommendation problem, where the click records database suffers from s-parseness of video inventory and video tags. Targeting the long tail phenomena of user behavior and sparsity of item features, we propose a personalized compound recommendation framework for online video recommendation called Dirichlet mixture probit model for information scarcity (DPIS). Assuming that each record is generated from a representation of user preferences, DPIS is a probit classifier utilizing record topical clustering on the user part for recommendation. As demonstrated by the real-world application, the proposed DPIS achieves better performance than traditional methods."
113,"With the soaring popularity of online social media like Twitter, analyzing short text has emerged as an increasingly important task which is challenging to classical topic models. as topic sparsity exists in short text. Topic sparsity refers to the observation that individual document usually concentrates on several salient topics, which may be rare in entire corpus. Understanding this sparse topical structure of short text has been recognized as the key ingredient for mining user-generated Web content and social medium, which are featured in the form of extremely short posts and discussions. However, the existing sparsity-enhanced topic models all assume over-complicated generative process, which severely limits their scalability and makes them unable to automatically infer the number of topics from data. In this paper, we propose a probabilistic Bayesian topic model, namely Sparse Dirichlet mixture Topic Model (S-parseDTM), based on Indian Buffet Process (IBP) prior, and infer our model on the large text corpora through a novel inference procedure called stochastic variational-Gibbs inference. Unlike prior work, the proposed approach is able to achieve exact sparse topical structure of large short text collections, and automatically identify the number of topics with a good balance between completeness and homogeneity of topic coherence. Experiments on different genres of large text corpora demonstrate that our approach outperforms various existing sparse topic models. The improvement is significant on large-scale collections of short text.",2016-10-24,2-s2.0-84996522100,"International Conference on Information and Knowledge Management, Proceedings",Understanding sparse topical structure of short text via stochastic variational-gibbs inference,"With the soaring popularity of online social media like Twitter, analyzing short text has emerged as an increasingly important task which is challenging to classical topic models. as topic sparsity exists in short text. Topic sparsity refers to the observation that individual document usually concentrates on several salient topics, which may be rare in entire corpus. Understanding this sparse topical structure of short text has been recognized as the key ingredient for mining user-generated Web content and social medium, which are featured in the form of extremely short posts and discussions. However, the existing sparsity-enhanced topic models all assume over-complicated generative process, which severely limits their scalability and makes them unable to automatically infer the number of topics from data. In this paper, we propose a probabilistic Bayesian topic model, namely Sparse Dirichlet mixture Topic Model (S-parseDTM), based on Indian Buffet Process (IBP) prior, and infer our model on the large text corpora through a novel inference procedure called stochastic variational-Gibbs inference. Unlike prior work, the proposed approach is able to achieve exact sparse topical structure of large short text collections, and automatically identify the number of topics with a good balance between completeness and homogeneity of topic coherence. Experiments on different genres of large text corpora demonstrate that our approach outperforms various existing sparse topic models. The improvement is significant on large-scale collections of short text."
114,"Mining topics in short texts (e.g. tweets, instant messages) can help people grasp essential information and understand key contents, and is widely used in many applications related to social media and text analysis. The sparsity and noise of short texts often restrict the performance of traditional topic models like LDA. Recently proposed Biterm Topic Model (BTM) which models word co-occurrence patterns directly, is revealed effective for topic detection in short texts. However, BTM has two main drawbacks. It needs to manually specify topic number, which is difficult to accurately determine when facing new corpora. Besides, BTM assumes that two words in same term should belong to the same topic, which is often too strong as it does not differentiate two types of words (i.e. general words and topical words). To tackle these problems, in this paper, we propose a non-parametric topic model npCTM with the above distinction. Our model incorporates the Chinese restaurant process (CRP) into the BTM model to determine topic number automatically. Our model also distinguishes general words from topical words by jointly considering the distribution of these two word types for each word as well as word coherence information as prior knowledge. We carry out experimental studies on real-world twitter dataset. The results demonstrate the effectiveness of our method to discover coherent topics compared with the baseline methods.",2016-10-24,2-s2.0-84996598809,"International Conference on Information and Knowledge Management, Proceedings",A non-parametric topic model for short texts incorporating word coherence knowledge,"Mining topics in short texts (e.g. tweets, instant messages) can help people grasp essential information and understand key contents, and is widely used in many applications related to social media and text analysis. The sparsity and noise of short texts often restrict the performance of traditional topic models like LDA. Recently proposed Biterm Topic Model (BTM) which models word co-occurrence patterns directly, is revealed effective for topic detection in short texts. However, BTM has two main drawbacks. It needs to manually specify topic number, which is difficult to accurately determine when facing new corpora. Besides, BTM assumes that two words in same term should belong to the same topic, which is often too strong as it does not differentiate two types of words (i.e. general words and topical words). To tackle these problems, in this paper, we propose a non-parametric topic model npCTM with the above distinction. Our model incorporates the Chinese restaurant process (CRP) into the BTM model to determine topic number automatically. Our model also distinguishes general words from topical words by jointly considering the distribution of these two word types for each word as well as word coherence information as prior knowledge. We carry out experimental studies on real-world twitter dataset. The results demonstrate the effectiveness of our method to discover coherent topics compared with the baseline methods."
115,"The newly emerging location-based social networks (LBSN) such as Tinder and Momo extends social interaction from friends to strangers, providing novel experiences of making new friends. Familiar strangers refer to the strangers who meet frequently in daily life and may share common interests; thus they may be good candidates for friend recommendation. In this paper, we study the problem of discovering familiar strangers, specifically, public transportation trip companions, and their common interests. We collect 5.7 million transaction records of smart cards from about 3.02 million people in the city of Beijing, China. We first analyze this dataset and reveal the temporal and spatial characteristics of passenger encounter behaviors. Then we propose a stability metric to measure hidden friend relations. This metric facilitates us to employ community detection techniques to capture the communities of trip companions. Further, we infer common interests of each community using a topic model, i.e., LDA4HFC (Latent Dirichlet Allocation for Hidden Friend Communities) model. Such topics for communities help to understand how hidden friend clusters are formed. We evaluate our method using large-scale and real-world datasets, consisting of two-week smart card records and 901,855 Point of Interests (POIs) in Beijing. The results show that our method outperforms three baseline methods with higher recommendation accuracy. Moreover, our case study demonstrates that the discovered topics interpret the communities very well.",2016-10-24,2-s2.0-84996527566,"International Conference on Information and Knowledge Management, Proceedings",Who are my familiar strangers? Revealing hidden friend relations and common interests from smart card data,"The newly emerging location-based social networks (LBSN) such as Tinder and Momo extends social interaction from friends to strangers, providing novel experiences of making new friends. Familiar strangers refer to the strangers who meet frequently in daily life and may share common interests; thus they may be good candidates for friend recommendation. In this paper, we study the problem of discovering familiar strangers, specifically, public transportation trip companions, and their common interests. We collect 5.7 million transaction records of smart cards from about 3.02 million people in the city of Beijing, China. We first analyze this dataset and reveal the temporal and spatial characteristics of passenger encounter behaviors. Then we propose a stability metric to measure hidden friend relations. This metric facilitates us to employ community detection techniques to capture the communities of trip companions. Further, we infer common interests of each community using a topic model, i.e., LDA4HFC (Latent Dirichlet Allocation for Hidden Friend Communities) model. Such topics for communities help to understand how hidden friend clusters are formed. We evaluate our method using large-scale and real-world datasets, consisting of two-week smart card records and 901,855 Point of Interests (POIs) in Beijing. The results show that our method outperforms three baseline methods with higher recommendation accuracy. Moreover, our case study demonstrates that the discovered topics interpret the communities very well."
116,"The past few years have witnessed millions of credit/debit cards flowing through the underground economy and ultimately causing significant financial loss. Examining key underground economy sellers has both practical and academic significance for cybercrime forensics and criminology research. Drawing on social media analytics, we have developed the AZSecure text mining system for identifying and profiling key sellers. The system identifies sellers using sentiment analysis of customer reviews and profiles sellers using topic modeling of advertisements. We evaluated the AZSecure system on eight international underground economy forums. The system significantly outperformed all benchmark machine-learning methods on identifying advertisement threads, classifying customer review sentiments, and profiling seller characteristics, with an average F-measure of about 80 percent to 90 percent. In our case study, we identified the famous carder, Rescator, who was affiliated with the Target breach, and captured important seller characteristics in terms of product type, payment options, and contact channels. Our research leverages social media analytics to probe into the underground economy in order to help law enforcement target key sellers and prevent future fraud. It also contributes to our understanding of the use of information technology in detecting deception in online systems.",2016-10-01,2-s2.0-85012186740,Journal of Management Information Systems,Identifying and Profiling Key Sellers in Cyber Carding Community: AZSecure Text Mining System,"The past few years have witnessed millions of credit/debit cards flowing through the underground economy and ultimately causing significant financial loss. Examining key underground economy sellers has both practical and academic significance for cybercrime forensics and criminology research. Drawing on social media analytics, we have developed the AZSecure text mining system for identifying and profiling key sellers. The system identifies sellers using sentiment analysis of customer reviews and profiles sellers using topic modeling of advertisements. We evaluated the AZSecure system on eight international underground economy forums. The system significantly outperformed all benchmark machine-learning methods on identifying advertisement threads, classifying customer review sentiments, and profiling seller characteristics, with an average F-measure of about 80 percent to 90 percent. In our case study, we identified the famous carder, Rescator, who was affiliated with the Target breach, and captured important seller characteristics in terms of product type, payment options, and contact channels. Our research leverages social media analytics to probe into the underground economy in order to help law enforcement target key sellers and prevent future fraud. It also contributes to our understanding of the use of information technology in detecting deception in online systems."
117,"The performance of cross-lingual sentiment classification is sharply limited by the language gap, which means that each language has its own ways to express sentiments. Many methods have been designed to transmit sentiment information across languages by making use of machine translation, parallel corpora, auxiliary unlabeled samples and other resources. In this paper, a new approach is proposed based on the selection of training data, where labeled samples highly similar to the target language are put into the training set. The refined training samples are used to build up an effective cross-lingual sentiment classifier focusing on the target language. The proposed approach contains two major strategies: the aligned-translation topic model and the semi-supervised training data adjustment. The aligned-translation topic model provides a cross-language representation space in which the semi-supervised training data adjustment procedure attempts to select effective training samples to eliminate the negative influence of the semantic distribution differences between the original and target languages. The experiments show that the proposed approach is feasible for cross-language sentiment classification tasks and provides insight into the semantic relationship between two different languages.",2016-09-01,2-s2.0-85000885429,Knowledge-Based Systems,Cross-lingual sentiment classification: Similarity discovery plus training data adjustment,"The performance of cross-lingual sentiment classification is sharply limited by the language gap, which means that each language has its own ways to express sentiments. Many methods have been designed to transmit sentiment information across languages by making use of machine translation, parallel corpora, auxiliary unlabeled samples and other resources. In this paper, a new approach is proposed based on the selection of training data, where labeled samples highly similar to the target language are put into the training set. The refined training samples are used to build up an effective cross-lingual sentiment classifier focusing on the target language. The proposed approach contains two major strategies: the aligned-translation topic model and the semi-supervised training data adjustment. The aligned-translation topic model provides a cross-language representation space in which the semi-supervised training data adjustment procedure attempts to select effective training samples to eliminate the negative influence of the semantic distribution differences between the original and target languages. The experiments show that the proposed approach is feasible for cross-language sentiment classification tasks and provides insight into the semantic relationship between two different languages."
118,"Traditional pseudo relevance feedback (PRF) models choose top k feedback documents for query expansion and treat those documents equally. When k is determined, feedback terms are selected without considering the reliability of these documents for relevance. Because the performance of PRF is sensitive to the selection of feedback terms, noisy terms imported from these irrelevant documents or partially relevant documents will harm the final results extensively. Intuitively, terms in these documents should be considered less important for feedback term selection. Nonetheless, how to measure the reliability of feedback documents is a difficult problem. Recently, topic modeling has become more and more popular in the information retrieval (IR) area. In order to identify how reliable a feedback document is to be relevant, we attempt to adapt the topical information into PRF. However, topics are hard to be quantified and therefore the identification of topic is usually fuzzy. It is very challenging for integrating the obtained topical information effectively into IR and other text-processing-related areas. Current research work mainly focuses on mining relevant information from particular topics. This is extremely difficult when the boundaries of different topics are hard to define. In this article, we investigate a key factor of this problem, the topic number for topic modeling and how it makes topics ""fuzzy."" To effectively and efficiently apply topical information, we propose a new probabilistic framework, ""TopPRF,"" and threemodels, TS-COS, TS-EU, and TS-Entropy, via integrating ""Topic Space"" (TS) information into pseudo relevance feedback. Thesemethods discover how reliable a document is to be relevant through both term and topical information.When selecting feedback terms, candidate terms in more reliable feedback documents should obtain extra weights. Experimental results on various public collections justify that our proposed methods can significantly reduce the influence of ""fuzzy topics"" and obtain stable, good results over the strong baseline models. Our proposed probabilistic framework, TopPRF, and three topicspace- based models are capable of searching documents beyond traditional term matching only and provide a promising avenue for constructing better topic-space-based IR systems. Moreover, in-depth discussions and conclusions are made to help other researchers apply topical information effectively.",2016-08-01,2-s2.0-84986596754,ACM Transactions on Information Systems,TopPRF: A probabilistic framework for integrating topic space into pseudo relevance feedback,"Traditional pseudo relevance feedback (PRF) models choose top k feedback documents for query expansion and treat those documents equally. When k is determined, feedback terms are selected without considering the reliability of these documents for relevance. Because the performance of PRF is sensitive to the selection of feedback terms, noisy terms imported from these irrelevant documents or partially relevant documents will harm the final results extensively. Intuitively, terms in these documents should be considered less important for feedback term selection. Nonetheless, how to measure the reliability of feedback documents is a difficult problem. Recently, topic modeling has become more and more popular in the information retrieval (IR) area. In order to identify how reliable a feedback document is to be relevant, we attempt to adapt the topical information into PRF. However, topics are hard to be quantified and therefore the identification of topic is usually fuzzy. It is very challenging for integrating the obtained topical information effectively into IR and other text-processing-related areas. Current research work mainly focuses on mining relevant information from particular topics. This is extremely difficult when the boundaries of different topics are hard to define. In this article, we investigate a key factor of this problem, the topic number for topic modeling and how it makes topics ""fuzzy."" To effectively and efficiently apply topical information, we propose a new probabilistic framework, ""TopPRF,"" and threemodels, TS-COS, TS-EU, and TS-Entropy, via integrating ""Topic Space"" (TS) information into pseudo relevance feedback. Thesemethods discover how reliable a document is to be relevant through both term and topical information.When selecting feedback terms, candidate terms in more reliable feedback documents should obtain extra weights. Experimental results on various public collections justify that our proposed methods can significantly reduce the influence of ""fuzzy topics"" and obtain stable, good results over the strong baseline models. Our proposed probabilistic framework, TopPRF, and three topicspace- based models are capable of searching documents beyond traditional term matching only and provide a promising avenue for constructing better topic-space-based IR systems. Moreover, in-depth discussions and conclusions are made to help other researchers apply topical information effectively."
119,"The surge of interest in big social data has led to growing demand for social media analytics (SMA). Having robust SMA can help firms create value and achieve competitive advantages. However, most firms do not always know how to embrace big social data to establish a path to value. This study addresses this key question to deepen our understanding of how different types of SMA can be applied to create value. Specifically, the findings show the significant uses of opinion mining or sentiment analysis, topic modeling, engagement analysis, predictive analysis, social network analysis, and trend analysis. Finally, the study provides directions for the challenges and opportunities of SMA to maximize value.",2016-07-01,2-s2.0-84973529689,Journal of Organizational and End User Computing,How does social media analytics create value?,"The surge of interest in big social data has led to growing demand for social media analytics (SMA). Having robust SMA can help firms create value and achieve competitive advantages. However, most firms do not always know how to embrace big social data to establish a path to value. This study addresses this key question to deepen our understanding of how different types of SMA can be applied to create value. Specifically, the findings show the significant uses of opinion mining or sentiment analysis, topic modeling, engagement analysis, predictive analysis, social network analysis, and trend analysis. Finally, the study provides directions for the challenges and opportunities of SMA to maximize value."
120,"This paper presents a general framework for short text classification by learning vector representations of both words and hidden topics together. We refer to a large-scale external data collection named ""corpus"" which is topic consistent with short texts to be classified and then use the corpus to build topic model with Latent Dirichlet Allocation (LDA). For all the texts of the corpus and short texts, topics of words are viewed as new words and integrated into texts for data enriching. On the enriched corpus, we can learn vector representations of both words and topics. In this way, feature representations of short texts can be performed based on vectors of both words and topics for training and classification. On an open short text classification data set, learning vectors of both words and topics can significantly help reduce the classification error comparing with learning only word vectors. We also compared the proposed classification method with various baselines and experimental results justified the effectiveness of our word/topic vector representations.",2016-06-15,2-s2.0-84964318898,Knowledge-Based Systems,Improving short text classification by learning vector representations of both words and hidden topics,"This paper presents a general framework for short text classification by learning vector representations of both words and hidden topics together. We refer to a large-scale external data collection named ""corpus"" which is topic consistent with short texts to be classified and then use the corpus to build topic model with Latent Dirichlet Allocation (LDA). For all the texts of the corpus and short texts, topics of words are viewed as new words and integrated into texts for data enriching. On the enriched corpus, we can learn vector representations of both words and topics. In this way, feature representations of short texts can be performed based on vectors of both words and topics for training and classification. On an open short text classification data set, learning vectors of both words and topics can significantly help reduce the classification error comparing with learning only word vectors. We also compared the proposed classification method with various baselines and experimental results justified the effectiveness of our word/topic vector representations."
121,"Explicit Semantic Analysis (ESA) is a knowledge-based method which builds the semantic representation of the words depending on the textual description of the concepts in the certain knowledge source. Due to its simplicity and success, ESA has received wide attention from researchers in the computational linguistics and information retrieval. However, the representation vectors formed by ESA method are generally very excessive, high dimensional, and may contain many redundant concepts. In this paper, we introduce a reduced semantic representation method that constructs the semantic interpretation of the words as the vectors over the latent topics from the original ESA representation vectors. For modeling the latent topics, the Latent Dirichlet Allocation (LDA) is adapted to the ESA vectors for extracting the topics as the probability distributions over the concepts rather than the words in the traditional model. The proposed method is applied to the wide knowledge sources used in the computational semantic analysis: WordNet and Wikipedia. For evaluation, we use the proposed method in two natural language processing tasks: measuring the semantic relatedness between words/texts and text clustering. The experimental results indicate that the proposed method overcomes the limitations of the representation of the ESA method.",2016-05-15,2-s2.0-84977992467,Knowledge-Based Systems,Reducing explicit semantic representation vectors using Latent Dirichlet Allocation,"Explicit Semantic Analysis (ESA) is a knowledge-based method which builds the semantic representation of the words depending on the textual description of the concepts in the certain knowledge source. Due to its simplicity and success, ESA has received wide attention from researchers in the computational linguistics and information retrieval. However, the representation vectors formed by ESA method are generally very excessive, high dimensional, and may contain many redundant concepts. In this paper, we introduce a reduced semantic representation method that constructs the semantic interpretation of the words as the vectors over the latent topics from the original ESA representation vectors. For modeling the latent topics, the Latent Dirichlet Allocation (LDA) is adapted to the ESA vectors for extracting the topics as the probability distributions over the concepts rather than the words in the traditional model. The proposed method is applied to the wide knowledge sources used in the computational semantic analysis: WordNet and Wikipedia. For evaluation, we use the proposed method in two natural language processing tasks: measuring the semantic relatedness between words/texts and text clustering. The experimental results indicate that the proposed method overcomes the limitations of the representation of the ESA method."
122,"Electronic Medical Record (EMR) has established itself as a valuable resource for large scale analysis of health data. A hospital EMR dataset typically consists of medical records of hospitalized patients. A medical record contains diagnostic information (diagnosis codes), procedures performed (procedure codes) and admission details. Traditional topic models, such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP), can be employed to discover disease topics from EMR data by treating patients as documents and diagnosis codes as words. This topic modeling helps to understand the constitution of patient diseases and offers a tool for better planning of treatment. In this paper, we propose a novel and flexible hierarchical Bayesian nonparametric model, the word distance dependent Chinese restaurant franchise (wddCRF), which incorporates word-to-word distances to discover semantically-coherent disease topics. We are motivated by the fact that diagnosis codes are connected in the form of ICD-10 tree structure which presents semantic relationships between codes. We exploit a decay function to incorporate distances between words at the bottom level of wddCRF. Efficient inference is derived for the wddCRF by using MCMC technique. Furthermore, since procedure codes are often correlated with diagnosis codes, we develop the correspondence wddCRF (Corr-wddCRF) to explore conditional relationships of procedure codes for a given disease pattern. Efficient collapsed Gibbs sampling is derived for the Corr-wddCRF. We evaluate the proposed models on two real-world medical datasets - PolyVascular disease and Acute Myocardial Infarction disease. We demonstrate that the Corr-wddCRF model discovers more coherent topics than the Corr-HDP. We also use disease topic proportions as new features and show that using features from the Corr-wddCRF outperforms the baselines on 14-days readmission prediction. Beside these, the prediction for procedure codes based on the Corr-wddCRF also shows considerable accuracy.",2016-05-01,2-s2.0-84961208266,Knowledge-Based Systems,Hierarchical Bayesian nonparametric models for knowledge discovery from electronic medical records,"Electronic Medical Record (EMR) has established itself as a valuable resource for large scale analysis of health data. A hospital EMR dataset typically consists of medical records of hospitalized patients. A medical record contains diagnostic information (diagnosis codes), procedures performed (procedure codes) and admission details. Traditional topic models, such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP), can be employed to discover disease topics from EMR data by treating patients as documents and diagnosis codes as words. This topic modeling helps to understand the constitution of patient diseases and offers a tool for better planning of treatment. In this paper, we propose a novel and flexible hierarchical Bayesian nonparametric model, the word distance dependent Chinese restaurant franchise (wddCRF), which incorporates word-to-word distances to discover semantically-coherent disease topics. We are motivated by the fact that diagnosis codes are connected in the form of ICD-10 tree structure which presents semantic relationships between codes. We exploit a decay function to incorporate distances between words at the bottom level of wddCRF. Efficient inference is derived for the wddCRF by using MCMC technique. Furthermore, since procedure codes are often correlated with diagnosis codes, we develop the correspondence wddCRF (Corr-wddCRF) to explore conditional relationships of procedure codes for a given disease pattern. Efficient collapsed Gibbs sampling is derived for the Corr-wddCRF. We evaluate the proposed models on two real-world medical datasets - PolyVascular disease and Acute Myocardial Infarction disease. We demonstrate that the Corr-wddCRF model discovers more coherent topics than the Corr-HDP. We also use disease topic proportions as new features and show that using features from the Corr-wddCRF outperforms the baselines on 14-days readmission prediction. Beside these, the prediction for procedure codes based on the Corr-wddCRF also shows considerable accuracy."
123,"Social media systems provide ever-growing huge volumes of information for dissemination and communication among communities of users, while recommender systems aim to mitigate information overload by filtering and providing users the most attractive and relevant items from information-sea. This paper aims at providing compound recommendation engine for social media systems, and focuses on exploiting multi-sourced information (e.g. social networks, item contents and user feedbacks) to predict the ratings of users to items and make recommendations. For this, we suppose the users' decisions on adopting item are affected both by their tastes and the favors of trusted friends, and extend Collaborative Topic Regression to jointly incorporates social trust ensemble, topic modeling and probabilistic matrix factorization. We propose corresponding approaches to learning the latent factors both of users and items, as well as additional parameters to be estimated. Empirical experiments on Lastfm and Delicious datasets show that our model is better and more robust than the state-of-the-art methods on making recommendations in term of accuracy. Experiments results also reveal some useful findings to enlighten the development of recommender systems in social media.",2016-04-01,2-s2.0-84956627416,Knowledge-Based Systems,Collaborative Topic Regression with social trust ensemble for recommendation in social media systems,"Social media systems provide ever-growing huge volumes of information for dissemination and communication among communities of users, while recommender systems aim to mitigate information overload by filtering and providing users the most attractive and relevant items from information-sea. This paper aims at providing compound recommendation engine for social media systems, and focuses on exploiting multi-sourced information (e.g. social networks, item contents and user feedbacks) to predict the ratings of users to items and make recommendations. For this, we suppose the users' decisions on adopting item are affected both by their tastes and the favors of trusted friends, and extend Collaborative Topic Regression to jointly incorporates social trust ensemble, topic modeling and probabilistic matrix factorization. We propose corresponding approaches to learning the latent factors both of users and items, as well as additional parameters to be estimated. Empirical experiments on Lastfm and Delicious datasets show that our model is better and more robust than the state-of-the-art methods on making recommendations in term of accuracy. Experiments results also reveal some useful findings to enlighten the development of recommender systems in social media."
124,"Understanding current technological changes is the basis for better forecasting of technological changes. Because technology is path dependent, monitoring past and current trends of technological development helps managers and decision makers to identify probable future technologies in order to prevent organizational failure. This study suggests a method based on patent-development paths, k-core analysis and topic modeling of past and current trends of technological development to identify technologies that have the potential to become disruptive technologies. We find that within the photovoltaic industry, thin-film technology is likely to replace the dominant technology, namely crystalline silicon. In addition, we identity the hidden technologies, namely multi-junction, dye-sensitized and concentration technologies, that have the potential to become disruptive technologies within the three main technologies of the photovoltaic industry.",2016-03-01,2-s2.0-84951842753,Technological Forecasting and Social Change,Identification and monitoring of possible disruptive technologies by patent-development paths and topic modeling,"Understanding current technological changes is the basis for better forecasting of technological changes. Because technology is path dependent, monitoring past and current trends of technological development helps managers and decision makers to identify probable future technologies in order to prevent organizational failure. This study suggests a method based on patent-development paths, k-core analysis and topic modeling of past and current trends of technological development to identify technologies that have the potential to become disruptive technologies. We find that within the photovoltaic industry, thin-film technology is likely to replace the dominant technology, namely crystalline silicon. In addition, we identity the hidden technologies, namely multi-junction, dye-sensitized and concentration technologies, that have the potential to become disruptive technologies within the three main technologies of the photovoltaic industry."
125,"Topic detection as a tool to detect topics from online media attracts much attention. Generally, a topic is characterized by a set of informative keywords/terms. Traditional approaches are usually based on various topic models, such as Latent Dirichlet Allocation (LDA). They cluster terms into a topic by mining semantic relations between terms. However, co-occurrence relations across the document are commonly neglected, which leads to the detection of incomplete information. Furthermore, the inability to discover latent co-occurrence relations via the context or other bridge terms prevents the important but rare topics from being detected. To tackle this issue, we propose a hybrid relations analysis approach to integrate semantic relations and co-occurrence relations for topic detection. Specifically, the approach fuses multiple relations into a term graph and detects topics from the graph using a graph analytical method. It can not only detect topics more effectively by combing mutually complementary relations, but also mine important rare topics by leveraging latent co-occurrence relations. Extensive experiments demonstrate the advantage of our approach over several benchmarks.",2016-02-01,2-s2.0-84955515486,Knowledge-Based Systems,A hybrid term-term relations analysis approach for topic detection,"Topic detection as a tool to detect topics from online media attracts much attention. Generally, a topic is characterized by a set of informative keywords/terms. Traditional approaches are usually based on various topic models, such as Latent Dirichlet Allocation (LDA). They cluster terms into a topic by mining semantic relations between terms. However, co-occurrence relations across the document are commonly neglected, which leads to the detection of incomplete information. Furthermore, the inability to discover latent co-occurrence relations via the context or other bridge terms prevents the important but rare topics from being detected. To tackle this issue, we propose a hybrid relations analysis approach to integrate semantic relations and co-occurrence relations for topic detection. Specifically, the approach fuses multiple relations into a term graph and detects topics from the graph using a graph analytical method. It can not only detect topics more effectively by combing mutually complementary relations, but also mine important rare topics by leveraging latent co-occurrence relations. Extensive experiments demonstrate the advantage of our approach over several benchmarks."
127,"User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses' strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers' minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.",2016-01-01,2-s2.0-84952982692,Business Horizons,Uncovering the message from the mess of big data,"User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses' strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers' minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers."
128,"We conduct a large-scale empirical study on the sharing behavior in social media to measure the effect of message features and initial messengers on information diffusion. Our analysis focuses on messages created by companies and utilizes both textual and visual semantic content by employing state-of-the-art machine learning methods: topic modeling and deep learning. We find that messages with multiple conspicuous images and messengers with similar content are crucial in the diffusion process. Our approach for semantic content analysis, particularly for visual content, bridges advanced machine learning techniques for effective marketing and social media strategies.",2016-01-01,2-s2.0-84988014531,Lecture Notes in Business Information Processing,Sharing behavior in online social media: An empirical analysis with deep learning,"We conduct a large-scale empirical study on the sharing behavior in social media to measure the effect of message features and initial messengers on information diffusion. Our analysis focuses on messages created by companies and utilizes both textual and visual semantic content by employing state-of-the-art machine learning methods: topic modeling and deep learning. We find that messages with multiple conspicuous images and messengers with similar content are crucial in the diffusion process. Our approach for semantic content analysis, particularly for visual content, bridges advanced machine learning techniques for effective marketing and social media strategies."
129,"Information diffusion model plays an important role in many real-world applications such as online marketing and e-government campaigns. Existing approaches often predict information diffusion by examining whether events are triggered by external trends or the social network itself. However, existing methods cannot take into account the semantically rich ""topics"" to estimate the correlations between users and messages describing some events. The main contribution of our work is the development of the Topic based Information Diffusion (TBID) model which can incorporate external trends model and topic based social descriptions to enhance the effectiveness of predicting information diffusion in online social networks. Experiments conducted based on real-world data sets confirm the distinct advantage of the proposed computational method. Our research opens the door to the development of a more effective personalized information recommendation model in online social media.",2015-12-08,2-s2.0-84964918090,"Proceedings - 12th IEEE International Conference on E-Business Engineering, ICEBE 2015",Topic Based Information Diffusion Prediction Model with External Trends,"Information diffusion model plays an important role in many real-world applications such as online marketing and e-government campaigns. Existing approaches often predict information diffusion by examining whether events are triggered by external trends or the social network itself. However, existing methods cannot take into account the semantically rich ""topics"" to estimate the correlations between users and messages describing some events. The main contribution of our work is the development of the Topic based Information Diffusion (TBID) model which can incorporate external trends model and topic based social descriptions to enhance the effectiveness of predicting information diffusion in online social networks. Experiments conducted based on real-world data sets confirm the distinct advantage of the proposed computational method. Our research opens the door to the development of a more effective personalized information recommendation model in online social media."
130,"Understanding technology convergence became crucial for pursuing innovation and economic growth. This paper attempts to predict the pattern of technology convergence by jointly applying the Association Rule and Link Prediction to entire IPCs related to triadic patents filed during the period from 1955 to 2011. We further use a topic model to discover emerging areas of the predicted technology convergence. The results show that the medical area is in the center of convergence, and we predict that technologies for treating respiratory system/blood/sense disorders are associated with the technologies of genetic engineering/peptide/heterocyclic compounds. After eliminating the majority of convergence, we found the convergence pattern among activating catalysts, printing, advanced networking, controlling devices, secured communication with in-memory system, television system with pattern recognition, and image processing and analyzing technologies. The results of our study are expected to contribute to firms that seek new innovative technological domain.",2015-11-01,2-s2.0-84949531077,Technological Forecasting and Social Change,Predicting the pattern of technology convergence using big-data technology on large-scale triadic patents,"Understanding technology convergence became crucial for pursuing innovation and economic growth. This paper attempts to predict the pattern of technology convergence by jointly applying the Association Rule and Link Prediction to entire IPCs related to triadic patents filed during the period from 1955 to 2011. We further use a topic model to discover emerging areas of the predicted technology convergence. The results show that the medical area is in the center of convergence, and we predict that technologies for treating respiratory system/blood/sense disorders are associated with the technologies of genetic engineering/peptide/heterocyclic compounds. After eliminating the majority of convergence, we found the convergence pattern among activating catalysts, printing, advanced networking, controlling devices, secured communication with in-memory system, television system with pattern recognition, and image processing and analyzing technologies. The results of our study are expected to contribute to firms that seek new innovative technological domain."
131,"Social networking services, such as Twitter and Sina Weibo, have tremendous popularity in recent years. Mass of short texts and social links are aggregated into these service platforms. To realize personalized services on social network, topic inference from both short texts and social links plays more and more important role. Most conventional topic modeling methods focus on analyzing formal texts, e.g., papers, news and blogs, and usually assume that the links are only generated by topical factors. As a result, on social network, the learned topics of these methods are usually affected by topic-irrelevant links. Recently, a few approaches use artificial priors to recognize the links generated by the popularity factor in topic modeling. However, employing global priors, these methods can not well capture the distinct properties of each link and still suffer from the effect of topic-irrelevant links. To address the above limitations, we propose a novel Social-Relational Topic Model (SRTM), which can alleviate the effect of topic-irrelevant links by analyzing relational users' topics of each link. SRTM jointly models texts and social links for learning the topic distribution and topical influence of each user. The experimental results show that, our model outperforms the state-of-the-arts in topic modeling and social link prediction.",2015-10-17,2-s2.0-84959275720,"International Conference on Information and Knowledge Management, Proceedings",Social-relational topic model for social networks,"Social networking services, such as Twitter and Sina Weibo, have tremendous popularity in recent years. Mass of short texts and social links are aggregated into these service platforms. To realize personalized services on social network, topic inference from both short texts and social links plays more and more important role. Most conventional topic modeling methods focus on analyzing formal texts, e.g., papers, news and blogs, and usually assume that the links are only generated by topical factors. As a result, on social network, the learned topics of these methods are usually affected by topic-irrelevant links. Recently, a few approaches use artificial priors to recognize the links generated by the popularity factor in topic modeling. However, employing global priors, these methods can not well capture the distinct properties of each link and still suffer from the effect of topic-irrelevant links. To address the above limitations, we propose a novel Social-Relational Topic Model (SRTM), which can alleviate the effect of topic-irrelevant links by analyzing relational users' topics of each link. SRTM jointly models texts and social links for learning the topic distribution and topical influence of each user. The experimental results show that, our model outperforms the state-of-the-arts in topic modeling and social link prediction."
132,"The main objective of the workshop is to bring together researchers who are interested in applications of topic models and improving their output. Our goal is to create a broad platform for researchers to share ideas that could improve the usability and interpretation of topic models. We expect this will promote topic model applications in other research areas, making their use more effective.",2015-10-17,2-s2.0-84958248001,"International Conference on Information and Knowledge Management, Proceedings",TM 2015 - Topic models: Post-processing and applications workshop,"The main objective of the workshop is to bring together researchers who are interested in applications of topic models and improving their output. Our goal is to create a broad platform for researchers to share ideas that could improve the usability and interpretation of topic models. We expect this will promote topic model applications in other research areas, making their use more effective."
133,"Document network is a kind of intriguing dataset which can provide both topical (textual content) and topological (relational link) information. A key point in viably modeling such datasets is to discover proper denominators beneath the two different types of data, text and link. Most previous work introduces the assumption that documents closely linked with each other share common latent topics. However, the heterophily (i.e., tendency to link to different others) of nodes is neglected, which is pervasive in social networks. In this paper, we simultaneously incorporate community detection and topic modeling in a unified framework, and appeal to Canonical Correlation Analysis (CCA) to capture the latent semantic correlations between the two heterogeneous latent factors, community and topic. Despite of the homophily (i.e., tendency to link to similar others) or heterophily, CCA can properly capture the inherent correlations which fit the dataset itself without any prior hypothesis. Logistic normal prior is also employed in modeling network to better capture the community correlations. We derive efficient inference and learning algorithms based on variational EM methods. The effectiveness of our proposed model is comprehensively verified on three different types of datasets which are namely hyperlinked networks of web pages, social networks of friends and coauthor networks of publications. Experimental results show that our approach achieves significant improvements on both topic modeling and community detection compared with the current state of the art. Meanwhile, our model is impressive in discovering correlations between extracted topics and communities.",2015-10-17,2-s2.0-84958254390,"International Conference on Information and Knowledge Management, Proceedings",Discovering canonical correlations between topical and topological information in document networks,"Document network is a kind of intriguing dataset which can provide both topical (textual content) and topological (relational link) information. A key point in viably modeling such datasets is to discover proper denominators beneath the two different types of data, text and link. Most previous work introduces the assumption that documents closely linked with each other share common latent topics. However, the heterophily (i.e., tendency to link to different others) of nodes is neglected, which is pervasive in social networks. In this paper, we simultaneously incorporate community detection and topic modeling in a unified framework, and appeal to Canonical Correlation Analysis (CCA) to capture the latent semantic correlations between the two heterogeneous latent factors, community and topic. Despite of the homophily (i.e., tendency to link to similar others) or heterophily, CCA can properly capture the inherent correlations which fit the dataset itself without any prior hypothesis. Logistic normal prior is also employed in modeling network to better capture the community correlations. We derive efficient inference and learning algorithms based on variational EM methods. The effectiveness of our proposed model is comprehensively verified on three different types of datasets which are namely hyperlinked networks of web pages, social networks of friends and coauthor networks of publications. Experimental results show that our approach achieves significant improvements on both topic modeling and community detection compared with the current state of the art. Meanwhile, our model is impressive in discovering correlations between extracted topics and communities."
134,"Inferring interests of users in social network is important for many applications such as personalized search, recommender systems and online advertising. Most previous studies inferred users' interests based on text posted in social network, which is usually not related to their interests. In this paper, we propose a modified topic model, Bi-Labeled LDA with a term weighting scheme, to extract interest tags for users in social network. The proposed model utilize only users' relationship information without requirement for text information, and incorporates supervision into traditional LDA. Specifically, we introduce method to extract tags for non-famous user through their relationship with famous users in Twitter, and study why a non-famous user follows famous users simultaneously. Comparison with state-of-the-art methods on real dataset shows that our method is far more superior in terms of precision and recall of the extracted tag set, and also more applicable for many personalized applications. Besides, we find that a reasonable term weighting scheme can actually improve the performance further.",2015-10-17,2-s2.0-84958251275,"International Conference on Information and Knowledge Management, Proceedings",Extracting interest tags for non-famous users in social network,"Inferring interests of users in social network is important for many applications such as personalized search, recommender systems and online advertising. Most previous studies inferred users' interests based on text posted in social network, which is usually not related to their interests. In this paper, we propose a modified topic model, Bi-Labeled LDA with a term weighting scheme, to extract interest tags for users in social network. The proposed model utilize only users' relationship information without requirement for text information, and incorporates supervision into traditional LDA. Specifically, we introduce method to extract tags for non-famous user through their relationship with famous users in Twitter, and study why a non-famous user follows famous users simultaneously. Comparison with state-of-the-art methods on real dataset shows that our method is far more superior in terms of precision and recall of the extracted tag set, and also more applicable for many personalized applications. Besides, we find that a reasonable term weighting scheme can actually improve the performance further."
135,"Accurate mortality prediction is an important task in intensive care units in order to channel prompt care to patients in the most critical condition and to reduce nurses' alarm fatigue. Nursing notes carry valuable information in this regard, but nothing has been reported about the effectiveness of temporal analysis of nursing notes in mortality prediction tasks. We propose a time series model that uncovers the temporal dynamics of patients' underlying states from nursing notes. The effectiveness of this information in mortality prediction is examined for mortality prediction for five different time spans ranging from one day to one year. Our experiments show that the model captures both patient states and their temporal dynamics that have a strong correlation with patient mortality. The results also show that incorporating temporal information improves performance in long-term mortality prediction, but has no significant effect in short-term prediction.",2015-10-17,2-s2.0-84958246551,"International Conference on Information and Knowledge Management, Proceedings",Time series analysis of nursing notes for mortality prediction via a state transition topic model,"Accurate mortality prediction is an important task in intensive care units in order to channel prompt care to patients in the most critical condition and to reduce nurses' alarm fatigue. Nursing notes carry valuable information in this regard, but nothing has been reported about the effectiveness of temporal analysis of nursing notes in mortality prediction tasks. We propose a time series model that uncovers the temporal dynamics of patients' underlying states from nursing notes. The effectiveness of this information in mortality prediction is examined for mortality prediction for five different time spans ranging from one day to one year. Our experiments show that the model captures both patient states and their temporal dynamics that have a strong correlation with patient mortality. The results also show that incorporating temporal information improves performance in long-term mortality prediction, but has no significant effect in short-term prediction."
136,"Formulating and reformulating reliable textual queries have been recognized as a challenging task in Information Retrieval (IR), even for experienced users. Most existing query expansion methods, especially those based on implicit relevance feedback, utilize the user's historical interaction data, such as clicks, scrolling and viewing time on documents, to derive a refined query model. It is further expected that the user's search experience would be largely improved if we could dig out user's latent query intention, in real-time, by capturing the user's current interaction at the term level directly. In this paper, we propose a real-time eye tracking based query expansion method, which is able to: (1) automatically capture the terms that the user is viewing by utilizing eye tracking techniques; (2) derive the user's latent intent based on the eye tracking terms and by using the Latent Dirichlet Allocation (LDA) approach. A systematic user study has been carried out and the experimental results demonstrate the effectiveness of our proposed methods.",2015-10-17,2-s2.0-84959312260,"International Conference on Information and Knowledge Management, Proceedings",A real-time eye tracking based query expansion approach via latent topic modeling,"Formulating and reformulating reliable textual queries have been recognized as a challenging task in Information Retrieval (IR), even for experienced users. Most existing query expansion methods, especially those based on implicit relevance feedback, utilize the user's historical interaction data, such as clicks, scrolling and viewing time on documents, to derive a refined query model. It is further expected that the user's search experience would be largely improved if we could dig out user's latent query intention, in real-time, by capturing the user's current interaction at the term level directly. In this paper, we propose a real-time eye tracking based query expansion method, which is able to: (1) automatically capture the terms that the user is viewing by utilizing eye tracking techniques; (2) derive the user's latent intent based on the eye tracking terms and by using the Latent Dirichlet Allocation (LDA) approach. A systematic user study has been carried out and the experimental results demonstrate the effectiveness of our proposed methods."
137,"To date, data generates and arrives in the form of stream to propagate discussions of public events in microblog services. Discovering event-oriented topics from the stream will lead to a better understanding of the change of public concern. However, as the massive scale of the data stream, traditional static topic models, such as LDA, are no longer fit for topic detection and tracking tasks. In this paper, we propose a central topic model (CenTM), where a Multi-view Clustering algorithm with Two-phase Random Walk (MC-TRW) is devised to aggregate the LDA's latent topics into central topics. Furthermore, we leverage the aggregation of central topics alternately with MC-TRW and sequential topic inference to improve the scalability in the stream fashion, so as to derive the dynamic central topic model (DCenTM). Specifically, our model is able to uncover the intrinsic characteristics of the central topics and predict the trend of their intensity along a life cycle. Experimental results demonstrate that the proposed central topic model is event-oriented and of high generalization, it therefore can dispose the topic trend prediction effectively and precisely in massive data stream.",2015-10-17,2-s2.0-84959303766,"International Conference on Information and Knowledge Management, Proceedings",Central topic model for event-oriented topics mining in microblog stream,"To date, data generates and arrives in the form of stream to propagate discussions of public events in microblog services. Discovering event-oriented topics from the stream will lead to a better understanding of the change of public concern. However, as the massive scale of the data stream, traditional static topic models, such as LDA, are no longer fit for topic detection and tracking tasks. In this paper, we propose a central topic model (CenTM), where a Multi-view Clustering algorithm with Two-phase Random Walk (MC-TRW) is devised to aggregate the LDA's latent topics into central topics. Furthermore, we leverage the aggregation of central topics alternately with MC-TRW and sequential topic inference to improve the scalability in the stream fashion, so as to derive the dynamic central topic model (DCenTM). Specifically, our model is able to uncover the intrinsic characteristics of the central topics and predict the trend of their intensity along a life cycle. Experimental results demonstrate that the proposed central topic model is event-oriented and of high generalization, it therefore can dispose the topic trend prediction effectively and precisely in massive data stream."
138,"Microblogs contain the most up-to-date and abundant opinion information on current events. Aspect-based opinion mining is a good way to get a comprehensive summarization of events. The most popular aspect based opinion mining models are used in the field of product and service. However, existing models are not suitable for event mining. In this paper we propose a novel probabilistic generative model (ASEM) to simultaneously discover aspects and the specified opinions. ASEM incorporate a sequence labeling model(CRF) into a generative topic model. Additionally, we adopt a set of features for separating aspects and sentiments. Moreover, we novelly present a continuously learning model. It can utilize the knowledge of one event to learn another, and get a better performance. We use five real world events to do experiment. The experimental results show that ASEM extracts aspects and sentiments well, and ASEM outperforms other state-of-art models and the intuitive two-step method.",2015-10-17,2-s2.0-84958256507,"International Conference on Information and Knowledge Management, Proceedings",ASEM: Mining aspects and sentiment of events from microblog,"Microblogs contain the most up-to-date and abundant opinion information on current events. Aspect-based opinion mining is a good way to get a comprehensive summarization of events. The most popular aspect based opinion mining models are used in the field of product and service. However, existing models are not suitable for event mining. In this paper we propose a novel probabilistic generative model (ASEM) to simultaneously discover aspects and the specified opinions. ASEM incorporate a sequence labeling model(CRF) into a generative topic model. Additionally, we adopt a set of features for separating aspects and sentiments. Moreover, we novelly present a continuously learning model. It can utilize the knowledge of one event to learn another, and get a better performance. We use five real world events to do experiment. The experimental results show that ASEM extracts aspects and sentiments well, and ASEM outperforms other state-of-art models and the intuitive two-step method."
139,"In social media, users have contributed enormous behavior data online which can be leveraged for user modeling and conduct personalized services. Temporal user modeling, which incorporates the timestamp of these behavior data and understands users' interest evolution, have attracted attention recently. With the recognition that user interests are vulnerable to transient events, many current temporal user modeling solutions propose to first identify the transient events and then consider the identified events into user behavior modeling. In this work, in the context of microblogs, we propose a unified probabilistic framework to simultaneously model the process of transient event detection and temporal user tweeting. The outputs of the framework include: (1) one long-term topic space spanning over general categories, (2) one short-term topic space for each time interval corresponding to the transient events, and (3) users' interest distributions over the long- and short-term topic spaces. Qualitative and quantitative experimental evaluation are conducted on a large-scale Twitter dataset, with more than 2 million users and 0.3 billion tweets. The promising results demonstrate the advantage of the proposed topic models.",2015-10-17,2-s2.0-84958252637,"International Conference on Information and Knowledge Management, Proceedings",A probabilistic framework for temporal user modeling on microblogs,"In social media, users have contributed enormous behavior data online which can be leveraged for user modeling and conduct personalized services. Temporal user modeling, which incorporates the timestamp of these behavior data and understands users' interest evolution, have attracted attention recently. With the recognition that user interests are vulnerable to transient events, many current temporal user modeling solutions propose to first identify the transient events and then consider the identified events into user behavior modeling. In this work, in the context of microblogs, we propose a unified probabilistic framework to simultaneously model the process of transient event detection and temporal user tweeting. The outputs of the framework include: (1) one long-term topic space spanning over general categories, (2) one short-term topic space for each time interval corresponding to the transient events, and (3) users' interest distributions over the long- and short-term topic spaces. Qualitative and quantitative experimental evaluation are conducted on a large-scale Twitter dataset, with more than 2 million users and 0.3 billion tweets. The promising results demonstrate the advantage of the proposed topic models."
140,"A common and convenient approach for user to describe his information need is to provide a set of keywords. Therefore, the technique to understand the need becomes crucial. In this paper, for the information need about a topic or category, we propose a novel method called TDCS(Topic Distilling with Compressive Sensing) for explicit and accurate modeling the topic implied by several keywords. The task is transformed as a topic reconstruction problem in the semantic space with a reasonable intuition that the topic is sparse in the semantic space. The latent semantic space could be mined from documents via unsupervised methods, e.g. LSI. Compressive sensing is leveraged to obtain a sparse representation from only a few keywords. In order to make the distilled topic more robust, an iterative learning approach is adopted. The experiment results show the effectiveness of our method. Moreover, with only a few semantic concepts remained for the topic, our method is efficient for subsequent text mining tasks.",2015-10-17,2-s2.0-84958232629,"International Conference on Information and Knowledge Management, Proceedings",Topic modeling in semantic space with keywords,"A common and convenient approach for user to describe his information need is to provide a set of keywords. Therefore, the technique to understand the need becomes crucial. In this paper, for the information need about a topic or category, we propose a novel method called TDCS(Topic Distilling with Compressive Sensing) for explicit and accurate modeling the topic implied by several keywords. The task is transformed as a topic reconstruction problem in the semantic space with a reasonable intuition that the topic is sparse in the semantic space. The latent semantic space could be mined from documents via unsupervised methods, e.g. LSI. Compressive sensing is leveraged to obtain a sparse representation from only a few keywords. In order to make the distilled topic more robust, an iterative learning approach is adopted. The experiment results show the effectiveness of our method. Moreover, with only a few semantic concepts remained for the topic, our method is efficient for subsequent text mining tasks."
141,"As the volume of publications has increased dramatically, an urgent need has developed to assist researchers in locating high-quality, candidate-cited papers from a research repository. Traditional scholarly recommendation approaches ignore the chronological nature of citation recommendations. In this study, we propose a novel method called ""Chronological Citation Recommendation"" which assumes initial user information needs could shift while users are searching for papers in different time slices. We model the information-need shifts with two-level modeling: dynamic time-related ranking feature construction and dynamic evolving feature weight training. In more detail, we employed a supervised document influence model to characterize the content ""time-varying"" dynamics and constructed a novel heterogeneous graph that encapsulates dynamic topic-based information, time-decay paper/topic citation information, and word-based information. We applied multiple meta-paths for different ranking hypotheses which carried different types of information for citation recommendation in various time slices, along with information-need shifting. We also used multiple learning-to-rank models to optimize the feature weights for different time slices to generate the final ""Chronological Citation Recommendation"" rankings. The use of Chronological Citation Recommendation suggests time-series ranking lists based on initial user textual information need and characterizes the information-need shifting. Experiments on the ACM corpus show that Chronological Citation Recommendation can significantly enhance citation recommendation performance.",2015-10-17,2-s2.0-84958257628,"International Conference on Information and Knowledge Management, Proceedings",Chronological citation recommendation with information-need shifting,"As the volume of publications has increased dramatically, an urgent need has developed to assist researchers in locating high-quality, candidate-cited papers from a research repository. Traditional scholarly recommendation approaches ignore the chronological nature of citation recommendations. In this study, we propose a novel method called ""Chronological Citation Recommendation"" which assumes initial user information needs could shift while users are searching for papers in different time slices. We model the information-need shifts with two-level modeling: dynamic time-related ranking feature construction and dynamic evolving feature weight training. In more detail, we employed a supervised document influence model to characterize the content ""time-varying"" dynamics and constructed a novel heterogeneous graph that encapsulates dynamic topic-based information, time-decay paper/topic citation information, and word-based information. We applied multiple meta-paths for different ranking hypotheses which carried different types of information for citation recommendation in various time slices, along with information-need shifting. We also used multiple learning-to-rank models to optimize the feature weights for different time slices to generate the final ""Chronological Citation Recommendation"" rankings. The use of Chronological Citation Recommendation suggests time-series ranking lists based on initial user textual information need and characterizes the information-need shifting. Experiments on the ACM corpus show that Chronological Citation Recommendation can significantly enhance citation recommendation performance."
142,"The classification of web pages content is essential to many information retrieval tasks. In this paper, we propose a new methodology for a multilayer soft classification. Our approach is based on the connection between the semi-supervised Latent Dirichlet Allocation (LDA) and the Random Forest classifier. We compute with LDA the distribution of topics in each document and use the results to train the Random Forest classifier. The trained classifier is then able to categorize each web document in different layers of the categories hierarchy. We have applied our methodology on a collected data set from dmoz and have obtained satisfactory results.",2015-10-07,2-s2.0-84954551242,"2015 15th International Conference on Innovations for Community Services, I4CS 2015",Multilayer classification of web pages using random forest and semi-supervised latent dirichlet allocation,"The classification of web pages content is essential to many information retrieval tasks. In this paper, we propose a new methodology for a multilayer soft classification. Our approach is based on the connection between the semi-supervised Latent Dirichlet Allocation (LDA) and the Random Forest classifier. We compute with LDA the distribution of topics in each document and use the results to train the Random Forest classifier. The trained classifier is then able to categorize each web document in different layers of the categories hierarchy. We have applied our methodology on a collected data set from dmoz and have obtained satisfactory results."
143,"Buyers express their opinions openly in free text feedback comments. The users are attracted to online-shopping not only due to the convenience in accessing the information of items on-sold, but also the availability of the other buyer's feedback on their purchasing experience, item-related and/or seller-related. The Mining E-Commerce feedback comments for trust evaluation System proposes a novel technique that uses a multi-dimensional trust evaluation model, for computing comprehensive trust scores for sellers in e-commerce applications. The system computes dimension trust scores and the dimension weights. It automatically extracting dimension ratings from feedback comments and by combining natural language processing with opinion mining and summarization techniques in trust evaluation to improve the accuracy for mining process.",2015-09-23,2-s2.0-84965071508,ICETECH 2015 - 2015 IEEE International Conference on Engineering and Technology,Mining E-commerce feedback comments for trust evaluation,"Buyers express their opinions openly in free text feedback comments. The users are attracted to online-shopping not only due to the convenience in accessing the information of items on-sold, but also the availability of the other buyer's feedback on their purchasing experience, item-related and/or seller-related. The Mining E-Commerce feedback comments for trust evaluation System proposes a novel technique that uses a multi-dimensional trust evaluation model, for computing comprehensive trust scores for sellers in e-commerce applications. The system computes dimension trust scores and the dimension weights. It automatically extracting dimension ratings from feedback comments and by combining natural language processing with opinion mining and summarization techniques in trust evaluation to improve the accuracy for mining process."
144,"In their quest for data-driven insight, firms align their resources to produce information that is actionable. Moreover, the bundling and utilization of these valuable resources is what defines an organizational capability. Thus, in this paper we conceptualize a new type of capability - data analytics capabilities, DAC, as the ability to assemble, coordinate, mobilize, and deploy analytics-based resources with strategic purpose. Using text as data, we explore the use of probabilistic topic modeling on historical press releases, in an attempt to identify types of DAC from successful data analytics investments. Press and news releases frequently articulate a firm's resource allocation strategy, proving an opportunity to automatically classify these into topics that can suggest categorization of DAC. We explore 8-year historical press releases and apply Latent Dirichlet Allocation topic modeling to 273 press releases.",2015-09-21,2-s2.0-84955621444,Portland International Conference on Management of Engineering and Technology,Decoding data analytics capabilities from topic modeling on press releases,"In their quest for data-driven insight, firms align their resources to produce information that is actionable. Moreover, the bundling and utilization of these valuable resources is what defines an organizational capability. Thus, in this paper we conceptualize a new type of capability - data analytics capabilities, DAC, as the ability to assemble, coordinate, mobilize, and deploy analytics-based resources with strategic purpose. Using text as data, we explore the use of probabilistic topic modeling on historical press releases, in an attempt to identify types of DAC from successful data analytics investments. Press and news releases frequently articulate a firm's resource allocation strategy, proving an opportunity to automatically classify these into topics that can suggest categorization of DAC. We explore 8-year historical press releases and apply Latent Dirichlet Allocation topic modeling to 273 press releases."
145,"In this chapter, the authors survey the general problem of analyzing a social network in order to make predictions about its behavior, content, or the systems and phenomena that generated it. They begin by defining five basic tasks that can be performed using social networks: (1) link prediction; (2) pathway and community formation; (3) recommendation and decision support; (4) risk analysis; and (5) planning, especially causal interventional planning. Next, they discuss frameworks for using predictive analytics, availability of annotation, text associated with (or produced within) a social network, information propagation history (e.g., upvotes and shares), trust, and reputation data. They also review challenges such as imbalanced and partial data, concept drift especially as it manifests within social media, and the need for active learning, online learning, and transfer learning. They then discuss general methodologies for predictive analytics involving network topology and dynamics, heterogeneous information network analysis, stochastic simulation, and topic modeling using the abovementioned text corpora. They continue by describing applications such as predicting ""who will follow whom?"" in a social network, making entity-to-entity recommendations (person-to-person, business-to-business [B2B], consumer-tobusiness [C2B], or business-to-consumer [B2C]), and analyzing big data (especially transactional data) for Customer Relationship Management (CRM) applications. Finally, the authors examine a few specific recommender systems and systems for interaction discovery, as part of brief case studies.",2015-09-21,2-s2.0-84956714680,Emerging Methods in Predictive Analytics: Risk Management and Decision-Making,Predictive analytics of social networks: A survey of tasks and techniques,"In this chapter, the authors survey the general problem of analyzing a social network in order to make predictions about its behavior, content, or the systems and phenomena that generated it. They begin by defining five basic tasks that can be performed using social networks: (1) link prediction; (2) pathway and community formation; (3) recommendation and decision support; (4) risk analysis; and (5) planning, especially causal interventional planning. Next, they discuss frameworks for using predictive analytics, availability of annotation, text associated with (or produced within) a social network, information propagation history (e.g., upvotes and shares), trust, and reputation data. They also review challenges such as imbalanced and partial data, concept drift especially as it manifests within social media, and the need for active learning, online learning, and transfer learning. They then discuss general methodologies for predictive analytics involving network topology and dynamics, heterogeneous information network analysis, stochastic simulation, and topic modeling using the abovementioned text corpora. They continue by describing applications such as predicting ""who will follow whom?"" in a social network, making entity-to-entity recommendations (person-to-person, business-to-business [B2B], consumer-tobusiness [C2B], or business-to-consumer [B2C]), and analyzing big data (especially transactional data) for Customer Relationship Management (CRM) applications. Finally, the authors examine a few specific recommender systems and systems for interaction discovery, as part of brief case studies."
146,"We explore the double-edged sword of recombination in generating breakthrough innovation: recombination of distant or diverse knowledge is needed because knowledge in a narrow domain might trigger myopia, but recombination can be counterproductive when local search is needed to identify anomalies. We take into account how creativity shapes both the cognitive novelty of the idea and the subsequent realization of economic value. We develop a text-based measure of novel ideas in patents using topic modeling to identify those patents that originate new topics in a body of knowledge. We find that, counter to theories of recombination, patents that originate new topics are more likely to be associated with local search, while economic value is the product of broader recombinations as well as novelty.",2015-01-01,2-s2.0-84941187958,Strategic Management Journal,The double-edged sword of recombination in breakthrough innovation,"We explore the double-edged sword of recombination in generating breakthrough innovation: recombination of distant or diverse knowledge is needed because knowledge in a narrow domain might trigger myopia, but recombination can be counterproductive when local search is needed to identify anomalies. We take into account how creativity shapes both the cognitive novelty of the idea and the subsequent realization of economic value. We develop a text-based measure of novel ideas in patents using topic modeling to identify those patents that originate new topics in a body of knowledge. We find that, counter to theories of recombination, patents that originate new topics are more likely to be associated with local search, while economic value is the product of broader recombinations as well as novelty."
147,"Studies have shown that perceptual maps derived from online consumer-generated data are effective for depicting market structure such as demonstrating positioning of competitive brands. However, most text mining algorithms would require manual reading to merge extracted product features with synonyms. In response, Topic modeling is introduced to group synonyms together under a topic automatically, leading to convenient and accurate evaluation of brands based on consumers' online reviews. To ensure the feasibility of employing Topic modeling in assessing competitive brands, we developed a unique and novel framework named WVAP (Weights from Valid Posterior Probability) based on Scree plot technique. WVAP can filter the noises in posterior distribution obtained from Topic modeling, and improve accuracy in brand evaluation. A case study exploring online reviews of mobile phones is conducted. We extract topics to reflect the features of the cell phones with a qualified validity. In addition to perceptual maps derived by multi-dimensional scaling (MDS) for product positioning, we also rank these products by TOPSIS (Technique for Order Performance by Similarity to Ideal Solution) so as to visualize the market structure from different perspectives. Our case study of cell phones shows that the proposed framework is effective in mining online reviews and providing insights into the competitive landscape.",2015-01-01,2-s2.0-84925010368,Electronic Commerce Research and Applications,"Visualizing market structure through online product reviews: Integrate topic modeling, TOPSIS, and multi-dimensional scaling approaches","Studies have shown that perceptual maps derived from online consumer-generated data are effective for depicting market structure such as demonstrating positioning of competitive brands. However, most text mining algorithms would require manual reading to merge extracted product features with synonyms. In response, Topic modeling is introduced to group synonyms together under a topic automatically, leading to convenient and accurate evaluation of brands based on consumers' online reviews. To ensure the feasibility of employing Topic modeling in assessing competitive brands, we developed a unique and novel framework named WVAP (Weights from Valid Posterior Probability) based on Scree plot technique. WVAP can filter the noises in posterior distribution obtained from Topic modeling, and improve accuracy in brand evaluation. A case study exploring online reviews of mobile phones is conducted. We extract topics to reflect the features of the cell phones with a qualified validity. In addition to perceptual maps derived by multi-dimensional scaling (MDS) for product positioning, we also rank these products by TOPSIS (Technique for Order Performance by Similarity to Ideal Solution) so as to visualize the market structure from different perspectives. Our case study of cell phones shows that the proposed framework is effective in mining online reviews and providing insights into the competitive landscape."
148,"People often share their opinions or impressions about TV shows (e.g., dramas) with other viewers through social media such as personal blogs and Twitter. As such, broadcast media, especially TV, lead to audience engagement on social media. Moreover, the audience engagement, in turn, impacts broadcast media ratings. Social TV analyzes audience's TV-related social media behaviors and tries to use the behaviors in marketing activities such as advertisement; however, this is purely based on the quantity o f engagement in social media. In this study, we analyze the subjects of the audience engagement on social media about specific TV dramas through topic modeling, and examines the relationship between changes in the topics and viewer ratings of the TV dramas.",2015-01-01,2-s2.0-84959256115,Proceedings of the International Conference on Electronic Business (ICEB),Relationship between audience engagement on social media and broadcast media ratings,"People often share their opinions or impressions about TV shows (e.g., dramas) with other viewers through social media such as personal blogs and Twitter. As such, broadcast media, especially TV, lead to audience engagement on social media. Moreover, the audience engagement, in turn, impacts broadcast media ratings. Social TV analyzes audience's TV-related social media behaviors and tries to use the behaviors in marketing activities such as advertisement; however, this is purely based on the quantity o f engagement in social media. In this study, we analyze the subjects of the audience engagement on social media about specific TV dramas through topic modeling, and examines the relationship between changes in the topics and viewer ratings of the TV dramas."
149,"This study examines the effects of organizational attention on technological search in the multibusiness firm. We argue that attentional specialization and coupling, or (respectively) attention given to problems within and across units, affect a unit's ability to engage in distant and local search by shaping how problems are perceived and addressed. We test this theory by applying a probabilistic topic model to all Motorola patents issued from 1974 to 1997, thus identifying and measuring attention to technical problems. Our results suggest that (a) subunits with specialized attention are not myopic but instead explore broadly and (b) tight attentional coupling across units increases the breadth of search. This study contributes to attention-based views of the firm and to studies on organizational design and search.",2015-01-01,2-s2.0-84939780271,Advances in Strategic Management,Organizational attention and technological search in the multibusiness firm: Motorola from 1974 to 1997,"This study examines the effects of organizational attention on technological search in the multibusiness firm. We argue that attentional specialization and coupling, or (respectively) attention given to problems within and across units, affect a unit's ability to engage in distant and local search by shaping how problems are perceived and addressed. We test this theory by applying a probabilistic topic model to all Motorola patents issued from 1974 to 1997, thus identifying and measuring attention to technical problems. Our results suggest that (a) subunits with specialized attention are not myopic but instead explore broadly and (b) tight attentional coupling across units increases the breadth of search. This study contributes to attention-based views of the firm and to studies on organizational design and search."
150,"Patent classification systems and citation networks are used extensively in innovation studies. However, non-unique mapping of classification codes onto specific products/markets and the difficulties in accurately capturing knowledge flows based just on citation linkages present limitations to these conventional patent analysis approaches. We present a natural language processing based hierarchical technique that enables the automatic identification and classification of patent datasets into technology areas and sub-areas. The key novelty of our technique is to use topic modeling to map patents to probability distributions over real world categories/topics. Accuracy and usefulness of our technique are tested on a dataset of 10,201 patents in solar photovoltaics filed in the United States Patent and Trademark Office (USPTO) between 2002 and 2013. We show that linguistic features from topic models can be used to effectively identify the main technology area that a patent's invention applies to. Our computational experiments support the view that the topic distribution of a patent offers a reduced-form representation of the knowledge content in a patent. Accordingly, we suggest that this hidden thematic structure in patents can be useful in studies of the policy-innovation-geography nexus. To that end, we also demonstrate an application of our technique for identifying patterns in technological convergence.",2015-01-01,2-s2.0-84954123056,Technological Forecasting and Social Change,Topic based classification and pattern identification in patents,"Patent classification systems and citation networks are used extensively in innovation studies. However, non-unique mapping of classification codes onto specific products/markets and the difficulties in accurately capturing knowledge flows based just on citation linkages present limitations to these conventional patent analysis approaches. We present a natural language processing based hierarchical technique that enables the automatic identification and classification of patent datasets into technology areas and sub-areas. The key novelty of our technique is to use topic modeling to map patents to probability distributions over real world categories/topics. Accuracy and usefulness of our technique are tested on a dataset of 10,201 patents in solar photovoltaics filed in the United States Patent and Trademark Office (USPTO) between 2002 and 2013. We show that linguistic features from topic models can be used to effectively identify the main technology area that a patent's invention applies to. Our computational experiments support the view that the topic distribution of a patent offers a reduced-form representation of the knowledge content in a patent. Accordingly, we suggest that this hidden thematic structure in patents can be useful in studies of the policy-innovation-geography nexus. To that end, we also demonstrate an application of our technique for identifying patterns in technological convergence."
151,"Micro-blogging services and location-based social networks, such as Twitter, Weibo, and Foursquare, enable users to post short messages with timestamps and geographical annotations. The rich spatial-temporalsemantic information of individuals embedded in these geo-annotated short messages provides exciting opportunity to develop many context-aware applications in ubiquitous computing environments. Example applications include contextual recommendation and contextual search. To obtain accurate recommendations and most relevant search results, it is important to capture users' contextual information (e.g., time and location) and to understand users' topical interests and intentions. While time and location can be readily captured by smartphones, understanding user's interests and intentions calls for effective methods in modeling user mobility behavior. Here, user mobility refers to who visits which place at what time for what activity. That is, user mobility behavior modeling must consider user (Who), spatial (Where), temporal (When), and activity (What) aspects. Unfortunately, no previous studies on user mobility behavior modeling have considered all of the four aspects jointly, which have complex interdependencies. In our preliminary study, we propose the first solution named W",2015-01-01,2-s2.0-84923814459,ACM Transactions on Information Systems,"Who, where, when, and what: A nonparametric Bayesian approach to context-aware recommendation and search for Twitter users","Micro-blogging services and location-based social networks, such as Twitter, Weibo, and Foursquare, enable users to post short messages with timestamps and geographical annotations. The rich spatial-temporalsemantic information of individuals embedded in these geo-annotated short messages provides exciting opportunity to develop many context-aware applications in ubiquitous computing environments. Example applications include contextual recommendation and contextual search. To obtain accurate recommendations and most relevant search results, it is important to capture users' contextual information (e.g., time and location) and to understand users' topical interests and intentions. While time and location can be readily captured by smartphones, understanding user's interests and intentions calls for effective methods in modeling user mobility behavior. Here, user mobility refers to who visits which place at what time for what activity. That is, user mobility behavior modeling must consider user (Who), spatial (Where), temporal (When), and activity (What) aspects. Unfortunately, no previous studies on user mobility behavior modeling have considered all of the four aspects jointly, which have complex interdependencies. In our preliminary study, we propose the first solution named W"
152,"In recent years, topic modeling is gaining significant momentum in information retrieval (IR). Researchers have found that utilizing the topic information generated through topic modeling together with traditional TF-IDF information generates superior results in document retrieval. However, in order to apply this idea to real-life IR systems, some critical problems need to be solved: how to store the topic information and how to utilize it with the TF-IDF information for efficient document retrieval. In this paper, we propose the Topic Enhanced Inverted Index (TEII) to incorporate the topic information into the inverted index for efficient top-k document retrieval. Specifically, we explore two different types of TEIIs. We first propose the incremental TEII, which includes the topic information into the traditional inverted index by adding topic-based inverted lists. The incremental TEII is beneficial for legacy IR systems, since it does not change the existing TF-IDF-based inverted lists. As a more flexible alternative, we propose the hybrid TEII to incorporate the topic information into each posting of the inverted index. In the hybrid TEII, two relaxation methods are proposed to support dynamic estimation of the upper bound impact of each posting. The hybrid TEII is highly extensible for incorporating different ranking factors and we show an extension of the hybrid TEII by considering the static quality of the documents in the corpus. Based on the incremental and hybrid TEIIs, we develop several query processing algorithms to support efficient top-k document retrieval on TEIIs. Empirical evaluation on the TREC dataset verifies the effectiveness and efficiency of the proposed index structures and query processing algorithms.",2015-01-01,2-s2.0-84944352133,Knowledge-Based Systems,TEII: Topic enhanced inverted index for top-k document retrieval,"In recent years, topic modeling is gaining significant momentum in information retrieval (IR). Researchers have found that utilizing the topic information generated through topic modeling together with traditional TF-IDF information generates superior results in document retrieval. However, in order to apply this idea to real-life IR systems, some critical problems need to be solved: how to store the topic information and how to utilize it with the TF-IDF information for efficient document retrieval. In this paper, we propose the Topic Enhanced Inverted Index (TEII) to incorporate the topic information into the inverted index for efficient top-k document retrieval. Specifically, we explore two different types of TEIIs. We first propose the incremental TEII, which includes the topic information into the traditional inverted index by adding topic-based inverted lists. The incremental TEII is beneficial for legacy IR systems, since it does not change the existing TF-IDF-based inverted lists. As a more flexible alternative, we propose the hybrid TEII to incorporate the topic information into each posting of the inverted index. In the hybrid TEII, two relaxation methods are proposed to support dynamic estimation of the upper bound impact of each posting. The hybrid TEII is highly extensible for incorporating different ranking factors and we show an extension of the hybrid TEII by considering the static quality of the documents in the corpus. Based on the incremental and hybrid TEIIs, we develop several query processing algorithms to support efficient top-k document retrieval on TEIIs. Empirical evaluation on the TREC dataset verifies the effectiveness and efficiency of the proposed index structures and query processing algorithms."
153,"The essential work of feature-specific opinion mining is centered on the product features. Previous related research work has often taken into account explicit features but ignored implicit features, However, implicit feature identification, which can help us better understand the reviews, is an essential aspect of feature-specific opinion mining. This paper is mainly centered on implicit feature identification in Chinese product reviews. We think that based on the explicit synonymous feature group and the sentences which contain explicit features, several Support Vector Machine (SVM) classifiers can be established to classify the non-explicit sentences. Nevertheless, instead of simply using traditional feature selection methods, we believe an explicit topic model in which each topic is pre-defined could perform better. In this paper, we first extend a popular topic modeling method, called Latent Dirichlet Allocation (LDA), to construct an explicit topic model. Then some types of prior knowledge, such as: must-links, cannot-links and relevance-based prior knowledge, are extracted and incorporated into the explicit topic model automatically. Experiments show that the explicit topic model, which incorporates pre-existing knowledge, outperforms traditional feature selection methods and other existing methods by a large margin and the identification task can be completed better.",2015-01-01,2-s2.0-84923080938,Knowledge-Based Systems,Implicit feature identification in Chinese reviews using explicit topic mining model,"The essential work of feature-specific opinion mining is centered on the product features. Previous related research work has often taken into account explicit features but ignored implicit features, However, implicit feature identification, which can help us better understand the reviews, is an essential aspect of feature-specific opinion mining. This paper is mainly centered on implicit feature identification in Chinese product reviews. We think that based on the explicit synonymous feature group and the sentences which contain explicit features, several Support Vector Machine (SVM) classifiers can be established to classify the non-explicit sentences. Nevertheless, instead of simply using traditional feature selection methods, we believe an explicit topic model in which each topic is pre-defined could perform better. In this paper, we first extend a popular topic modeling method, called Latent Dirichlet Allocation (LDA), to construct an explicit topic model. Then some types of prior knowledge, such as: must-links, cannot-links and relevance-based prior knowledge, are extracted and incorporated into the explicit topic model automatically. Experiments show that the explicit topic model, which incorporates pre-existing knowledge, outperforms traditional feature selection methods and other existing methods by a large margin and the identification task can be completed better."
154,"This article reviews 40 years of the Journal of Consumer Research (JCR). Using text mining, we uncover the key phrases associated with consumer research. We use a topic modeling procedure to uncover 16 topics that have been featured in the journal since its inception and to show the trends in topics over time. For example, we highlight the decline in family decision-making research and the flourishing of social identity and influence research since the journal’s inception. A citation analysis shows which JCR articles have had the most impact and compares the topics in top-cited articles with all JCR journal articles. We show that methodological and consumer culture articles tend to be heavily cited. We conclude by investigating the scholars who have been the top contributors to the journal across the four decades of its existence. And to better understand which schools have contributed most to the knowledge of consumer research over this history, we provide an analysis of where these top-performing scholars were trained. Our approach shows that the JCR archives can be an excellent source of data for scholars trying to understand the complicated, challenging, and dynamic field of consumer research.",2015-01-01,2-s2.0-84936752613,Journal of Consumer Research,The journal of consumer research at 40: A historical analysis,"This article reviews 40 years of the Journal of Consumer Research (JCR). Using text mining, we uncover the key phrases associated with consumer research. We use a topic modeling procedure to uncover 16 topics that have been featured in the journal since its inception and to show the trends in topics over time. For example, we highlight the decline in family decision-making research and the flourishing of social identity and influence research since the journal’s inception. A citation analysis shows which JCR articles have had the most impact and compares the topics in top-cited articles with all JCR journal articles. We show that methodological and consumer culture articles tend to be heavily cited. We conclude by investigating the scholars who have been the top contributors to the journal across the four decades of its existence. And to better understand which schools have contributed most to the knowledge of consumer research over this history, we provide an analysis of where these top-performing scholars were trained. Our approach shows that the JCR archives can be an excellent source of data for scholars trying to understand the complicated, challenging, and dynamic field of consumer research."
155,"A number of algorithms exist in measuring clothing similarity for clothing recommendations in E-commerce. The clothing similarity mostly depends on its shape, texture and style. In this paper we introduce three models of defining feature space for clothing recommendations. The sketch-based image search mainly focuses on defining similarity of clothing in contour dimension. The spatial bagof-feature approach is employed to measure the clothing similarity of local image patterns. Finally, we introduce a query adaptive shape model which combines shape characteristics and labels of clothing, in order to take the semantic information of clothing. A large number of simulations are given to show the feasibility and performance of the clothing recommendations by using content-based image search.",2015-01-01,2-s2.0-84943811019,Multimedia Data Mining and Analytics: Disruptive Innovation,Content based image search for clothing recommendations in E-Commerce,"A number of algorithms exist in measuring clothing similarity for clothing recommendations in E-commerce. The clothing similarity mostly depends on its shape, texture and style. In this paper we introduce three models of defining feature space for clothing recommendations. The sketch-based image search mainly focuses on defining similarity of clothing in contour dimension. The spatial bagof-feature approach is employed to measure the clothing similarity of local image patterns. Finally, we introduce a query adaptive shape model which combines shape characteristics and labels of clothing, in order to take the semantic information of clothing. A large number of simulations are given to show the feasibility and performance of the clothing recommendations by using content-based image search."
156,"This article aims to explain the widespread attention to contemporary protesting artists among Western audiences by focusing on the case of Pussy Riot. Social movement scholarship provides a first step into understanding how Pussy Riot legitimately protests Russian politics through its punk performances. It then turns to the concept of cosmopolitanism as a performance in everyday life to explain Pussy Riot's appeal among Western audiences. By collecting and analyzing 9001 tweets through a thematic hashtag analysis and topic modeling, this article analyzes how audiences talk about Pussy Riot and shows how Twitter affords users to perform cosmopolitan selves by sharing their ideas and experiences on Pussy Riot with others. Although we distinguish between four types of cosmopolitan selves, the results clearly show Pussy Riot is mainly reflected upon in a media context: Twitter users predominantly talk about Pussy Riot's media appearances rather than readily engage with its explicit political advocacy.",2015-01-01,2-s2.0-84939772902,International Journal of Consumer Studies,Western solidarity with Pussy Riot and the Twittering of cosmopolitan selves,"This article aims to explain the widespread attention to contemporary protesting artists among Western audiences by focusing on the case of Pussy Riot. Social movement scholarship provides a first step into understanding how Pussy Riot legitimately protests Russian politics through its punk performances. It then turns to the concept of cosmopolitanism as a performance in everyday life to explain Pussy Riot's appeal among Western audiences. By collecting and analyzing 9001 tweets through a thematic hashtag analysis and topic modeling, this article analyzes how audiences talk about Pussy Riot and shows how Twitter affords users to perform cosmopolitan selves by sharing their ideas and experiences on Pussy Riot with others. Although we distinguish between four types of cosmopolitan selves, the results clearly show Pussy Riot is mainly reflected upon in a media context: Twitter users predominantly talk about Pussy Riot's media appearances rather than readily engage with its explicit political advocacy."
157,"As news events on the same subject occur, our knowledge about the subject will accumulate and become more comprehensive. In this paper, we formally define the problem of incremental knowledge learning from similar news events on the same subject, where each event consists of a set of news articles reporting about it. The knowledge is represented by a topic hierarchy presenting topics at different levels of granularity. Though topic (hierarchy) mining from text has been researched a lot, incremental learning from similar events remains under developed. In this paper, we propose a scalable two-phase framework to incrementally learn a topic hierarchy for a subject from events on the subject as the events occur. First, we recursively construct a topic hierarchy for each event based on a novel topic model considering the named entities and entity types in news articles. Second, we incrementally merge the topic hierarchies through top-down hierarchical topic alignment. Extensive experimental results on real datasets demonstrate the effectiveness and efficiency of the proposed framework in terms of both qualitative and quantitative measures.",2015-01-01,2-s2.0-84944322844,Knowledge-Based Systems,Incremental learning from news events,"As news events on the same subject occur, our knowledge about the subject will accumulate and become more comprehensive. In this paper, we formally define the problem of incremental knowledge learning from similar news events on the same subject, where each event consists of a set of news articles reporting about it. The knowledge is represented by a topic hierarchy presenting topics at different levels of granularity. Though topic (hierarchy) mining from text has been researched a lot, incremental learning from similar events remains under developed. In this paper, we propose a scalable two-phase framework to incrementally learn a topic hierarchy for a subject from events on the subject as the events occur. First, we recursively construct a topic hierarchy for each event based on a novel topic model considering the named entities and entity types in news articles. Second, we incrementally merge the topic hierarchies through top-down hierarchical topic alignment. Extensive experimental results on real datasets demonstrate the effectiveness and efficiency of the proposed framework in terms of both qualitative and quantitative measures."
158,"Abstract The reviews in social media are produced continuously by a large and uncontrolled number of users. To capture the mixture of sentiment and topics simultaneously in reviews is still a challenging task. In this paper, we present a novel probabilistic model framework based on the non-parametric hierarchical Dirichlet process (HDP) topic model, called non-parametric joint sentiment topic mixture model (NJST), which adds a sentiment level to the HDP topic model and detects sentiment and topics simultaneously from reviews. Then considered the dynamic nature of social media data, we propose dynamic NJST (dNJST) which adds time decay dependencies of historical epochs to the current epochs. Compared with the existing sentiment topic mixture models which are based on latent Dirichlet allocation (LDA), the biggest difference of NJST and dNJST is that they can determine topic number automatically. We implement NJST and dNJST with online variational inference algorithms, and incorporate the sentiment priors of words into NJST and dNJST with HowNet lexicon. The experiment results in some Chinese social media dataset show that dNJST can effectively detect and track dynamic sentiment and topics.",2015-01-01,2-s2.0-84928068815,Knowledge-Based Systems,Dynamic non-parametric joint sentiment topic mixture model,"Abstract The reviews in social media are produced continuously by a large and uncontrolled number of users. To capture the mixture of sentiment and topics simultaneously in reviews is still a challenging task. In this paper, we present a novel probabilistic model framework based on the non-parametric hierarchical Dirichlet process (HDP) topic model, called non-parametric joint sentiment topic mixture model (NJST), which adds a sentiment level to the HDP topic model and detects sentiment and topics simultaneously from reviews. Then considered the dynamic nature of social media data, we propose dynamic NJST (dNJST) which adds time decay dependencies of historical epochs to the current epochs. Compared with the existing sentiment topic mixture models which are based on latent Dirichlet allocation (LDA), the biggest difference of NJST and dNJST is that they can determine topic number automatically. We implement NJST and dNJST with online variational inference algorithms, and incorporate the sentiment priors of words into NJST and dNJST with HowNet lexicon. The experiment results in some Chinese social media dataset show that dNJST can effectively detect and track dynamic sentiment and topics."
159,"Search engine query logs are recognized as an important information source that contains millions of users' web search needs. Discovering Geographic Web Search Topics (G-WSTs) from a query log can support a variety of downstream web applications such as finding commonality between locations and profiling search engine users. However, the task of discovering G-WSTs is nontrivial, not only because of the diversity of the information in web search but also due to the sheer size of query log. In this paper, we propose a new framework, Scalable Geographic Web Search Topic Discovery (SG-WSTD), which contains highly scalable functionalities such as search session derivation, geographic information extraction and geographic web search topic discovery to discover G-WSTs from query log. Within SG-WSTD, two probabilistic topic models are proposed to discover G-WSTs from two complementary perspectives. The first one is the Discrete Search Topic Model (DSTM), which discovers G-WSTs that capture the commonalities between discrete locations. The second is the Regional Search Topic Model (RSTM), which focuses on a specific geographic region on the map and discovers G-WSTs that demonstrate geographic locality. Since query log is typically voluminous, we implement the functionalities in SG-WSTD based on the MapReduce paradigm to solve the efficiency bottleneck. We evaluate SG-WSTD against several strong baselines on a real-life query log from AOL. The proposed framework demonstrates significantly improved data interpretability, better prediction performance, higher topic distinctiveness and superior scalability in the experimentation.",2015-01-01,2-s2.0-84929502247,Knowledge-Based Systems,SG-WSTD: A framework for scalable geographic web search topic discovery,"Search engine query logs are recognized as an important information source that contains millions of users' web search needs. Discovering Geographic Web Search Topics (G-WSTs) from a query log can support a variety of downstream web applications such as finding commonality between locations and profiling search engine users. However, the task of discovering G-WSTs is nontrivial, not only because of the diversity of the information in web search but also due to the sheer size of query log. In this paper, we propose a new framework, Scalable Geographic Web Search Topic Discovery (SG-WSTD), which contains highly scalable functionalities such as search session derivation, geographic information extraction and geographic web search topic discovery to discover G-WSTs from query log. Within SG-WSTD, two probabilistic topic models are proposed to discover G-WSTs from two complementary perspectives. The first one is the Discrete Search Topic Model (DSTM), which discovers G-WSTs that capture the commonalities between discrete locations. The second is the Regional Search Topic Model (RSTM), which focuses on a specific geographic region on the map and discovers G-WSTs that demonstrate geographic locality. Since query log is typically voluminous, we implement the functionalities in SG-WSTD based on the MapReduce paradigm to solve the efficiency bottleneck. We evaluate SG-WSTD against several strong baselines on a real-life query log from AOL. The proposed framework demonstrates significantly improved data interpretability, better prediction performance, higher topic distinctiveness and superior scalability in the experimentation."
160,"In the context-based image retrieval, the textual information surrounding the image plays a central role for ranking returned results. Although this technique outperforms content-based approaches, it may fail when the query keywords does not match the textual content of many documents containing relevant images. In addition, users are usually not experts and provide ambiguous queries that lead to heterogeneous results. To solve these problems, researchers are trying to re-rank primary results using other techniques such as query expansion, concept-based retrieval, etc. In this paper, we propose to use LDA topic model to re-rank results and therefore improve retrieval precision. We apply this model in two levels: global level represented by the whole document containing the image and local level represented by the paragraph containing an image (considered as a specific textual information for the image). Results show a significant improvement over the standard text retrieval approach by re-ranking with the LDA model applied to the local level.",2015-01-01,2-s2.0-84955284482,Lecture Notes in Business Information Processing,An LDA topic model adaptation for context-based image retrieval,"In the context-based image retrieval, the textual information surrounding the image plays a central role for ranking returned results. Although this technique outperforms content-based approaches, it may fail when the query keywords does not match the textual content of many documents containing relevant images. In addition, users are usually not experts and provide ambiguous queries that lead to heterogeneous results. To solve these problems, researchers are trying to re-rank primary results using other techniques such as query expansion, concept-based retrieval, etc. In this paper, we propose to use LDA topic model to re-rank results and therefore improve retrieval precision. We apply this model in two levels: global level represented by the whole document containing the image and local level represented by the paragraph containing an image (considered as a specific textual information for the image). Results show a significant improvement over the standard text retrieval approach by re-ranking with the LDA model applied to the local level."
161,"A variety of generative topic models have been successfully applied to model corpus of documents with continuous metadata. But there is no efficient model dealing with documents having a user-item-word structure. This structure forms a 3-way text tensor, and texts correlate with each other through users and items. In this paper we propose an elegant Tensor topic model (TTM) for text tensors inspired by Tucker decomposition, in which both user and item dimensions are co-reduced together with vocabulary space. So we get low-rank representations for not only words but also users and items from TTM. Also, general rules are developed to transform decomposition model into a probabilistic one. Experiments show that TTM outperforms existing topic models in modeling texts with a user-item-word structure.",2014-01-01,2-s2.0-84920712716,"Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014",Collaborative topic modeling for text tensors,"A variety of generative topic models have been successfully applied to model corpus of documents with continuous metadata. But there is no efficient model dealing with documents having a user-item-word structure. This structure forms a 3-way text tensor, and texts correlate with each other through users and items. In this paper we propose an elegant Tensor topic model (TTM) for text tensors inspired by Tucker decomposition, in which both user and item dimensions are co-reduced together with vocabulary space. So we get low-rank representations for not only words but also users and items from TTM. Also, general rules are developed to transform decomposition model into a probabilistic one. Experiments show that TTM outperforms existing topic models in modeling texts with a user-item-word structure."
162,"Many articles in the online encyclopedia Wikipedia have hyperlinks to ambiguous article titles; these ambiguous links should be replaced with links to unambiguous articles, a process known as disambiguation. We propose a novel statistical topic model based on link text, which we refer to as the Link Text Topic Model (LTTM), that we use to suggest new link targets for ambiguous links. To evaluate our model, we describe a method for extracting ground truth for this link disambiguation task from edits made to Wikipedia in a specific time period. We use this ground truth to demonstrate the superiority of LTTM over other existing link- and content-based approaches to disambiguating links in Wikipedia. Finally, we build a web service that uses LTTM to make suggestions to human editors wanting to fix ambiguous links in Wikipedia.",2014-01-01,2-s2.0-84904755752,ACM Transactions on Information Systems,Topic modeling for wikipedia link disambiguation,"Many articles in the online encyclopedia Wikipedia have hyperlinks to ambiguous article titles; these ambiguous links should be replaced with links to unambiguous articles, a process known as disambiguation. We propose a novel statistical topic model based on link text, which we refer to as the Link Text Topic Model (LTTM), that we use to suggest new link targets for ambiguous links. To evaluate our model, we describe a method for extracting ground truth for this link disambiguation task from edits made to Wikipedia in a specific time period. We use this ground truth to demonstrate the superiority of LTTM over other existing link- and content-based approaches to disambiguating links in Wikipedia. Finally, we build a web service that uses LTTM to make suggestions to human editors wanting to fix ambiguous links in Wikipedia."
163,"What the network learners have said in network learning forum posts directly reflects the needs of the learners during the network learning. By mining topics from forum posts, the network learning support service could be greatly improved. For this purpose, the Learner-Topic model was employed in this paper to mine the learner's posts for discovering its topics. And then use the topics for learning resources recommendation. © 2014 WIT Press.",2014-01-01,2-s2.0-84903172778,WIT Transactions on Information and Communication Technologies,Network learning forum posts topic discovery,"What the network learners have said in network learning forum posts directly reflects the needs of the learners during the network learning. By mining topics from forum posts, the network learning support service could be greatly improved. For this purpose, the Learner-Topic model was employed in this paper to mine the learner's posts for discovering its topics. And then use the topics for learning resources recommendation. "
164,The proceedings contain 56 papers. The topics discussed include: computational experiment of service policy in collaborative procurement; a visualized framework of automatic orchestration engine supporting hybrid cloud resources; a human-centric user model for intelligent healthcare; dominant bidding strategy in mobile app advertising auction; personal healthcare record integration method based on linked data model; optimistic fair-exchange with anonymity for bitcoin users; value evaluation of enterprise management informatization based on comprehensive method; research on undergraduates' perception of WeChat acceptance; in-house crowdsourcing-based entity resolution: dealing with common names; collaborative topic modeling for text tensors; and price pattern detection using finite state machine with fuzzy transitions.,2014-01-01,2-s2.0-84925679353,"Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014","Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014",The proceedings contain 56 papers. The topics discussed include: computational experiment of service policy in collaborative procurement; a visualized framework of automatic orchestration engine supporting hybrid cloud resources; a human-centric user model for intelligent healthcare; dominant bidding strategy in mobile app advertising auction; personal healthcare record integration method based on linked data model; optimistic fair-exchange with anonymity for bitcoin users; value evaluation of enterprise management informatization based on comprehensive method; research on undergraduates' perception of WeChat acceptance; in-house crowdsourcing-based entity resolution: dealing with common names; collaborative topic modeling for text tensors; and price pattern detection using finite state machine with fuzzy transitions.
165,"We are developing indicators for the emergence of science and technology (S&T) topics. To do so, we extract information from various S&T information resources. This paper compares alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and term commonality weighting. We compare topic modeling to Principal Components Analysis for a test set of 4104 abstract records on Dye-Sensitized Solar Cells. Results suggest potential to enhance understanding regarding technological topics to help track technological emergence. © 2013 Elsevier B.V.",2014-01-01,2-s2.0-84902280190,Journal of Engineering and Technology Management - JET-M,Comparing methods to extract technical content for technological intelligence,"We are developing indicators for the emergence of science and technology (S&T) topics. To do so, we extract information from various S&T information resources. This paper compares alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and term commonality weighting. We compare topic modeling to Principal Components Analysis for a test set of 4104 abstract records on Dye-Sensitized Solar Cells. Results suggest potential to enhance understanding regarding technological topics to help track technological emergence. "
166,"The rise of online social media has led to an explosion in user-generated content. However, user-generated content is difficult to analyze in isolation from its context. Accordingly, context detection and tracking its evolution is essential to understanding social media. This paper presents a statistical model that can detect interpretable topics along with their contexts. A topic is represented by a cluster of words that frequently occur together, and a context is represented by a cluster of hashtags that frequently occur with a topic. The model combines a context with a related topic by jointly modeling words with hashtags and time. Experiments on real datasets demonstrate that the proposed model successfully discovers both meaningful topics and contexts, and tracks their evolution.",2014-01-01,2-s2.0-84937715645,"International Conference on Information and Knowledge Management, Proceedings",Context over time: Modeling context evolution in social media,"The rise of online social media has led to an explosion in user-generated content. However, user-generated content is difficult to analyze in isolation from its context. Accordingly, context detection and tracking its evolution is essential to understanding social media. This paper presents a statistical model that can detect interpretable topics along with their contexts. A topic is represented by a cluster of words that frequently occur together, and a context is represented by a cluster of hashtags that frequently occur with a topic. The model combines a context with a related topic by jointly modeling words with hashtags and time. Experiments on real datasets demonstrate that the proposed model successfully discovers both meaningful topics and contexts, and tracks their evolution."
167,"It remains a challenge to deal with the diversity of the cross-domain feature space when using transfer learning in the recommendation system. To solve the difficulty, we propose a fusion topic model to extract the latent topic in the crossdomain. There are two layers in the proposed model. Firstly, the model simulates the user–item relationship in every subdomain with an author–topic model separately, and extracts the subdomain level topics. In addition, the model extracts the full-domain level topics in the whole domain using subdomain level topics as words in the author–topic model. By using Gibbs sampling, the method can extract two different levels of topics. The experiment on the public dataset shows our method has good performance. The results of the experiment indicate that extracting multilevel topics can help to discover the correlation in the MovieLens dataset and the Book-Crossing dataset, and to extract the crossdomain feature space.",2014-01-01,2-s2.0-84907864904,WIT Transactions on Information and Communication Technologies,Fusion topic model transfer learning for cross-domain recommendation,"It remains a challenge to deal with the diversity of the cross-domain feature space when using transfer learning in the recommendation system. To solve the difficulty, we propose a fusion topic model to extract the latent topic in the crossdomain. There are two layers in the proposed model. Firstly, the model simulates the user–item relationship in every subdomain with an author–topic model separately, and extracts the subdomain level topics. In addition, the model extracts the full-domain level topics in the whole domain using subdomain level topics as words in the author–topic model. By using Gibbs sampling, the method can extract two different levels of topics. The experiment on the public dataset shows our method has good performance. The results of the experiment indicate that extracting multilevel topics can help to discover the correlation in the MovieLens dataset and the Book-Crossing dataset, and to extract the crossdomain feature space."
168,"There are many news articles reported online everyday. Within an ongoing topic, people can find a huge amount of news articles. A topic often consists of several events, and people are interested in the whole evolution of a topic along a timeline. This requests for finding and identifying the dependent relationships between events. In order to understand the whole evolution of a topic effectively, we propose a framework of event relationship analysis. We define three kinds of event relationships which are coccurrence dependence relationship, event reference relationship, and temporal proximity relationship for modeling how an event is dependent on another event within a topic. Through combining three kinds of relationships, we can discover an Event Evolution Graph (EEG) for users to view the evolution of a topic. Experiments conducted on a real data set show that our method outperforms baseline methods.",2014-01-01,2-s2.0-84920730935,"Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014",Discovering event evolution graphs based on news articles relationships,"There are many news articles reported online everyday. Within an ongoing topic, people can find a huge amount of news articles. A topic often consists of several events, and people are interested in the whole evolution of a topic along a timeline. This requests for finding and identifying the dependent relationships between events. In order to understand the whole evolution of a topic effectively, we propose a framework of event relationship analysis. We define three kinds of event relationships which are coccurrence dependence relationship, event reference relationship, and temporal proximity relationship for modeling how an event is dependent on another event within a topic. Through combining three kinds of relationships, we can discover an Event Evolution Graph (EEG) for users to view the evolution of a topic. Experiments conducted on a real data set show that our method outperforms baseline methods."
169,"Managers and researchers alike have long recognized the importance of corporate textual risk disclosures. Yet it is a nontrivial task to discover and quantify variables of interest from unstructured text. In this paper, we develop a variation of the latent Dirichlet allocation topic model and its learning algorithm for simultaneously discovering and quantifying risk types from textual risk disclosures. We conduct comprehensive evaluations in terms of both conventional statistical fit and substantive fit with respect to the quality of discovered information. Experimental results show that our proposed method outperforms all competing methods, and could find more meaningful topics (risk types). By taking advantage of our proposed method for measuring risk types from textual data, we study how risk disclosures in 10-K forms affect the risk perceptions of investors. Different from prior studies, our results provide support for all three competing arguments regarding whether and how risk disclosures affect the risk perceptions of investors, depending on the specific risk types disclosed. We find that around two-thirds of risk types lack informativeness and have no significant influence. Moreover, we find that the informative risk types do not necessarily increase the risk perceptions of investors-the disclosure of three types of systematic and liquidity risks will increase the risk perceptions of investors, whereas the other five types of unsystematic risks will decrease them. © 2014 INFORMS.",2014-01-01,2-s2.0-84902288534,Management Science,Simultaneously discovering and quantifying risk types from textual risk disclosures,"Managers and researchers alike have long recognized the importance of corporate textual risk disclosures. Yet it is a nontrivial task to discover and quantify variables of interest from unstructured text. In this paper, we develop a variation of the latent Dirichlet allocation topic model and its learning algorithm for simultaneously discovering and quantifying risk types from textual risk disclosures. We conduct comprehensive evaluations in terms of both conventional statistical fit and substantive fit with respect to the quality of discovered information. Experimental results show that our proposed method outperforms all competing methods, and could find more meaningful topics (risk types). By taking advantage of our proposed method for measuring risk types from textual data, we study how risk disclosures in 10-K forms affect the risk perceptions of investors. Different from prior studies, our results provide support for all three competing arguments regarding whether and how risk disclosures affect the risk perceptions of investors, depending on the specific risk types disclosed. We find that around two-thirds of risk types lack informativeness and have no significant influence. Moreover, we find that the informative risk types do not necessarily increase the risk perceptions of investors-the disclosure of three types of systematic and liquidity risks will increase the risk perceptions of investors, whereas the other five types of unsystematic risks will decrease them. "
170,"In consideration of the features of micro-blogging content such as short text, sparse feature words and the huge scale, a method to detect micro-blogging hot topic was proposed in this paper based on MapReduce programming model. This method first employs the hidden topic analysis to solve the problem of short micro-blogging content and sparse feature words. Then the CURE algorithm is used to alleviate the problem that the Kmeans algorithm is sensitive to the initial points. Finally, the hot topic clustering results are obtained through the parallel Kmeans clustering algorithm based on the MapReduce programming model. The experimental results show that proposed method can effectively improve the micro-blogging hot topic detection efficiency.",2014-01-01,2-s2.0-84928012766,"International Conference on Logistics, Engineering, Management and Computer Science, LEMCS 2014",A distributed approach for Chinese microblog hot topic detection,"In consideration of the features of micro-blogging content such as short text, sparse feature words and the huge scale, a method to detect micro-blogging hot topic was proposed in this paper based on MapReduce programming model. This method first employs the hidden topic analysis to solve the problem of short micro-blogging content and sparse feature words. Then the CURE algorithm is used to alleviate the problem that the Kmeans algorithm is sensitive to the initial points. Finally, the hot topic clustering results are obtained through the parallel Kmeans clustering algorithm based on the MapReduce programming model. The experimental results show that proposed method can effectively improve the micro-blogging hot topic detection efficiency."
171,"With the considerable growth of user-generated content, online reviews are becoming extremely valuable sources for mining customers' opinions on products and services. However, most of the traditional opinion mining methods are coarse-grained and cannot understand natural languages. Thus, aspect-based opinion mining and summarization are of great interest in academic and industrial research. In this paper, we study an approach to extract product and service aspect words, as well as sentiment words, automatically from reviews. An unsupervised dependency analysis-based approach is presented to extract Appraisal Expression Patterns (AEPs) from reviews, which represent the manner in which people express opinions regarding products or services and can be regarded as a condensed representation of the syntactic relationship between aspect and sentiment words. AEPs are high-level, domain-independent types of information, and have excellent domain adaptability. An AEP-based Latent Dirichlet Allocation (AEP-LDA) model is also proposed. This is a sentence-level, probabilistic generative model which assumes that all words in a sentence are drawn from one topic - a generally true assumption, based on our observation. The model also assumes that every review corpus is composed of several mutually corresponding aspect and sentiment topics, as well as a background word topic. The AEP information is incorporated into the AEP-LDA model for mining aspect and sentiment words simultaneously. The experimental results on reviews of restaurants, hotels, MP3 players, and cameras show that the AEP-LDA model outperforms other approaches in identifying aspect and sentiment words. © 2014 Elsevier B.V. All rights reserved.",2014-01-01,2-s2.0-84897411279,Knowledge-Based Systems,Incorporating appraisal expression patterns into topic modeling for aspect and sentiment word identification,"With the considerable growth of user-generated content, online reviews are becoming extremely valuable sources for mining customers' opinions on products and services. However, most of the traditional opinion mining methods are coarse-grained and cannot understand natural languages. Thus, aspect-based opinion mining and summarization are of great interest in academic and industrial research. In this paper, we study an approach to extract product and service aspect words, as well as sentiment words, automatically from reviews. An unsupervised dependency analysis-based approach is presented to extract Appraisal Expression Patterns (AEPs) from reviews, which represent the manner in which people express opinions regarding products or services and can be regarded as a condensed representation of the syntactic relationship between aspect and sentiment words. AEPs are high-level, domain-independent types of information, and have excellent domain adaptability. An AEP-based Latent Dirichlet Allocation (AEP-LDA) model is also proposed. This is a sentence-level, probabilistic generative model which assumes that all words in a sentence are drawn from one topic - a generally true assumption, based on our observation. The model also assumes that every review corpus is composed of several mutually corresponding aspect and sentiment topics, as well as a background word topic. The AEP information is incorporated into the AEP-LDA model for mining aspect and sentiment words simultaneously. The experimental results on reviews of restaurants, hotels, MP3 players, and cameras show that the AEP-LDA model outperforms other approaches in identifying aspect and sentiment words. "
172,"One of the most challenging problems in aspect-based opinion mining is aspect extraction, which aims to identify expressions that describe aspects of products (called aspect expressions) and categorize domain-specific synonymous expressions. Although a number of methods of aspect extraction have been proposed before, very few of them are designed to improve the interpretability of generated aspects. Existing methods either generate multiple fine-grained aspects without proper categorization or categorize semantically unrelated product aspects (e.g., by unsupervised topic modeling). In this paper, we first examine previous studies on product aspect extraction. To overcome the limitations of existing methods, two novel semi-supervised models for product aspect extraction are then proposed. More specifically, the proposed methodology first extracts seeding aspects and related terms from detailed product descriptions readily available on E-commerce websites. Next, product reviews are regrouped according to these seeding aspects so that more effective textual contexts for topic modeling are built. Finally, two novel semi-supervised topic models are developed to extract human-comprehensible product aspects. For the first proposed topic model, the Fine-grained Labeled LDA (FL-LDA), seeding aspects are applied to guide the model to discover words that are related to these seeding aspects. For the second model, the Unified Fine-grained Labeled LDA (UFL-LDA), we incorporate unlabeled documents to extend the FL-LDA model so that words related to the seeding aspects or other high-frequency words in customer reviews are extracted. Our experimental results demonstrate that the proposed methods outperform state-of-The-art methods.",2014-01-01,2-s2.0-84908025051,Knowledge-Based Systems,Product aspect extraction supervised with online domain knowledge,"One of the most challenging problems in aspect-based opinion mining is aspect extraction, which aims to identify expressions that describe aspects of products (called aspect expressions) and categorize domain-specific synonymous expressions. Although a number of methods of aspect extraction have been proposed before, very few of them are designed to improve the interpretability of generated aspects. Existing methods either generate multiple fine-grained aspects without proper categorization or categorize semantically unrelated product aspects (e.g., by unsupervised topic modeling). In this paper, we first examine previous studies on product aspect extraction. To overcome the limitations of existing methods, two novel semi-supervised models for product aspect extraction are then proposed. More specifically, the proposed methodology first extracts seeding aspects and related terms from detailed product descriptions readily available on E-commerce websites. Next, product reviews are regrouped according to these seeding aspects so that more effective textual contexts for topic modeling are built. Finally, two novel semi-supervised topic models are developed to extract human-comprehensible product aspects. For the first proposed topic model, the Fine-grained Labeled LDA (FL-LDA), seeding aspects are applied to guide the model to discover words that are related to these seeding aspects. For the second model, the Unified Fine-grained Labeled LDA (UFL-LDA), we incorporate unlabeled documents to extend the FL-LDA model so that words related to the seeding aspects or other high-frequency words in customer reviews are extracted. Our experimental results demonstrate that the proposed methods outperform state-of-The-art methods."
173,"Twitter is used extensively in the United States as well as globally, creating many opportunities to augment decision support systems with Twitter-driven predictive analytics. Twitter is an ideal data source for decision support: its users, who number in the millions, publicly discuss events, emotions, and innumerable other topics; its content is authored and distributed in real time at no charge; and individual messages (also known as tweets) are often tagged with precise spatial and temporal coordinates. This article presents research investigating the use of spatiotemporally tagged tweets for crime prediction. We use Twitter-specific linguistic analysis and statistical topic modeling to automatically identify discussion topics across a major city in the United States. We then incorporate these topics into a crime prediction model and show that, for 19 of the 25 crime types we studied, the addition of Twitter data improves crime prediction performance versus a standard approach based on kernel density estimation. We identify a number of performance bottlenecks that could impact the use of Twitter in an actual decision support system. We also point out important areas of future work for this research, including deeper semantic analysis of message content, temporal modeling, and incorporation of auxiliary data sources. This research has implications specifically for criminal justice decision makers in charge of resource allocation for crime prevention. More generally, this research has implications for decision makers concerned with geographic spaces occupied by Twitter-using individuals. ©2014 Elsevier B.V. All rights reserved.",2014-01-01,2-s2.0-84897492871,Decision Support Systems,Predicting crime using Twitter and kernel density estimation,"Twitter is used extensively in the United States as well as globally, creating many opportunities to augment decision support systems with Twitter-driven predictive analytics. Twitter is an ideal data source for decision support: its users, who number in the millions, publicly discuss events, emotions, and innumerable other topics; its content is authored and distributed in real time at no charge; and individual messages (also known as tweets) are often tagged with precise spatial and temporal coordinates. This article presents research investigating the use of spatiotemporally tagged tweets for crime prediction. We use Twitter-specific linguistic analysis and statistical topic modeling to automatically identify discussion topics across a major city in the United States. We then incorporate these topics into a crime prediction model and show that, for 19 of the 25 crime types we studied, the addition of Twitter data improves crime prediction performance versus a standard approach based on kernel density estimation. We identify a number of performance bottlenecks that could impact the use of Twitter in an actual decision support system. We also point out important areas of future work for this research, including deeper semantic analysis of message content, temporal modeling, and incorporation of auxiliary data sources. This research has implications specifically for criminal justice decision makers in charge of resource allocation for crime prevention. More generally, this research has implications for decision makers concerned with geographic spaces occupied by Twitter-using individuals. "
174,"We study the problem of recommending scientific articles to users in an online community and present a novel matrix factorization model, the topic regression Matrix Factorization (tr-MF), to solve the problem. The main idea of tr-MF lies in extending the matrix factorization with a probabilistic topic modeling. Instead of regularizing item factors through the probabilistic topic modeling as in the framework of the CTR model, tr-MF introduces a regression model to regularize user factors through the probabilistic topic modeling under the basic hypothesis that users share the similar preferences if they rate similar sets of items. Consequently, tr-MF provides interpretable latent factors for users and items, and makes accurate predictions for community users. Specifically, it is effective in making predictions for users with only few ratings or even no ratings, and supports tasks that are specific to a certain field, neither of which is addressed in the existing literature. Further, we demonstrate the efficacy of tr-MF on a large subset of the data from CiteULike, a bibliography sharing service dataset. The proposed model outperforms the state-of-the-art matrix factorization models with a significant margin. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889559998,"International Conference on Information and Knowledge Management, Proceedings",Scientific articles recommendation,"We study the problem of recommending scientific articles to users in an online community and present a novel matrix factorization model, the topic regression Matrix Factorization (tr-MF), to solve the problem. The main idea of tr-MF lies in extending the matrix factorization with a probabilistic topic modeling. Instead of regularizing item factors through the probabilistic topic modeling as in the framework of the CTR model, tr-MF introduces a regression model to regularize user factors through the probabilistic topic modeling under the basic hypothesis that users share the similar preferences if they rate similar sets of items. Consequently, tr-MF provides interpretable latent factors for users and items, and makes accurate predictions for community users. Specifically, it is effective in making predictions for users with only few ratings or even no ratings, and supports tasks that are specific to a certain field, neither of which is addressed in the existing literature. Further, we demonstrate the efficacy of tr-MF on a large subset of the data from CiteULike, a bibliography sharing service dataset. The proposed model outperforms the state-of-the-art matrix factorization models with a significant margin. "
175,"Many applications require analyzing textual topics in conjunction with external time series variables such as stock prices. We develop a novel general text mining framework for discovering such causal topics from text. Our framework naturally combines any given probabilistic topic model with time-series causal analysis to discover topics that are both coherent semantically and correlated with time series data. We iteratively refine topics, increasing the correlation of discovered topics with the time series. Time series data provides feedback at each iteration by imposing prior distributions on parameters. Experimental results show that the proposed framework is effective. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889604012,"International Conference on Information and Knowledge Management, Proceedings",Mining causal topics in text data: Iterative topic modeling with time series feedback,"Many applications require analyzing textual topics in conjunction with external time series variables such as stock prices. We develop a novel general text mining framework for discovering such causal topics from text. Our framework naturally combines any given probabilistic topic model with time-series causal analysis to discover topics that are both coherent semantically and correlated with time series data. We iteratively refine topics, increasing the correlation of discovered topics with the time series. Time series data provides feedback at each iteration by imposing prior distributions on parameters. Experimental results show that the proposed framework is effective. "
176,"Interactive web search involves selecting which documents to read further and locating the parts of the documents that are relevant to the user's current activity. In this paper, we introduce UIMaP: User Interest Modeling and Personalization, a search task based personal user interest model to support users' information gathering tasks. The novelty of our approach lies in the use of topic modeling to generate fine-grained models of user interest and visualizations that direct user's attention to documents or parts of documents that match user's inferred interests. User annotations are used to help generate personalized visualizations for user's search tasks. Based on 1267 user annotations from 17 users, we show the performance comparisons of four different topic models: LDA+H, LDA+KL, LDA+JSD, and LDA+TopN. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889589230,"International Conference on Information and Knowledge Management, Proceedings",Mining user interest from search tasks and annotations,"Interactive web search involves selecting which documents to read further and locating the parts of the documents that are relevant to the user's current activity. In this paper, we introduce UIMaP: User Interest Modeling and Personalization, a search task based personal user interest model to support users' information gathering tasks. The novelty of our approach lies in the use of topic modeling to generate fine-grained models of user interest and visualizations that direct user's attention to documents or parts of documents that match user's inferred interests. User annotations are used to help generate personalized visualizations for user's search tasks. Based on 1267 user annotations from 17 users, we show the performance comparisons of four different topic models: LDA+H, LDA+KL, LDA+JSD, and LDA+TopN. "
177,"In this paper, we propose a framework of recommending users and communities in social media. Given a user's profile, our framework is capable of recommending influential users and topic-cohesive interactive communities that are most relevant to the given user. In our framework, we present a generative topic model to discover user-oriented and community-oriented topics simultaneously, which enables us to capture the exact topic interests of users, as well as the focuses of communities. Extensive evaluation on a data set obtained from Twitter has demonstrated the effectiveness of our proposed framework compared with other probabilistic topic model based recommendation methods. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889590408,"International Conference on Information and Knowledge Management, Proceedings",FRec: A novel framework of recommending users and communities in social media,"In this paper, we propose a framework of recommending users and communities in social media. Given a user's profile, our framework is capable of recommending influential users and topic-cohesive interactive communities that are most relevant to the given user. In our framework, we present a generative topic model to discover user-oriented and community-oriented topics simultaneously, which enables us to capture the exact topic interests of users, as well as the focuses of communities. Extensive evaluation on a data set obtained from Twitter has demonstrated the effectiveness of our proposed framework compared with other probabilistic topic model based recommendation methods. "
178,"This paper studies text summarization by extracting hierarchical topics from a given collection of documents. We propose a new approach of text modeling via network analysis. We convert documents into a word influence network, and find the words summarizing the major topics with an efficient influence maximization algorithm. Besides, the influence capability of the topic words on other words in the network reveal the relations among the topic words. Then we cluster the words and build hierarchies for the topics. Experiments on large collections of Web documents show that a simple method based on the influence analysis is effective, compared with existing generative topic modeling and random walk based ranking. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889577016,"International Conference on Information and Knowledge Management, Proceedings",Content coverage maximization on word networks for hierarchical topic summarization,"This paper studies text summarization by extracting hierarchical topics from a given collection of documents. We propose a new approach of text modeling via network analysis. We convert documents into a word influence network, and find the words summarizing the major topics with an efficient influence maximization algorithm. Besides, the influence capability of the topic words on other words in the network reveal the relations among the topic words. Then we cluster the words and build hierarchies for the topics. Experiments on large collections of Web documents show that a simple method based on the influence analysis is effective, compared with existing generative topic modeling and random walk based ranking. "
179,"We build a system to extract user interests from Twitter messages. Specifically, we extract interest candidates using linguistic patterns and rank them using four different keyphrase ranking techniques: TFIDF, TextRank, LDA-TextRank, and Relevance-Interestingness-Rank (RI-Rank). We also explore the complementary relation between TFIDF and TextRank in ranking interest candidates. Top ranked interests are evaluated with user feedback gathered from an online survey. The results show that TFIDF and TextRank are both suitable for extracting user interests from tweets. Moreover, the combination of TFIDF and TextRank consistently yields the highest user positive feedback. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889571444,"International Conference on Information and Knowledge Management, Proceedings",Interest mining from user tweets,"We build a system to extract user interests from Twitter messages. Specifically, we extract interest candidates using linguistic patterns and rank them using four different keyphrase ranking techniques: TFIDF, TextRank, LDA-TextRank, and Relevance-Interestingness-Rank (RI-Rank). We also explore the complementary relation between TFIDF and TextRank in ranking interest candidates. Top ranked interests are evaluated with user feedback gathered from an online survey. The results show that TFIDF and TextRank are both suitable for extracting user interests from tweets. Moreover, the combination of TFIDF and TextRank consistently yields the highest user positive feedback. "
180,"Microblogging platforms, such as Twitter, already play an important role in cultural, social and political events around the world. Discovering high-level topics from social streams is therefore important for many downstream applications. However, traditional text mining methods that rely on the bag-of-words model are insufficient to uncover the rich semantics and temporal aspects of topics in Twitter. In particular, topics in Twitter are inherently dynamic and often focus on specific entities, such as people or organizations. In this paper, we therefore propose a method for mining multi-faceted topics from Twitter streams. The Multi-Faceted Topic Model (MfTM) is proposed to jointly model latent semantics among terms and entities and captures the temporal characteristics of each topic. We develop an efficient online inference method for MfTM, which enables our model to be applied to large-scale and streaming data. Our experimental evaluation shows the effectiveness and efficiency of our model compared with state-of-the-art baselines. We further demonstrate the effectiveness of our framework in the context of tweet clustering. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889594913,"International Conference on Information and Knowledge Management, Proceedings",Dynamic multi-faceted topic discovery in twitter,"Microblogging platforms, such as Twitter, already play an important role in cultural, social and political events around the world. Discovering high-level topics from social streams is therefore important for many downstream applications. However, traditional text mining methods that rely on the bag-of-words model are insufficient to uncover the rich semantics and temporal aspects of topics in Twitter. In particular, topics in Twitter are inherently dynamic and often focus on specific entities, such as people or organizations. In this paper, we therefore propose a method for mining multi-faceted topics from Twitter streams. The Multi-Faceted Topic Model (MfTM) is proposed to jointly model latent semantics among terms and entities and captures the temporal characteristics of each topic. We develop an efficient online inference method for MfTM, which enables our model to be applied to large-scale and streaming data. Our experimental evaluation shows the effectiveness and efficiency of our model compared with state-of-the-art baselines. We further demonstrate the effectiveness of our framework in the context of tweet clustering. "
181,"Community Question Answering (CQA) websites, where people share expertise on open platforms, have become large repositories of valuable knowledge. To bring the best value out of these knowledge repositories, it is critically important for CQA services to know how to find the right experts, retrieve archived similar questions and recommend best answers to new questions. To tackle this cluster of closely related problems in a principled approach, we proposed Topic Expertise Model (TEM), a novel probabilistic generative model with GMM hybrid, to jointly model topics and expertise by integrating textual content model and link structure analysis. Based on TEM results, we proposed CQARank to measure user interests and expertise score under different topics. Leveraging the question answering history based on long-term community reviews and voting, our method could find experts with both similar topical preference and high topical expertise. Experiments carried out on Stack Overflow data, the largest CQA focused on computer programming, show that our method achieves significant improvement over existing methods on multiple metrics. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889610414,"International Conference on Information and Knowledge Management, Proceedings",CQARank: Jointly model topics and expertise in Community Question Answering,"Community Question Answering (CQA) websites, where people share expertise on open platforms, have become large repositories of valuable knowledge. To bring the best value out of these knowledge repositories, it is critically important for CQA services to know how to find the right experts, retrieve archived similar questions and recommend best answers to new questions. To tackle this cluster of closely related problems in a principled approach, we proposed Topic Expertise Model (TEM), a novel probabilistic generative model with GMM hybrid, to jointly model topics and expertise by integrating textual content model and link structure analysis. Based on TEM results, we proposed CQARank to measure user interests and expertise score under different topics. Leveraging the question answering history based on long-term community reviews and voting, our method could find experts with both similar topical preference and high topical expertise. Experiments carried out on Stack Overflow data, the largest CQA focused on computer programming, show that our method achieves significant improvement over existing methods on multiple metrics. "
182,"A large number of studies have been devoted to modeling the contents and interactions between users on Twitter. In this paper, we propose a method inspired from Social Role Theory (SRT), which assumes that a user behaves differently in different roles in the generation process of Twitter content. We consider the two most distinctive social roles on Twitter: originator and propagator, who respectively posts original messages and retweets or forwards the messages from others. In addition, we also consider role-specific social interactions, especially implicit interactions between users who share some common interests. All the above elements are integrated into a novel regularized topic model. We evaluate the proposed method on real Twitter data. The results show that our method is more effective than the existing ones which do not distinguish social roles. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889562628,"International Conference on Information and Knowledge Management, Proceedings",Originator or propagator? Incorporating social role theory into topic models for twitter content analysis,"A large number of studies have been devoted to modeling the contents and interactions between users on Twitter. In this paper, we propose a method inspired from Social Role Theory (SRT), which assumes that a user behaves differently in different roles in the generation process of Twitter content. We consider the two most distinctive social roles on Twitter: originator and propagator, who respectively posts original messages and retweets or forwards the messages from others. In addition, we also consider role-specific social interactions, especially implicit interactions between users who share some common interests. All the above elements are integrated into a novel regularized topic model. We evaluate the proposed method on real Twitter data. The results show that our method is more effective than the existing ones which do not distinguish social roles. "
183,"Cross-domain text classification aims to automatically train a precise text classifier for a target domain by using labelled text data from a related source domain. To this end, one of the most promising ideas is to induce a new feature representation so that the distributional difference between domains can be reduced and a more accurate classifier can be learned in this new feature space. However, most existing methods do not explore the duality of the marginal distribution of examples and the conditional distribution of class labels given labeled training examples in the source domain. Besides, few previous works attempt to explicitly distinguish the domain-independent and domain-specific latent features and align the domain-specific features to further improve the cross-domain learning. In this paper, we propose a model called Partially Supervised Cross-Collection LDA topic model (PSCCLDA) for cross-domain learning with the purpose of addressing these two issues in a unified way. Experimental results on nine datasets show that our model outperforms two standard classifiers and four state-of-the-art methods, which demonstrates the effectiveness of our proposed model. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889569618,"International Conference on Information and Knowledge Management, Proceedings",A partially supervised cross-collection topic model for cross-domain text classification,"Cross-domain text classification aims to automatically train a precise text classifier for a target domain by using labelled text data from a related source domain. To this end, one of the most promising ideas is to induce a new feature representation so that the distributional difference between domains can be reduced and a more accurate classifier can be learned in this new feature space. However, most existing methods do not explore the duality of the marginal distribution of examples and the conditional distribution of class labels given labeled training examples in the source domain. Besides, few previous works attempt to explicitly distinguish the domain-independent and domain-specific latent features and align the domain-specific features to further improve the cross-domain learning. In this paper, we propose a model called Partially Supervised Cross-Collection LDA topic model (PSCCLDA) for cross-domain learning with the purpose of addressing these two issues in a unified way. Experimental results on nine datasets show that our model outperforms two standard classifiers and four state-of-the-art methods, which demonstrates the effectiveness of our proposed model. "
184,"Dirichlet process mixture (DPM) model is one of the most important Bayesian nonparametric models owing to its efficiency of inference and flexibility for various applications. A fundamental assumption made by DPM model is that all data items are generated from a single, shared DP. This assumption, however, is restrictive in many practical settings where samples are generated from a collection of dependent DPs, each associated with a point in some covariate space. For example, documents in the proceedings of a conference are organized by year, or photos may be tagged and recorded with GPS locations. We present a general method for constructing dependent Dirichlet processes (DP) on arbitrary covariate space. The approach is based on restricting and projecting a DP defined on a space of continuous functions with different domains, which results in a collection of dependent random measures, each associated with a point in covariate space and is marginally DP distributed. The constructed collection of dependent DPs can be used as a nonparametric prior of infinite dynamic mixture models, which allow each mixture component to appear/disappear and vary in a subspace of covariate space. Furthermore, we discuss choices of base distributions of functions in a variety of settings as a flexible method to control dependencies. In addition, we develop an efficient Gibbs sampler for model inference where all underlying random measures are integrated out. Finally, experiment results on temporal modeling and spatial modeling datasets demonstrate the effectiveness of the method in modeling dynamic mixture models on different types of covariates. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889599699,"International Conference on Information and Knowledge Management, Proceedings",Functional dirichlet process,"Dirichlet process mixture (DPM) model is one of the most important Bayesian nonparametric models owing to its efficiency of inference and flexibility for various applications. A fundamental assumption made by DPM model is that all data items are generated from a single, shared DP. This assumption, however, is restrictive in many practical settings where samples are generated from a collection of dependent DPs, each associated with a point in some covariate space. For example, documents in the proceedings of a conference are organized by year, or photos may be tagged and recorded with GPS locations. We present a general method for constructing dependent Dirichlet processes (DP) on arbitrary covariate space. The approach is based on restricting and projecting a DP defined on a space of continuous functions with different domains, which results in a collection of dependent random measures, each associated with a point in covariate space and is marginally DP distributed. The constructed collection of dependent DPs can be used as a nonparametric prior of infinite dynamic mixture models, which allow each mixture component to appear/disappear and vary in a subspace of covariate space. Furthermore, we discuss choices of base distributions of functions in a variety of settings as a flexible method to control dependencies. In addition, we develop an efficient Gibbs sampler for model inference where all underlying random measures are integrated out. Finally, experiment results on temporal modeling and spatial modeling datasets demonstrate the effectiveness of the method in modeling dynamic mixture models on different types of covariates. "
185,"The advent of social media is changing the existing information behavior by letting users access to real-time online information channels without the constraints of time and space. It also generates a huge amount of data worth discovering novel knowledge. Social media, therefore, has created an enormous challenge for scientists trying to keep pace with developments in their field. Most of the previous studies have adopted broadbrush approaches which tend to result in providing limited analysis. To handle these problems properly, we introduce our real-time Twitter trend mining system, RT",2013-08-12,2-s2.0-84881177326,"Proceedings - 2013 International Conference on Social Intelligence and Technology, SOCIETY 2013",RT2M : Real-time twitter trend mining system,"The advent of social media is changing the existing information behavior by letting users access to real-time online information channels without the constraints of time and space. It also generates a huge amount of data worth discovering novel knowledge. Social media, therefore, has created an enormous challenge for scientists trying to keep pace with developments in their field. Most of the previous studies have adopted broadbrush approaches which tend to result in providing limited analysis. To handle these problems properly, we introduce our real-time Twitter trend mining system, RT"
186,"Public health related tweets are difficult to identify in large conversational datasets like Twitter.com. Even more challenging is the visualization and analyses of the spatial patterns encoded in tweets. This study has the following objectives: how can topic modeling be used to identify relevant public health topics such as obesity on Twitter.com? What are the common obesity related themes? What is the spatial pattern of the themes? What are the research challenges of using large conversational datasets from social networking sites? Obesity is chosen as a test theme to demonstrate the effectiveness of topic modeling using Latent Dirichlet Allocation (LDA) and spatial analysis using Geographic Information System (GIS). The dataset is constructed from tweets (originating from the United States) extracted from Twitter.com on obesityrelated queries. Examples of such queries are 'food deserts', 'fast food', and 'childhood obesity'. The tweets are also georeferenced and time stamped. Three cohesive and meaningful themes such as 'childhood obesity and schools', 'obesity prevention', and 'obesity and food habits' are extracted from the LDA model. The GIS analysis of the extracted themes show distinct spatial pattern between rural and urban areas, northern and southern states, and between coasts and inland states. Further, relating the themes with ancillary datasets such as US census and locations of fast food restaurants based upon the location of the tweets in a GIS environment opened new avenues for spatial analyses and mapping. Therefore the techniques used in this study provide a possible toolset for computational social scientists in general, and health researchers in specific, to better understand health problems from large conversational datasets. © 2013 Cartography and Geographic Information Society.",2013-06-10,2-s2.0-84878526821,Cartography and Geographic Information Science,What are we 'tweeting' about obesity? Mapping tweets with topic modeling and Geographic Information System,"Public health related tweets are difficult to identify in large conversational datasets like Twitter.com. Even more challenging is the visualization and analyses of the spatial patterns encoded in tweets. This study has the following objectives: how can topic modeling be used to identify relevant public health topics such as obesity on Twitter.com? What are the common obesity related themes? What is the spatial pattern of the themes? What are the research challenges of using large conversational datasets from social networking sites? Obesity is chosen as a test theme to demonstrate the effectiveness of topic modeling using Latent Dirichlet Allocation (LDA) and spatial analysis using Geographic Information System (GIS). The dataset is constructed from tweets (originating from the United States) extracted from Twitter.com on obesityrelated queries. Examples of such queries are 'food deserts', 'fast food', and 'childhood obesity'. The tweets are also georeferenced and time stamped. Three cohesive and meaningful themes such as 'childhood obesity and schools', 'obesity prevention', and 'obesity and food habits' are extracted from the LDA model. The GIS analysis of the extracted themes show distinct spatial pattern between rural and urban areas, northern and southern states, and between coasts and inland states. Further, relating the themes with ancillary datasets such as US census and locations of fast food restaurants based upon the location of the tweets in a GIS environment opened new avenues for spatial analyses and mapping. Therefore the techniques used in this study provide a possible toolset for computational social scientists in general, and health researchers in specific, to better understand health problems from large conversational datasets. "
187,"With the widespread adoption of social media, online learning communities are perceived as a network of knowledge comprised of interconnected individuals with varying roles. Opinion leaders are important in social networks because of their ability to influence the attitudes and behaviours of others via their superior status, education, and social prestige. Many theories have been put forward to explain the formation, characteristics, and durability of social networks, but few address the issue of opinion leader identification. This paper proposes an improved mix framework for opinion leader identification in online learning communities. The framework is validated by an experimental study. By analysing textual content, user behaviour and time, this study ranked opinion leaders based on four distinguishing features: expertise, novelty, influence, and activity. Furthermore, the performances of opinion leaders were further investigated in terms of longevity and centrality. Experimental study on real datasets has shown that our framework effectively identifies opinion leaders in online learning communities. © 2013 Elsevier B.V. All rights reserved.",2013-03-01,2-s2.0-84875275748,Knowledge-Based Systems,An improved mix framework for opinion leader identification in online learning communities,"With the widespread adoption of social media, online learning communities are perceived as a network of knowledge comprised of interconnected individuals with varying roles. Opinion leaders are important in social networks because of their ability to influence the attitudes and behaviours of others via their superior status, education, and social prestige. Many theories have been put forward to explain the formation, characteristics, and durability of social networks, but few address the issue of opinion leader identification. This paper proposes an improved mix framework for opinion leader identification in online learning communities. The framework is validated by an experimental study. By analysing textual content, user behaviour and time, this study ranked opinion leaders based on four distinguishing features: expertise, novelty, influence, and activity. Furthermore, the performances of opinion leaders were further investigated in terms of longevity and centrality. Experimental study on real datasets has shown that our framework effectively identifies opinion leaders in online learning communities. "
188,"Topic modeling provides a powerful way to analyze the content of a collection of documents. It has become a popular tool in many research areas, such as text mining, information retrieval, natural language processing, and other related fields. In real-world applications, however, the usefulness of topic modeling is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps, such as vastly reducing input vocabulary. In this article we introduce Regularized Latent Semantic Indexing (RLSI)-including a batch version and an online version, referred to as batch RLSI and online RLSI, respectively-to scale up topic modeling. Batch RLSI and online RLSI are as effective as existing topic modeling techniques and can scale to larger datasets without reducing input vocabulary. Moreover, online RLSI can be applied to stream data and can capture the dynamic evolution of topics. Both versions of RLSI formalize topic modeling as a problem of minimizing a quadratic loss function regularized by L",2013-01-01,2-s2.0-84873891666,ACM Transactions on Information Systems,Regularized latent semantic indexing: A new approach to large-scale topic modeling,"Topic modeling provides a powerful way to analyze the content of a collection of documents. It has become a popular tool in many research areas, such as text mining, information retrieval, natural language processing, and other related fields. In real-world applications, however, the usefulness of topic modeling is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps, such as vastly reducing input vocabulary. In this article we introduce Regularized Latent Semantic Indexing (RLSI)-including a batch version and an online version, referred to as batch RLSI and online RLSI, respectively-to scale up topic modeling. Batch RLSI and online RLSI are as effective as existing topic modeling techniques and can scale to larger datasets without reducing input vocabulary. Moreover, online RLSI can be applied to stream data and can capture the dynamic evolution of topics. Both versions of RLSI formalize topic modeling as a problem of minimizing a quadratic loss function regularized by L"
189,"User-generated reviews on the Web reflect users' sentiment about products, services and social events. Existing researches mostly focus on the sentiment classification of the product and service reviews in document level. Reviews of social events such as economic and political activities, which are called social reviews, have specific characteristics different to the reviews of products and services. In this paper, we propose an unsupervised approach to automatically discover the aspects discussed in Chinese social reviews and also the sentiments expressed in different aspects. The approach is called Multi-aspect Sentiment Analysis for Chinese Online Social Reviews (MSA-COSRs). We first apply the Latent Dirichlet Allocation (LDA) model to discover multi-aspect global topics of social reviews, and then extract the local topic and associated sentiment based on a sliding window context over the review text. The aspect of the local topic is identified by a trained LDA model, and the polarity of the associated sentiment is classified by HowNet lexicon. The experiment results show that MSA-COSR cannot only obtain good topic partitioning results, but also help to improve sentiment analysis accuracy. It helps to simultaneously discover multi-aspect fine-grained topics and associated sentiment. © 2012 Elsevier B.V. All rights reserved.",2013-01-01,2-s2.0-84870067197,Knowledge-Based Systems,Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling and HowNet lexicon,"User-generated reviews on the Web reflect users' sentiment about products, services and social events. Existing researches mostly focus on the sentiment classification of the product and service reviews in document level. Reviews of social events such as economic and political activities, which are called social reviews, have specific characteristics different to the reviews of products and services. In this paper, we propose an unsupervised approach to automatically discover the aspects discussed in Chinese social reviews and also the sentiments expressed in different aspects. The approach is called Multi-aspect Sentiment Analysis for Chinese Online Social Reviews (MSA-COSRs). We first apply the Latent Dirichlet Allocation (LDA) model to discover multi-aspect global topics of social reviews, and then extract the local topic and associated sentiment based on a sliding window context over the review text. The aspect of the local topic is identified by a trained LDA model, and the polarity of the associated sentiment is classified by HowNet lexicon. The experiment results show that MSA-COSR cannot only obtain good topic partitioning results, but also help to improve sentiment analysis accuracy. It helps to simultaneously discover multi-aspect fine-grained topics and associated sentiment. "
190,"Contextual factors can greatly influence users' decisions in selecting items, such as songs when listening to music. The goal of a context-aware recommender system is to adapt its recommendations not just to the general preferences of users, but also to the context in which users are seeking those rec- ommendations. In the domain of music recommendation, the explicit contextual factors and their values might not be known to the system, a priori. Moreover, the contextual state of a user can be dynamic and change during an inter- action with the system. In this paper, we present a hybrid context-aware recommender system which infers contextual information from the sequence of songs listened to or specified by a user and uses this information to produce context- aware recommendations. Our system mines popular tags for songs from social media Web sites and uses a topic modeling approach to learn latent topics representing various contexts. We then model each song as a set of latent topics capturing the general characteristics of that song. This representation is used to track and detect changes in user's choice of mu- sic, as reffected in a playlist of song sequence, and adjust the recommendations to better meet the current context of the user. Using our approach, the contextual information can be integrated with any traditional recommendation al- gorithm to produce context-aware recommendations. For our system, we designed and evaluated two hybrid meth- ods. The first hybrid combines collaborative filtering and content-based recommendation techniques, and the second hybrid additionally incorporates information about pairwise song associations. Our evaluation results show that both the hybrid approach and the contextualization can enhance the performance of baseline music recommendation method. Copyright 2012 ACM.",2012-12-10,2-s2.0-84870562922,"International Conference on Information and Knowledge Management, Proceedings",Using social tags to infer context in hybrid music recommendation,"Contextual factors can greatly influence users' decisions in selecting items, such as songs when listening to music. The goal of a context-aware recommender system is to adapt its recommendations not just to the general preferences of users, but also to the context in which users are seeking those rec- ommendations. In the domain of music recommendation, the explicit contextual factors and their values might not be known to the system, a priori. Moreover, the contextual state of a user can be dynamic and change during an inter- action with the system. In this paper, we present a hybrid context-aware recommender system which infers contextual information from the sequence of songs listened to or specified by a user and uses this information to produce context- aware recommendations. Our system mines popular tags for songs from social media Web sites and uses a topic modeling approach to learn latent topics representing various contexts. We then model each song as a set of latent topics capturing the general characteristics of that song. This representation is used to track and detect changes in user's choice of mu- sic, as reffected in a playlist of song sequence, and adjust the recommendations to better meet the current context of the user. Using our approach, the contextual information can be integrated with any traditional recommendation al- gorithm to produce context-aware recommendations. For our system, we designed and evaluated two hybrid meth- ods. The first hybrid combines collaborative filtering and content-based recommendation techniques, and the second hybrid additionally incorporates information about pairwise song associations. Our evaluation results show that both the hybrid approach and the contextualization can enhance the performance of baseline music recommendation method. "
191,"We are developing indicators for the emergence of science and technology (S&T) topics. We are targeting various S&T information resources, including metadata (i.e., bibliographic information) and full text. We explore alternative text analysis approaches - principal components analysis (PCA) and topic modeling - to extract technical topic information. We analyze the topical content to pursue potential applications and innovation pathways. In this presentation we compare alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing (NLP) on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and tf-idf weighting. We compare PCA results to topic modeling results. Our key test set consists of 4104 Web of Science records on Dye-Sensitized Solar Cells (DSSCs). Results suggest good potential to enhance our technical intelligence payoffs from database searches on topics of interest. © 2012 IEEE.",2012-11-01,2-s2.0-84867934136,"2012 Proceedings of Portland International Center for Management of Engineering and Technology: Technology Management for Emerging Technologies, PICMET'12",Comparing methods to extract technical content for technological intelligence,"We are developing indicators for the emergence of science and technology (S&T) topics. We are targeting various S&T information resources, including metadata (i.e., bibliographic information) and full text. We explore alternative text analysis approaches - principal components analysis (PCA) and topic modeling - to extract technical topic information. We analyze the topical content to pursue potential applications and innovation pathways. In this presentation we compare alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing (NLP) on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and tf-idf weighting. We compare PCA results to topic modeling results. Our key test set consists of 4104 Web of Science records on Dye-Sensitized Solar Cells (DSSCs). Results suggest good potential to enhance our technical intelligence payoffs from database searches on topics of interest. "
192,"Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. The advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. By focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. The results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches. © 2012 Elsevier B.V. All rights reserved.",2012-11-01,2-s2.0-84865521691,Decision Support Systems,Feature selection for sentiment analysis based on content and syntax models,"Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. The advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. By focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. The results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches. "
193,"Researchers interests finding has been an active area of investigation for different recommendation tasks. Previous approaches for finding researchers interests exploit writing styles and links connectivity by considering time of documents, while semantics-based intrinsic structure of words is ignored. Consequently, a topic model named Author-Topic model is proposed, which exploits semantics-based intrinsic structure of words present between the authors of research papers. It ignores simultaneous modeling of time factor which results in exchangeability of topics problem, which is, important factor to deal with when finding dynamic research interests. For example, in many real world applications, like finding reviewers for papers and finding taggers in the social tagging systems one needs to consider different time periods. In this paper, we present time topic modeling approach named Temporal-Author-Topic (TAT) which can simultaneously model text, researchers and time of research papers to overcome the exchangeability of topic problem. The mixture distribution over topics is influenced by both co-occurrences of words and timestamps of the research papers. Consequently, topics occurrence and their related researchers change over time, while the meaning of particular topic almost remains unchanged. Proposed approach is used to discover topically related researchers for different time periods. We also show how their interests and relationships change over a time period. Empirical results on large research papers corpus show the effectiveness of our proposed approach and dominance over Author-Topic (AT) model, by handling the exchangeability of topics problem, which enables it to obtain similar meaning of a particular topic overtime. © 2011 Elsevier B.V. All rights reserved.",2012-02-01,2-s2.0-84155189116,Knowledge-Based Systems,Using time topic modeling for semantics-based dynamic research interest finding,"Researchers interests finding has been an active area of investigation for different recommendation tasks. Previous approaches for finding researchers interests exploit writing styles and links connectivity by considering time of documents, while semantics-based intrinsic structure of words is ignored. Consequently, a topic model named Author-Topic model is proposed, which exploits semantics-based intrinsic structure of words present between the authors of research papers. It ignores simultaneous modeling of time factor which results in exchangeability of topics problem, which is, important factor to deal with when finding dynamic research interests. For example, in many real world applications, like finding reviewers for papers and finding taggers in the social tagging systems one needs to consider different time periods. In this paper, we present time topic modeling approach named Temporal-Author-Topic (TAT) which can simultaneously model text, researchers and time of research papers to overcome the exchangeability of topic problem. The mixture distribution over topics is influenced by both co-occurrences of words and timestamps of the research papers. Consequently, topics occurrence and their related researchers change over time, while the meaning of particular topic almost remains unchanged. Proposed approach is used to discover topically related researchers for different time periods. We also show how their interests and relationships change over a time period. Empirical results on large research papers corpus show the effectiveness of our proposed approach and dominance over Author-Topic (AT) model, by handling the exchangeability of topics problem, which enables it to obtain similar meaning of a particular topic overtime. "
195,"This paper explores correspondence and mixture topic modeling of documents tagged from two different perspectives. There has been ongoing work in topic modeling of documents with tags (tag-topic models) where words and tags typically reflect a single perspective, namely document content. However, words in documents can also be tagged from different perspectives, for example, syntactic perspective as in part-of-speech tagging or an opinion perspective as in sentiment tagging. The models proposed in this paper are novel in: (i) the consideration of two different tag perspectives - a document level tag perspective that is relevant to the document as a whole and a word level tag perspective pertaining to each word in the document; (ii) the attribution of latent topics with word level tags and labeling latent topics with images in case of multimedia documents; and (iii) discovering the possible correspondence of the words to document level tags. The proposed correspondence tag-topic model shows better predictive power i.e. higher likelihood on heldout test data than all existing tag topic models and even a supervised topic model. To evaluate the models in practical scenarios, quantitative measures between the outputs of the proposed models and the ground truth domain knowledge have been explored. Manually assigned (gold standard) document category labels in Wikipedia pages are used to validate model-generated tag suggestions using a measure of pairwise concept similarity within an ontological hierarchy like WordNet. Using a news corpus, automatic relationship discovery between person names was performed and compared to a robust baseline. © 2011 ACM.",2011-12-13,2-s2.0-83055161660,"International Conference on Information and Knowledge Management, Proceedings",Simultaneous joint and conditional modeling of documents tagged from two perspectives,"This paper explores correspondence and mixture topic modeling of documents tagged from two different perspectives. There has been ongoing work in topic modeling of documents with tags (tag-topic models) where words and tags typically reflect a single perspective, namely document content. However, words in documents can also be tagged from different perspectives, for example, syntactic perspective as in part-of-speech tagging or an opinion perspective as in sentiment tagging. The models proposed in this paper are novel in: (i) the consideration of two different tag perspectives - a document level tag perspective that is relevant to the document as a whole and a word level tag perspective pertaining to each word in the document; (ii) the attribution of latent topics with word level tags and labeling latent topics with images in case of multimedia documents; and (iii) discovering the possible correspondence of the words to document level tags. The proposed correspondence tag-topic model shows better predictive power i.e. higher likelihood on heldout test data than all existing tag topic models and even a supervised topic model. To evaluate the models in practical scenarios, quantitative measures between the outputs of the proposed models and the ground truth domain knowledge have been explored. Manually assigned (gold standard) document category labels in Wikipedia pages are used to validate model-generated tag suggestions using a measure of pairwise concept similarity within an ontological hierarchy like WordNet. Using a news corpus, automatic relationship discovery between person names was performed and compared to a robust baseline. "
196,"Named entities are observed in a large portion of web search queries (named entity queries), where each entity can be associated with many different query terms that refer to various aspects of this entity. Organizing these query terms into topics helps understand major search intents about entities and the discovered topics are useful for applications such as query suggestion. Furthermore, we notice that named entities can often be organized into categories and those from the same category share many generic topics. Therefore, working on a category of named entities instead of individual ones helps avoid the problems caused by the sparsity and noise in the data. In this paper, Named Entity Topic Model (NETM) is proposed to discover generic topics for a category of named entities, where the quality of the generic topics is improved through the model design and the parameter initialization. Experiments based on query log data show that NETM discovers high-quality topics and outperforms the state-of-the-art techniques by 12.8% based on F1 measure. © 2011 ACM.",2011-12-13,2-s2.0-83055165976,"International Conference on Information and Knowledge Management, Proceedings",Topic modeling for named entity queries,"Named entities are observed in a large portion of web search queries (named entity queries), where each entity can be associated with many different query terms that refer to various aspects of this entity. Organizing these query terms into topics helps understand major search intents about entities and the discovered topics are useful for applications such as query suggestion. Furthermore, we notice that named entities can often be organized into categories and those from the same category share many generic topics. Therefore, working on a category of named entities instead of individual ones helps avoid the problems caused by the sparsity and noise in the data. In this paper, Named Entity Topic Model (NETM) is proposed to discover generic topics for a category of named entities, where the quality of the generic topics is improved through the model design and the parameter initialization. Experiments based on query log data show that NETM discovers high-quality topics and outperforms the state-of-the-art techniques by 12.8% based on F1 measure. "
197,"Name ambiguity arises from the polysemy of names and causes uncertainty about the true identity of entities referenced in unstructured text. This is a major problem in areas like information retrieval or knowledge management, for example when searching for a specific entity or updating an existing knowledge base. We approach this problem of named entity disambiguation (NED) using thematic information derived from Latent Dirichlet Allocation (LDA) to compare the entity mention's context with candidate entities in Wikipedia represented by their respective articles. We evaluate various distances over topic distributions in a supervised classification setting to find the best suited candidate entity, which is either covered in Wikipedia or unknown. We compare our approach to a state of the art method and show that it achieves significantly better results in predictive performance, regarding both entities covered in Wikipedia as well as uncovered entities. We show that our approach is in general language independent as we obtain equally good results for named entity disambiguation using the English, the German and the French Wikipedia. © 2011 ACM.",2011-12-13,2-s2.0-83055161746,"International Conference on Information and Knowledge Management, Proceedings",From names to entities using thematic context distance,"Name ambiguity arises from the polysemy of names and causes uncertainty about the true identity of entities referenced in unstructured text. This is a major problem in areas like information retrieval or knowledge management, for example when searching for a specific entity or updating an existing knowledge base. We approach this problem of named entity disambiguation (NED) using thematic information derived from Latent Dirichlet Allocation (LDA) to compare the entity mention's context with candidate entities in Wikipedia represented by their respective articles. We evaluate various distances over topic distributions in a supervised classification setting to find the best suited candidate entity, which is either covered in Wikipedia or unknown. We compare our approach to a state of the art method and show that it achieves significantly better results in predictive performance, regarding both entities covered in Wikipedia as well as uncovered entities. We show that our approach is in general language independent as we obtain equally good results for named entity disambiguation using the English, the German and the French Wikipedia. "
198,"With an increasingly amount of information in web forums, quick comprehension of threads in web forums has become a challenging research problem. To handle this issue, this paper investigates the task of Web Forum Thread Summarization (WFTS), aiming to give a brief statement of each thread that involving multiple dynamic topics. When applied to the task of WFTS, traditional summarization methods are cramped by topic dependencies, topic drifting and text sparseness. Consequently, we explore an unsupervised topic propagation model in this paper, the Post Propagation Model (PPM), to burst through these problems by simultaneously modeling the semantics and the reply relationship existing in each thread. Each post in PPM is considered as a mixture of topics, and a product of Dirichlet distributions in previous posts is employed to model each topic dependencies during the asynchronous discussion. Based on this model, the task of WFTS is accomplished by extracting most significant sentences in a thread. The experimental results on two different forum data sets show that WFTS based on the PPM outperforms several state-of-the-art summarization methods in terms of ROUGE metrics. © 2011 ACM.",2011-12-13,2-s2.0-83055179512,"International Conference on Information and Knowledge Management, Proceedings",Summarizing web forum threads based on a latent topic propagation process,"With an increasingly amount of information in web forums, quick comprehension of threads in web forums has become a challenging research problem. To handle this issue, this paper investigates the task of Web Forum Thread Summarization (WFTS), aiming to give a brief statement of each thread that involving multiple dynamic topics. When applied to the task of WFTS, traditional summarization methods are cramped by topic dependencies, topic drifting and text sparseness. Consequently, we explore an unsupervised topic propagation model in this paper, the Post Propagation Model (PPM), to burst through these problems by simultaneously modeling the semantics and the reply relationship existing in each thread. Each post in PPM is considered as a mixture of topics, and a product of Dirichlet distributions in previous posts is employed to model each topic dependencies during the asynchronous discussion. Based on this model, the task of WFTS is accomplished by extracting most significant sentences in a thread. The experimental results on two different forum data sets show that WFTS based on the PPM outperforms several state-of-the-art summarization methods in terms of ROUGE metrics. "
199,"We propose a hierarchical nonparametric topic model, based on the hierarchical Dirichlet process (HDP), that accounts for dependencies among the data. The HDP mixture models are useful for discovering an unknown semantic structure (i.e., topics) from a set of unstructured data such as a corpus of documents. For simplicity, HDP makes an exchangeability assumption that any permutation of the data points would result in the same joint probability of the data being generated. This exchangeability assumption poses a problem for some domains where there are clear and strong dependencies among the data. A model that allows for non-exchangeability of data can capture these dependencies and assign higher probabilities to clusters that account for data dependencies, for example, inferring topics that reflect the temporal patterns of the data. Our model incorporates the distance dependent Chinese restaurant process (ddCRP), which clusters data with an inherent bias toward clusters of data points that are near to one another, into a hierarchical construction analogous to the HDP, and we call this new prior the distance dependent Chinese restaurant franchise (ddCRF). When tested with temporal datasets, the ddCRF mixture model shows clear improvements in data fit compared to the HDP in terms of heldout likelihood and complexity. The resulting set of topics shows the sequential emergence and disappearance patterns of topics. © 2011 ACM.",2011-12-13,2-s2.0-83055161763,"International Conference on Information and Knowledge Management, Proceedings",Accounting for data dependencies within a hierarchical dirichlet process mixture model,"We propose a hierarchical nonparametric topic model, based on the hierarchical Dirichlet process (HDP), that accounts for dependencies among the data. The HDP mixture models are useful for discovering an unknown semantic structure (i.e., topics) from a set of unstructured data such as a corpus of documents. For simplicity, HDP makes an exchangeability assumption that any permutation of the data points would result in the same joint probability of the data being generated. This exchangeability assumption poses a problem for some domains where there are clear and strong dependencies among the data. A model that allows for non-exchangeability of data can capture these dependencies and assign higher probabilities to clusters that account for data dependencies, for example, inferring topics that reflect the temporal patterns of the data. Our model incorporates the distance dependent Chinese restaurant process (ddCRP), which clusters data with an inherent bias toward clusters of data points that are near to one another, into a hierarchical construction analogous to the HDP, and we call this new prior the distance dependent Chinese restaurant franchise (ddCRF). When tested with temporal datasets, the ddCRF mixture model shows clear improvements in data fit compared to the HDP in terms of heldout likelihood and complexity. The resulting set of topics shows the sequential emergence and disappearance patterns of topics. "
200,"With the advent of the Web and various specialized digital libraries, the automatic extraction of useful information from text has become an increasingly important research in Data mining. In this paper we present a new MH based algorithm that extracts both the topics expressed in large text document collections and also models how the authors of documents use those topics. The methodology is illustrated using a sample of 1740 documents and 2037 authors of NIPS conference papers. A novel feature of our model is the inclusion of MH sampling for author topic models, in which authors are modeled as probability distributions over topics. The author-topic models can be used to support a variety of interactive and exploratory queries on the dataset. Algorithm proposed in this paper is the implementation of enhanced author topic modeling in text collection for extraction of topics from documents which will be useful for efficient search and retrieval. This paper presents an unsupervised learning technique for extracting information from the real world large text collections. This involves clustering which is used for extracting a representation from a collection of documents. Each cluster is associated with a topic and a single document is associated in only one cluster. Traditional Author Topic Model encounters problem in case of multi topic documents. Experimental results using proposed algorithm achieved the same classification accuracy with reduced time (50%) to extract the topics. © 2011 IEEE.",2011-09-05,2-s2.0-80052213571,"International Conference on Recent Trends in Information Technology, ICRTIT 2011",A author topic model based unsupervised algorithm for learning topics from large text collections,"With the advent of the Web and various specialized digital libraries, the automatic extraction of useful information from text has become an increasingly important research in Data mining. In this paper we present a new MH based algorithm that extracts both the topics expressed in large text document collections and also models how the authors of documents use those topics. The methodology is illustrated using a sample of 1740 documents and 2037 authors of NIPS conference papers. A novel feature of our model is the inclusion of MH sampling for author topic models, in which authors are modeled as probability distributions over topics. The author-topic models can be used to support a variety of interactive and exploratory queries on the dataset. Algorithm proposed in this paper is the implementation of enhanced author topic modeling in text collection for extraction of topics from documents which will be useful for efficient search and retrieval. This paper presents an unsupervised learning technique for extracting information from the real world large text collections. This involves clustering which is used for extracting a representation from a collection of documents. Each cluster is associated with a topic and a single document is associated in only one cluster. Traditional Author Topic Model encounters problem in case of multi topic documents. Experimental results using proposed algorithm achieved the same classification accuracy with reduced time (50%) to extract the topics. "
201,"Community Question Answering (CQA) services have evolved into a popular way of information seeking and providing. User-posted questions in CQA are generally organized into hierarchical categories. In this paper, we define and study a novel problem which is referred to as New Category Identification (NCI) in CQA question archives. New Category Identification is primarily concerned with detecting and characterizing new or emerging categories which are not included in the existing category hierarchy. We define this problem formally, and propose both unsupervised and semi-supervised topic modeling methods to solve it. Experiments with a ground-truth set built from Yahoo! Answers show that our methods identify and interpret new categories effectively. © 2010 ACM.",2010-12-01,2-s2.0-78651318340,"International Conference on Information and Knowledge Management, Proceedings",Identifying new categories in Community Question Answering archives: A topic modeling approach,"Community Question Answering (CQA) services have evolved into a popular way of information seeking and providing. User-posted questions in CQA are generally organized into hierarchical categories. In this paper, we define and study a novel problem which is referred to as New Category Identification (NCI) in CQA question archives. New Category Identification is primarily concerned with detecting and characterizing new or emerging categories which are not included in the existing category hierarchy. We define this problem formally, and propose both unsupervised and semi-supervised topic modeling methods to solve it. Experiments with a ground-truth set built from Yahoo! Answers show that our methods identify and interpret new categories effectively. "
202,"Exploring community is fundamental for uncovering the connections between structure and function of complex networks and for practical applications in many disciplines such as biology and sociology. In this paper, we propose a TTR-LDA-Community model which combines the Latent Dirichlet Allocation model (LDA) and the Girvan-Newman community detection algorithm with an inference mechanism. The model is then applied to data from Delicious, a popular social tagging system, over the time period of 2005-2008. Our results show that 1) users in the same community tend to be interested in similar set of topics in all time periods; and 2) topics may divide into several sub-topics and scatter into different communities over time. We evaluate the effectiveness of our model and show that the TTR-LDA-Community model is meaningful for understanding communities and outperforms TTR-LDA and LDA models in tag prediction. © 2010 ACM.",2010-12-01,2-s2.0-78651324362,"International Conference on Information and Knowledge Management, Proceedings",Community-based topic modeling for social tagging,"Exploring community is fundamental for uncovering the connections between structure and function of complex networks and for practical applications in many disciplines such as biology and sociology. In this paper, we propose a TTR-LDA-Community model which combines the Latent Dirichlet Allocation model (LDA) and the Girvan-Newman community detection algorithm with an inference mechanism. The model is then applied to data from Delicious, a popular social tagging system, over the time period of 2005-2008. Our results show that 1) users in the same community tend to be interested in similar set of topics in all time periods; and 2) topics may divide into several sub-topics and scatter into different communities over time. We evaluate the effectiveness of our model and show that the TTR-LDA-Community model is meaningful for understanding communities and outperforms TTR-LDA and LDA models in tag prediction. "
203,"This paper presents a hierarchical generative model that captures the latent relation of cause and effect underlying user behavioral-originated data such as papers, twitter, and purchase history. Our proposal, the Latent Interest Topic model (LIT), introduces a latent variable into each document and each author layer in a coherent generative model. We call the former variable the document class, and the latter variable the author class, where these classes are indicator variables that allow the inclusion of different types of probability, and can be shared over documents with similar content and authors with similar interests, respectively. Significantly, unlike other works, LIT differentiates, respectively, document topics and user interests by using these classes. Consequently, LIT is superior to previous models in explaining the causal relationships behind the data by merging similar distributions; it also makes the computation process easier. Experiments on a research paper corpus show that the proposed model can well capture document and author classes, and reduce the dimensionality of documents to a low-dimensional author-document space, making it useful as a generative model. © 2010 ACM.",2010-12-01,2-s2.0-78651285299,"International Conference on Information and Knowledge Management, Proceedings",Latent interest-topic model: Finding the causal relationships behind dyadic data,"This paper presents a hierarchical generative model that captures the latent relation of cause and effect underlying user behavioral-originated data such as papers, twitter, and purchase history. Our proposal, the Latent Interest Topic model (LIT), introduces a latent variable into each document and each author layer in a coherent generative model. We call the former variable the document class, and the latter variable the author class, where these classes are indicator variables that allow the inclusion of different types of probability, and can be shared over documents with similar content and authors with similar interests, respectively. Significantly, unlike other works, LIT differentiates, respectively, document topics and user interests by using these classes. Consequently, LIT is superior to previous models in explaining the causal relationships behind the data by merging similar distributions; it also makes the computation process easier. Experiments on a research paper corpus show that the proposed model can well capture document and author classes, and reduce the dimensionality of documents to a low-dimensional author-document space, making it useful as a generative model. "
204,"This paper addresses the problem of semantics-based temporal expert finding, which means identifying a person with given expertise for different time periods. For example, many real world applications like reviewer matching for papers and finding hot topics in newswire articles need to consider time dynamics. Intuitively there will be different reviewers and reporters for different topics during different time periods. Traditional approaches used graph-based link structure by using keywords based matching and ignored semantic information, while topic modeling considered semantics-based information without conferences influence (richer text semantics and relationships between authors) and time information simultaneously. Consequently they result in not finding appropriate experts for different time periods. We propose a novel Temporal-Expert-Topic (TET) approach based on Semantics and Temporal Information based Expert Search (STMS) for temporal expert finding, which simultaneously models conferences influence and time information. Consequently, topics (semantically related probabilistic clusters of words) occurrence and correlations change over time, while the meaning of a particular topic almost remains unchanged. By using Bayes Theorem we can obtain topically related experts for different time periods and show how experts' interests and relationships change over time. Experimental results on scientific literature dataset show that the proposed generalized time topic modeling approach significantly outperformed the non-generalized time topic modeling approaches, due to simultaneously capturing conferences influence with time information. © 2010 Elsevier B.V. All rights reserved.",2010-08-01,2-s2.0-77955671812,Knowledge-Based Systems,Temporal expert finding through generalized time topic modeling,"This paper addresses the problem of semantics-based temporal expert finding, which means identifying a person with given expertise for different time periods. For example, many real world applications like reviewer matching for papers and finding hot topics in newswire articles need to consider time dynamics. Intuitively there will be different reviewers and reporters for different topics during different time periods. Traditional approaches used graph-based link structure by using keywords based matching and ignored semantic information, while topic modeling considered semantics-based information without conferences influence (richer text semantics and relationships between authors) and time information simultaneously. Consequently they result in not finding appropriate experts for different time periods. We propose a novel Temporal-Expert-Topic (TET) approach based on Semantics and Temporal Information based Expert Search (STMS) for temporal expert finding, which simultaneously models conferences influence and time information. Consequently, topics (semantically related probabilistic clusters of words) occurrence and correlations change over time, while the meaning of a particular topic almost remains unchanged. By using Bayes Theorem we can obtain topically related experts for different time periods and show how experts' interests and relationships change over time. Experimental results on scientific literature dataset show that the proposed generalized time topic modeling approach significantly outperformed the non-generalized time topic modeling approaches, due to simultaneously capturing conferences influence with time information. "
205,"In this paper, the task of text segmentation is approached from a topic modeling perspective. We investigate the use of latent Dirichlet allocation (LDA) topic model to segment a text into semantically coherent segments. A major benefit of the proposed approach is that along with the segment boundaries, it outputs the topic distribution associated with each segment. This information is of potential use in applications like segment retrieval and discourse analysis. The new approach outperforms a standard baseline method and yields significantly better performance than most of the available unsupervised methods on a benchmark dataset. Copyright 2009 ACM.",2009-12-01,2-s2.0-74549168970,"International Conference on Information and Knowledge Management, Proceedings",Text segmentation via topic modeling: An analytical study,"In this paper, the task of text segmentation is approached from a topic modeling perspective. We investigate the use of latent Dirichlet allocation (LDA) topic model to segment a text into semantically coherent segments. A major benefit of the proposed approach is that along with the segment boundaries, it outputs the topic distribution associated with each segment. This information is of potential use in applications like segment retrieval and discourse analysis. The new approach outperforms a standard baseline method and yields significantly better performance than most of the available unsupervised methods on a benchmark dataset. "
206,"Topic-based text summaries promise to help average users quickly understand a text collection and derive insights. Recent research has shown that the Latent Dirichlet Allocation (LDA) model is one of the most effective approaches to topic analysis. However, the LDA-based results may not be ideal for human understanding and consumption. In this paper, we present several topic and keyword re-ranking approaches that can help users better understand and consume the LDA-derived topics in their text analysis. Our methods process the LDA output based on a set of criteria that model a user's information needs. Our evaluation demonstrates the usefulness of the methods in summarizing several large-scale, real world data sets. Copyright 2009 ACM.",2009-12-01,2-s2.0-74549195580,"International Conference on Information and Knowledge Management, Proceedings",Topic and keyword re-ranking for LDA-based topic modeling,"Topic-based text summaries promise to help average users quickly understand a text collection and derive insights. Recent research has shown that the Latent Dirichlet Allocation (LDA) model is one of the most effective approaches to topic analysis. However, the LDA-based results may not be ideal for human understanding and consumption. In this paper, we present several topic and keyword re-ranking approaches that can help users better understand and consume the LDA-derived topics in their text analysis. Our methods process the LDA output based on a set of criteria that model a user's information needs. Our evaluation demonstrates the usefulness of the methods in summarizing several large-scale, real world data sets. "
207,"The proceedings contain 9 papers. The topics discussed include: integrating web-based intelligence retrieval and decision-making from the twitter trends knowledge base; a tag recommendation system for folksonomy; annotating Wikipedia articles with semantic tags for structured retrieval; automobile, car and BMW: horizontal and hierarchical approach in social tagging systems; characterizing the evolution of collaboration network; privacy-enhanced public view for social graphs; commentary-based video categorization and concept discovery; cross-language linking of news stories on the web using interlingual topic modeling; and detecting opinion leaders and trends in online social networks.",2009-12-01,2-s2.0-74249122055,"International Conference on Information and Knowledge Management, Proceedings","2nd ACM Workshop on Social Web Search and Mining, SWSM'09, Co-located with the 18th ACM International Conference on Information and Knowledge Management, CIKM 2009","The proceedings contain 9 papers. The topics discussed include: integrating web-based intelligence retrieval and decision-making from the twitter trends knowledge base; a tag recommendation system for folksonomy; annotating Wikipedia articles with semantic tags for structured retrieval; automobile, car and BMW: horizontal and hierarchical approach in social tagging systems; characterizing the evolution of collaboration network; privacy-enhanced public view for social graphs; commentary-based video categorization and concept discovery; cross-language linking of news stories on the web using interlingual topic modeling; and detecting opinion leaders and trends in online social networks."
208,"We have studied the problem of linking event information across different languages without the use of translation systems or dictionaries. The linking is based on interlingua information obtained through probabilistic topic models trained on comparable corpora written in two languages (in our case English and Dutch). The achieve this, we expand the Latent Dirichlet Allocation model to process documents in two languages. We demonstrate the validity of the learned interlingual topics in a document clustering task, where the evaluation is performed on Google News. Copyright 2009 ACM.",2009-12-01,2-s2.0-74049123165,"International Conference on Information and Knowledge Management, Proceedings",Cross-language linking of news stories on the web using interlingual topic modelling,"We have studied the problem of linking event information across different languages without the use of translation systems or dictionaries. The linking is based on interlingua information obtained through probabilistic topic models trained on comparable corpora written in two languages (in our case English and Dutch). The achieve this, we expand the Latent Dirichlet Allocation model to process documents in two languages. We demonstrate the validity of the learned interlingual topics in a document clustering task, where the evaluation is performed on Google News. "
209,"This paper presents a new Bayesian topical trend analysis. We regard the parameters of topic Dirichlet priors in latent Dirichlet allocation as a function of document timestamps and optimize the parameters by a gradient-based algorithm. Since our method gives similar hyperparameters to the documents having similar timestamps, topic assignment in collapsed Gibbs sampling is affected by timestamp similarities. We compute TFIDF-based document similarities by using a result of collapsed Gibbs sampling and evaluate our proposal by link detection task of Topic Detection and Tracking. Copyright 2009 ACM.",2009-12-01,2-s2.0-74549123327,"International Conference on Information and Knowledge Management, Proceedings",Dynamic hyperparameter optimization for bayesian topical trend analysis,"This paper presents a new Bayesian topical trend analysis. We regard the parameters of topic Dirichlet priors in latent Dirichlet allocation as a function of document timestamps and optimize the parameters by a gradient-based algorithm. Since our method gives similar hyperparameters to the documents having similar timestamps, topic assignment in collapsed Gibbs sampling is affected by timestamp similarities. We compute TFIDF-based document similarities by using a result of collapsed Gibbs sampling and evaluate our proposal by link detection task of Topic Detection and Tracking. "
210,"Topic modeling has been a key problem for document analysis. One of the canonical approaches for topic modeling is Probabilistic Latent Semantic Indexing, which maximizes the joint probability of documents and terms in the corpus. The major disadvantage of PLSI is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting. Latent Dirichlet Allocation (LDA) is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random variable. Both of these two methods discover the hidden topics in the Euclidean space. However, there is no convincing evidence that the document space is Euclidean, or flat. Therefore, it is more natural and reasonable to assume that the document space is a manifold, either linear or nonlinear. In this paper, we consider the problem of topic modeling on intrinsic document manifold. Specifically, we propose a novel algorithm called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI) for topic modeling. LapPLSI models the document space as a submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question. We compare the proposed LapPLSI approach with PLSI and LDA on three text data sets. Experimental results show that LapPLSI provides better representation in the sense of semantic structure. Copyright 2008 ACM.",2008-12-01,2-s2.0-70349247055,"International Conference on Information and Knowledge Management, Proceedings",Modeling hidden topics on document manifold,"Topic modeling has been a key problem for document analysis. One of the canonical approaches for topic modeling is Probabilistic Latent Semantic Indexing, which maximizes the joint probability of documents and terms in the corpus. The major disadvantage of PLSI is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting. Latent Dirichlet Allocation (LDA) is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random variable. Both of these two methods discover the hidden topics in the Euclidean space. However, there is no convincing evidence that the document space is Euclidean, or flat. Therefore, it is more natural and reasonable to assume that the document space is a manifold, either linear or nonlinear. In this paper, we consider the problem of topic modeling on intrinsic document manifold. Specifically, we propose a novel algorithm called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI) for topic modeling. LapPLSI models the document space as a submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question. We compare the proposed LapPLSI approach with PLSI and LDA on three text data sets. Experimental results show that LapPLSI provides better representation in the sense of semantic structure. "
211,"The rapidly increasing popularity of community-based Question Answering (cQA) services, e.g. Yahoo! Answers, Baidu Zhidao, etc. have attracted great attention from both academia and indus-try. Besides the basic problems, like question searching and an-swer finding, it should be noted that the low participation rate of users in cQA service is the crucial problem which limits its de-velopment potential. In this paper, we focus on addressing this problem by recommending answer providers, in which a question is given as a query and a ranked list of users is returned according to the likelihood of answering the question. Based on the intuitive idea for recommendation, we try to introduce topic-level model to improve heuristic term-level methods, which are treated as the baselines. The proposed approach consists of two steps: (1) dis-covering latent topics in the content of questions and answers as well as latent interests of users to build user profiles; (2) recom-mending question answerers for new arrival questions based on latent topics and term-level model. Specifically, we develop a general generative model for questions and answers in cQA, which is then altered to obtain a novel computationally tractable Bayesian network model. Experiments are carried out on a real-world data crawled from Yahoo! Answers during Jun 12 2007 to Aug 04 2007, which consists of 118510 questions, 772962 answers and 150324 users. The experimental results reveal signif-icant improvements over the baseline methods and validate the positive influence of topic-level information. Copyright 2008 ACM.",2008-12-01,2-s2.0-70349236044,"International Conference on Information and Knowledge Management, Proceedings",Tapping on the potential of Q&A community by recommending answer providers,"The rapidly increasing popularity of community-based Question Answering (cQA) services, e.g. Yahoo! Answers, Baidu Zhidao, etc. have attracted great attention from both academia and indus-try. Besides the basic problems, like question searching and an-swer finding, it should be noted that the low participation rate of users in cQA service is the crucial problem which limits its de-velopment potential. In this paper, we focus on addressing this problem by recommending answer providers, in which a question is given as a query and a ranked list of users is returned according to the likelihood of answering the question. Based on the intuitive idea for recommendation, we try to introduce topic-level model to improve heuristic term-level methods, which are treated as the baselines. The proposed approach consists of two steps: (1) dis-covering latent topics in the content of questions and answers as well as latent interests of users to build user profiles; (2) recom-mending question answerers for new arrival questions based on latent topics and term-level model. Specifically, we develop a general generative model for questions and answers in cQA, which is then altered to obtain a novel computationally tractable Bayesian network model. Experiments are carried out on a real-world data crawled from Yahoo! Answers during Jun 12 2007 to Aug 04 2007, which consists of 118510 questions, 772962 answers and 150324 users. The experimental results reveal signif-icant improvements over the baseline methods and validate the positive influence of topic-level information. "
212,"Unstructured information in the form of natural language text is abundant in various kinds of organisations. To increase information sharing, organisational learning, decision-making and productivity, large amounts of unstructured text need to be analysed on a daily basis. Full text searching alone is not sufficient as a first approach to help users understand what a collection of electronic documents is about, since it does not provide the user with an overview of the underlying concepts in the document collection. A topic model is a useful mechanism for identifying and characterising various concepts embedded in a document collection allowing the user to navigate the collection in a topic-guided manner. Topics, made up of significant words, provide the user with an overview of the content of the document collection. Each document is represented as a mixture of automatically constructed topics and the user may select documents related to a specific topic of interest and vice versa. Similarities between documents may be found by looking at what documents are assigned to a specific topic enabling the user to find other documents related to a given document. This methodology enables users to digest a larger number of documents, assisting them in spending more of their time in actually reading than finding relevant information. © 2008 PICMET.",2008-09-30,2-s2.0-52449110280,"PICMET: Portland International Center for Management of Engineering and Technology, Proceedings",Leveraging unstructured information using topic modelling,"Unstructured information in the form of natural language text is abundant in various kinds of organisations. To increase information sharing, organisational learning, decision-making and productivity, large amounts of unstructured text need to be analysed on a daily basis. Full text searching alone is not sufficient as a first approach to help users understand what a collection of electronic documents is about, since it does not provide the user with an overview of the underlying concepts in the document collection. A topic model is a useful mechanism for identifying and characterising various concepts embedded in a document collection allowing the user to navigate the collection in a topic-guided manner. Topics, made up of significant words, provide the user with an overview of the content of the document collection. Each document is represented as a mixture of automatically constructed topics and the user may select documents related to a specific topic of interest and vice versa. Similarities between documents may be found by looking at what documents are assigned to a specific topic enabling the user to find other documents related to a given document. This methodology enables users to digest a larger number of documents, assisting them in spending more of their time in actually reading than finding relevant information. "
