,abstract,date,id,publicationName,title,abstract_cleaned
0,"Well-being as an intangible, philosophical, and multi-faceted phenomenon is hard to measure. By taking a psycholinguistic expression of well-being, we measure how tourists' experiencing holiday destinations affects their well-being states. The well-being state includes increases or decreases in Hedonia and Eudaimonia as a result of destination experiences. We apply text topic modelling to analyse big Web 2.0 datasets. These include tourists’ self-reports of their experiences during their visit to destination countries collected from Travelblog.org weblog. The findings from a Global sample, New Zealand, and France are compared to generalise Hedonia and Eudaimonia conceptualisations across these destinations. The outputs also characterise experiences with maximum well-being and ill-being that include both Hedonia and Eudaimonia. Managerial implications include how a destination image may be managed according to desired well-being states. This also helps tourists make informed decisions about their holiday destinations.",2018-12-01,2-s2.0-85048543703,Tourism Management,Hedonic and eudaimonic well-being: A psycholinguistic view,"Well-being as an intangible, philosophical, and multi-faceted phenomenon is hard to measure. By taking a psycholinguistic expression of well-being, we measure how tourists' experiencing holiday destinations affects their well-being states. The well-being state includes increases or decreases in Hedonia and Eudaimonia as a result of destination experiences. We apply text topic modelling to analyse big Web 2.0 datasets. These include tourists’ self-reports of their experiences during their visit to destination countries collected from Travelblog.org weblog. The findings from a Global sample, New Zealand, and France are compared to generalise Hedonia and Eudaimonia conceptualisations across these destinations. The outputs also characterise experiences with maximum well-being and ill-being that include both Hedonia and Eudaimonia. Managerial implications include how a destination image may be managed according to desired well-being states. This also helps tourists make informed decisions about their holiday destinations."
1,"The rapid growth of the Chinese economy has resulted in great pressure on the environment: technological innovation is the fundamental pathway for improvement the efficiency in the process of energy saving and emission reduction. Based on large-scale technical text data in 31 Chinese provinces from 1985 to 2017, the Latent Dirichlet Allocation (LDA) topic model is introduced to technology content analysis. Then the LDA provincial-topic model is constructed, the subject and object of energy technology are jointly modelled, and the relationship between technology subject and technology have been region studied. The energy saving and emission reduction technology research direction in 31 provinces of China in the past 30 years has been examined: the status and level of technical reserves in each province have been evaluated, and the heterogeneity of provincial patent subject content has been compared. The research found that, despite the cumulative patent record (No. 1 world ranking), similarities in research directions in different provinces of China caused by poor regional technology mobility are a growing problem for Chinese energy technology, indicating much repeated research and development in different regions of China. Targeted suggestions on inter-provincial technology transfer and cooperation on energy technology development have been put forward.",2018-11-20,2-s2.0-85053080182,Journal of Cleaner Production,Heterogeneity evaluation of China's provincial energy technology based on large-scale technical text data mining,"The rapid growth of the Chinese economy has resulted in great pressure on the environment: technological innovation is the fundamental pathway for improvement the efficiency in the process of energy saving and emission reduction. Based on large-scale technical text data in 31 Chinese provinces from 1985 to 2017, the Latent Dirichlet Allocation (LDA) topic model is introduced to technology content analysis. Then the LDA provincial-topic model is constructed, the subject and object of energy technology are jointly modelled, and the relationship between technology subject and technology have been region studied. The energy saving and emission reduction technology research direction in 31 provinces of China in the past 30 years has been examined: the status and level of technical reserves in each province have been evaluated, and the heterogeneity of provincial patent subject content has been compared. The research found that, despite the cumulative patent record (No. 1 world ranking), similarities in research directions in different provinces of China caused by poor regional technology mobility are a growing problem for Chinese energy technology, indicating much repeated research and development in different regions of China. Targeted suggestions on inter-provincial technology transfer and cooperation on energy technology development have been put forward."
2,"The aim of this study is to introduce a decision framework that integrates Quality Function Deployment (QFD) methodology and topic modelling to facilitate analysing online reviews, that is, a digital source of voice of the customer (VoC), to extract the true customer needs for developing a product/service. Within the framework, after collecting and preprocessing customer reviews, Latent Dirichlet Allocation is used for grouping them as topics. Most representative reviews are determined based on corresponding topic distributions, and QFD methodology is employed to clarify these reviews and reveal true customer needs. For demonstration, this framework was used to analyse 83,545 reviews on Italian restaurants gathered from Yelp.com via topic modelling with a more objective perspective. The number of documents to be examined was decreased by approximately 95%. Thereupon the completion time of customer voice table, one of the most important tools in QFD studies, reduced dramatically. Besides, text mining process is enhanced for retrieving meaningful information from customer reviews by using QFD tools and their tailored approach. This study is the first attempt that integrates QFD and topic modelling in order to increase the efficiency of the VoC clarification process.",2018-11-10,2-s2.0-85007493460,Total Quality Management and Business Excellence,Topic modelling-based decision framework for analysing digital voice of the customer,"The aim of this study is to introduce a decision framework that integrates Quality Function Deployment (QFD) methodology and topic modelling to facilitate analysing online reviews, that is, a digital source of voice of the customer (VoC), to extract the true customer needs for developing a product/service. Within the framework, after collecting and preprocessing customer reviews, Latent Dirichlet Allocation is used for grouping them as topics. Most representative reviews are determined based on corresponding topic distributions, and QFD methodology is employed to clarify these reviews and reveal true customer needs. For demonstration, this framework was used to analyse 83,545 reviews on Italian restaurants gathered from Yelp.com via topic modelling with a more objective perspective. The number of documents to be examined was decreased by approximately 95%. Thereupon the completion time of customer voice table, one of the most important tools in QFD studies, reduced dramatically. Besides, text mining process is enhanced for retrieving meaningful information from customer reviews by using QFD tools and their tailored approach. This study is the first attempt that integrates QFD and topic modelling in order to increase the efficiency of the VoC clarification process."
3,"The new reality of networked publics on social media calls for crisis communication practitioners and researchers to understand the narratives generated by publics on social media during organizational crises. As social media publics possess diverse, unique characteristics and communicative needs during a crisis, they form interpretative communities and co-create various symbolic interpretations of the crisis. Extending the public-centric and narrative perspective to the context of social media crises, we examined what crisis narratives were constructed by social media publics (i.e., multiplicity) and how these narratives changed by crisis stages (i.e., dynamics). Using topic modelling based on large-scale Twitter data of the Chipotle E. coli crisis (N = 40,610), we identified ten narratives subsumed under two themes (i.e., sharing-based and conversation-based) based on publics’ social constructions of their perceived risks and crisis experience. On the one hand, sharing-based narratives, heavily impacted by publics’ shared media coverage, reflected media crisis narratives and salient risk perceptions aligning with the news agenda. On the other hand, conversation-based narratives, fueled by publics’ opinion expression and emotional venting, demonstrated publics’ interpretations of their experience with the organization in the crisis with less salient but more diversified risk perceptions. Crisis managers are recommended to produce and deliver compelling narratives resonating with different groups of social media publics during crises.",2018-11-01,2-s2.0-85050884991,Public Relations Review,Examining multiplicity and dynamics of publics’ crisis narratives with large-scale Twitter data,"The new reality of networked publics on social media calls for crisis communication practitioners and researchers to understand the narratives generated by publics on social media during organizational crises. As social media publics possess diverse, unique characteristics and communicative needs during a crisis, they form interpretative communities and co-create various symbolic interpretations of the crisis. Extending the public-centric and narrative perspective to the context of social media crises, we examined what crisis narratives were constructed by social media publics (i.e., multiplicity) and how these narratives changed by crisis stages (i.e., dynamics). Using topic modelling based on large-scale Twitter data of the Chipotle E. coli crisis (N = 40,610), we identified ten narratives subsumed under two themes (i.e., sharing-based and conversation-based) based on publics’ social constructions of their perceived risks and crisis experience. On the one hand, sharing-based narratives, heavily impacted by publics’ shared media coverage, reflected media crisis narratives and salient risk perceptions aligning with the news agenda. On the other hand, conversation-based narratives, fueled by publics’ opinion expression and emotional venting, demonstrated publics’ interpretations of their experience with the organization in the crisis with less salient but more diversified risk perceptions. Crisis managers are recommended to produce and deliver compelling narratives resonating with different groups of social media publics during crises."
4,"Topic models often produce unexplainable topics that are filled with noisy words. The reason is that words in topic modeling have equal weights. High frequency words dominate the top topic word lists, but most of them are meaningless words, e.g., domain-specific stopwords. To address this issue, in this paper we aim to investigate how to weight words, and then develop a straightforward but effective term weighting scheme, namely entropy weighting (EW). The proposed EW scheme is based on conditional entropy measured by word co-occurrences. Compared with existing term weighting schemes, the highlight of EW is that it can automatically reward informative words. For more robust word weight, we further suggest a combination form of EW (CEW) with two existing weighting schemes. Basically, our CEW assigns meaningless words lower weights and informative words higher weights, leading to more coherent topics during topic modeling inference. We apply CEW to Dirichlet multinomial mixture and latent Dirichlet allocation, and evaluate it by topic quality, document clustering and classification tasks on 8 real world data sets. Experimental results show that weighting words can effectively improve the topic modeling performance over both short texts and normal long texts. More importantly, the proposed CEW significantly outperforms the existing term weighting schemes, since it further considers which words are informative.",2018-11-01,2-s2.0-85048519194,Information Processing and Management,Exploring coherent topics by topic modeling with term weighting,"Topic models often produce unexplainable topics that are filled with noisy words. The reason is that words in topic modeling have equal weights. High frequency words dominate the top topic word lists, but most of them are meaningless words, e.g., domain-specific stopwords. To address this issue, in this paper we aim to investigate how to weight words, and then develop a straightforward but effective term weighting scheme, namely entropy weighting (EW). The proposed EW scheme is based on conditional entropy measured by word co-occurrences. Compared with existing term weighting schemes, the highlight of EW is that it can automatically reward informative words. For more robust word weight, we further suggest a combination form of EW (CEW) with two existing weighting schemes. Basically, our CEW assigns meaningless words lower weights and informative words higher weights, leading to more coherent topics during topic modeling inference. We apply CEW to Dirichlet multinomial mixture and latent Dirichlet allocation, and evaluate it by topic quality, document clustering and classification tasks on 8 real world data sets. Experimental results show that weighting words can effectively improve the topic modeling performance over both short texts and normal long texts. More importantly, the proposed CEW significantly outperforms the existing term weighting schemes, since it further considers which words are informative."
5,"Aspect mining, which aims to extract ad hoc aspects from online reviews and predict rating or opinion on each aspect, can satisfy the personalized needs for evaluation of specific aspect on product quality. Recently, with the increase of related research, how to effectively integrate rating and review information has become the key issue for addressing this problem. Considering that matrix factorization is an effective tool for rating prediction and topic modeling is widely used for review processing, it is a natural idea to combine matrix factorization and topic modeling for aspect mining (or called aspect rating prediction). However, this idea faces several challenges on how to address suitable sharing factors, scale mismatch, and dependency relation of rating and review information. In this paper, we propose a novel model to effectively integrate Matrix factorization and Topic modeling for Aspect rating prediction (MaToAsp). To overcome the above challenges and ensure the performance, MaToAsp employs items as the sharing factors to combine matrix factorization and topic modeling, and introduces an interpretive preference probability to eliminate scale mismatch. In the hybrid model, we establish a dependency relation from ratings to sentiment terms in phrases. The experiments on two real datasets including Chinese Dianping and English Tripadvisor prove that MaToAsp not only obtains reasonable aspect identification but also achieves the best aspect rating prediction performance, compared to recent representative baselines.",2018-11-01,2-s2.0-85048528031,Information Processing and Management,Coupled matrix factorization and topic modeling for aspect mining,"Aspect mining, which aims to extract ad hoc aspects from online reviews and predict rating or opinion on each aspect, can satisfy the personalized needs for evaluation of specific aspect on product quality. Recently, with the increase of related research, how to effectively integrate rating and review information has become the key issue for addressing this problem. Considering that matrix factorization is an effective tool for rating prediction and topic modeling is widely used for review processing, it is a natural idea to combine matrix factorization and topic modeling for aspect mining (or called aspect rating prediction). However, this idea faces several challenges on how to address suitable sharing factors, scale mismatch, and dependency relation of rating and review information. In this paper, we propose a novel model to effectively integrate Matrix factorization and Topic modeling for Aspect rating prediction (MaToAsp). To overcome the above challenges and ensure the performance, MaToAsp employs items as the sharing factors to combine matrix factorization and topic modeling, and introduces an interpretive preference probability to eliminate scale mismatch. In the hybrid model, we establish a dependency relation from ratings to sentiment terms in phrases. The experiments on two real datasets including Chinese Dianping and English Tripadvisor prove that MaToAsp not only obtains reasonable aspect identification but also achieves the best aspect rating prediction performance, compared to recent representative baselines."
6,"E-petitions have become a popular vehicle for political activism, but studying them has been difficult because efficient methods for analyzing their content are currently lacking. Researchers have used topic modeling for content analysis, but current practices carry some serious limitations. While modeling may be more efficient than manually reading each petition, it generally relies on unsupervised machine learning and so requires a dependable training and validation process. And so this paper describes a framework to train and validate Latent Dirichlet Allocation (LDA), the simplest and most popular topic modeling algorithm, using e-petition data. With rigorous training and evaluation, 87% of LDA-generated topics made sense to human judges. Topics also aligned well with results from an independent content analysis by the Pew Research Center, and were strongly associated with corresponding social events. Computer-assisted content analysts can benefit from our guidelines to supervise every process of training and evaluation of LDA. Software developers can benefit from learning the demands of social scientists when using LDA for content analysis. These findings have significant implications for developing LDA tools and assuring validity and interpretability of LDA content analysis. In addition, LDA topics can have some advantages over subjects extracted by manual content analysis by reflecting multiple themes expressed in texts, by extracting new themes that are not highlighted by human coders, and by being less prone to human bias.",2018-11-01,2-s2.0-85047760326,Information Processing and Management,Content analysis of e-petitions with topic modeling: How to train and evaluate LDA models?,"E-petitions have become a popular vehicle for political activism, but studying them has been difficult because efficient methods for analyzing their content are currently lacking. Researchers have used topic modeling for content analysis, but current practices carry some serious limitations. While modeling may be more efficient than manually reading each petition, it generally relies on unsupervised machine learning and so requires a dependable training and validation process. And so this paper describes a framework to train and validate Latent Dirichlet Allocation (LDA), the simplest and most popular topic modeling algorithm, using e-petition data. With rigorous training and evaluation, 87% of LDA-generated topics made sense to human judges. Topics also aligned well with results from an independent content analysis by the Pew Research Center, and were strongly associated with corresponding social events. Computer-assisted content analysts can benefit from our guidelines to supervise every process of training and evaluation of LDA. Software developers can benefit from learning the demands of social scientists when using LDA for content analysis. These findings have significant implications for developing LDA tools and assuring validity and interpretability of LDA content analysis. In addition, LDA topics can have some advantages over subjects extracted by manual content analysis by reflecting multiple themes expressed in texts, by extracting new themes that are not highlighted by human coders, and by being less prone to human bias."
7,"Identifying the activities that individuals conduct in a city is key to understanding urban dynamics. It is difficult, however, to identify different human activities on a large scale without incurring significant costs. This study focuses on modeling the spatiotemporal patterns of different activity types within cities by employing user-contributed, geosocial content as a proxy for human activities. In this work, we use linguistic topic modeling to analyze georeferenced twitter data in order to differentiate different activity types. We then examine the spatial and temporal patterns of the derived activity types in three U.S. cities: Baltimore, MD., Washington, D.C., and New York City, NY. The linguistic patterns reflect the spatiotemporal context of the places where the social media content is posted. We further construct a method to link what people post online to the activities conducted within a city. We then use these derived activities to profile the characteristics of neighborhoods in the three cities, and apply the activity signatures to discover similar neighborhoods both within and between the cities. This approach represents a novel activity-based method for assessing similarity between neighborhoods.",2018-11-01,2-s2.0-85050504898,"Computers, Environment and Urban Systems",Identifying spatiotemporal urban activities through linguistic signatures,"Identifying the activities that individuals conduct in a city is key to understanding urban dynamics. It is difficult, however, to identify different human activities on a large scale without incurring significant costs. This study focuses on modeling the spatiotemporal patterns of different activity types within cities by employing user-contributed, geosocial content as a proxy for human activities. In this work, we use linguistic topic modeling to analyze georeferenced twitter data in order to differentiate different activity types. We then examine the spatial and temporal patterns of the derived activity types in three U.S. cities: Baltimore, MD., Washington, D.C., and New York City, NY. The linguistic patterns reflect the spatiotemporal context of the places where the social media content is posted. We further construct a method to link what people post online to the activities conducted within a city. We then use these derived activities to profile the characteristics of neighborhoods in the three cities, and apply the activity signatures to discover similar neighborhoods both within and between the cities. This approach represents a novel activity-based method for assessing similarity between neighborhoods."
8,"This paper follows a stepwise application of Driving forces, Pressures, States, Impacts, and Responses (DPSIR) framework, and explores the use of data analytics to analyze oil spillages in the oil producing Niger Delta area of Nigeria. Our empirical results show that the major concerns of Shell Petroleum Development Corporation (SPDC) are implementing the 2011 UNEP report on oil spills in Ogoniland, and the cleanup program in the Niger Delta area. However, for the communities, the key issues are Shell Oil spillages and water pollution. The most important actionable option derived in the last step of the DPSIR framework is environmental mediation. This methodological approach uses both numerical and text data in proffering solutions to sustainable development crisis in the area, and can be replicated by policy makers at global levels to tackle similar problems.",2018-11-01,2-s2.0-85049335668,Land Use Policy,Using the DPSIR framework and data analytics to analyze oil spillages in the Niger delta area,"This paper follows a stepwise application of Driving forces, Pressures, States, Impacts, and Responses (DPSIR) framework, and explores the use of data analytics to analyze oil spillages in the oil producing Niger Delta area of Nigeria. Our empirical results show that the major concerns of Shell Petroleum Development Corporation (SPDC) are implementing the 2011 UNEP report on oil spills in Ogoniland, and the cleanup program in the Niger Delta area. However, for the communities, the key issues are Shell Oil spillages and water pollution. The most important actionable option derived in the last step of the DPSIR framework is environmental mediation. This methodological approach uses both numerical and text data in proffering solutions to sustainable development crisis in the area, and can be replicated by policy makers at global levels to tackle similar problems."
9,"This study investigated Hillary Clinton's and Donald Trump's speeches during the 2016 presidential election to identify their sentiments and discourse themes and strategies by using machine-based methods, including computerized sentence-level sentiment analysis, structural topic modeling for themes, and word2vec exploration for thematic associations. The machine-based automatic analyses were also complemented by a qualitative examination of the speech data motivated by the top thematic terms identified by the automatic analyses. The results of the study revealed that Trump's speeches were significantly more negative than Clinton's. The results also provided evidence supporting many previous findings regarding Clinton's and Trump's discourse/rhetoric styles and major campaign themes produced by studies using different research methods. The results of this study might also help explain Trump's victory despite the significant more negative sentiment in his discourse.",2018-10-01,2-s2.0-85046693635,"Discourse, Context and Media",The appeal to political sentiment: An analysis of Donald Trump's and Hillary Clinton's speech themes and discourse strategies in the 2016 US presidential election,"This study investigated Hillary Clinton's and Donald Trump's speeches during the 2016 presidential election to identify their sentiments and discourse themes and strategies by using machine-based methods, including computerized sentence-level sentiment analysis, structural topic modeling for themes, and word2vec exploration for thematic associations. The machine-based automatic analyses were also complemented by a qualitative examination of the speech data motivated by the top thematic terms identified by the automatic analyses. The results of the study revealed that Trump's speeches were significantly more negative than Clinton's. The results also provided evidence supporting many previous findings regarding Clinton's and Trump's discourse/rhetoric styles and major campaign themes produced by studies using different research methods. The results of this study might also help explain Trump's victory despite the significant more negative sentiment in his discourse."
10,"The study of technology and innovation management (TIM) has continued to evolve and expand with great speed over the last three decades. This research aims to identify core topics in TIM studies and explore their dynamic changes. The conventional approach, based on discrete assignments by subjective judgment with predetermined categories, cannot effectively capture latent topics from large volumes of scholarly data. Hence, this study adopts the topic model approach, which automatically discovers topics that pervade a large and unstructured collection of documents, to uncover research topics in TIM research. The 50 topics of TIM research are identified through the Latent Dirichlet Allocation model from 11,693 articles published from 1997 to 2016 in 11 TIM journals, and top 10 most popular topics in TIM research are briefly reviewed. We then explore topic trends by examining the changes in topics rankings over different time periods and identifying hot and cold topics of TIM research over the last two decades. For each of the 11 TIM journals, the areas of subspecialty and the effects of editor changes on topic portfolios are also investigated. The findings of this study are expected to provide implications for researchers, journal editors, and policy makers in the field of TIM.",2018-10-01,2-s2.0-85012236978,Journal of Technology Transfer,Identifying core topics in technology and innovation management studies: a topic model approach,"The study of technology and innovation management (TIM) has continued to evolve and expand with great speed over the last three decades. This research aims to identify core topics in TIM studies and explore their dynamic changes. The conventional approach, based on discrete assignments by subjective judgment with predetermined categories, cannot effectively capture latent topics from large volumes of scholarly data. Hence, this study adopts the topic model approach, which automatically discovers topics that pervade a large and unstructured collection of documents, to uncover research topics in TIM research. The 50 topics of TIM research are identified through the Latent Dirichlet Allocation model from 11,693 articles published from 1997 to 2016 in 11 TIM journals, and top 10 most popular topics in TIM research are briefly reviewed. We then explore topic trends by examining the changes in topics rankings over different time periods and identifying hot and cold topics of TIM research over the last two decades. For each of the 11 TIM journals, the areas of subspecialty and the effects of editor changes on topic portfolios are also investigated. The findings of this study are expected to provide implications for researchers, journal editors, and policy makers in the field of TIM."
11,"The study of discursive understandings of cybervictimisation draws on a dataset of crime news reporting and asks the question of if and how cybervictimisation is construed in ways that differ from other types of (non-digital) victimisation. Building on a critical discourse perspective employing corpus-based text analysis methods, the composition of news discourses about cybervictimisation are analysed, alongside the relationship between such representations and news media discourse on crime victimisation generally. The aim is to see what effect the presence of a digital dimension has for how the notion of victimisation is socially and culturally understood. The study shows, first, that news reporting on cybervictimisation has a strong bias towards crimes that fit well with the notion of ‘the ideal victim’ (such as sexual victimisation and bullying) while excluding other types like hacking and identity theft. The question is raised whether ‘victim’ discourse is able to account for the latter types or if new understandings and concepts will emerge. Second, the study shows that discourses promoting understandings of technology as contributing to amplifying danger, and that represent technology as potentially undermining social order, are strong in cybervictimisation news reports. These discourses are consequential for who is seen as a legitimate victim and not. Just as it can be very difficult to identify and apprehend perpetrators of cybercrime, so is also the identification and definition of cybervictims ambiguous and demands to be further researched.",2018-10-01,2-s2.0-85047839151,Discourse and Communication,A ghost in the machine: Tracing the role of ‘the digital’ in discursive processes of cybervictimisation,"The study of discursive understandings of cybervictimisation draws on a dataset of crime news reporting and asks the question of if and how cybervictimisation is construed in ways that differ from other types of (non-digital) victimisation. Building on a critical discourse perspective employing corpus-based text analysis methods, the composition of news discourses about cybervictimisation are analysed, alongside the relationship between such representations and news media discourse on crime victimisation generally. The aim is to see what effect the presence of a digital dimension has for how the notion of victimisation is socially and culturally understood. The study shows, first, that news reporting on cybervictimisation has a strong bias towards crimes that fit well with the notion of ‘the ideal victim’ (such as sexual victimisation and bullying) while excluding other types like hacking and identity theft. The question is raised whether ‘victim’ discourse is able to account for the latter types or if new understandings and concepts will emerge. Second, the study shows that discourses promoting understandings of technology as contributing to amplifying danger, and that represent technology as potentially undermining social order, are strong in cybervictimisation news reports. These discourses are consequential for who is seen as a legitimate victim and not. Just as it can be very difficult to identify and apprehend perpetrators of cybercrime, so is also the identification and definition of cybervictims ambiguous and demands to be further researched."
12,"An information retrieval (IR) system can often fail to retrieve relevant documents due to the incomplete specification of information need in the user’s query. Pseudo-relevance feedback (PRF) aims to improve IR effectiveness by exploiting potentially relevant aspects of the information need present in the documents retrieved in an initial search. Standard PRF approaches utilize the information contained in these top ranked documents from the initial search with the assumption that documents as a whole are relevant to the information need. However, in practice, documents are often multi-topical where only a portion of the documents may be relevant to the query. In this situation, exploitation of the topical composition of the top ranked documents, estimated with statistical topic modeling based approaches, can potentially be a useful cue to improve PRF effectiveness. The key idea behind our PRF method is to use the term-topic and the document-topic distributions obtained from topic modeling over the set of top ranked documents to re-rank the initially retrieved documents. The objective is to improve the ranks of documents that are primarily composed of the relevant topics expressed in the information need of the query. Our RF model can further be improved by making use of non-parametric topic modeling, where the number of topics can grow according to the document contents, thus giving the RF model the capability to adjust the number of topics based on the content of the top ranked documents. We empirically validate our topic model based RF approach on two document collections of diverse length and topical composition characteristics: (1) ad-hoc retrieval using the TREC 6-8 and the TREC Robust ’04 dataset, and (2) tweet retrieval using the TREC Microblog ’11 dataset. Results indicate that our proposed approach increases MAP by up to 9% in comparison to the results obtained with an LDA based language model (for initial retrieval) coupled with the relevance model (for feedback). Moreover, the non-parametric version of our proposed approach is shown to be more effective than its parametric counterpart due to its advantage of adapting the number of topics, improving results by up to 5.6% of MAP compared to the parametric version.",2018-10-01,2-s2.0-85044920646,Information Retrieval Journal,A non-parametric topical relevance model,"An information retrieval (IR) system can often fail to retrieve relevant documents due to the incomplete specification of information need in the user’s query. Pseudo-relevance feedback (PRF) aims to improve IR effectiveness by exploiting potentially relevant aspects of the information need present in the documents retrieved in an initial search. Standard PRF approaches utilize the information contained in these top ranked documents from the initial search with the assumption that documents as a whole are relevant to the information need. However, in practice, documents are often multi-topical where only a portion of the documents may be relevant to the query. In this situation, exploitation of the topical composition of the top ranked documents, estimated with statistical topic modeling based approaches, can potentially be a useful cue to improve PRF effectiveness. The key idea behind our PRF method is to use the term-topic and the document-topic distributions obtained from topic modeling over the set of top ranked documents to re-rank the initially retrieved documents. The objective is to improve the ranks of documents that are primarily composed of the relevant topics expressed in the information need of the query. Our RF model can further be improved by making use of non-parametric topic modeling, where the number of topics can grow according to the document contents, thus giving the RF model the capability to adjust the number of topics based on the content of the top ranked documents. We empirically validate our topic model based RF approach on two document collections of diverse length and topical composition characteristics: (1) ad-hoc retrieval using the TREC 6-8 and the TREC Robust ’04 dataset, and (2) tweet retrieval using the TREC Microblog ’11 dataset. Results indicate that our proposed approach increases MAP by up to 9% in comparison to the results obtained with an LDA based language model (for initial retrieval) coupled with the relevance model (for feedback). Moreover, the non-parametric version of our proposed approach is shown to be more effective than its parametric counterpart due to its advantage of adapting the number of topics, improving results by up to 5.6% of MAP compared to the parametric version."
13,"The current debate on corporate social responsibility was triggered by the understanding that companies should be responsible for mitigating the impacts of their activities on the society and the environment. In this paper, drawing on the structural power of business, we examine the rule-setting power in which sugarcane ethanol companies engage, and then discuss why these companies develop and use institutions to promote sustainability. We use a machine learning algorithm, latent Dirichlet allocation, to identify companies’ commitment to sustainability and business-led governance by analyzing a large volume of data from public corporate documents. The results reveal 36 main themes that demonstrate the rule-setting power of the sugarcane ethanol industry through voluntary standards, codes of conduct, and corporate social responsibility these companies use as indicators of superior social and environmental performance and to show sustainability is embedded in companies’ priorities. However, the results also show critical issues of unsustainable production practices can affect industry image and stress socio-environmental relationships. These issues can be ameliorated with integrative governance that considers the independence of the sectors involved in ethanol production.",2018-10-01,2-s2.0-85049884289,Journal of Cleaner Production,Sustainability and governance of sugarcane ethanol companies in Brazil: Topic modeling analysis of CSR reporting,"The current debate on corporate social responsibility was triggered by the understanding that companies should be responsible for mitigating the impacts of their activities on the society and the environment. In this paper, drawing on the structural power of business, we examine the rule-setting power in which sugarcane ethanol companies engage, and then discuss why these companies develop and use institutions to promote sustainability. We use a machine learning algorithm, latent Dirichlet allocation, to identify companies’ commitment to sustainability and business-led governance by analyzing a large volume of data from public corporate documents. The results reveal 36 main themes that demonstrate the rule-setting power of the sugarcane ethanol industry through voluntary standards, codes of conduct, and corporate social responsibility these companies use as indicators of superior social and environmental performance and to show sustainability is embedded in companies’ priorities. However, the results also show critical issues of unsustainable production practices can affect industry image and stress socio-environmental relationships. These issues can be ameliorated with integrative governance that considers the independence of the sectors involved in ethanol production."
14,"One challenge in using naturalistic driving data is producing a holistic analysis of these highly variable datasets. Typical analyses focus on isolated events, such as large g-force accelerations indicating a possible near-crash. Examining isolated events is ill-suited for identifying patterns in continuous activities such as maintaining vehicle control. We present an alternative approach that converts driving data into a text representation and uses topic modeling to identify patterns across the dataset. This approach enables the discovery of non-linear patterns, reduces the dimensionality of the data, and captures subtle variations in driver behavior. In this study topic models were used to concisely described patterns in trips from drivers with and without untreated obstructive sleep apnea (OSA). The analysis included 5000 trips (50 trips from 100 drivers; 66 drivers with OSA; 34 comparison drivers). Trips were treated as documents, and speed and acceleration data from the trips were converted to “driving words.” The identified patterns, called topics, were determined based on regularities in the co-occurrence of the driving words within the trips. This representation was used in random forest models to predict the driver condition (i.e., OSA or comparison) for each trip. Models with 10, 15 and 20 topics had better accuracy in predicting the driver condition, with a maximum AUC of 0.73 for a model with 20 topics. Trips from drivers with OSA were more likely to be defined by topics for smaller lateral accelerations at low speeds. The results demonstrate topic modeling as a useful tool for extracting meaningful information from naturalistic driving datasets.",2018-10-01,2-s2.0-85048196361,Transportation Research Part F: Traffic Psychology and Behaviour,Using topic modeling to develop multi-level descriptions of naturalistic driving data from drivers with and without sleep apnea,"One challenge in using naturalistic driving data is producing a holistic analysis of these highly variable datasets. Typical analyses focus on isolated events, such as large g-force accelerations indicating a possible near-crash. Examining isolated events is ill-suited for identifying patterns in continuous activities such as maintaining vehicle control. We present an alternative approach that converts driving data into a text representation and uses topic modeling to identify patterns across the dataset. This approach enables the discovery of non-linear patterns, reduces the dimensionality of the data, and captures subtle variations in driver behavior. In this study topic models were used to concisely described patterns in trips from drivers with and without untreated obstructive sleep apnea (OSA). The analysis included 5000 trips (50 trips from 100 drivers; 66 drivers with OSA; 34 comparison drivers). Trips were treated as documents, and speed and acceleration data from the trips were converted to “driving words.” The identified patterns, called topics, were determined based on regularities in the co-occurrence of the driving words within the trips. This representation was used in random forest models to predict the driver condition (i.e., OSA or comparison) for each trip. Models with 10, 15 and 20 topics had better accuracy in predicting the driver condition, with a maximum AUC of 0.73 for a model with 20 topics. Trips from drivers with OSA were more likely to be defined by topics for smaller lateral accelerations at low speeds. The results demonstrate topic modeling as a useful tool for extracting meaningful information from naturalistic driving datasets."
15,"Cybercrime has become a growing business. The marketplaces for such businesses tend to be online forums. Much of the research on carding forums has been qualitative, but there have been quantitative analyses as well. One such type of analysis is topic modeling, a clustering technique that groups forum users according to the textual comments they leave. However, this type of research so far has been exclusively quantitative, without qualitatively examining the topics. The following study attempts to add to this research by analyzing the comment histories from 30,469 users from three carding forums. The results have revealed that users belong to one or more of 21 different topics. The topics are grouped into six broader categories, consisting of a customer base, identity fraud market, crimeware market, free content market, and two others. Descriptives are provided displaying how the topics are distributed across the three websites and directions for future research are discussed.",2018-10-01,2-s2.0-85053705445,Social Science Computer Review,Profiling Cybercriminals: Topic Model Clustering of Carding Forum Member Comment Histories,"Cybercrime has become a growing business. The marketplaces for such businesses tend to be online forums. Much of the research on carding forums has been qualitative, but there have been quantitative analyses as well. One such type of analysis is topic modeling, a clustering technique that groups forum users according to the textual comments they leave. However, this type of research so far has been exclusively quantitative, without qualitatively examining the topics. The following study attempts to add to this research by analyzing the comment histories from 30,469 users from three carding forums. The results have revealed that users belong to one or more of 21 different topics. The topics are grouped into six broader categories, consisting of a customer base, identity fraud market, crimeware market, free content market, and two others. Descriptives are provided displaying how the topics are distributed across the three websites and directions for future research are discussed."
16,"The CL-SciSumm 2016 shared task introduced an interesting problem: given a document D and a piece of text that cites D, how do we identify the text spans of D being referenced by the piece of text? The shared task provided the first annotated dataset for studying this problem. We present an analysis of our continued work in improving our system’s performance on this task. We demonstrate how topic models and word embeddings can be used to surpass the previously best performing system.",2018-09-01,2-s2.0-85021160173,International Journal on Digital Libraries,Identifying reference spans: topic modeling and word embeddings help IR,"The CL-SciSumm 2016 shared task introduced an interesting problem: given a document D and a piece of text that cites D, how do we identify the text spans of D being referenced by the piece of text? The shared task provided the first annotated dataset for studying this problem. We present an analysis of our continued work in improving our system’s performance on this task. We demonstrate how topic models and word embeddings can be used to surpass the previously best performing system."
17,"In this article, we use topic modeling to systematically explore topics discussed in contemporary art criticism. Analyzing 6965 articles published between 1991 and 2015 in Frieze, a leading art magazine, we find a plurality of topics characterizing professional discourse on contemporary art. Not surprisingly, media- or genre-specific topics such as film/cinema, photography, sculpture/installations, etc. emerge. Interestingly, extra-artistic topics also characterize contemporary art criticism: there is room for articles on new digital technology and on art and philosophy; there is also growing interest in the relationship between art and society. Our analysis shows that despite evolutions in the field of contemporary art – such as the ‘social turn’, in which contemporary art starts paying more attention to social forms and content – the prevalence of certain topics in contemporary art criticism has barely changed over the past 25 years. With this article, we demonstrate the unique value of topic modeling for cultural sociology: it is both a powerful computational technique to generate a bird’s-eye view of a huge text corpus and a heuristic device that locates key texts for further close reading.",2018-09-01,2-s2.0-85051520460,Cultural Sociology,Trends in Contemporary Art Discourse: Using Topic Models to Analyze 25 years of Professional Art Criticism,"In this article, we use topic modeling to systematically explore topics discussed in contemporary art criticism. Analyzing 6965 articles published between 1991 and 2015 in Frieze, a leading art magazine, we find a plurality of topics characterizing professional discourse on contemporary art. Not surprisingly, media- or genre-specific topics such as film/cinema, photography, sculpture/installations, etc. emerge. Interestingly, extra-artistic topics also characterize contemporary art criticism: there is room for articles on new digital technology and on art and philosophy; there is also growing interest in the relationship between art and society. Our analysis shows that despite evolutions in the field of contemporary art – such as the ‘social turn’, in which contemporary art starts paying more attention to social forms and content – the prevalence of certain topics in contemporary art criticism has barely changed over the past 25 years. With this article, we demonstrate the unique value of topic modeling for cultural sociology: it is both a powerful computational technique to generate a bird’s-eye view of a huge text corpus and a heuristic device that locates key texts for further close reading."
18,"Does domestic contestation of European Union legitimacy affect the behaviour of the European Commission as an economic and fiscal supervisor? We draw on theories of bureaucratic responsiveness and employ multilevel and topic modelling to examine the extent to which the politicisation of European integration affects the outputs of the European Semester: the Country-Specific Recommendations. We develop two competing sets of hypotheses and test these on an original large-N data set on Commission behaviour with observations covering the period 2011–2017. We detect a twofold effect on the Commission's recommendations: member states that experience greater politicisation receive recommendations that are larger in scope but whose substance is less oriented towards social investment. We argue that this effect is best explained as an outcome of the Commission's institutional risk management strategy of regulatory ‘entrenchment’. The supranational agent issues additional recommendations while simultaneously entrenching on a stronger mandate substantively, which allows it to maintain its regulatory reputation and signal regulatory resolve to observing audiences.",2018-09-01,2-s2.0-85046728892,European Union Politics,Bread and butter or bread and circuses? Politicisation and the European Commission in the European Semester,"Does domestic contestation of European Union legitimacy affect the behaviour of the European Commission as an economic and fiscal supervisor? We draw on theories of bureaucratic responsiveness and employ multilevel and topic modelling to examine the extent to which the politicisation of European integration affects the outputs of the European Semester: the Country-Specific Recommendations. We develop two competing sets of hypotheses and test these on an original large-N data set on Commission behaviour with observations covering the period 2011–2017. We detect a twofold effect on the Commission's recommendations: member states that experience greater politicisation receive recommendations that are larger in scope but whose substance is less oriented towards social investment. We argue that this effect is best explained as an outcome of the Commission's institutional risk management strategy of regulatory ‘entrenchment’. The supranational agent issues additional recommendations while simultaneously entrenching on a stronger mandate substantively, which allows it to maintain its regulatory reputation and signal regulatory resolve to observing audiences."
19,"Large-scale infrastructure projects have important strategic positions in economy and social development, especially in developing countries, and are growing constantly larger and more complex. During the life cycle of a large-scale infrastructure project, technological problems and environmental and socio-economic impacts drive relevant scientific research. However, few empirical studies have been conducted to examine the interaction between project and research. In this paper, we use the Three Gorges Project (TGP), which is the world's largest hydropower project, as a case study to find the patterns of scientific research on specific large-scale infrastructure projects. Text mining and other quantitative methods have been used to reveal: (1) the growth of the publication outputs of the TGP relevant research, (2) major research topics and their temporal trends and (3) the effects of language and corresponding author's nationality on topic proportions in papers. We find that topics of engineering issues were discussed more in domestic journals and in earlier stage of the project's life cycle. Meanwhile, topics of environmental issues were discussed more in international journals and became increasingly popular during the life cycle of the project. These findings are useful for policy-makers and project managers to better establish collaborations with the academia and to more properly allocate resources in future project management practices.",2018-09-01,2-s2.0-85047395068,Technological Forecasting and Social Change,Scientific research driven by large-scale infrastructure projects: A case study of the Three Gorges Project in China,"Large-scale infrastructure projects have important strategic positions in economy and social development, especially in developing countries, and are growing constantly larger and more complex. During the life cycle of a large-scale infrastructure project, technological problems and environmental and socio-economic impacts drive relevant scientific research. However, few empirical studies have been conducted to examine the interaction between project and research. In this paper, we use the Three Gorges Project (TGP), which is the world's largest hydropower project, as a case study to find the patterns of scientific research on specific large-scale infrastructure projects. Text mining and other quantitative methods have been used to reveal: (1) the growth of the publication outputs of the TGP relevant research, (2) major research topics and their temporal trends and (3) the effects of language and corresponding author's nationality on topic proportions in papers. We find that topics of engineering issues were discussed more in domestic journals and in earlier stage of the project's life cycle. Meanwhile, topics of environmental issues were discussed more in international journals and became increasingly popular during the life cycle of the project. These findings are useful for policy-makers and project managers to better establish collaborations with the academia and to more properly allocate resources in future project management practices."
20,"This paper presents a novel system, synthetic high-fidelity social media data generator (SHIELD), for generating the synthetic social media data. SHIELD jointly generates time-varying, directed and weighted interaction graph structures and topic-driven text features similar to the input social media data. A synthetic interaction graph is generated by a social network model to minimize the distance to real graph and is enhanced by adding various patterns, such as anomalies and information cascades, interaction types, and temporal dynamics. A synthetic text generator based on the n-gram Markov model is trained under each topic identified by topic modeling. Synthetic text and graph structures are combined through the assignment of synthetic social media entities. Extensive performance evaluation via a graph and text analysis is provided to demonstrate the statistical fidelity of large-scale synthetic data generated by SHIELD. A data evaluation exercise with human participants is executed to identify how difficult it is for a human to distinguish between tweets that were generated by SHIELD and tweets that were posted by real users. Experimental results followed by a statistical significance analysis showed that human participants cannot reliably distinguish between real and synthetic tweets.",2018-09-01,2-s2.0-85051699146,IEEE Transactions on Computational Social Systems,Synthetic social media data generation,"This paper presents a novel system, synthetic high-fidelity social media data generator (SHIELD), for generating the synthetic social media data. SHIELD jointly generates time-varying, directed and weighted interaction graph structures and topic-driven text features similar to the input social media data. A synthetic interaction graph is generated by a social network model to minimize the distance to real graph and is enhanced by adding various patterns, such as anomalies and information cascades, interaction types, and temporal dynamics. A synthetic text generator based on the n-gram Markov model is trained under each topic identified by topic modeling. Synthetic text and graph structures are combined through the assignment of synthetic social media entities. Extensive performance evaluation via a graph and text analysis is provided to demonstrate the statistical fidelity of large-scale synthetic data generated by SHIELD. A data evaluation exercise with human participants is executed to identify how difficult it is for a human to distinguish between tweets that were generated by SHIELD and tweets that were posted by real users. Experimental results followed by a statistical significance analysis showed that human participants cannot reliably distinguish between real and synthetic tweets."
21,"The rapid expansion of Big Data Analytics is forcing companies to rethink their Human Resource (HR) needs. However, at the same time, it is unclear which types of job roles and skills constitute this area. To this end, this study pursues to drive clarity across the heterogeneous nature of skills required in Big Data professions, by analyzing a large amount of real-world job posts published online. More precisely we: 1) identify four Big Data ‘job families’; 2) recognize nine homogeneous groups of Big Data skills (skill sets) that are being demanded by companies; 3) characterize each job family with the appropriate level of competence required within each Big Data skill set. We propose a novel, semi-automated, fully replicable, analytical methodology based on a combination of machine learning algorithms and expert judgement. Our analysis leverages a significant amount of online job posts, obtained through web scraping, to generate an intelligible classification of job roles and skill sets. The results can support business leaders and HR managers in establishing clear strategies for the acquisition and the development of the right skills needed to leverage Big Data at best. Moreover, the structured classification of job families and skill sets will help establish a common dictionary to be used by HR recruiters and education providers, so that supply and demand can more effectively meet in the job marketplace.",2018-09-01,2-s2.0-85020243753,Information Processing and Management,Human resources for Big Data professions: A systematic classification of job roles and required skill sets,"The rapid expansion of Big Data Analytics is forcing companies to rethink their Human Resource (HR) needs. However, at the same time, it is unclear which types of job roles and skills constitute this area. To this end, this study pursues to drive clarity across the heterogeneous nature of skills required in Big Data professions, by analyzing a large amount of real-world job posts published online. More precisely we: 1) identify four Big Data ‘job families’; 2) recognize nine homogeneous groups of Big Data skills (skill sets) that are being demanded by companies; 3) characterize each job family with the appropriate level of competence required within each Big Data skill set. We propose a novel, semi-automated, fully replicable, analytical methodology based on a combination of machine learning algorithms and expert judgement. Our analysis leverages a significant amount of online job posts, obtained through web scraping, to generate an intelligible classification of job roles and skill sets. The results can support business leaders and HR managers in establishing clear strategies for the acquisition and the development of the right skills needed to leverage Big Data at best. Moreover, the structured classification of job families and skill sets will help establish a common dictionary to be used by HR recruiters and education providers, so that supply and demand can more effectively meet in the job marketplace."
22,"This article presents a case study in which we have mined the contents of the University of Chicago's ARTFL digital library as part of a digital history experiment. Two corpora of articles were extracted from the online version of Diderot's Encyclopédie in order to answer a two-part question about the production of an imagined geography through the reading of texts. An explicitly geographical corpus of 14,547 articles allowed us to characterize the evolution of the way the Encyclopédie produces places, an evolution caused by the transition between Diderot's initial vision and that of the corpus' most important author, knight Louis de Jaucourt. A second corpus, containing 6,053 articles from all areas of knowledge, was then used to study the production of space, specifically the representation of America as an object of scientific curiosity and of potential riches. Some methodological issues related to the use of digital libraries by historians are also discussed.",2018-08-31,2-s2.0-85053207448,Document Numerique,La production de l'espace dans l'Encyclopédie Portraits d'une géographie imaginée,"This article presents a case study in which we have mined the contents of the University of Chicago's ARTFL digital library as part of a digital history experiment. Two corpora of articles were extracted from the online version of Diderot's Encyclopédie in order to answer a two-part question about the production of an imagined geography through the reading of texts. An explicitly geographical corpus of 14,547 articles allowed us to characterize the evolution of the way the Encyclopédie produces places, an evolution caused by the transition between Diderot's initial vision and that of the corpus' most important author, knight Louis de Jaucourt. A second corpus, containing 6,053 articles from all areas of knowledge, was then used to study the production of space, specifically the representation of America as an object of scientific curiosity and of potential riches. Some methodological issues related to the use of digital libraries by historians are also discussed."
23,"Although it is obvious that research regarding Sustainable Forest Management (SFM) is context specific and developed over time, not many research papers yet intended to investigate these changes. As a matter of fact, the number of scientific publications addressing SFM is relatively high. Hence, such a wide field cannot be sufficiently covered by traditional literature review approaches. With this paper, we aim at identifying the most convergent narratives within the SFM-research landscape by applying a text mining methodology to recent scientific literature. By doing so, we generated results that indicate that there may have been three phases in the evolution of SFM-research: the early phase covers in particular issues regarding land use in tropical and developing countries. Furthermore, papers in this phase tend to focus on general concepts or policy issues. In contrast, the second phase is characterized by a larger share of publications in forestry focused journals. This process is seemingly connected with issues like forest management, certification, forest stand management and the development of sustainability indicators. A third phase can be observed by the relative downturn of publications in forest-focused journals between 2005 and 2010. A new focus in this period is climate change.",2018-08-18,2-s2.0-85041517626,Journal of Sustainable Forestry,Identifying sustainable forest management research narratives: a text mining approach,"Although it is obvious that research regarding Sustainable Forest Management (SFM) is context specific and developed over time, not many research papers yet intended to investigate these changes. As a matter of fact, the number of scientific publications addressing SFM is relatively high. Hence, such a wide field cannot be sufficiently covered by traditional literature review approaches. With this paper, we aim at identifying the most convergent narratives within the SFM-research landscape by applying a text mining methodology to recent scientific literature. By doing so, we generated results that indicate that there may have been three phases in the evolution of SFM-research: the early phase covers in particular issues regarding land use in tropical and developing countries. Furthermore, papers in this phase tend to focus on general concepts or policy issues. In contrast, the second phase is characterized by a larger share of publications in forestry focused journals. This process is seemingly connected with issues like forest management, certification, forest stand management and the development of sustainability indicators. A third phase can be observed by the relative downturn of publications in forest-focused journals between 2005 and 2010. A new focus in this period is climate change."
24,"Course forums offer an interactive channel for learners to express opinions and feedback, which contain valuable emotions and topic information towards courses. In this paper, we propose an emotion oriented topic probabilistic model that can be used to calculate distributions of emotion-Topic over words to discover what students are most concerned about. An experiment on real-life data indicates that students had a positive attitude for knowledge applications, a negative experience for the learning system, and expressed confusion about the final exam. We also visualize the temporal trends of emotions of the whole group and the groups with different levels of achievement. The proposed model has a potential in discovering students' emotions in their feedback, thus improving the online learning experience, and identifying at-risk students timely.",2018-08-10,2-s2.0-85052497868,"Proceedings - IEEE 18th International Conference on Advanced Learning Technologies, ICALT 2018",An emotion oriented topic modeling approach to discover what students are concerned about in course forums,"Course forums offer an interactive channel for learners to express opinions and feedback, which contain valuable emotions and topic information towards courses. In this paper, we propose an emotion oriented topic probabilistic model that can be used to calculate distributions of emotion-Topic over words to discover what students are most concerned about. An experiment on real-life data indicates that students had a positive attitude for knowledge applications, a negative experience for the learning system, and expressed confusion about the final exam. We also visualize the temporal trends of emotions of the whole group and the groups with different levels of achievement. The proposed model has a potential in discovering students' emotions in their feedback, thus improving the online learning experience, and identifying at-risk students timely."
25,"Social media today are playing a more important role as a news source than ever before. Yet, there have been no longitudinal studies on journalists’ sourcing practices in recent years that allow us to consider the mechanisms of innovation diffusion. Comparative studies of different social platforms in different media systems are just as rare. We therefore examine the use of Facebook and Twitter as journalistic sources in newspapers of three countries. A main finding is that, after a period of stagnation at the beginning of this decade, the use of social media sources has resurged massively in recent years. The patterns of this second rise of social media in journalism are almost identical in the analyzed newspapers. A comparison of the platforms has shown that Twitter is more commonly used as a news source than Facebook. Compared to Facebook, Twitter is primarily used as an elite channel. An unsupervised topic clustering approach (LDA) also revealed that the issues on which social media are sourced and the quantities of social media references are similar in The New York Times and The Guardian. In Süddeutsche Zeitung, however, journalists source social media considerably less, and in different thematic contexts.",2018-08-09,2-s2.0-85053631665,Digital Journalism,"Sourcing the Sources: An analysis of the use of Twitter and Facebook as a journalistic source over 10 years in The New York Times, The Guardian, and Süddeutsche Zeitung","Social media today are playing a more important role as a news source than ever before. Yet, there have been no longitudinal studies on journalists’ sourcing practices in recent years that allow us to consider the mechanisms of innovation diffusion. Comparative studies of different social platforms in different media systems are just as rare. We therefore examine the use of Facebook and Twitter as journalistic sources in newspapers of three countries. A main finding is that, after a period of stagnation at the beginning of this decade, the use of social media sources has resurged massively in recent years. The patterns of this second rise of social media in journalism are almost identical in the analyzed newspapers. A comparison of the platforms has shown that Twitter is more commonly used as a news source than Facebook. Compared to Facebook, Twitter is primarily used as an elite channel. An unsupervised topic clustering approach (LDA) also revealed that the issues on which social media are sourced and the quantities of social media references are similar in The New York Times and The Guardian. In Süddeutsche Zeitung, however, journalists source social media considerably less, and in different thematic contexts."
26,"Growing competition among manufacturing businesses and the advent of the Fourth Industrial Revolution has meant that many countries are conducting various research projects to understand how to introduce and populate smart factories. Smart factories are expected to provide a way of solving the manufacturing industries' complex problems, to take a role in breakthroughs in factories and to carry on a sustainable business. Smart factories are currently in the introduction stage, so we should follow up on the majorities and check their tendencies. However, smart-factory research is an interdisciplinary field that should be studied by researchers with diverse backgrounds in various domains. Thus, studying the past and present overall research trends of smart factory studies is required for their successful introduction and sustainable research. In this study, we explored the research trends of smart factories in both international and specifically Korean research, as an example of a nation case, to determine the major research directions. We determined trends using latent semantic analysis, which is a known topic-modeling technique, and analyzed the trends with regression-based methods. As a result, we could read the clear trends by analyzing existing studies related to smart factories. In addition, it is possible to compare research trends in Korea and international research trends for the commonly appeared topics, such as 'ICT' (Information and Communications Technology) and 'R & D (Research and Development)/Technology Innovation'. We expect that the quantitative analysis results and suggestions presented in this study can be used to formulate strategies for the future diffusion of smart factories.",2018-08-06,2-s2.0-85051119899,Sustainability (Switzerland),Exploring the research trend of smart factory with topic modeling,"Growing competition among manufacturing businesses and the advent of the Fourth Industrial Revolution has meant that many countries are conducting various research projects to understand how to introduce and populate smart factories. Smart factories are expected to provide a way of solving the manufacturing industries' complex problems, to take a role in breakthroughs in factories and to carry on a sustainable business. Smart factories are currently in the introduction stage, so we should follow up on the majorities and check their tendencies. However, smart-factory research is an interdisciplinary field that should be studied by researchers with diverse backgrounds in various domains. Thus, studying the past and present overall research trends of smart factory studies is required for their successful introduction and sustainable research. In this study, we explored the research trends of smart factories in both international and specifically Korean research, as an example of a nation case, to determine the major research directions. We determined trends using latent semantic analysis, which is a known topic-modeling technique, and analyzed the trends with regression-based methods. As a result, we could read the clear trends by analyzing existing studies related to smart factories. In addition, it is possible to compare research trends in Korea and international research trends for the commonly appeared topics, such as 'ICT' (Information and Communications Technology) and 'R & D (Research and Development)/Technology Innovation'. We expect that the quantitative analysis results and suggestions presented in this study can be used to formulate strategies for the future diffusion of smart factories."
27,"Entrepreneurial ecosystems are vital sources of innovation and critical engines for economic growth. In this study, we use text-based analysis, network visualizations, and topic modeling of nearly 60&#x2009;000 venture business descriptions&#x2014;i.e., how ventures present technologies and products to key stakeholders including customers, employees, and investors&#x2014;to examine the structure of 35 global entrepreneurial ecosystems. Rather than using predefined industry classifications, we allow the structural configurations to emerge endogenously revealing a more variegated perspective of venture strategic positioning in entrepreneurial ecosystems. Our study makes several important contributions. First, by examining strategic positioning statements of geographically defined ventures, we contribute and advance our understanding of the geography of innovation and structure of entrepreneurial ecosystems. Our results indicate that there are wide differences in entrepreneurial ecosystem size, structure, composition, and venture strategic positioning. Second, methodologically, we use novel computational approaches and introduce visualization as a powerful means to understand entrepreneurial ecosystems. Third, our results show that ventures from widely different industries often use similar position statements, thus highlighting that ecosystems are indeed not just defined by industries, but also strategic positioning. We conclude with theoretical and managerial implications.",2018-08-03,2-s2.0-85051011185,IEEE Transactions on Engineering Management,Visual Analysis of Venture Similarity in Entrepreneurial Ecosystems,"Entrepreneurial ecosystems are vital sources of innovation and critical engines for economic growth. In this study, we use text-based analysis, network visualizations, and topic modeling of nearly 60&#x2009;000 venture business descriptions&#x2014;i.e., how ventures present technologies and products to key stakeholders including customers, employees, and investors&#x2014;to examine the structure of 35 global entrepreneurial ecosystems. Rather than using predefined industry classifications, we allow the structural configurations to emerge endogenously revealing a more variegated perspective of venture strategic positioning in entrepreneurial ecosystems. Our study makes several important contributions. First, by examining strategic positioning statements of geographically defined ventures, we contribute and advance our understanding of the geography of innovation and structure of entrepreneurial ecosystems. Our results indicate that there are wide differences in entrepreneurial ecosystem size, structure, composition, and venture strategic positioning. Second, methodologically, we use novel computational approaches and introduce visualization as a powerful means to understand entrepreneurial ecosystems. Third, our results show that ventures from widely different industries often use similar position statements, thus highlighting that ecosystems are indeed not just defined by industries, but also strategic positioning. We conclude with theoretical and managerial implications."
28,"Modern large-scale IT operations are getting complex every day and generate thousands of log and event files, which contain essential information about the systems and their behavior. System errors are examined manually by human experts, which takes a considerable amount of time and efforts. The system log files, besides other attributes such as time and location, contain messages in textual form, which is essential for analyzing logs behavior. In this paper, we propose a method to extract meaningful information through topic modeling from log messages, combine them into a sequence of events and present them to the practitioners to visually identify abnormal behavioral patterns, which is used for predictive analysis.",2018-08-02,2-s2.0-85052306307,"Proceedings - 2018 IEEE 19th International Conference on Information Reuse and Integration for Data Science, IRI 2018",Visualization of server log data for detecting abnormal behaviour,"Modern large-scale IT operations are getting complex every day and generate thousands of log and event files, which contain essential information about the systems and their behavior. System errors are examined manually by human experts, which takes a considerable amount of time and efforts. The system log files, besides other attributes such as time and location, contain messages in textual form, which is essential for analyzing logs behavior. In this paper, we propose a method to extract meaningful information through topic modeling from log messages, combine them into a sequence of events and present them to the practitioners to visually identify abnormal behavioral patterns, which is used for predictive analysis."
29,"Due to recent explosion of text data, researchers have been overwhelmed by ever-increasing volume of articles produced by different research communities. Various scholarly search websites, citation recommendation engines, and research databases have been created to simplify the text search tasks. However, it is still difficult for researchers to be able to identify potential research topics without doing intensive reviews on a tremendous number of articles published by journals, conferences, meetings, and workshops. In this paper, we consider a novel topic diffusion discovery technique that incorporates sparseness-constrained Non-negative Matrix Factorization with generalized Jensen-Shannon divergence to help understand term-topic evolutions and identify topic diffusions. Our experimental result shows that this approach can extract more prominent topics from large article databases, visualize relationships between terms of interest and abstract topics, and further help researchers understand whether given terms/topics have been widely explored or whether new topics are emerging from literature.",2018-08-02,2-s2.0-85052303344,"Proceedings - 2018 IEEE 19th International Conference on Information Reuse and Integration for Data Science, IRI 2018",Topic diffusion discovery based on sparseness-constrained non-negative matrix factorization,"Due to recent explosion of text data, researchers have been overwhelmed by ever-increasing volume of articles produced by different research communities. Various scholarly search websites, citation recommendation engines, and research databases have been created to simplify the text search tasks. However, it is still difficult for researchers to be able to identify potential research topics without doing intensive reviews on a tremendous number of articles published by journals, conferences, meetings, and workshops. In this paper, we consider a novel topic diffusion discovery technique that incorporates sparseness-constrained Non-negative Matrix Factorization with generalized Jensen-Shannon divergence to help understand term-topic evolutions and identify topic diffusions. Our experimental result shows that this approach can extract more prominent topics from large article databases, visualize relationships between terms of interest and abstract topics, and further help researchers understand whether given terms/topics have been widely explored or whether new topics are emerging from literature."
30,"The purpose of this paper is to demonstrate that user-generated online contents can be used as an alternative data source for assessing airport service quality, which effectively complements and cross-validates the conventional service quality surveys. We apply sentiment analysis and topic modeling technique to 42,137 reviews collected from Google Maps. The results are compared to the well-publicized ASQ ratings conducted by Airport Council International. The sentiment scores computed from the textual Google reviews are very good predictors of the associated Google star ratings, with r",2018-08-01,2-s2.0-85048149191,Journal of Air Transport Management,Assessment of airport service quality: A complementary approach to measure perceived service quality based on Google reviews,"The purpose of this paper is to demonstrate that user-generated online contents can be used as an alternative data source for assessing airport service quality, which effectively complements and cross-validates the conventional service quality surveys. We apply sentiment analysis and topic modeling technique to 42,137 reviews collected from Google Maps. The results are compared to the well-publicized ASQ ratings conducted by Airport Council International. The sentiment scores computed from the textual Google reviews are very good predictors of the associated Google star ratings, with r"
31,"In recent decades, the amount of text available for organizational science research has grown tremendously. Despite the availability of text and advances in text analysis methods, many of these techniques remain largely segmented by discipline. Moreover, there is an increasing number of open-source tools (R, Python) for text analysis, yet these tools are not easily taken advantage of by social science researchers who likely have limited programming knowledge and exposure to computational methods. In this article, we compare quantitative and qualitative text analysis methods used across social sciences. We describe basic terminology and the overlooked, but critically important, steps in pre-processing raw text (e.g., selection of stop words; stemming). Next, we provide an exploratory analysis of open-ended responses from a prototypical survey dataset using topic modeling with R. We provide a list of best practice recommendations for text analysis focused on (1) hypothesis and question formation, (2) design and data collection, (3) data pre-processing, and (4) topic modeling. We also discuss the creation of scale scores for more traditional correlation and regression analyses. All the data are available in an online repository for the interested reader to practice with, along with a reference list for additional reading, an R markdown file, and an open source interactive topic model tool (topicApp; see https://github.com/wesslen/topicApp, https://github.com/wesslen/text-analysis-org-science, https://dataverse.unc.edu/dataset.xhtml?persistentId=doi:10.15139/S3/R4W7ZS).",2018-08-01,2-s2.0-85040335587,Journal of Business and Psychology,A Review of Best Practice Recommendations for Text Analysis in R (and a User-Friendly App),"In recent decades, the amount of text available for organizational science research has grown tremendously. Despite the availability of text and advances in text analysis methods, many of these techniques remain largely segmented by discipline. Moreover, there is an increasing number of open-source tools (R, Python) for text analysis, yet these tools are not easily taken advantage of by social science researchers who likely have limited programming knowledge and exposure to computational methods. In this article, we compare quantitative and qualitative text analysis methods used across social sciences. We describe basic terminology and the overlooked, but critically important, steps in pre-processing raw text (e.g., selection of stop words; stemming). Next, we provide an exploratory analysis of open-ended responses from a prototypical survey dataset using topic modeling with R. We provide a list of best practice recommendations for text analysis focused on (1) hypothesis and question formation, (2) design and data collection, (3) data pre-processing, and (4) topic modeling. We also discuss the creation of scale scores for more traditional correlation and regression analyses. All the data are available in an online repository for the interested reader to practice with, along with a reference list for additional reading, an R markdown file, and an open source interactive topic model tool (topicApp; see https://github.com/wesslen/topicApp, https://github.com/wesslen/text-analysis-org-science, https://dataverse.unc.edu/dataset.xhtml?persistentId=doi:10.15139/S3/R4W7ZS)."
32,"Research has shown that adornment strategies such as “theming” are a prominent and patterned phenomenon in institutional organizational life. Scholars have yet to investigate if, and how, extra-institutional organizations such as terrorist organizations similarly utilize theming as a strategy to signal to their primary consumer base: recruits. To remedy this, we utilize a variety of computational topic modeling techniques built from Latent Dirichlet Analysis (LDA) to analyze a corpus of propaganda aimed at Western audiences. Drawing on the markets-as-politics paradigm we show how Al-Qaeda in the Arabian Peninsula (AQAP) and the Islamic State organization (ISIS) maintain similar strategies of signaling global illegitimacy while cultivating distinct themes of terrorism in an effort to differentiate themselves from one another. We argue that terrorist organizations use propaganda to signal distinct organizational identity, even while broadly operating under the umbrella identity of Islamic fundamentalism. This focus on extra-legal organizations elaborates upon fundamental assumption of markets-as-politics and problematizes prior research asserting that strategies utilized by terrorist organizations to convey meaning are fundamentally the same.",2018-08-01,2-s2.0-85046879488,Poetics,Theming for terror: Organizational adornment in terrorist propaganda,"Research has shown that adornment strategies such as “theming” are a prominent and patterned phenomenon in institutional organizational life. Scholars have yet to investigate if, and how, extra-institutional organizations such as terrorist organizations similarly utilize theming as a strategy to signal to their primary consumer base: recruits. To remedy this, we utilize a variety of computational topic modeling techniques built from Latent Dirichlet Analysis (LDA) to analyze a corpus of propaganda aimed at Western audiences. Drawing on the markets-as-politics paradigm we show how Al-Qaeda in the Arabian Peninsula (AQAP) and the Islamic State organization (ISIS) maintain similar strategies of signaling global illegitimacy while cultivating distinct themes of terrorism in an effort to differentiate themselves from one another. We argue that terrorist organizations use propaganda to signal distinct organizational identity, even while broadly operating under the umbrella identity of Islamic fundamentalism. This focus on extra-legal organizations elaborates upon fundamental assumption of markets-as-politics and problematizes prior research asserting that strategies utilized by terrorist organizations to convey meaning are fundamentally the same."
33,"The need to cluster small text corpora composed of a few hundreds of short texts rises in various applications; e.g., clustering top-retrieved documents based on their snippets. This clustering task is challenging due to the vocabulary mismatch between short texts and the insufficient corpus-based statistics (e.g., term co-occurrence statistics) due to the corpus size. We address this clustering challenge using a framework that utilizes a set of external knowledge resources that provide information about term relations. Specifically, we use information induced from the resources to estimate similarity between terms and produce term clusters. We also utilize the resources to expand the vocabulary used in the given corpus and thus enhance term clustering. We then project the texts in the corpus onto the term clusters to cluster the texts. We evaluate various instantiations of the proposed framework by varying the term clustering method used, the approach of projecting the texts onto the term clusters, and the way of applying external knowledge resources. Extensive empirical evaluation demonstrates the merits of our approach with respect to applying clustering algorithms directly on the text corpus, and using state-of-the-art co-clustering and topic modeling methods.",2018-08-01,2-s2.0-85035786981,Information Retrieval Journal,Clustering small-sized collections of short texts,"The need to cluster small text corpora composed of a few hundreds of short texts rises in various applications; e.g., clustering top-retrieved documents based on their snippets. This clustering task is challenging due to the vocabulary mismatch between short texts and the insufficient corpus-based statistics (e.g., term co-occurrence statistics) due to the corpus size. We address this clustering challenge using a framework that utilizes a set of external knowledge resources that provide information about term relations. Specifically, we use information induced from the resources to estimate similarity between terms and produce term clusters. We also utilize the resources to expand the vocabulary used in the given corpus and thus enhance term clustering. We then project the texts in the corpus onto the term clusters to cluster the texts. We evaluate various instantiations of the proposed framework by varying the term clustering method used, the approach of projecting the texts onto the term clusters, and the way of applying external knowledge resources. Extensive empirical evaluation demonstrates the merits of our approach with respect to applying clustering algorithms directly on the text corpus, and using state-of-the-art co-clustering and topic modeling methods."
34,"Social media platforms such as Facebook, Instagram, and Twitter have drastically altered the way information is generated and disseminated. These platforms allow their users to report events and express their opinions toward these events. The profusion of data generated through social media has proved to have the potential for improving the efficiency of existing traffic management systems and transportation analytics. This study complements existing literature by proposing a framework to evaluate transit riders’ opinion about quality of transit service using Twitter data. Although previous studies used keyword search to extract transit-related tweets, the extracted tweets can still be noisy and might not be relevant to transit quality of service at all. In this study, we leverage topic modeling, an unsupervised machine learning technique, to sift tweets that are relevant to the actual user experience of the transit system. Sentiment analysis is further performed based on the tweet-per-topic index we developed, to gauge transit riders’ feedback and explore the underlying reasons causing their dissatisfaction on the service. This framework can be potentially quite useful to transit agencies for user-oriented analysis and to assist with investment decision making.",2018-08-01,2-s2.0-85052294717,Public Transport,Using Twitter data for transit performance assessment: a framework for evaluating transit riders’ opinions about quality of service,"Social media platforms such as Facebook, Instagram, and Twitter have drastically altered the way information is generated and disseminated. These platforms allow their users to report events and express their opinions toward these events. The profusion of data generated through social media has proved to have the potential for improving the efficiency of existing traffic management systems and transportation analytics. This study complements existing literature by proposing a framework to evaluate transit riders’ opinion about quality of transit service using Twitter data. Although previous studies used keyword search to extract transit-related tweets, the extracted tweets can still be noisy and might not be relevant to transit quality of service at all. In this study, we leverage topic modeling, an unsupervised machine learning technique, to sift tweets that are relevant to the actual user experience of the transit system. Sentiment analysis is further performed based on the tweet-per-topic index we developed, to gauge transit riders’ feedback and explore the underlying reasons causing their dissatisfaction on the service. This framework can be potentially quite useful to transit agencies for user-oriented analysis and to assist with investment decision making."
35,"The Triple Helix concept of innovation systems holds that consensus space among industry, government and university is required to bring together their competences to achieve enhanced economic and social development on a systemic scale. In line with this argument, this article analyses empirically how the concept of circular economy is conceived in the institutional spheres of ""industry"", ""government"" and ""university"". Innovation systems are constantly being reconstructed through knowledge production and communication, which is reflected in how concepts develop in the different spheres. By applying natural language processing tools to key contributions from each of the three spheres (the ""Triple Helix""), it is shown that, although institutional backgrounds do contribute to differing conceptualizations of circular economy, there is a substantial but limited conceptual consensus space, which, according to the Triple Helix, should open new opportunities for innovations. The consensus space shared across the three spheres focuses on materials and products and sees circular economy as a way to create new resources, businesses and products from waste. The industry sphere highlights business opportunities on global scale, which are also evident in the government sphere. The government sphere connects circular economy to waste-related innovation policies targeted at industrial renewal, economic growth, investments and jobs. The university sphere, in turn, focuses on production and environmental issues, waste and knowledge, and is rather distinct from the two other spheres. The importance of the differing conceptions of circular economy is based on the logic of Triple Helix systems. Accordingly, sufficient consensus between the Triple Helix spheres can advance the application of the concept of circular economy beyond the individual spheres to achieve systemic changes.",2018-07-27,2-s2.0-85051088778,Sustainability (Switzerland),Circular economy in the Triple Helix of innovation systems,"The Triple Helix concept of innovation systems holds that consensus space among industry, government and university is required to bring together their competences to achieve enhanced economic and social development on a systemic scale. In line with this argument, this article analyses empirically how the concept of circular economy is conceived in the institutional spheres of ""industry"", ""government"" and ""university"". Innovation systems are constantly being reconstructed through knowledge production and communication, which is reflected in how concepts develop in the different spheres. By applying natural language processing tools to key contributions from each of the three spheres (the ""Triple Helix""), it is shown that, although institutional backgrounds do contribute to differing conceptualizations of circular economy, there is a substantial but limited conceptual consensus space, which, according to the Triple Helix, should open new opportunities for innovations. The consensus space shared across the three spheres focuses on materials and products and sees circular economy as a way to create new resources, businesses and products from waste. The industry sphere highlights business opportunities on global scale, which are also evident in the government sphere. The government sphere connects circular economy to waste-related innovation policies targeted at industrial renewal, economic growth, investments and jobs. The university sphere, in turn, focuses on production and environmental issues, waste and knowledge, and is rather distinct from the two other spheres. The importance of the differing conceptions of circular economy is based on the logic of Triple Helix systems. Accordingly, sufficient consensus between the Triple Helix spheres can advance the application of the concept of circular economy beyond the individual spheres to achieve systemic changes."
36,"With the rapid development of social media and e-commerce on the Internet, many short texts such as instant messages, tweets and product comments have become an important form of Internet information. Compared with regular texts, short texts suffer severe data sparsity and informal format. Because of that, mining latent semantics such as topics in short texts is a critical challenge. There are many limitations directly applying common topic models like Latent Dirichlet Allocation (LDA) to short texts. In result of that, many short text topic models were put forward. The Word Network Topic Model (WNTM) is the typical one among them, which transfers sparse document to word space into dense word to word space and then learn topics from it. However, WNTM has its own limitation. The word co-occurrence matrix of the original document is used to construct the word network. This method is simple and intuitive, which can't express the deep meaning between words and words. To solve this problem, we proposed W2V-WNTM, which uses Word2Vec model instead of word co-occurrence to construct word network. Experiment shows it can describe relationship between words better.",2018-07-05,2-s2.0-85050691415,"Proceedings - IEEE 4th International Conference on Big Data Computing Service and Applications, BigDataService 2018",Word network topic model based on Word2Vector,"With the rapid development of social media and e-commerce on the Internet, many short texts such as instant messages, tweets and product comments have become an important form of Internet information. Compared with regular texts, short texts suffer severe data sparsity and informal format. Because of that, mining latent semantics such as topics in short texts is a critical challenge. There are many limitations directly applying common topic models like Latent Dirichlet Allocation (LDA) to short texts. In result of that, many short text topic models were put forward. The Word Network Topic Model (WNTM) is the typical one among them, which transfers sparse document to word space into dense word to word space and then learn topics from it. However, WNTM has its own limitation. The word co-occurrence matrix of the original document is used to construct the word network. This method is simple and intuitive, which can't express the deep meaning between words and words. To solve this problem, we proposed W2V-WNTM, which uses Word2Vec model instead of word co-occurrence to construct word network. Experiment shows it can describe relationship between words better."
37,"Because of the increasing popularity of Sina micro-blog, its data volume gets larger and larger. Friend recommendation gets harder. Users' behavior on Sina micro-blog reflects their value and interests. People who have similar interests are more likely to become friends. In view of the above-mentioned facts, we build micro-blog topic model based on users' operations and the concept of time slices. Then calculate the user similarity based on topic probability distribution that we get through the topic model. After that, clustering the users and getting social circles. Recalculating the user similarity based on circle structure and calculates user's trust degree of other users. In the end we can finish the friend recommendation based on user similarity and trust degree. Experimental results show this algorithm is better than traditional methods.",2018-07-05,2-s2.0-85050695967,"Proceedings - IEEE 4th International Conference on Big Data Computing Service and Applications, BigDataService 2018",Micro-blog friend-recommendation based on topic analysis and circle found,"Because of the increasing popularity of Sina micro-blog, its data volume gets larger and larger. Friend recommendation gets harder. Users' behavior on Sina micro-blog reflects their value and interests. People who have similar interests are more likely to become friends. In view of the above-mentioned facts, we build micro-blog topic model based on users' operations and the concept of time slices. Then calculate the user similarity based on topic probability distribution that we get through the topic model. After that, clustering the users and getting social circles. Recalculating the user similarity based on circle structure and calculates user's trust degree of other users. In the end we can finish the friend recommendation based on user similarity and trust degree. Experimental results show this algorithm is better than traditional methods."
38,"Do parties change their platform in anticipation of electoral losses? Or do parties respond to experienced losses at the previous election? These questions relate to two mechanisms to align public opinion with party platforms: (1) rational anticipation, and (2) electoral performance. While extant work empirically tested, and found support for, the latter mechanism, the effect of rational anticipation has not been put to an empirical test yet. We contribute to the literature on party platform change by theorizing and assessing how party performance motivates parties to change their platform in-between elections. We built a new and unique dataset of >20,000 press releases issued by 15 Dutch national political parties that were in parliament between 1997 and 2014. Utilizing automated text analysis (topic modeling) to measure parties’ platform change, we show that electoral defeat motivates party platform change in-between elections. In line with existing findings, we demonstrate that parties are backward-looking.",2018-07-03,2-s2.0-85049990512,Political Communication,"Living in the Past or Living in the Future? Analyzing Parties’ Platform Change In Between Elections,The Netherlands 1997–2014","Do parties change their platform in anticipation of electoral losses? Or do parties respond to experienced losses at the previous election? These questions relate to two mechanisms to align public opinion with party platforms: (1) rational anticipation, and (2) electoral performance. While extant work empirically tested, and found support for, the latter mechanism, the effect of rational anticipation has not been put to an empirical test yet. We contribute to the literature on party platform change by theorizing and assessing how party performance motivates parties to change their platform in-between elections. We built a new and unique dataset of >20,000 press releases issued by 15 Dutch national political parties that were in parliament between 1997 and 2014. Utilizing automated text analysis (topic modeling) to measure parties’ platform change, we show that electoral defeat motivates party platform change in-between elections. In line with existing findings, we demonstrate that parties are backward-looking."
39,"Data(-driven) journalism has triggered debates about whether this innovative storytelling and investigative approach, using data analytical and computational methods, better serves the public. Applying the concept of articulation, wherein an array of terms are juxtaposed and expressed together, this paper examines how the term “data-driven journalism” is represented on social media. Focusing on the Twittersphere as the research context, the paper employed the Twitter search application programming interface (API) to harvest all available public tweets (N = 6951) containing hashtags or keywords related to data-driven journalism within a four-week period in late 2016. A text-mining analysis of the contents of these tweets found that they focused extensively on journalistic practices, data visualization, and data analytical techniques. Further analysis on the hashtag co-occurrence network revealed that a number of hashtags bridged and organized the discussion of data-driven journalism in the Twittersphere. Some hashtags on technologies and commercial applications, such as “#dataviz,” “#bigdata,” and “#datajournalism,” were located at important positions in the network. In contrast, public-related terms, such as “#opendata” or “#opengovernment,” were mentioned in a limited way and positioned peripherally. Implications for journalism and society are discussed.",2018-07-03,2-s2.0-85023169954,Digital Journalism,"Visualization, Technologies, or the Public?: Exploring the articulation of data-driven journalism in the Twittersphere","Data(-driven) journalism has triggered debates about whether this innovative storytelling and investigative approach, using data analytical and computational methods, better serves the public. Applying the concept of articulation, wherein an array of terms are juxtaposed and expressed together, this paper examines how the term “data-driven journalism” is represented on social media. Focusing on the Twittersphere as the research context, the paper employed the Twitter search application programming interface (API) to harvest all available public tweets (N = 6951) containing hashtags or keywords related to data-driven journalism within a four-week period in late 2016. A text-mining analysis of the contents of these tweets found that they focused extensively on journalistic practices, data visualization, and data analytical techniques. Further analysis on the hashtag co-occurrence network revealed that a number of hashtags bridged and organized the discussion of data-driven journalism in the Twittersphere. Some hashtags on technologies and commercial applications, such as “#dataviz,” “#bigdata,” and “#datajournalism,” were located at important positions in the network. In contrast, public-related terms, such as “#opendata” or “#opengovernment,” were mentioned in a limited way and positioned peripherally. Implications for journalism and society are discussed."
40,"User-Generated Content (UGC) provides a potential data source which can help us to better describe and understand how places are conceptualized, and in turn better represent the places in Geographic Information Science (GIScience). In this article, we aim at aggregating the shared meanings associated with places and linking these to a conceptual model of place. Our focus is on the metadata of Flickr images, in the form of locations and tags. We use topic modeling to identify regions associated with shared meanings. We choose a grid approach and generate topics associated with one or more cells using Latent Dirichlet Allocation. We analyze the sensitivity of our results to both grid resolution and the chosen number of topics using a range of measures including corpus distance and the coherence value. Using a resolution of 500 m and with 40 topics, we are able to generate meaningful topics which characterize places in London based on 954 unique tags associated with around 300,000 images and more than 7000 individuals.",2018-07-03,2-s2.0-85049866856,Geo-Spatial Information Science,Description and characterization of place properties using topic modeling on georeferenced tags,"User-Generated Content (UGC) provides a potential data source which can help us to better describe and understand how places are conceptualized, and in turn better represent the places in Geographic Information Science (GIScience). In this article, we aim at aggregating the shared meanings associated with places and linking these to a conceptual model of place. Our focus is on the metadata of Flickr images, in the form of locations and tags. We use topic modeling to identify regions associated with shared meanings. We choose a grid approach and generate topics associated with one or more cells using Latent Dirichlet Allocation. We analyze the sensitivity of our results to both grid resolution and the chosen number of topics using a range of measures including corpus distance and the coherence value. Using a resolution of 500 m and with 40 topics, we are able to generate meaningful topics which characterize places in London based on 954 unique tags associated with around 300,000 images and more than 7000 individuals."
41,"Do administrative boundaries correspond to the observable ways in which people interact in urban space? As cities grow in complexity, and people interact over long distances with greater ease, so partitioning of cities needs to depart from conventional gravity models. The current state-of-the-art for uncovering interactional regions, i.e. regions reflective of observable human mobility and interaction patterns, is to apply community detection to networks constructed from vast amounts of human interactions, such as phone calls or flights. This approach is well suited for origin–destination activities, but not for activities involving multiple locations, such as police patrols, and is blind to spatial anomalies. As a result of the latter, community detection generates geographically coherent regions, which may appear plausible but give no insights into forces other than gravity that shape our interaction patterns. This paper proposes novel approaches to regional delineation that address the aforementioned shortcomings. Firstly, it introduces topic modelling as an alternative tool for extracting interactional regions from tracking data. Secondly, it presents refinements of the topic modelling and community detection approaches that can uncover interaction patterns driven by forces other than spatial proximity. When applied to police patrol data, our methodology partitions the street network into non-overlapping patrol zones and detects popular long-distance routes between police stations. These findings could be used in the design of effective police districts, especially in light of recent funding cuts that promise to impact upon the ways in which policing and specifically patrols are carried out.",2018-07-03,2-s2.0-85039159649,International Journal of Geographical Information Science,Interactional regions in cities: making sense of flows across networked systems,"Do administrative boundaries correspond to the observable ways in which people interact in urban space? As cities grow in complexity, and people interact over long distances with greater ease, so partitioning of cities needs to depart from conventional gravity models. The current state-of-the-art for uncovering interactional regions, i.e. regions reflective of observable human mobility and interaction patterns, is to apply community detection to networks constructed from vast amounts of human interactions, such as phone calls or flights. This approach is well suited for origin–destination activities, but not for activities involving multiple locations, such as police patrols, and is blind to spatial anomalies. As a result of the latter, community detection generates geographically coherent regions, which may appear plausible but give no insights into forces other than gravity that shape our interaction patterns. This paper proposes novel approaches to regional delineation that address the aforementioned shortcomings. Firstly, it introduces topic modelling as an alternative tool for extracting interactional regions from tracking data. Secondly, it presents refinements of the topic modelling and community detection approaches that can uncover interaction patterns driven by forces other than spatial proximity. When applied to police patrol data, our methodology partitions the street network into non-overlapping patrol zones and detects popular long-distance routes between police stations. These findings could be used in the design of effective police districts, especially in light of recent funding cuts that promise to impact upon the ways in which policing and specifically patrols are carried out."
42,"These days, technological advances are being made through technological conversion. Following this trend, companies need to adapt and secure their own sustainable technological strategies. Technology transfer is one such strategy. This method is especially effective in coping with recent technological developments. In addition, universities and research institutes are able to secure new research opportunities through technology transfer. The aim of our study is to provide a technology transfer prediction model for the sustainable growth of companies. In the proposed method, we first collected patent data from a Korean patent information service provider. Next, we used latent Dirichlet allocation, which is a topic modeling method used to identify the technical field of the collected patents. Quantitative indicators on the patent data were also extracted. Finally, we used the variables that we obtained to create a technology transfer prediction model using the AdaBoost algorithm. The model was found to have sufficient classification performance. It is expected that the proposed model will enable universities and research institutes to secure new technology development opportunities more efficiently. In addition, companies using this model can maintain sustainable growth in line, coping with the changing pace of society.",2018-07-02,2-s2.0-85049387495,Sustainability (Switzerland),Ensemble modeling for sustainable technology transfer,"These days, technological advances are being made through technological conversion. Following this trend, companies need to adapt and secure their own sustainable technological strategies. Technology transfer is one such strategy. This method is especially effective in coping with recent technological developments. In addition, universities and research institutes are able to secure new research opportunities through technology transfer. The aim of our study is to provide a technology transfer prediction model for the sustainable growth of companies. In the proposed method, we first collected patent data from a Korean patent information service provider. Next, we used latent Dirichlet allocation, which is a topic modeling method used to identify the technical field of the collected patents. Quantitative indicators on the patent data were also extracted. Finally, we used the variables that we obtained to create a technology transfer prediction model using the AdaBoost algorithm. The model was found to have sufficient classification performance. It is expected that the proposed model will enable universities and research institutes to secure new technology development opportunities more efficiently. In addition, companies using this model can maintain sustainable growth in line, coping with the changing pace of society."
43,"Despite the ubiquity of textual data, so far few researchers have applied text mining to answer organizational research questions. Text mining, which essentially entails a quantitative approach to the analysis of (usually) voluminous textual data, helps accelerate knowledge discovery by radically increasing the amount data that can be analyzed. This article aims to acquaint organizational researchers with the fundamental logic underpinning text mining, the analytical stages involved, and contemporary techniques that may be used to achieve different types of objectives. The specific analytical techniques reviewed are (a) dimensionality reduction, (b) distance and similarity computing, (c) clustering, (d) topic modeling, and (e) classification. We describe how text mining may extend contemporary organizational research by allowing the testing of existing or new research questions with data that are likely to be rich, contextualized, and ecologically valid. After an exploration of how evidence for the validity of text mining output may be generated, we conclude the article by illustrating the text mining process in a job analysis setting using a dataset composed of job vacancies.",2018-07-01,2-s2.0-85046617359,Organizational Research Methods,Text Mining in Organizational Research,"Despite the ubiquity of textual data, so far few researchers have applied text mining to answer organizational research questions. Text mining, which essentially entails a quantitative approach to the analysis of (usually) voluminous textual data, helps accelerate knowledge discovery by radically increasing the amount data that can be analyzed. This article aims to acquaint organizational researchers with the fundamental logic underpinning text mining, the analytical stages involved, and contemporary techniques that may be used to achieve different types of objectives. The specific analytical techniques reviewed are (a) dimensionality reduction, (b) distance and similarity computing, (c) clustering, (d) topic modeling, and (e) classification. We describe how text mining may extend contemporary organizational research by allowing the testing of existing or new research questions with data that are likely to be rich, contextualized, and ecologically valid. After an exploration of how evidence for the validity of text mining output may be generated, we conclude the article by illustrating the text mining process in a job analysis setting using a dataset composed of job vacancies."
44,"Darknet markets, which can be considered as online black markets, in general sell illegal items such as drugs, firearms, and malware. In July 2017, significant law enforcement operations compromised or completely took down multiple international darknet markets. To quickly understand how this affected the markets and the choice of tools utilized by users of darknet markets, we use unsupervised topic modeling techniques on the DarkNetMarkets subreddit in order to determine prominent topics and terms, and how they have changed over a year's time. After extracting, filtering out irrelevant posts, and preprocessing the text crawled from the subreddit, we perform Latent Dirichlet Allocation (LDA) topic modeling on a corpus of posts for each month from November 5th, 2016 to November 5th, 2017. Our results indicate that even analyzing public forums such as the DarkNetMarkets subreddit can reveal trends and keywords related to criminal activity and their methods, and that users of the dark web appear to be becoming increasingly more security-minded due to the recent law enforcement events.",2018-07-01,2-s2.0-85048551767,Digital Investigation,Analyzing the DarkNetMarkets subreddit for evolutions of tools and trends using LDA topic modeling,"Darknet markets, which can be considered as online black markets, in general sell illegal items such as drugs, firearms, and malware. In July 2017, significant law enforcement operations compromised or completely took down multiple international darknet markets. To quickly understand how this affected the markets and the choice of tools utilized by users of darknet markets, we use unsupervised topic modeling techniques on the DarkNetMarkets subreddit in order to determine prominent topics and terms, and how they have changed over a year's time. After extracting, filtering out irrelevant posts, and preprocessing the text crawled from the subreddit, we perform Latent Dirichlet Allocation (LDA) topic modeling on a corpus of posts for each month from November 5th, 2016 to November 5th, 2017. Our results indicate that even analyzing public forums such as the DarkNetMarkets subreddit can reveal trends and keywords related to criminal activity and their methods, and that users of the dark web appear to be becoming increasingly more security-minded due to the recent law enforcement events."
45,Previous studies on design behaviour indicate that focus shifts positively influence ideational productivity. In this study we want to take a closer look at how these focus shifts look on the verbal level. We describe a mutually influencing relationship between mental focus shifts and verbal low coherent statements. In a case study based on the DTRS11 dataset we identify 297 low coherent statements via a combined topic modelling and manual approach. We introduce a categorization of the different instances of low coherent statements. The results indicate that designers tend to shift topics within an existing design issue instead of completely disrupting it.,2018-07-01,2-s2.0-85045474979,Design Studies,Verbal focus shifts: Forms of low coherent statements in design conversations,Previous studies on design behaviour indicate that focus shifts positively influence ideational productivity. In this study we want to take a closer look at how these focus shifts look on the verbal level. We describe a mutually influencing relationship between mental focus shifts and verbal low coherent statements. In a case study based on the DTRS11 dataset we identify 297 low coherent statements via a combined topic modelling and manual approach. We introduce a categorization of the different instances of low coherent statements. The results indicate that designers tend to shift topics within an existing design issue instead of completely disrupting it.
46,"Mathematics educators have been publishing their work in international research journals for nearly 5 decades. How has the field developed over this period? We analyzed the full text of all articles published in Educational Studies 1n Mathematics and the Journalfor Research in Mathematics Education since their foundation. Using Lakatos's (1978) notion of a research programme, we focus on the field's changing theoretical orientations and pay particular attention to the relative prominence of the experimental psychology, constructivist, and sociocultural programmes. We quantitatively assess the extent of the ""social turn,"" observe that the field is currently experiencing a period of theoretical diversity, and identify and discuss the ""experimental cliff,"" a period during which experimental investigations migrated away from mathematics education journals.",2018-07-01,2-s2.0-85052811311,Journal for Research in Mathematics Education,Five decades of mathematics education research,"Mathematics educators have been publishing their work in international research journals for nearly 5 decades. How has the field developed over this period? We analyzed the full text of all articles published in Educational Studies 1n Mathematics and the Journalfor Research in Mathematics Education since their foundation. Using Lakatos's (1978) notion of a research programme, we focus on the field's changing theoretical orientations and pay particular attention to the relative prominence of the experimental psychology, constructivist, and sociocultural programmes. We quantitatively assess the extent of the ""social turn,"" observe that the field is currently experiencing a period of theoretical diversity, and identify and discuss the ""experimental cliff,"" a period during which experimental investigations migrated away from mathematics education journals."
47,"Social media data can provide valuable information regarding people's behaviors and health outcomes. Previous studies have shown that social media data can be extracted to monitor and predict infectious disease outbreaks. These same approaches can be applied to other fields including physical activity research and forensic science. Social media data have the potential to provide real-time monitoring and prediction of physical activity level in a given region. This tool can be valuable to public health organizations as it can overcome the time lag in the reporting of physical activity epidemiology data faced by traditional research methods (e.g. surveys, observational studies). As a result, this tool could help public health organizations better mobilize and target physical activity interventions. The first part of this paper aims to describe current approaches (e.g. topic modeling, sentiment analysis and social network analysis) that could be used to analyze social media data to provide real-time monitoring of physical activity level. The second aim of this paper was to discuss ways to apply social media analysis to other fields such as forensic sciences and provide recommendations to further social media research.",2018-07-01,2-s2.0-85006124420,Journal of Forensic and Legal Medicine,A survey of social media data analysis for physical activity surveillance,"Social media data can provide valuable information regarding people's behaviors and health outcomes. Previous studies have shown that social media data can be extracted to monitor and predict infectious disease outbreaks. These same approaches can be applied to other fields including physical activity research and forensic science. Social media data have the potential to provide real-time monitoring and prediction of physical activity level in a given region. This tool can be valuable to public health organizations as it can overcome the time lag in the reporting of physical activity epidemiology data faced by traditional research methods (e.g. surveys, observational studies). As a result, this tool could help public health organizations better mobilize and target physical activity interventions. The first part of this paper aims to describe current approaches (e.g. topic modeling, sentiment analysis and social network analysis) that could be used to analyze social media data to provide real-time monitoring of physical activity level. The second aim of this paper was to discuss ways to apply social media analysis to other fields such as forensic sciences and provide recommendations to further social media research."
48,"Corporate social responsibility (CSR) is an essential business practice in industry and a popular topic in academic research. Several studies have attempted to understand topics or categories in CSR contexts and some have used qualitative techniques to analyze data from traditional communication channels such as corporate reports, newspapers, and websites. This study adopts computational content analysis for understanding themes or topics from CSR-related conversations in the Twitter-sphere, the largest microblogging social media platform. Specifically, a probabilistic topic modeling-based computational text analysis framework is introduced to answer three questions: (1)What CSR-related topics are being communicated in the Twitter-sphere and what are the prevalent topics or themes in CSR conversation? (topic prevalence); (2) How are those topics interrelated? (topic correlation); (3) How have those topics changed over time? (topic evolution). The topicmodeling results are discussed, and the direction for future research is presented.",2018-06-28,2-s2.0-85049209460,Sustainability (Switzerland),Corporate social responsibility (CSR): A survey of topics and trends using Twitter data and topic modeling,"Corporate social responsibility (CSR) is an essential business practice in industry and a popular topic in academic research. Several studies have attempted to understand topics or categories in CSR contexts and some have used qualitative techniques to analyze data from traditional communication channels such as corporate reports, newspapers, and websites. This study adopts computational content analysis for understanding themes or topics from CSR-related conversations in the Twitter-sphere, the largest microblogging social media platform. Specifically, a probabilistic topic modeling-based computational text analysis framework is introduced to answer three questions: (1)What CSR-related topics are being communicated in the Twitter-sphere and what are the prevalent topics or themes in CSR conversation? (topic prevalence); (2) How are those topics interrelated? (topic correlation); (3) How have those topics changed over time? (topic evolution). The topicmodeling results are discussed, and the direction for future research is presented."
49,"Human rights discourse has been likened to a global lingua franca, and in more ways than one, the analogy seems apt. Human rights discourse is a language that is used by all yet belongs uniquely to no particular place. It crosses not only the borders between nation-states, but also the divide between national law and international law: It appears in national constitutions and international treaties alike. But is it possible to conceive of human rights as a global language or lingua franca not just in a figurative or metaphorical sense, but in a literal or linguistic sense as a legal dialect defined by distinctive patterns of word choice and usage? Does there exist a global language of human rights that transcends not only national borders, but also the divide between domestic and international law? Empirical analysis suggests that the answer is yes, but this global language comes in at least two variants or dialects. New techniques for performing automated content analysis enable us to analyze the bulk of all national constitutions over the last two centuries, together with the world's leading regional and international human rights instruments, for patterns of linguistic similarity and to evaluate how much language, if any, they share in common. Specifically, we employ a technique known as topic modeling that disassembles texts into recurring verbal patterns. The results highlight the existence of two species or dialects of rights talk-the universalist dialect and the positive-rights dialect-both of which are global in reach and rising in popularity. The universalist dialect is generic in content and draws heavily on the type of language found in international and regional human rights instruments. It appears in particularly large doses in the constitutions of transitional states, developing states, and states that have been heavily exposed to the influence of the international community. The positive-rights dialect, by contrast, is characterized by its substantive emphasis on positive rights of a social or economic variety, and by its prevalence in lengthier constitutions and constitutions from outside the common law world, especially those of the Spanish-speaking world. Both dialects of rights talk are truly transnational, in the sense that they appear simultaneously in national, regional, and international legal instruments and transcend the distinction between domestic and international law. Their existence attests to the blurring of the boundary between constitutional law and international law.",2018-06-26,2-s2.0-85049135815,Law and Ethics of Human Rights,The global language of human rights: A computational linguistic analysis,"Human rights discourse has been likened to a global lingua franca, and in more ways than one, the analogy seems apt. Human rights discourse is a language that is used by all yet belongs uniquely to no particular place. It crosses not only the borders between nation-states, but also the divide between national law and international law: It appears in national constitutions and international treaties alike. But is it possible to conceive of human rights as a global language or lingua franca not just in a figurative or metaphorical sense, but in a literal or linguistic sense as a legal dialect defined by distinctive patterns of word choice and usage? Does there exist a global language of human rights that transcends not only national borders, but also the divide between domestic and international law? Empirical analysis suggests that the answer is yes, but this global language comes in at least two variants or dialects. New techniques for performing automated content analysis enable us to analyze the bulk of all national constitutions over the last two centuries, together with the world's leading regional and international human rights instruments, for patterns of linguistic similarity and to evaluate how much language, if any, they share in common. Specifically, we employ a technique known as topic modeling that disassembles texts into recurring verbal patterns. The results highlight the existence of two species or dialects of rights talk-the universalist dialect and the positive-rights dialect-both of which are global in reach and rising in popularity. The universalist dialect is generic in content and draws heavily on the type of language found in international and regional human rights instruments. It appears in particularly large doses in the constitutions of transitional states, developing states, and states that have been heavily exposed to the influence of the international community. The positive-rights dialect, by contrast, is characterized by its substantive emphasis on positive rights of a social or economic variety, and by its prevalence in lengthier constitutions and constitutions from outside the common law world, especially those of the Spanish-speaking world. Both dialects of rights talk are truly transnational, in the sense that they appear simultaneously in national, regional, and international legal instruments and transcend the distinction between domestic and international law. Their existence attests to the blurring of the boundary between constitutional law and international law."
50,"With the rapid development of online social networking services, the recommendation systems are facing new challenges in recommending resources to a target group of users. How to make a trade-off between group's preference and influencer's impact is one of important problems, especially in the group recommendation on learning resources. In this paper, we propose a User Topic Influence (UTI) model, which fully exploits user topic influence together with group's preference and item content for group recommendation on learning resources. Based on the UTI model, we mine the topic influence and group's preference through statistical inference, then we develop a parameter learning algorithm through Expectation Maximization (EM) algorithm. In addition, we propose a group recommendation algorithm with the consideration of group's preference and influencers' impact. The experimental results demonstrate that our proposed group recommendation algorithm performs better than other five alternatives.",2018-06-26,2-s2.0-85050183623,"2017 IEEE SmartWorld Ubiquitous Intelligence and Computing, Advanced and Trusted Computed, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People and Smart City Innovation, SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI 2017 - Conference Proceedings",Exploring user topic influence for group recommendation on learning resources,"With the rapid development of online social networking services, the recommendation systems are facing new challenges in recommending resources to a target group of users. How to make a trade-off between group's preference and influencer's impact is one of important problems, especially in the group recommendation on learning resources. In this paper, we propose a User Topic Influence (UTI) model, which fully exploits user topic influence together with group's preference and item content for group recommendation on learning resources. Based on the UTI model, we mine the topic influence and group's preference through statistical inference, then we develop a parameter learning algorithm through Expectation Maximization (EM) algorithm. In addition, we propose a group recommendation algorithm with the consideration of group's preference and influencers' impact. The experimental results demonstrate that our proposed group recommendation algorithm performs better than other five alternatives."
51,"A topic model is a statistical model for modeling high dimensional count data. Many different parameters (solutions) of a topic model can be obtained through a learning algorithm due to different initial conditions. This paper focuses on diversity of solutions. To utilize diversity of solutions, it is necessary to acquire distribution structure of them. Therefore, this paper proposes a novel method to define similarity (inner product) of solutions using normalized mutual information to analyze distribution of solutions. Experimental results for text data are presented to show the usefulness of the proposed method.",2018-06-20,2-s2.0-85050125812,"Proceedings of 2018 5th International Conference on Business and Industrial Research: Smart Technology for Next Generation of Information, Engineering, Business and Social Science, ICBIR 2018",A method for analyzing solution diversity in topic models,"A topic model is a statistical model for modeling high dimensional count data. Many different parameters (solutions) of a topic model can be obtained through a learning algorithm due to different initial conditions. This paper focuses on diversity of solutions. To utilize diversity of solutions, it is necessary to acquire distribution structure of them. Therefore, this paper proposes a novel method to define similarity (inner product) of solutions using normalized mutual information to analyze distribution of solutions. Experimental results for text data are presented to show the usefulness of the proposed method."
52,"Natural language processing (NLP) in Thai language is notoriously complicated. One major problem is the lack of word boundary in a sentence, introducing ambiguity in word tokenization. For topic extraction, semantic ambiguity adds another layer of complexity to the problem. Topic model that disregards word order, such as Latent Dirichlet Allocation (LDA), performs poorly in Thai Language. In this paper, we experimented and tested a probabilistic language model equipped with word location information, the so-called Topic N-grams model (TNG). We deployed several testing tasks to assess TNG's capabilities of modeling the generative process of Thai text and established benchmarks that compare the performance of LDA and TNG for various NLP tasks in Thai language. To our knowledge, this paper is the first to explore word-order model in Thai language topic extraction. We concluded that TNG can help boosting performance of Thai language processing in word cutting, semantic checking, word prediction, and document generation task. We also explored how we can measure performance of LDA and TNG on such tasks using perplexity.",2018-06-20,2-s2.0-85050154385,"Proceedings of 2018 5th International Conference on Business and Industrial Research: Smart Technology for Next Generation of Information, Engineering, Business and Social Science, ICBIR 2018",Probabilistic learning models for topic extraction i Thai language,"Natural language processing (NLP) in Thai language is notoriously complicated. One major problem is the lack of word boundary in a sentence, introducing ambiguity in word tokenization. For topic extraction, semantic ambiguity adds another layer of complexity to the problem. Topic model that disregards word order, such as Latent Dirichlet Allocation (LDA), performs poorly in Thai Language. In this paper, we experimented and tested a probabilistic language model equipped with word location information, the so-called Topic N-grams model (TNG). We deployed several testing tasks to assess TNG's capabilities of modeling the generative process of Thai text and established benchmarks that compare the performance of LDA and TNG for various NLP tasks in Thai language. To our knowledge, this paper is the first to explore word-order model in Thai language topic extraction. We concluded that TNG can help boosting performance of Thai language processing in word cutting, semantic checking, word prediction, and document generation task. We also explored how we can measure performance of LDA and TNG on such tasks using perplexity."
53,"Bioinformatics is an emerging field that is constantly evolving as technology progresses and new biomedical discoveries are made. Bioinformatics research has led to several scientific breakthroughs in the past two decades and remains an active driver of scientific progress and technological advances. In this work, we conduct an analysis of bioinformatics scholarly literature consisting of 143,000 research papers between 1987 and 2018. We apply topic modeling to identify the salient themes in bioinformatics research. We examine the research trends by performing temporal analysis to determine exciting areas of research and predict future trends. In addition, we evaluate the impact of bioinformatics research on the industry by cross-linking the literature with patent databases. We also survey the author backgrounds and the publishing journals, both of which were found to have changed significantly within the past decade. This study provides valuable insight on the progress and current state of bioinformatics research.",2018-06-08,2-s2.0-85049879446,"2018 IEEE Long Island Systems, Applications and Technology Conference, LISAT 2018",Exploring trends and themes in bioinformatics literature using topic modeling and temporal analysis,"Bioinformatics is an emerging field that is constantly evolving as technology progresses and new biomedical discoveries are made. Bioinformatics research has led to several scientific breakthroughs in the past two decades and remains an active driver of scientific progress and technological advances. In this work, we conduct an analysis of bioinformatics scholarly literature consisting of 143,000 research papers between 1987 and 2018. We apply topic modeling to identify the salient themes in bioinformatics research. We examine the research trends by performing temporal analysis to determine exciting areas of research and predict future trends. In addition, we evaluate the impact of bioinformatics research on the industry by cross-linking the literature with patent databases. We also survey the author backgrounds and the publishing journals, both of which were found to have changed significantly within the past decade. This study provides valuable insight on the progress and current state of bioinformatics research."
54,"This study examines analyst information intermediary roles using a textual analysis of analyst reports and corporate disclosures. We employ a topic modeling methodology from computational linguistic research to compare the thematic content of a large sample of analyst reports issued promptly after earnings conference calls with the content of the calls themselves. We show that analysts discuss exclusive topics beyond those from conference calls and interpret topics from conference calls. In addition, we find that investors place a greater value on new information in analyst reports when managers face greater incentives to withhold value-relevant information. Analyst interpretation is particularly valuable when the processing costs of conference call information increase. Finally, we document that investors react to analyst report content that simply confirms managers’ conference call discussions. Overall, our study shows that analysts play the information intermediary roles by discovering information beyond corporate disclosures and by clarifying and confirming corporate disclosures.",2018-06-01,2-s2.0-85048106526,Management Science,Analyst information discovery and interpretation roles: A topic modeling approach,"This study examines analyst information intermediary roles using a textual analysis of analyst reports and corporate disclosures. We employ a topic modeling methodology from computational linguistic research to compare the thematic content of a large sample of analyst reports issued promptly after earnings conference calls with the content of the calls themselves. We show that analysts discuss exclusive topics beyond those from conference calls and interpret topics from conference calls. In addition, we find that investors place a greater value on new information in analyst reports when managers face greater incentives to withhold value-relevant information. Analyst interpretation is particularly valuable when the processing costs of conference call information increase. Finally, we document that investors react to analyst report content that simply confirms managers’ conference call discussions. Overall, our study shows that analysts play the information intermediary roles by discovering information beyond corporate disclosures and by clarifying and confirming corporate disclosures."
55,"With the ever increasing number of Web services, discovering an appropriate Web service requested by users has become a vital yet challenging task. We need a scalable and efficient search engine to deal with the large volume of Web services. The aim of this approach is to provide an efficient search engine that can retrieve the most relevant Web services in a short time. The proposed Web service search engine (WSSE) is based on the probabilistic topic modeling and clustering techniques that are integrated to support each other by discovering the semantic meaning of Web services and reducing the search space. The latent Dirichlet allocation (LDA) is used to extract topics from Web service descriptions. These topics are used to group similar Web services together. Each Web service description is represented as a topic vector, so the topic model is an efficient technique to reduce the dimensionality of word vectors and to discover the semantic meaning that is hidden in Web service descriptions. Also, the Web service description is represented as a word vector to address the drawbacks of the keyword-based search system. The accuracy of the proposed WSSE is compared with the keyword-based search system. Also, the precision and recall metrics are used to evaluate the performance of the proposed approach and the keyword-based search system. The results show that the proposed WSSE based on LDA and clustering outperforms the keyword-based search system.",2018-06-01,2-s2.0-85044388351,Service Oriented Computing and Applications,A Web service search engine for large-scale Web service discovery based on the probabilistic topic modeling and clustering,"With the ever increasing number of Web services, discovering an appropriate Web service requested by users has become a vital yet challenging task. We need a scalable and efficient search engine to deal with the large volume of Web services. The aim of this approach is to provide an efficient search engine that can retrieve the most relevant Web services in a short time. The proposed Web service search engine (WSSE) is based on the probabilistic topic modeling and clustering techniques that are integrated to support each other by discovering the semantic meaning of Web services and reducing the search space. The latent Dirichlet allocation (LDA) is used to extract topics from Web service descriptions. These topics are used to group similar Web services together. Each Web service description is represented as a topic vector, so the topic model is an efficient technique to reduce the dimensionality of word vectors and to discover the semantic meaning that is hidden in Web service descriptions. Also, the Web service description is represented as a word vector to address the drawbacks of the keyword-based search system. The accuracy of the proposed WSSE is compared with the keyword-based search system. Also, the precision and recall metrics are used to evaluate the performance of the proposed approach and the keyword-based search system. The results show that the proposed WSSE based on LDA and clustering outperforms the keyword-based search system."
56,"Using the novel technique of topic modelling, this paper examines thematic patterns and their changes over time in a large corpus of corporate social responsibility (CSR) reports produced in the oil sector. Whereas previous research on corporate communications has been small-scale or interested in selected lexical aspects and thematic categories identified ex ante, our approach allows for thematic patterns to emerge from the data. The analysis reveals a number of major trends and topic shifts pointing to changing practices of CSR. Nowadays 'people', 'communities', and 'rights' seem to be given more prominence, whereas 'environmental protection' appears to be less relevant. Using more established corpus-based methods, we subsequently explore two top phrases - 'human rights' and 'climate change' - that were identified as representative of the shifting thematic patterns. Our approach strikes a balance between the purely quantitative and qualitative methodologies and offers applied linguists new ways of exploring discourse in large collections of texts.",2018-06-01,2-s2.0-85048606560,Applied Linguistics,Doing Well by Talking Good: A Topic Modelling-Assisted Discourse Study of Corporate Social Responsibility,"Using the novel technique of topic modelling, this paper examines thematic patterns and their changes over time in a large corpus of corporate social responsibility (CSR) reports produced in the oil sector. Whereas previous research on corporate communications has been small-scale or interested in selected lexical aspects and thematic categories identified ex ante, our approach allows for thematic patterns to emerge from the data. The analysis reveals a number of major trends and topic shifts pointing to changing practices of CSR. Nowadays 'people', 'communities', and 'rights' seem to be given more prominence, whereas 'environmental protection' appears to be less relevant. Using more established corpus-based methods, we subsequently explore two top phrases - 'human rights' and 'climate change' - that were identified as representative of the shifting thematic patterns. Our approach strikes a balance between the purely quantitative and qualitative methodologies and offers applied linguists new ways of exploring discourse in large collections of texts."
57,"Legal reasoning requires identification through search of authoritative legal texts (such as statutes, constitutions, or prior judicial opinions) that apply to a given legal question. In this paper, using a network representation of US Supreme Court opinions that integrates citation connectivity and topical similarity, we model the activity of law search as an organizing principle in the evolution of the corpus of legal texts. The network model and (parametrized) probabilistic search behavior generates a Pagerank-style ranking of the texts that in turn gives rise to a natural geometry of the opinion corpus. This enables us to then measure the ways in which new judicial opinions affect the topography of the network and its future evolution. While we deploy it here on the US Supreme Court opinion corpus, there are obvious extensions to large evolving bodies of legal text (or text corpora in general). The model is a proxy for the way in which new opinions influence the search behavior of litigants and judges and thus affect the law. This type of “legal search effect” is a new legal consequence of research practice that has not been previously identified in jurisprudential thought and has never before been subject to empirical analysis. We quantitatively estimate the extent of this effect and find significant relationships between search-related network structures and propensity of future citation. This finding indicates that “search influence” is a pathway through which judicial opinions can affect future legal development.",2018-06-01,2-s2.0-85043385510,Artificial Intelligence and Law,Bending the law: geometric tools for quantifying influence in the multinetwork of legal opinions,"Legal reasoning requires identification through search of authoritative legal texts (such as statutes, constitutions, or prior judicial opinions) that apply to a given legal question. In this paper, using a network representation of US Supreme Court opinions that integrates citation connectivity and topical similarity, we model the activity of law search as an organizing principle in the evolution of the corpus of legal texts. The network model and (parametrized) probabilistic search behavior generates a Pagerank-style ranking of the texts that in turn gives rise to a natural geometry of the opinion corpus. This enables us to then measure the ways in which new judicial opinions affect the topography of the network and its future evolution. While we deploy it here on the US Supreme Court opinion corpus, there are obvious extensions to large evolving bodies of legal text (or text corpora in general). The model is a proxy for the way in which new opinions influence the search behavior of litigants and judges and thus affect the law. This type of “legal search effect” is a new legal consequence of research practice that has not been previously identified in jurisprudential thought and has never before been subject to empirical analysis. We quantitatively estimate the extent of this effect and find significant relationships between search-related network structures and propensity of future citation. This finding indicates that “search influence” is a pathway through which judicial opinions can affect future legal development."
58,"Purpose: The proliferation of socialized data offers an unprecedented opportunity for customer service measurement. This paper addresses the problem of adequately measuring service quality using socialized data. Design/methodology/approach: The theoretical basis for the study is the widely used SERVQUAL model and we leverage a dataset uniquely suited for the analysis: the full database of online reviews generated on the website of the leading price comparison engine in Italy. Adopting a weakly supervised topic model, we extract the dimensions of service quality from these reviews. We use a linear regression to compare service quality dimensions between positive and negative opinions. Findings: First, we show that socialized textual data, not just quantitative ratings, provide a wealth of customer service information that can be used to measure service quality. Second, we demonstrate that the distribution of topics in online opinions differs significantly between positive and negative reviews. Specifically, we find that concerns about merchant responsiveness dominate negative reviews. Practical implications: Our research has important implications for designers of online review systems and marketers seeking novel approaches to the measurement of service quality. Our study shows that evaluation systems designed considering the knowledge extracted directly from customers’ review lead to a service quality measurement that not only is theory-based, but also more accurate. Originality/value: We believe this is the first study to combine the advanced text mining technique of topic modeling and SERVQUAL to extract specific service dimensions from socialized data. Using these advanced techniques, we point to systematic differences between positive and negative customer opinions. We are not aware of any study that has shown these differences with either traditional approaches (i.e., survey data) or modern techniques (e.g. text mining).",2018-06-01,2-s2.0-85042178089,International Journal of Information Management,The relative importance of service quality dimensions in E-commerce experiences,"Purpose: The proliferation of socialized data offers an unprecedented opportunity for customer service measurement. This paper addresses the problem of adequately measuring service quality using socialized data. Design/methodology/approach: The theoretical basis for the study is the widely used SERVQUAL model and we leverage a dataset uniquely suited for the analysis: the full database of online reviews generated on the website of the leading price comparison engine in Italy. Adopting a weakly supervised topic model, we extract the dimensions of service quality from these reviews. We use a linear regression to compare service quality dimensions between positive and negative opinions. Findings: First, we show that socialized textual data, not just quantitative ratings, provide a wealth of customer service information that can be used to measure service quality. Second, we demonstrate that the distribution of topics in online opinions differs significantly between positive and negative reviews. Specifically, we find that concerns about merchant responsiveness dominate negative reviews. Practical implications: Our research has important implications for designers of online review systems and marketers seeking novel approaches to the measurement of service quality. Our study shows that evaluation systems designed considering the knowledge extracted directly from customers’ review lead to a service quality measurement that not only is theory-based, but also more accurate. Originality/value: We believe this is the first study to combine the advanced text mining technique of topic modeling and SERVQUAL to extract specific service dimensions from socialized data. Using these advanced techniques, we point to systematic differences between positive and negative customer opinions. We are not aware of any study that has shown these differences with either traditional approaches (i.e., survey data) or modern techniques (e.g. text mining)."
59,"Purpose: This paper aims to determine if the digital humanities technique of topic modeling would reveal interesting patters in a corpus of library-themed literature focused on the future of libraries and pioneer a collaboration model in librarian-led digital humanities projects. By developing the project, librarians learned how to better support digital humanities by actually doing digital humanities, as well as gaining insight on the variety of approaches taken by researchers and commenters to the idea of the future of libraries. Design/methodology/approach: The researchers collected a corpus of over 150 texts (articles, blog posts, book chapters, websites, etc.) that all addressed the future of the library. They ran several instances of latent Dirichlet allocation style topic modeling on the corpus using the programming language R. Once they produced a run in which the topics were cohesive and discrete, they produced word-clouds of the words associated with each topic, visualized topics through time and examined in detail the top five documents associated with each topic. Findings: The research project provided an effective way for librarians to gain practical experience in digital humanities and develop a greater understanding of collaborative workflows in digital humanities. By examining a corpus of library-themed literature, the researchers gained new insight into how the profession grapples with the idea of the future and an appreciation for topic modeling as a form of literature review. Originality/value: Topic modeling a future-themed corpus of library literature is a unique research project and provides a way to support collaboration between library faculty and researchers from outside the library.",2018-05-30,2-s2.0-85047836623,Digital Library Perspectives,"Kindles, card catalogs, and the future of libraries: a collaborative digital humanities project","Purpose: This paper aims to determine if the digital humanities technique of topic modeling would reveal interesting patters in a corpus of library-themed literature focused on the future of libraries and pioneer a collaboration model in librarian-led digital humanities projects. By developing the project, librarians learned how to better support digital humanities by actually doing digital humanities, as well as gaining insight on the variety of approaches taken by researchers and commenters to the idea of the future of libraries. Design/methodology/approach: The researchers collected a corpus of over 150 texts (articles, blog posts, book chapters, websites, etc.) that all addressed the future of the library. They ran several instances of latent Dirichlet allocation style topic modeling on the corpus using the programming language R. Once they produced a run in which the topics were cohesive and discrete, they produced word-clouds of the words associated with each topic, visualized topics through time and examined in detail the top five documents associated with each topic. Findings: The research project provided an effective way for librarians to gain practical experience in digital humanities and develop a greater understanding of collaborative workflows in digital humanities. By examining a corpus of library-themed literature, the researchers gained new insight into how the profession grapples with the idea of the future and an appreciation for topic modeling as a form of literature review. Originality/value: Topic modeling a future-themed corpus of library literature is a unique research project and provides a way to support collaboration between library faculty and researchers from outside the library."
60,"Social sensing plays an important role in crime analytics and predictive policing. When humans play the role of sensor, several issues around privacy and trust emerge that must be carefully handled. We provide a framework for deploying predictive crime models based upon crowd-sourced information (crime reports, tips, Nextdoor posts, etc.) while protecting individual privacy and striving for a high level of algorithmic transparency. For this purpose we introduce a novel online Hawkes process estimation algorithm requiring no event history coupled with an online k-means type algorithm based upon the word movers distance. We illustrate the methodology using synthetic data, crime report data from Los Angeles, and public safety posts from Nextdoor in Indianapolis. In particular, we show that privacy and transparency can be maintained without sacrificing accuracy in space-time models of criminal incidents. Furthermore, our methodology provides a framework for sharing of information between private companies collecting crime tips or public safety information, law enforcement agencies, and the general public.",2018-05-25,2-s2.0-85048491329,"Proceedings - 3rd International Workshop on Social Sensing, SocialSens 2018","Privacy preserving, crowd sourced crime hawkes processes","Social sensing plays an important role in crime analytics and predictive policing. When humans play the role of sensor, several issues around privacy and trust emerge that must be carefully handled. We provide a framework for deploying predictive crime models based upon crowd-sourced information (crime reports, tips, Nextdoor posts, etc.) while protecting individual privacy and striving for a high level of algorithmic transparency. For this purpose we introduce a novel online Hawkes process estimation algorithm requiring no event history coupled with an online k-means type algorithm based upon the word movers distance. We illustrate the methodology using synthetic data, crime report data from Los Angeles, and public safety posts from Nextdoor in Indianapolis. In particular, we show that privacy and transparency can be maintained without sacrificing accuracy in space-time models of criminal incidents. Furthermore, our methodology provides a framework for sharing of information between private companies collecting crime tips or public safety information, law enforcement agencies, and the general public."
61,"Why do issues “fade” from the problem stream? This is an important but underresearched question, which this article examines by looking at the dynamic interaction between frames and frame sponsors. We develop a novel methodological approach that combines algorithmic coding (topic modelling) with hand-coding to track changes in the presence of frames and frame sponsors during periods of intense problematisation (“problem windows”) both within continuous contexts and diachronically across different contexts. We apply this approach empirically in a corpus of newspaper articles that pertain to the coal seam gas controversy in Australia – a divisive policy issue where frame conflicts are common. We find that elite actors have a particularly decisive impact on the problem stream in terms of both the evolution and duration of debate. Further, problem windows close in response to three different mechanisms: elite frame convergence; public statements (by government and industry); and elections.",2018-05-16,2-s2.0-85047156293,Journal of Public Policy,Explaining the “ebb and flow” of the problem stream: frame conflicts over the future of coal seam gas (“fracking”) in Australia,"Why do issues “fade” from the problem stream? This is an important but underresearched question, which this article examines by looking at the dynamic interaction between frames and frame sponsors. We develop a novel methodological approach that combines algorithmic coding (topic modelling) with hand-coding to track changes in the presence of frames and frame sponsors during periods of intense problematisation (“problem windows”) both within continuous contexts and diachronically across different contexts. We apply this approach empirically in a corpus of newspaper articles that pertain to the coal seam gas controversy in Australia – a divisive policy issue where frame conflicts are common. We find that elite actors have a particularly decisive impact on the problem stream in terms of both the evolution and duration of debate. Further, problem windows close in response to three different mechanisms: elite frame convergence; public statements (by government and industry); and elections."
62,"An emerging research trend in climate change studies is to use user-generated-data collected from social media to investigate the public opinion and science communication of climate change issues. This study collected data from the social Q & A website Quora to explore the key factors influencing the public preferences in climate change knowledge and opinions. Using web crawler, topic modeling, and count data regression modeling, this study quantitatively analyzed the effects of an answer's textual and auxiliary features on the number of up-votes received by the answer. Compared with previous studies based on open-ended surveys of citizens, the topic modeling result indicates that Quora users are more likely to talk about the energy, human and societal issues, and scientific research rather than the natural phenomena of climate change. The regression modeling results show that: (i) answers with more emphasis on specific subjects, but not popular knowledge, about climate change can get significantly more up-votes; (ii) answers with more terms of daily dialogue will get significantly fewer up-votes; and (iii) answers written by an author with more followers, with a longer text, with more images, or belonging to a question with more followers, can get significantly more up-votes.",2018-05-10,2-s2.0-85047002403,Sustainability (Switzerland),Climate change communication in an online Q&A community: A case study of Quora,"An emerging research trend in climate change studies is to use user-generated-data collected from social media to investigate the public opinion and science communication of climate change issues. This study collected data from the social Q & A website Quora to explore the key factors influencing the public preferences in climate change knowledge and opinions. Using web crawler, topic modeling, and count data regression modeling, this study quantitatively analyzed the effects of an answer's textual and auxiliary features on the number of up-votes received by the answer. Compared with previous studies based on open-ended surveys of citizens, the topic modeling result indicates that Quora users are more likely to talk about the energy, human and societal issues, and scientific research rather than the natural phenomena of climate change. The regression modeling results show that: (i) answers with more emphasis on specific subjects, but not popular knowledge, about climate change can get significantly more up-votes; (ii) answers with more terms of daily dialogue will get significantly fewer up-votes; and (iii) answers written by an author with more followers, with a longer text, with more images, or belonging to a question with more followers, can get significantly more up-votes."
63,"Despite the popularity of online food and grocery shopping, little research has been conducted to understand the factors that influence consumers’ online food purchases. Using a topic modeling approach, our results show four interpretable factors have significant impacts on the helpfulness of customer reviews: Amazon Service, Physical Feature, Flavor Feature, and Subjective Expression. Readers of customer reviews perceive objective reviews as more helpful than subjective reviews. In addition, customer review helpfulness has a concave relationship with the length of the reviews. Our results provide important business implications on how to encourage more helpful reviews to assist potential shoppers in making better purchase decisions.",2018-05-01,2-s2.0-85044872428,Journal of Retailing and Consumer Services,Exploring hidden factors behind online food shopping from Amazon reviews: A topic mining approach,"Despite the popularity of online food and grocery shopping, little research has been conducted to understand the factors that influence consumers’ online food purchases. Using a topic modeling approach, our results show four interpretable factors have significant impacts on the helpfulness of customer reviews: Amazon Service, Physical Feature, Flavor Feature, and Subjective Expression. Readers of customer reviews perceive objective reviews as more helpful than subjective reviews. In addition, customer review helpfulness has a concave relationship with the length of the reviews. Our results provide important business implications on how to encourage more helpful reviews to assist potential shoppers in making better purchase decisions."
64,"The social sciences and humanities (SSH) traditionally have a close relationship to the nation-state and there are substantial disciplinary differences across countries. Drawing on Bourdieu’s theory of the academic field, the present article examines how academic autonomy and heteronomy are applied as discursive strategies as the SSH compete for funding in the transnational European arena established by the European Research Council (ERC). To this end, we analyze mission statements of ERC Starting Grant projects in the SSH, using a mixed methods approach of statistical text analysis (topic modeling) and qualitative content analysis. Although the ERC puts the SSH under the constraints of academic capitalism, the classical humanities secure a strong position by signaling academic autonomy and engaging in the construction and consecration of European culture. However, economics and other social sciences gravitate toward a more heteronomous self-representation, emphasizing the political and social utility of their research, while disciplines like neuro-science and psychology exhibit self-representations closely related to the “hard sciences”—and relatively alien to the SSH.",2018-05-01,2-s2.0-85047220531,Osterreichische Zeitschrift fur Soziologie,Academic Autonomy Beyond the Nation-State: The Social Sciences and Humanities in the European Research Council Akademische Autonomie jenseits des Nationalstaats: Die Sozial- und Geisteswissenschaften im Europäischen Forschungsrat,"The social sciences and humanities (SSH) traditionally have a close relationship to the nation-state and there are substantial disciplinary differences across countries. Drawing on Bourdieu’s theory of the academic field, the present article examines how academic autonomy and heteronomy are applied as discursive strategies as the SSH compete for funding in the transnational European arena established by the European Research Council (ERC). To this end, we analyze mission statements of ERC Starting Grant projects in the SSH, using a mixed methods approach of statistical text analysis (topic modeling) and qualitative content analysis. Although the ERC puts the SSH under the constraints of academic capitalism, the classical humanities secure a strong position by signaling academic autonomy and engaging in the construction and consecration of European culture. However, economics and other social sciences gravitate toward a more heteronomous self-representation, emphasizing the political and social utility of their research, while disciplines like neuro-science and psychology exhibit self-representations closely related to the “hard sciences”—and relatively alien to the SSH."
65,"Topic sentiment joint model aims to deal with the problem about the mixture of topics and sentiment simultaneously from online reviews. Most of existing topic sentiment modeling algorithms are mainly based on the state-of-art latent Dirichlet allocation (LDA) and probabilistic latent semantic analysis (PLSA), which infer sentiment and topic distributions from the co-occurrence of words. These methods have been proposed and successfully used for topic and sentiment analysis. However, when the training corpus is small or when the documents are short, the textual features become sparse, so that the results of the sentiment and topic distributions might be not very satisfied. In this paper, we propose a novel topic sentiment joint model called weakly supervised topic sentiment joint model with word embeddings (WS-TSWE), which incorporates word embeddings and HowNet lexicon simultaneously to improve the topic identification and sentiment recognition. The main contributions of WS-TSWE include the following two aspects. (1) Existing models generate the words only from the sentiment-topic-to-word Dirichlet multinomial component, but the WS-TSWE model replaces it with a mixture of two components, a Dirichlet multinomial component and a word embeddings component. Since the word embeddings are trained on a very large corpora and can be used to extend the semantic information of the words, they can provide a certain solution for the problem of the textual sparse. (2) Most of previous models incorporate sentiment knowledge in the β priors. And the priors are usually set from a dictionary and completely rely on previous domain knowledge to identify positive and negative words. In contrast, the WS-TSWE model calculates the sentiment orientation of each word with the HowNet lexicon and automatically infers sentiment-based β priors for sentiment analysis and opinion mining. Furthermore, we implement WS-TSWE with Gibbs sampling algorithms. The experimental results on Chinese and English data sets show that WS-TSWE achieved significant performance in the task of detecting sentiment and topics simultaneously.",2018-05-01,2-s2.0-85041922050,Knowledge-Based Systems,Weakly supervised topic sentiment joint model with word embeddings,"Topic sentiment joint model aims to deal with the problem about the mixture of topics and sentiment simultaneously from online reviews. Most of existing topic sentiment modeling algorithms are mainly based on the state-of-art latent Dirichlet allocation (LDA) and probabilistic latent semantic analysis (PLSA), which infer sentiment and topic distributions from the co-occurrence of words. These methods have been proposed and successfully used for topic and sentiment analysis. However, when the training corpus is small or when the documents are short, the textual features become sparse, so that the results of the sentiment and topic distributions might be not very satisfied. In this paper, we propose a novel topic sentiment joint model called weakly supervised topic sentiment joint model with word embeddings (WS-TSWE), which incorporates word embeddings and HowNet lexicon simultaneously to improve the topic identification and sentiment recognition. The main contributions of WS-TSWE include the following two aspects. (1) Existing models generate the words only from the sentiment-topic-to-word Dirichlet multinomial component, but the WS-TSWE model replaces it with a mixture of two components, a Dirichlet multinomial component and a word embeddings component. Since the word embeddings are trained on a very large corpora and can be used to extend the semantic information of the words, they can provide a certain solution for the problem of the textual sparse. (2) Most of previous models incorporate sentiment knowledge in the β priors. And the priors are usually set from a dictionary and completely rely on previous domain knowledge to identify positive and negative words. In contrast, the WS-TSWE model calculates the sentiment orientation of each word with the HowNet lexicon and automatically infers sentiment-based β priors for sentiment analysis and opinion mining. Furthermore, we implement WS-TSWE with Gibbs sampling algorithms. The experimental results on Chinese and English data sets show that WS-TSWE achieved significant performance in the task of detecting sentiment and topics simultaneously."
66,"Question answering systems assist users in satisfying their information needs more precisely by providing focused responses to their questions. Among the various systems developed for such a purpose, community-based question answering has recently received researchers’ attention due to the large amount of user-generated questions and answers in social question-and-answer platforms. Reusing such data sources requires an accurate information retrieval component enhanced by a question classifier. The question classification gives the system the possibility to have information about question categories to focus on questions and answers from relevant categories to the input question. In this paper, we propose a new method based on unsupervised Latent Dirichlet Allocation for classifying questions in community-based question answering. Our method first uses unsupervised topic modeling to extract topics from a large amount of unlabeled data. The learned topics are then used in the training phase to find their association with the available category labels in the training data. The category mixture of topics is finally used to predict the label of unseen data.",2018-05-01,2-s2.0-85042465787,Information Processing and Management,Unsupervised Latent Dirichlet Allocation for supervised question classification,"Question answering systems assist users in satisfying their information needs more precisely by providing focused responses to their questions. Among the various systems developed for such a purpose, community-based question answering has recently received researchers’ attention due to the large amount of user-generated questions and answers in social question-and-answer platforms. Reusing such data sources requires an accurate information retrieval component enhanced by a question classifier. The question classification gives the system the possibility to have information about question categories to focus on questions and answers from relevant categories to the input question. In this paper, we propose a new method based on unsupervised Latent Dirichlet Allocation for classifying questions in community-based question answering. Our method first uses unsupervised topic modeling to extract topics from a large amount of unlabeled data. The learned topics are then used in the training phase to find their association with the available category labels in the training data. The category mixture of topics is finally used to predict the label of unseen data."
67,"Despite a near unanimous agreement that human trafficking is a morally reprehensible practice, there is confusion around what qualifies as human trafficking in the United States. Adopting a mixed-method strategy, we examine how human trafficking is defined by the public; how contemporary (mis)understanding of human trafficking developed; and the public opinion consequence of this (mis)understanding. The definition of human trafficking has evolved over time to become nearly synonymous with slavery; however, we demonstrate that media and anti-trafficking organisations have been focussing their attention on the sexual exploitation of foreign women. We show that general public opinion reflects this skewed attention; the average citizen equates human trafficking with the smuggling of women for sexual slavery. Using a survey experiment, we find that shining light on other facets of human trafficking – the fact that human trafficking is a security problem and a domestic issue – can increase public response to the issue.",2018-04-25,2-s2.0-85046030225,Journal of Public Policy,The evolution of human trafficking messaging in the United States and its effect on public opinion,"Despite a near unanimous agreement that human trafficking is a morally reprehensible practice, there is confusion around what qualifies as human trafficking in the United States. Adopting a mixed-method strategy, we examine how human trafficking is defined by the public; how contemporary (mis)understanding of human trafficking developed; and the public opinion consequence of this (mis)understanding. The definition of human trafficking has evolved over time to become nearly synonymous with slavery; however, we demonstrate that media and anti-trafficking organisations have been focussing their attention on the sexual exploitation of foreign women. We show that general public opinion reflects this skewed attention; the average citizen equates human trafficking with the smuggling of women for sexual slavery. Using a survey experiment, we find that shining light on other facets of human trafficking – the fact that human trafficking is a security problem and a domestic issue – can increase public response to the issue."
68,"Interpersonal communication on online social networks has a significant impact on the society by not only diffusing information, but also forming social ties, norms, and behaviors. Knowing how the conversational discourse semantically and geographically vary over time can help uncover the changing dynamics of interpersonal ties and the digital traces of social events. This article introduces a framework for modeling and visualizing the semantic and spatio-temporal evolution of topics in a spatially embedded and time-stamped interpersonal communication network. The framework consists of (1) a topic modeling workflow for modeling topics and extracting the evolution of conversational discourse; (2) a geo-social network modeling and smoothing approach to projecting connection characteristics and semantics of communication onto geographic space and time; (3) a web-based geovisual analytics environment for exploring semantic and spatio-temporal evolution of topics in a spatially embedded and time-stamped interpersonal communication network. To demonstrate, geo-located and reciprocal user mention and reply tweets over the course of the 2016 primary and presidential elections in the United States from 1 August 2015 to 15 November 2016 were analyzed. The large portion of the topics extracted from mention tweets were related to daily life routines, human activities, and interests such as school, work, sports, dating, wearing, birthday celebration, music, food, and live-tweeting. Specific focus on the analysis of political conversations revealed that the content of conversational discourse was split between civil rights and election-related discussions of the political campaigns and candidates. These political topics exhibited major shifts in terms of content and the popularity in reaction to primaries, debates, and events throughout the study period. While civil rights discussions were more dominant and in higher intensity across the nation and throughout the whole time period, election-specific conversations resulted in temporally varying local hotspots that correlated with locations of primaries and events.",2018-04-15,2-s2.0-85046041019,International Journal of Geographical Information Science,Modeling and visualizing semantic and spatio-temporal evolution of topics in interpersonal communication on Twitter,"Interpersonal communication on online social networks has a significant impact on the society by not only diffusing information, but also forming social ties, norms, and behaviors. Knowing how the conversational discourse semantically and geographically vary over time can help uncover the changing dynamics of interpersonal ties and the digital traces of social events. This article introduces a framework for modeling and visualizing the semantic and spatio-temporal evolution of topics in a spatially embedded and time-stamped interpersonal communication network. The framework consists of (1) a topic modeling workflow for modeling topics and extracting the evolution of conversational discourse; (2) a geo-social network modeling and smoothing approach to projecting connection characteristics and semantics of communication onto geographic space and time; (3) a web-based geovisual analytics environment for exploring semantic and spatio-temporal evolution of topics in a spatially embedded and time-stamped interpersonal communication network. To demonstrate, geo-located and reciprocal user mention and reply tweets over the course of the 2016 primary and presidential elections in the United States from 1 August 2015 to 15 November 2016 were analyzed. The large portion of the topics extracted from mention tweets were related to daily life routines, human activities, and interests such as school, work, sports, dating, wearing, birthday celebration, music, food, and live-tweeting. Specific focus on the analysis of political conversations revealed that the content of conversational discourse was split between civil rights and election-related discussions of the political campaigns and candidates. These political topics exhibited major shifts in terms of content and the popularity in reaction to primaries, debates, and events throughout the study period. While civil rights discussions were more dominant and in higher intensity across the nation and throughout the whole time period, election-specific conversations resulted in temporally varying local hotspots that correlated with locations of primaries and events."
69,"Latent Dirichlet allocation (LDA) topic models are increasingly being used in communication research. Yet, questions regarding reliability and validity of the approach have received little attention thus far. In applying LDA to textual data, researchers need to tackle at least four major challenges that affect these criteria: (a) appropriate pre-processing of the text collection; (b) adequate selection of model parameters, including the number of topics to be generated; (c) evaluation of the model’s reliability; and (d) the process of validly interpreting the resulting topics. We review the research literature dealing with these questions and propose a methodology that approaches these challenges. Our overall goal is to make LDA topic modeling more accessible to communication researchers and to ensure compliance with disciplinary standards. Consequently, we develop a brief hands-on user guide for applying LDA topic modeling. We demonstrate the value of our approach with empirical data from an ongoing research project.",2018-04-03,2-s2.0-85042230495,Communication Methods and Measures,Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology,"Latent Dirichlet allocation (LDA) topic models are increasingly being used in communication research. Yet, questions regarding reliability and validity of the approach have received little attention thus far. In applying LDA to textual data, researchers need to tackle at least four major challenges that affect these criteria: (a) appropriate pre-processing of the text collection; (b) adequate selection of model parameters, including the number of topics to be generated; (c) evaluation of the model’s reliability; and (d) the process of validly interpreting the resulting topics. We review the research literature dealing with these questions and propose a methodology that approaches these challenges. Our overall goal is to make LDA topic modeling more accessible to communication researchers and to ensure compliance with disciplinary standards. Consequently, we develop a brief hands-on user guide for applying LDA topic modeling. We demonstrate the value of our approach with empirical data from an ongoing research project."
70,"Environmental sociology is a growing field producing a diverse body of literature while also moving into the mainstream of the larger discipline. The twin goals of this paper are to introduce environmental sociologists to innovations in content analysis, specifically a form of text-mining known as topic modeling, and then employing it to identify key themes and trends within our diverse field. We apply the topic modeling approach to a corpus of research articles within environmental sociology, identifying 25 central topics within the field and examining their prevalence over time, co-occurrence, impact (judged by citations), and prestige (judged by journal rankings). Our results indicate which topics are most prevalent, tend to occur together, and how both vary over time. They also indicate that the highest impact topics are not the most prevalent, the most prestigious topics are not the most prevalent, and topics can be prestigious without exerting much impact. We conclude with a discussion of the capabilities computational text analysis methods offer environmental sociologists.",2018-04-03,2-s2.0-85050778140,Environmental Sociology,"Key Topics in environmental sociology, 1990–2014: results from a computational text analysis","Environmental sociology is a growing field producing a diverse body of literature while also moving into the mainstream of the larger discipline. The twin goals of this paper are to introduce environmental sociologists to innovations in content analysis, specifically a form of text-mining known as topic modeling, and then employing it to identify key themes and trends within our diverse field. We apply the topic modeling approach to a corpus of research articles within environmental sociology, identifying 25 central topics within the field and examining their prevalence over time, co-occurrence, impact (judged by citations), and prestige (judged by journal rankings). Our results indicate which topics are most prevalent, tend to occur together, and how both vary over time. They also indicate that the highest impact topics are not the most prevalent, the most prestigious topics are not the most prevalent, and topics can be prestigious without exerting much impact. We conclude with a discussion of the capabilities computational text analysis methods offer environmental sociologists."
71,"Web 2.0 offers manifold ways in order to integrate community members via online communities (OCs) for innovation processes. OCs prove to be a valuable and dynamic source of information. External information sources are also important for foresight in order to be able to identify and monitor all relevant changes. However, traditional foresight methods are rather static in comparison with dynamic OCs. Thus, this study gives first insights into the use of OCs for foresight. First, based on literature, it is conceptually shown that OCs can contribute to foresight. Second, the question of how to assess the potential of OCs for foresight is considered. Renewable energies OCs are identified using a netnographic approach. One selected OC is analyzed in-depth by applying a prior developed criteria catalog which is based on Popper's (2008) foresight diamond. Each of its four dimensions – creativity, expertise, interaction, and evidence – is operationalized with measurement items taken from literature. In particular, the evidence dimension is supported by a text mining approach. Lastly, a focus group interview proves the usefulness of OCs for foresight. The findings show that OCs can contribute to each dimension of the foresight diamond and serve as an additional source of information for foresight.",2018-04-01,2-s2.0-85044349949,Technological Forecasting and Social Change,Foresight by online communities – The case of renewable energies,"Web 2.0 offers manifold ways in order to integrate community members via online communities (OCs) for innovation processes. OCs prove to be a valuable and dynamic source of information. External information sources are also important for foresight in order to be able to identify and monitor all relevant changes. However, traditional foresight methods are rather static in comparison with dynamic OCs. Thus, this study gives first insights into the use of OCs for foresight. First, based on literature, it is conceptually shown that OCs can contribute to foresight. Second, the question of how to assess the potential of OCs for foresight is considered. Renewable energies OCs are identified using a netnographic approach. One selected OC is analyzed in-depth by applying a prior developed criteria catalog which is based on Popper's (2008) foresight diamond. Each of its four dimensions – creativity, expertise, interaction, and evidence – is operationalized with measurement items taken from literature. In particular, the evidence dimension is supported by a text mining approach. Lastly, a focus group interview proves the usefulness of OCs for foresight. The findings show that OCs can contribute to each dimension of the foresight diamond and serve as an additional source of information for foresight."
72,"Nanotechnology is an emerging and promising field of research. Creating sufficient technological diversity among its alternatives is important for the long-term success of nanotechnologies, as well as for other emerging technologies. Diversity prevents early lock-in, facilitates recombinant innovation, increases resilience, and allows market growth. Creation of new technological alternatives usually takes place in innovation projects in which public and private partners often collaborate. Currently, there is little empirical evidence about which characteristics of innovation projects influence diversity. In this paper we study the influence of characteristics of EU-funded nanotechnology projects on the creation of technological diversity. In addition to actor diversity and the network of the project, we also include novel variables that have a plausible influence on diversity creation: the degree of multi-disciplinarity of the project and the size of the joint knowledge base of project partners. We apply topic modelling (Latent Dirichlet allocation) as a novel method to categorize technological alternatives. Using an ordinal logistic regression model, our results show that the largest contribution to diversity comes from the multi-disciplinary nature of a project. The joint knowledge base of project partners and the geographical distance between them were positively associated with technological diversity creation. In contrast, the number and diversity of actors and the degree of clustering showed a negative association with technological diversity creation. These results extend current micro-level explanations of how the diversity of an emerging technology is created. The contribution of this study could also be helpful for policy makers to influence the level of diversity in a technological field, and hence to contribute to survival of emerging technologies.",2018-04-01,2-s2.0-85011281203,Journal of Technology Transfer,Multi-disciplinarity breeds diversity: the influence of innovation project characteristics on diversity creation in nanotechnology,"Nanotechnology is an emerging and promising field of research. Creating sufficient technological diversity among its alternatives is important for the long-term success of nanotechnologies, as well as for other emerging technologies. Diversity prevents early lock-in, facilitates recombinant innovation, increases resilience, and allows market growth. Creation of new technological alternatives usually takes place in innovation projects in which public and private partners often collaborate. Currently, there is little empirical evidence about which characteristics of innovation projects influence diversity. In this paper we study the influence of characteristics of EU-funded nanotechnology projects on the creation of technological diversity. In addition to actor diversity and the network of the project, we also include novel variables that have a plausible influence on diversity creation: the degree of multi-disciplinarity of the project and the size of the joint knowledge base of project partners. We apply topic modelling (Latent Dirichlet allocation) as a novel method to categorize technological alternatives. Using an ordinal logistic regression model, our results show that the largest contribution to diversity comes from the multi-disciplinary nature of a project. The joint knowledge base of project partners and the geographical distance between them were positively associated with technological diversity creation. In contrast, the number and diversity of actors and the degree of clustering showed a negative association with technological diversity creation. These results extend current micro-level explanations of how the diversity of an emerging technology is created. The contribution of this study could also be helpful for policy makers to influence the level of diversity in a technological field, and hence to contribute to survival of emerging technologies."
73,"With the rapid development of e-commerce, a new type of secondhand e-commerce website has appeared in recent years. Any user can have his or her own shop and list superfluous items for sale online without much supervision. These secondhand e-commerce platforms maximize the economic value of secondhand markets online, but buyers risk conducting unpleasant transactions with low-reputation sellers. The main contribution of our research is the design of a text analytics framework to assess secondhand sellers' reputation. In addition, we develop a new aspect-extraction method that combines the results of domain ontology and topic modeling to extract topical features from product descriptions. We conduct our experiments based on a real-word dataset crawled from XianYu. The experimental results reveal that our ontology-based topic model method outperforms a traditional topic model method. Furthermore, the proposed framework performs well in different item categories. The managerial implication of our research is that potential buyers can prejudge the reputation of secondhand sellers when making purchase decisions. The results can support a more effective development of online secondhand markets.",2018-04-01,2-s2.0-85042661166,Decision Support Systems,Secondhand seller reputation in online markets: A text analytics framework,"With the rapid development of e-commerce, a new type of secondhand e-commerce website has appeared in recent years. Any user can have his or her own shop and list superfluous items for sale online without much supervision. These secondhand e-commerce platforms maximize the economic value of secondhand markets online, but buyers risk conducting unpleasant transactions with low-reputation sellers. The main contribution of our research is the design of a text analytics framework to assess secondhand sellers' reputation. In addition, we develop a new aspect-extraction method that combines the results of domain ontology and topic modeling to extract topical features from product descriptions. We conduct our experiments based on a real-word dataset crawled from XianYu. The experimental results reveal that our ontology-based topic model method outperforms a traditional topic model method. Furthermore, the proposed framework performs well in different item categories. The managerial implication of our research is that potential buyers can prejudge the reputation of secondhand sellers when making purchase decisions. The results can support a more effective development of online secondhand markets."
74,"Topic models were proposed to detect the underlying semantic structure of large collections of text documents to facilitate the process of browsing and accessing documents with similar ideas and topics. Applying topic models to short text documents to extract meaningful topics is challenging. The problem becomes even more complicated when dealing with short and noisy micro-posts in Twitter that are about one general topic. In such a case, the goal of applying topic models is to extract subtopics. This results in topics represented by similar sets of keywords, which in turn makes the process of topic interpretation more confusing. In this paper we propose a new method that incorporates Twitter-LDA, WordNet, and hashtags to enhance the keyword labels that represent each topic. We emphasize the importance of different keywords to different topics based on the semantic relationships and the co-occurrences of keywords in hashtags. We also propose a method to find the best number of topics to represent the text document collection. Experiments on two real-life Twitter datasets on fashion suggest that our method performs better than the original Twitter-LDA in terms of perplexity, topic coherence, and the quality of keywords for topic labeling.",2018-04-01,2-s2.0-85035148483,Journal of the Association for Information Science and Technology,Improving interpretations of topic modeling in microblogs,"Topic models were proposed to detect the underlying semantic structure of large collections of text documents to facilitate the process of browsing and accessing documents with similar ideas and topics. Applying topic models to short text documents to extract meaningful topics is challenging. The problem becomes even more complicated when dealing with short and noisy micro-posts in Twitter that are about one general topic. In such a case, the goal of applying topic models is to extract subtopics. This results in topics represented by similar sets of keywords, which in turn makes the process of topic interpretation more confusing. In this paper we propose a new method that incorporates Twitter-LDA, WordNet, and hashtags to enhance the keyword labels that represent each topic. We emphasize the importance of different keywords to different topics based on the semantic relationships and the co-occurrences of keywords in hashtags. We also propose a method to find the best number of topics to represent the text document collection. Experiments on two real-life Twitter datasets on fashion suggest that our method performs better than the original Twitter-LDA in terms of perplexity, topic coherence, and the quality of keywords for topic labeling."
75,"What comes to mind when people think about rank-and-file party supporters? What stereotypes do people hold regarding ordinary partisans, and are these views politically consequential? We utilize open-ended survey items and structural topic modeling to document stereotypes about rank-and-file Democrats and Republicans. Many subjects report stereotypes consistent with the parties’ actual composition, but individual differences in political knowledge, interest, and partisan affiliation predict their specific content. Respondents varied in their tendency to characterize partisans in terms of group memberships, issue preferences, or individual traits, lending support to both ideological and identity-based conceptions of partisanship. Most importantly, we show that partisan stereotype content is politically significant: individuals who think of partisans in a predominantly trait-based manner—that is, in a way consistent with partisanship as a social identity—display dramatically higher levels of both affective and ideological polarization.",2018-03-28,2-s2.0-85044459291,Political Behavior,Pigeonholing Partisans: Stereotypes of Party Supporters and Partisan Polarization,"What comes to mind when people think about rank-and-file party supporters? What stereotypes do people hold regarding ordinary partisans, and are these views politically consequential? We utilize open-ended survey items and structural topic modeling to document stereotypes about rank-and-file Democrats and Republicans. Many subjects report stereotypes consistent with the parties’ actual composition, but individual differences in political knowledge, interest, and partisan affiliation predict their specific content. Respondents varied in their tendency to characterize partisans in terms of group memberships, issue preferences, or individual traits, lending support to both ideological and identity-based conceptions of partisanship. Most importantly, we show that partisan stereotype content is politically significant: individuals who think of partisans in a predominantly trait-based manner—that is, in a way consistent with partisanship as a social identity—display dramatically higher levels of both affective and ideological polarization."
76,"The Men's Rights Activism (MRA) movement and its sub-movement The Red Pill (TRP), has flourished online, offering support and advice to men who feel their masculinity is being challenged by societal shifts. Whilst some insightful studies have been carried out, the small samples analysed by researchers limits the scope of studies, which is small compared to the large amounts of data that TRP produces. By extracting a significant quantity of content from a prominent MRA website, ReturnOfKings.com (RoK), whose creator is one of the most prominent figures in the manosphere and who has been featured in multiple studies. Research already completed can be expanded upon with topic modelling and neural networked machine learning, computational analysis that is proposed to augment methodologies of open coding by automatically and unbiasedly analysing conceptual clusters. The successes and limitations of this computational methodology shed light on its further uses in sociological research and has answered the question: What can topic modeling demonstrate about the men's rights activism movement's prescriptive masculinity? This methodology not only proved that it could replicate the results of a previous study, but also delivered insights into an increasingly political focus within TRP, and deeper perspectives into the concepts identified within the movement.",2018-03-09,2-s2.0-85043401600,Social Sciences,Topic modeling the Red Pill,"The Men's Rights Activism (MRA) movement and its sub-movement The Red Pill (TRP), has flourished online, offering support and advice to men who feel their masculinity is being challenged by societal shifts. Whilst some insightful studies have been carried out, the small samples analysed by researchers limits the scope of studies, which is small compared to the large amounts of data that TRP produces. By extracting a significant quantity of content from a prominent MRA website, ReturnOfKings.com (RoK), whose creator is one of the most prominent figures in the manosphere and who has been featured in multiple studies. Research already completed can be expanded upon with topic modelling and neural networked machine learning, computational analysis that is proposed to augment methodologies of open coding by automatically and unbiasedly analysing conceptual clusters. The successes and limitations of this computational methodology shed light on its further uses in sociological research and has answered the question: What can topic modeling demonstrate about the men's rights activism movement's prescriptive masculinity? This methodology not only proved that it could replicate the results of a previous study, but also delivered insights into an increasingly political focus within TRP, and deeper perspectives into the concepts identified within the movement."
77,"Online platforms are prone to abuse and manipulations from strategic parties. For example, social media and review websites suffer from sentiment manipulations, manifested in the form of opinion spam and fake reviews. The consequence of such manipulations is the deterioration of information quality as well as loss in consumer welfare. We study one of movie studios' operation activities, sentiment manipulation, in the context of movie tweets. Using the movie release and movie studios' earning announcement dates as sources of exogenous shocks, we find that both the average Twitter sentiment and the proportion of highly positive tweets exhibit a significant drop on the movie's release day or movie studios' earnings announcement day. In addition, independent productions and low budget movies tend to experience a larger drop than major studio productions and high budget movies. To examine the effect of competition on firm manipulation, we construct a movie competition measure based on both the time and theme dimensions through topic modeling, and we find that a higher level of competition leads to a larger drop in Twitter sentiment. Overall, these observations suggest that firms might be actively managing online sentiment in a strategic manner. Our study sheds light on the reliability of sentiment analysis and contributes to our understanding of potential strategic manipulation in the operation of movie studios.",2018-03-01,2-s2.0-85043713983,Production and Operations Management,Sentiment Manipulation in Online Platforms: An Analysis of Movie Tweets,"Online platforms are prone to abuse and manipulations from strategic parties. For example, social media and review websites suffer from sentiment manipulations, manifested in the form of opinion spam and fake reviews. The consequence of such manipulations is the deterioration of information quality as well as loss in consumer welfare. We study one of movie studios' operation activities, sentiment manipulation, in the context of movie tweets. Using the movie release and movie studios' earning announcement dates as sources of exogenous shocks, we find that both the average Twitter sentiment and the proportion of highly positive tweets exhibit a significant drop on the movie's release day or movie studios' earnings announcement day. In addition, independent productions and low budget movies tend to experience a larger drop than major studio productions and high budget movies. To examine the effect of competition on firm manipulation, we construct a movie competition measure based on both the time and theme dimensions through topic modeling, and we find that a higher level of competition leads to a larger drop in Twitter sentiment. Overall, these observations suggest that firms might be actively managing online sentiment in a strategic manner. Our study sheds light on the reliability of sentiment analysis and contributes to our understanding of potential strategic manipulation in the operation of movie studios."
78,"One of the biggest challenges associatedwith semantic analysis is to obtain a distance from existing semantic knowledge. Methods from the field of Digital Humanities allow us to find new ways to cope with that challenge. Text mining or topic modeling tools offer a new perspective on words and their combination within a text. The following article highlights the relevance of digital methods for historical semantics, using the Latin term virtus and its medieval use as an example. It raises the question of genre and diachronic semantic change and demonstrates how digital tools have the potential to not only challenge our knowledge about texts but also help to reorganize what we already know.",2018-03-01,2-s2.0-85047624453,Geschichte und Gesellschaft,Digital methods for historical semantics. Tracing concepts in digital corpora Digitale Methoden für die Historische Semantik Auf den Spuren von Begriffen in digitalen Korpora,"One of the biggest challenges associatedwith semantic analysis is to obtain a distance from existing semantic knowledge. Methods from the field of Digital Humanities allow us to find new ways to cope with that challenge. Text mining or topic modeling tools offer a new perspective on words and their combination within a text. The following article highlights the relevance of digital methods for historical semantics, using the Latin term virtus and its medieval use as an example. It raises the question of genre and diachronic semantic change and demonstrates how digital tools have the potential to not only challenge our knowledge about texts but also help to reorganize what we already know."
79,"We explore how ideas from infectious disease and genetics can be used to uncover patterns of cultural inheritance and innovation in a corpus of 591 national constitutions spanning 1789–2008. Legal “ideas” are encoded as “topics”—words statistically linked in documents—derived from topic modeling the corpus of constitutions. Using these topics we derive a diffusion network for borrowing from ancestral constitutions back to the US Constitution of 1789 and reveal that constitutions are complex cultural recombinants. We find systematic variation in patterns of borrowing from ancestral texts and “biological”-like behavior in patterns of inheritance, with the distribution of “offspring” arising through a bounded preferential-attachment process. This process leads to a small number of highly innovative (influential) constitutions some of which have yet to have been identified as so in the current literature. Our findings thus shed new light on the critical nodes of the constitution-making network. The constitutional network structure reflects periods of intense constitution creation, and systematic patterns of variation in constitutional lifespan and temporal influence.",2018-03-01,2-s2.0-85034814427,Journal of the Association for Information Science and Technology,The cultural evolution of national constitutions,"We explore how ideas from infectious disease and genetics can be used to uncover patterns of cultural inheritance and innovation in a corpus of 591 national constitutions spanning 1789–2008. Legal “ideas” are encoded as “topics”—words statistically linked in documents—derived from topic modeling the corpus of constitutions. Using these topics we derive a diffusion network for borrowing from ancestral constitutions back to the US Constitution of 1789 and reveal that constitutions are complex cultural recombinants. We find systematic variation in patterns of borrowing from ancestral texts and “biological”-like behavior in patterns of inheritance, with the distribution of “offspring” arising through a bounded preferential-attachment process. This process leads to a small number of highly innovative (influential) constitutions some of which have yet to have been identified as so in the current literature. Our findings thus shed new light on the critical nodes of the constitution-making network. The constitutional network structure reflects periods of intense constitution creation, and systematic patterns of variation in constitutional lifespan and temporal influence."
80,"Many actors claim to be experts of specialized knowledge, but for this expertise to be perceived as legitimate, other actors in the field must recognize them as authorities. Using an automated topic-model analysis of historical texts associated with the U.S. amateur radio operator movement between 1899 and 1927, we propose a process model for lay-expertise legitimation as an alternative to professionalization. While the professionalization account depends on specialized work, credentialing, and restrictive jurisdictional control by powerful field actors, our model emphasizes four mechanisms leading to lay-expert recognition: building an advanced collective competence, operating in an unrestricted public space, providing transformational social contributions, and expanding an original collective role identity. Our analysis shows how field expertise can be achieved outside of professional spaces by non-professionalized actors who master activities as a labor of love. Our work also reveals that lay-expertise recognition depends on the interplay between collective identities and collective competence among non-professional actors, and it addresses the shifting power dynamics when professional and non-professional actors coexist and strive for expertise recognition.",2018-03-01,2-s2.0-85041561346,Administrative Science Quarterly,Labor of Love: Amateurs and Lay-expertise Legitimation in the Early U.S. Radio Field,"Many actors claim to be experts of specialized knowledge, but for this expertise to be perceived as legitimate, other actors in the field must recognize them as authorities. Using an automated topic-model analysis of historical texts associated with the U.S. amateur radio operator movement between 1899 and 1927, we propose a process model for lay-expertise legitimation as an alternative to professionalization. While the professionalization account depends on specialized work, credentialing, and restrictive jurisdictional control by powerful field actors, our model emphasizes four mechanisms leading to lay-expert recognition: building an advanced collective competence, operating in an unrestricted public space, providing transformational social contributions, and expanding an original collective role identity. Our analysis shows how field expertise can be achieved outside of professional spaces by non-professionalized actors who master activities as a labor of love. Our work also reveals that lay-expertise recognition depends on the interplay between collective identities and collective competence among non-professional actors, and it addresses the shifting power dynamics when professional and non-professional actors coexist and strive for expertise recognition."
81,"Information about road traffic is the one of the most important information for people around the world. TMC (Traffic Management Center) as one of the unit within the Indonesian National Police institution who in charge of traffic management has utilized Twitter as the medium to share about traffic information to the Indonesian citizen. This research aims to create the topic model regarding traffic information on Indonesian Twitter messages. The data used in this research were retrieved from the official Twitter account of the Traffic Management Center in Java using the method of LDA (Latent Dirichlet Allocation) to build the topic model from the dataset. The topic model obtained will represent what kind of topics which posted by TMC in each region in Java. Therefore, the result of this experiment could illustrate valuable and important information that happened in Java Island.",2018-02-27,2-s2.0-85049357538,"Proceedings - 2017 International Conference on Sustainable Information Engineering and Technology, SIET 2017",Road traffic topic modeling on Twitter using latent dirichlet allocation,"Information about road traffic is the one of the most important information for people around the world. TMC (Traffic Management Center) as one of the unit within the Indonesian National Police institution who in charge of traffic management has utilized Twitter as the medium to share about traffic information to the Indonesian citizen. This research aims to create the topic model regarding traffic information on Indonesian Twitter messages. The data used in this research were retrieved from the official Twitter account of the Traffic Management Center in Java using the method of LDA (Latent Dirichlet Allocation) to build the topic model from the dataset. The topic model obtained will represent what kind of topics which posted by TMC in each region in Java. Therefore, the result of this experiment could illustrate valuable and important information that happened in Java Island."
82,"This study focuses on examining the thematic landscape of the history of scholarly publication in business ethics. We analyze the titles, abstracts, full texts, and citation information of all research papers published in the field’s leading journal, the Journal of Business Ethics, from its inaugural issue in February 1982 until December 2016—a dataset that comprises 6308 articles and 42 million words. Our key method is a computational algorithm known as probabilistic topic modeling, which we use to examine objectively the field’s latent thematic landscape based on the vast volume of scholarly texts. This “big-data” approach allows us not only to provide time-specific snapshots of various research topics, but also to track the dynamic evolution of each topic over time. We further examine the pattern of individual papers’ topic diversity and the influence of individual papers’ topic diversity on their impact over time. We conclude this study with our recommendation for future studies in business ethics research.",2018-02-26,2-s2.0-85042535268,Journal of Business Ethics,"A Big-Data Approach to Understanding the Thematic Landscape of the Field of Business Ethics, 1982–2016","This study focuses on examining the thematic landscape of the history of scholarly publication in business ethics. We analyze the titles, abstracts, full texts, and citation information of all research papers published in the field’s leading journal, the Journal of Business Ethics, from its inaugural issue in February 1982 until December 2016—a dataset that comprises 6308 articles and 42 million words. Our key method is a computational algorithm known as probabilistic topic modeling, which we use to examine objectively the field’s latent thematic landscape based on the vast volume of scholarly texts. This “big-data” approach allows us not only to provide time-specific snapshots of various research topics, but also to track the dynamic evolution of each topic over time. We further examine the pattern of individual papers’ topic diversity and the influence of individual papers’ topic diversity on their impact over time. We conclude this study with our recommendation for future studies in business ethics research."
83,"Service innovation is intertwined with service design, and knowledge from both fields should be integrated to advance theoretical and normative insights. However, studies bridging service innovation and service design are in their infancy. This is because the body of service innovation and service design research is large and heterogeneous, which makes it difficult, if not impossible, for any human to read and understand its entire content and to delineate appropriate guidelines on how to broaden the scope of either field. Our work addresses this challenge by presenting the first application of topic modeling, a type of machine learning, to review and analyze currently available service innovation and service design research (n = 641 articles with 10,543 pages of written text or 4,119,747 words). We provide an empirical contribution to service research by identifying and analyzing 69 distinct research topics in the published text corpus, a theoretical contribution by delineating an extensive research agenda consisting of four research directions and 12 operationalizable guidelines to facilitate cross-fertilization between the two fields, and a methodological contribution by introducing and demonstrating the applicability of topic modeling and machine learning as a novel type of big data analytics to our discipline.",2018-02-01,2-s2.0-85040034771,Journal of Service Research,"Big Data, Big Insights? Advancing Service Innovation and Design With Machine Learning","Service innovation is intertwined with service design, and knowledge from both fields should be integrated to advance theoretical and normative insights. However, studies bridging service innovation and service design are in their infancy. This is because the body of service innovation and service design research is large and heterogeneous, which makes it difficult, if not impossible, for any human to read and understand its entire content and to delineate appropriate guidelines on how to broaden the scope of either field. Our work addresses this challenge by presenting the first application of topic modeling, a type of machine learning, to review and analyze currently available service innovation and service design research (n = 641 articles with 10,543 pages of written text or 4,119,747 words). We provide an empirical contribution to service research by identifying and analyzing 69 distinct research topics in the published text corpus, a theoretical contribution by delineating an extensive research agenda consisting of four research directions and 12 operationalizable guidelines to facilitate cross-fertilization between the two fields, and a methodological contribution by introducing and demonstrating the applicability of topic modeling and machine learning as a novel type of big data analytics to our discipline."
84,"This article analyses all articles published in Accounting History using a topic modeling technique. Previous studies focus on the content of accounting history, but not how the field has evolved. The article complements prior assessments of the research published in Accounting History by providing measures of the relative prevalence of research areas and their evolution over time. The analysis offers insights into accounting history by refining previous categorisations, uncovering overlooked topic areas and substantiating trends, such as the demise of interest in the technical core of accounting in favour of more variegated and fragmented approaches. The findings are discussed in light of the claimed pluralisation of methodological and theoretical approaches in this field.",2018-02-01,2-s2.0-85044145991,Accounting History,Accounting for Accounting History: A topic modeling approach (1996–2015),"This article analyses all articles published in Accounting History using a topic modeling technique. Previous studies focus on the content of accounting history, but not how the field has evolved. The article complements prior assessments of the research published in Accounting History by providing measures of the relative prevalence of research areas and their evolution over time. The analysis offers insights into accounting history by refining previous categorisations, uncovering overlooked topic areas and substantiating trends, such as the demise of interest in the technical core of accounting in favour of more variegated and fragmented approaches. The findings are discussed in light of the claimed pluralisation of methodological and theoretical approaches in this field."
85,"The Aviation Safety Reporting System includes over a million confidential reports describing aviation safety incidents. Natural language processing techniques allow for relatively rapid and largely automated analysis of large collections of text data. Interpretation of the results and further investigations by subject matter experts can produce meaningful results. This explains the many commercial and academic applications of natural language processing to aviation safety reports. Relatively few published articles have, however, employed topic modeling, an approach that can identify latent structure within a corpus of documents. Topic modeling is more flexible and relies less on subject matter experts than alternative document categorization and clustering methods. It can, for example, uncover any number of topics hidden in a set of incident reports that have been, or would be, assigned to the same category when using labels and methods applied in earlier research. This article describes the application of structural topic modeling to Aviation Safety Reporting System data. The application identifies known issues. The method also reveals previously unreported connections. Sample results reported here highlight fuel pump, tank, and landing gear issues and the relative insignificance of smoke and fire issues for private aircraft. The results also reveal the prominence of the Quiet Bridge Visual and Tip Toe Visual approach paths at San Francisco International Airport in safety incident reports. These results would, ideally, be verified by subject matter experts before being used to set priorities when planning future safety studies.",2018-02-01,2-s2.0-85044635854,Transportation Research Part C: Emerging Technologies,Using structural topic modeling to identify latent topics and trends in aviation incident reports,"The Aviation Safety Reporting System includes over a million confidential reports describing aviation safety incidents. Natural language processing techniques allow for relatively rapid and largely automated analysis of large collections of text data. Interpretation of the results and further investigations by subject matter experts can produce meaningful results. This explains the many commercial and academic applications of natural language processing to aviation safety reports. Relatively few published articles have, however, employed topic modeling, an approach that can identify latent structure within a corpus of documents. Topic modeling is more flexible and relies less on subject matter experts than alternative document categorization and clustering methods. It can, for example, uncover any number of topics hidden in a set of incident reports that have been, or would be, assigned to the same category when using labels and methods applied in earlier research. This article describes the application of structural topic modeling to Aviation Safety Reporting System data. The application identifies known issues. The method also reveals previously unreported connections. Sample results reported here highlight fuel pump, tank, and landing gear issues and the relative insignificance of smoke and fire issues for private aircraft. The results also reveal the prominence of the Quiet Bridge Visual and Tip Toe Visual approach paths at San Francisco International Airport in safety incident reports. These results would, ideally, be verified by subject matter experts before being used to set priorities when planning future safety studies."
86,"Nowadays plenty of user-generated posts, e.g., sina weibos, are published on the social media. The posts contain the public's sentiments (i.e., positive or negative) towards various topics. Bursty sentiment-aware topics from these posts reveal sentiment-aware events which have attracted much attention. To detect sentiment-aware topics, we attempt to utilize Joint Sentiment/Topic models, these models are achieved with Latent Dirichlet Allocation (LDA) based models. However, most of the existing sentiment/topic models cannot be directly utilized to detect sentiment-aware topics on the posts, since applying the models to the posts directly suffers from the context sparsity problem. In this paper, we propose a Time-User Sentiment/Topic Latent Dirichlet Allocation (TUS-LDA) which simultaneously models sentiments and topics for posts. Thereinto, TUS-LDA aggregates posts in the same timeslices or from the same users as pseudo-documents to alleviate the context sparsity problem. Based on TUS-LDA, we further design an approach to detect bursty sentiment-aware topics and these sentiment-ware topics can reflect bursty real-world events. Experiments on the Chinese sina weibos show that TUS-LDA outperforms previous models in the tasks of sentiment classification and burst detection in sentiment-aware topics. Finally, we visualize the bursty sentiment-aware topics discovered by TUS-LDA.",2018-02-01,2-s2.0-85034631338,Knowledge-Based Systems,Detecting bursts in sentiment-aware topics from social media,"Nowadays plenty of user-generated posts, e.g., sina weibos, are published on the social media. The posts contain the public's sentiments (i.e., positive or negative) towards various topics. Bursty sentiment-aware topics from these posts reveal sentiment-aware events which have attracted much attention. To detect sentiment-aware topics, we attempt to utilize Joint Sentiment/Topic models, these models are achieved with Latent Dirichlet Allocation (LDA) based models. However, most of the existing sentiment/topic models cannot be directly utilized to detect sentiment-aware topics on the posts, since applying the models to the posts directly suffers from the context sparsity problem. In this paper, we propose a Time-User Sentiment/Topic Latent Dirichlet Allocation (TUS-LDA) which simultaneously models sentiments and topics for posts. Thereinto, TUS-LDA aggregates posts in the same timeslices or from the same users as pseudo-documents to alleviate the context sparsity problem. Based on TUS-LDA, we further design an approach to detect bursty sentiment-aware topics and these sentiment-ware topics can reflect bursty real-world events. Experiments on the Chinese sina weibos show that TUS-LDA outperforms previous models in the tasks of sentiment classification and burst detection in sentiment-aware topics. Finally, we visualize the bursty sentiment-aware topics discovered by TUS-LDA."
87,"Social media web sites have become major media platforms to share personal information, news, photos, videos and more. Users can even share live streams whenever they want to reach out to many other. This prevalent usage of social media attracted companies, data scientists, and researchers who are trying to infer meaningful information from this vast amount of data. Information diffusion and maximizing the spread of words is one of the most important focus for researchers working on social media. This information can serve many purposes such as; user or content recommendation, viral marketing, and user modeling. In this research, finding topical influential/authority users on Twitter is addressed. Since Twitter is a good platform to spread knowledge as a word of mouth approach and it has many more public profiles than protected ones, it is a target media for marketers. In this paper, we introduce a novel methodology, called Personalized PageRank, that integrates both the information obtained from network topology and the information obtained from user actions and activities in Twitter. The proposed approach aims to determine the topical influencers who are experts on a specific topic. Experimental results on a large dataset consisting of Turkish tweets show that using user specific features like topical focus rate, activeness, authenticity and speed of getting reaction on specific topics positively affects identifying influencers and lead to higher information diffusion. Algorithms are implemented on a distributed computing environment which makes high-cost graph processing more efficient.",2018-02-01,2-s2.0-85035234575,Knowledge-Based Systems,Identifying topical influencers on twitter based on user behavior and network topology,"Social media web sites have become major media platforms to share personal information, news, photos, videos and more. Users can even share live streams whenever they want to reach out to many other. This prevalent usage of social media attracted companies, data scientists, and researchers who are trying to infer meaningful information from this vast amount of data. Information diffusion and maximizing the spread of words is one of the most important focus for researchers working on social media. This information can serve many purposes such as; user or content recommendation, viral marketing, and user modeling. In this research, finding topical influential/authority users on Twitter is addressed. Since Twitter is a good platform to spread knowledge as a word of mouth approach and it has many more public profiles than protected ones, it is a target media for marketers. In this paper, we introduce a novel methodology, called Personalized PageRank, that integrates both the information obtained from network topology and the information obtained from user actions and activities in Twitter. The proposed approach aims to determine the topical influencers who are experts on a specific topic. Experimental results on a large dataset consisting of Turkish tweets show that using user specific features like topical focus rate, activeness, authenticity and speed of getting reaction on specific topics positively affects identifying influencers and lead to higher information diffusion. Algorithms are implemented on a distributed computing environment which makes high-cost graph processing more efficient."
88,"We propose a methodological approach to analyze the content of hyperlink networks which represent networked public spheres on the Internet. Using the case of the food safety movement in the United States, we demonstrate how to generate a hyperlink network with the web crawling tool Issue Crawler and merge it with the results of a probabilistic topic model of the network’s content. Combining hyperlink networks and content analysis allows us to interpret such a network in its entirety and with regard to the mobilizing potentials of specific sub-issues of the movement. We focus on two specific sub-issues in the food safety network, genetically modified food and food control, in order to trace the involved websites and their interlinking structures, respectively.",2018-02-01,2-s2.0-85040463535,Social Science Computer Review,Exploring Issues in a Networked Public Sphere: Combining Hyperlink Network Analysis and Topic Modeling,"We propose a methodological approach to analyze the content of hyperlink networks which represent networked public spheres on the Internet. Using the case of the food safety movement in the United States, we demonstrate how to generate a hyperlink network with the web crawling tool Issue Crawler and merge it with the results of a probabilistic topic model of the network’s content. Combining hyperlink networks and content analysis allows us to interpret such a network in its entirety and with regard to the mobilizing potentials of specific sub-issues of the movement. We focus on two specific sub-issues in the food safety network, genetically modified food and food control, in order to trace the involved websites and their interlinking structures, respectively."
89,"The study highlights how digital marketing is often detrimental, when it is done by unskilled service providers. It highlights how the hyped services of search engine marketing (SEM) are not as successful as they seem to be and sometimes affect firms negatively. This study uses social media analytics to derive insights from Twitter using descriptive, content and network analytics. Methods like hashtag analysis, polarity and emotion analysis, word analysis, topic modeling and other relevant approaches have been used to mine user generated content. A qualitative case study on an e-market is used for validation of findings. SEM services provided by small organizations and freelancers are not as beneficial as the ones by established players. The services provided by these firms proved detrimental for the customers based on user experiences surrounding these services in the social media and forum specific discussions. This study highlights how SEM often not only fails to provide benefits but also destructs value if not done properly. Transaction costs like agency problems, coordination costs, loss of non-contractible value and cost of fit are also identified with potential fallouts which affect the long-term benefits. Inputs will be beneficial to practice in planning SEM and outsourcing.",2018-02-01,2-s2.0-85029686687,International Journal of Information Management,Search engine marketing is not all gold: Insights from Twitter and SEOClerks,"The study highlights how digital marketing is often detrimental, when it is done by unskilled service providers. It highlights how the hyped services of search engine marketing (SEM) are not as successful as they seem to be and sometimes affect firms negatively. This study uses social media analytics to derive insights from Twitter using descriptive, content and network analytics. Methods like hashtag analysis, polarity and emotion analysis, word analysis, topic modeling and other relevant approaches have been used to mine user generated content. A qualitative case study on an e-market is used for validation of findings. SEM services provided by small organizations and freelancers are not as beneficial as the ones by established players. The services provided by these firms proved detrimental for the customers based on user experiences surrounding these services in the social media and forum specific discussions. This study highlights how SEM often not only fails to provide benefits but also destructs value if not done properly. Transaction costs like agency problems, coordination costs, loss of non-contractible value and cost of fit are also identified with potential fallouts which affect the long-term benefits. Inputs will be beneficial to practice in planning SEM and outsourcing."
90,"Consumers usually do not know the complicated links between related health problems. This fact may cause troubles when they wish to seek complete information regarding such problems. This study detects the associations among health problems by extending the meaning of health terms with methods based on the latent Dirichlet allocation (LDA) probability topic model, the Medical Subject Headings (MeSH) thesaurus structure and the Wikipedia concept mapping. The terms represented health problems are selected from and extended by the consumer-level medical text. The vocabulary is different between the consumer-level and the professional-level medical text. Thus, the findings can be easily understood by the general public and be suitable to consumer-oriented applications. The methods were evaluated in two ways: (1) correlation analysis with expert rating to show the overall performance and (2) P@N to reflect the ability of detecting strong associations. The LDA topic-model-based method outperforms the other two types. The judgment incongruence between the best method and the expert ratings has been examined, and the evidence shows that the automatic method sometimes detects real associations beyond those identified by human experts.",2018-02-01,2-s2.0-85041116117,Journal of Information Science,Detecting the association of health problems in consumer-level medical text,"Consumers usually do not know the complicated links between related health problems. This fact may cause troubles when they wish to seek complete information regarding such problems. This study detects the associations among health problems by extending the meaning of health terms with methods based on the latent Dirichlet allocation (LDA) probability topic model, the Medical Subject Headings (MeSH) thesaurus structure and the Wikipedia concept mapping. The terms represented health problems are selected from and extended by the consumer-level medical text. The vocabulary is different between the consumer-level and the professional-level medical text. Thus, the findings can be easily understood by the general public and be suitable to consumer-oriented applications. The methods were evaluated in two ways: (1) correlation analysis with expert rating to show the overall performance and (2) P@N to reflect the ability of detecting strong associations. The LDA topic-model-based method outperforms the other two types. The judgment incongruence between the best method and the expert ratings has been examined, and the evidence shows that the automatic method sometimes detects real associations beyond those identified by human experts."
91,"How does the collaboration network of researchers coalesce around a scientific topic? What sort of social restructuring occurs as a new field develops? Previous empirical explorations of these questions have examined the evolution of co-authorship networks associated with several fields of science, each noting a characteristic shift in network structure as fields develop. Historically, however, such studies have tended to rely on manually annotated datasets and therefore only consider a handful of disciplines, calling into question the universality of the observed structural signature. To overcome this limitation and test the robustness of this phenomenon, we use a comprehensive dataset of over 189,000 scientific articles and develop a framework for partitioning articles and their authors into coherent, semantically related groups representing scientific fields of varying size and specificity. We then use the resulting population of fields to study the structure of evolving co-authorship networks. Consistent with earlier findings, we observe a global topological transition as the co-authorship networks coalesce from a disjointed aggregate into a dense giant connected component that dominates the network. We validate these results using a separate, complimentary corpus of scientific articles, and, overall, we find that the previously reported characteristic structural evolution of a scientific field's associated co-authorship network is robust across a large number of scientific fields of varying size, scope, and specificity. Additionally, the framework developed in this study may be used in other scientometric contexts in order to extend studies to compare across a larger range of scientific disciplines.",2018-02-01,2-s2.0-85040006123,Journal of Informetrics,Network assembly of scientific communities of varying size and specificity,"How does the collaboration network of researchers coalesce around a scientific topic? What sort of social restructuring occurs as a new field develops? Previous empirical explorations of these questions have examined the evolution of co-authorship networks associated with several fields of science, each noting a characteristic shift in network structure as fields develop. Historically, however, such studies have tended to rely on manually annotated datasets and therefore only consider a handful of disciplines, calling into question the universality of the observed structural signature. To overcome this limitation and test the robustness of this phenomenon, we use a comprehensive dataset of over 189,000 scientific articles and develop a framework for partitioning articles and their authors into coherent, semantically related groups representing scientific fields of varying size and specificity. We then use the resulting population of fields to study the structure of evolving co-authorship networks. Consistent with earlier findings, we observe a global topological transition as the co-authorship networks coalesce from a disjointed aggregate into a dense giant connected component that dominates the network. We validate these results using a separate, complimentary corpus of scientific articles, and, overall, we find that the previously reported characteristic structural evolution of a scientific field's associated co-authorship network is robust across a large number of scientific fields of varying size, scope, and specificity. Additionally, the framework developed in this study may be used in other scientometric contexts in order to extend studies to compare across a larger range of scientific disciplines."
92,"Social media provide a platform for users to express their opinions and share information. Understanding public health opinions on social media, such as Twitter, offers a unique approach to characterizing common health issues such as diabetes, diet, exercise, and obesity (DDEO); however, collecting and analyzing a large scale conversational public health data set is a challenging research task. The goal of this research is to analyze the characteristics of the general public's opinions in regard to diabetes, diet, exercise and obesity (DDEO) as expressed on Twitter. A multi-component semantic and linguistic framework was developed to collect Twitter data, discover topics of interest about DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8% of tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity. The strongest correlation among the topics was determined between exercise and obesity (p <.0002). Other notable correlations were: diabetes and obesity (p <.0005), and diet and obesity (p <.001). DDEO terms were also identified as subtopics of each of the DDEO topics. The frequent subtopics discussed along with “Diabetes”, excluding the DDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer. The non-DDEO subtopics for “Diet” included vegetarian, pregnancy, celebrities, weight loss, religious, and mental health, while subtopics for “Exercise” included computer games, brain, fitness, and daily plan. Non-DDEO subtopics for “Obesity” included Alzheimer, cancer, and children. With 2.67 billion social media users in 2016, publicly available data such as Twitter posts can be utilized to support clinical providers, public health experts, and social scientists in better understanding common public opinions in regard to diabetes, diet, exercise, and obesity.",2018-02-01,2-s2.0-85029596678,International Journal of Information Management,"Characterizing diabetes, diet, exercise, and obesity comments on Twitter","Social media provide a platform for users to express their opinions and share information. Understanding public health opinions on social media, such as Twitter, offers a unique approach to characterizing common health issues such as diabetes, diet, exercise, and obesity (DDEO); however, collecting and analyzing a large scale conversational public health data set is a challenging research task. The goal of this research is to analyze the characteristics of the general public's opinions in regard to diabetes, diet, exercise and obesity (DDEO) as expressed on Twitter. A multi-component semantic and linguistic framework was developed to collect Twitter data, discover topics of interest about DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8% of tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity. The strongest correlation among the topics was determined between exercise and obesity (p <.0002). Other notable correlations were: diabetes and obesity (p <.0005), and diet and obesity (p <.001). DDEO terms were also identified as subtopics of each of the DDEO topics. The frequent subtopics discussed along with “Diabetes”, excluding the DDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer. The non-DDEO subtopics for “Diet” included vegetarian, pregnancy, celebrities, weight loss, religious, and mental health, while subtopics for “Exercise” included computer games, brain, fitness, and daily plan. Non-DDEO subtopics for “Obesity” included Alzheimer, cancer, and children. With 2.67 billion social media users in 2016, publicly available data such as Twitter posts can be utilized to support clinical providers, public health experts, and social scientists in better understanding common public opinions in regard to diabetes, diet, exercise, and obesity."
93,"This paper presents the Bayesian nonparametric (BNP) learning for hierarchical and sparse topics from natural language. Traditionally, the Indian buffet process provides the BNP prior on a binary matrix for an infinite latent feature model consisting of a flat layer of topics. The nested model paves an avenue to construct a tree model instead of a flat-layer model. This paper presents the nested Indian buffet process (nIBP) to achieve the sparsity and flexibility in topic model where the model complexity and topic hierarchy are learned from the groups of words. The mixed membership modeling is conducted by representing a document using the tree nodes or dishes that a document or a customer chooses according to the nIBP scenario. A tree stick-breaking process is implemented to select topic weights from a subtree for flexible topic modeling. Such an nIBP relaxes the constraint of adopting a single tree path in the nested Chinese restaurant process (nCRP) and, therefore, improves the variety of topic representation for heterogeneous documents. A Gibbs sampling procedure is developed to infer the nIBP topic model. Compared to the nested hierarchical Dirichlet process (nhDP), the compactness of the estimated topics in a tree using nIBP is improved. Experimental results show that the proposed nIBP reduces the error rate of nCRP and nhDP by 18% and 8% on Reuters task for document classification, respectively.",2018-02-01,2-s2.0-85038374410,IEEE/ACM Transactions on Audio Speech and Language Processing,Bayesian Nonparametric Learning for Hierarchical and Sparse Topics,"This paper presents the Bayesian nonparametric (BNP) learning for hierarchical and sparse topics from natural language. Traditionally, the Indian buffet process provides the BNP prior on a binary matrix for an infinite latent feature model consisting of a flat layer of topics. The nested model paves an avenue to construct a tree model instead of a flat-layer model. This paper presents the nested Indian buffet process (nIBP) to achieve the sparsity and flexibility in topic model where the model complexity and topic hierarchy are learned from the groups of words. The mixed membership modeling is conducted by representing a document using the tree nodes or dishes that a document or a customer chooses according to the nIBP scenario. A tree stick-breaking process is implemented to select topic weights from a subtree for flexible topic modeling. Such an nIBP relaxes the constraint of adopting a single tree path in the nested Chinese restaurant process (nCRP) and, therefore, improves the variety of topic representation for heterogeneous documents. A Gibbs sampling procedure is developed to infer the nIBP topic model. Compared to the nested hierarchical Dirichlet process (nhDP), the compactness of the estimated topics in a tree using nIBP is improved. Experimental results show that the proposed nIBP reduces the error rate of nCRP and nhDP by 18% and 8% on Reuters task for document classification, respectively."
94,"In this study, we propose a framework for detecting topic evolutions in weighted citation networks. Citation networks are important in studying knowledge flows; however, citation network analysis has primarily focused on binary networks in which the individual citation influences of each cited paper in a citing paper are considered identical, even though not all cited papers have a significant influence on the cited publication. Accordingly, it is necessary to build and analyze a citation network comprising scholarly publications that notably impact one another, thus identifying topic evolution in a more precise manner. To measure the strength of citation influence and identify paper topics, we employ a citation influence topic model primarily based on topical inheritance between cited and citing papers. Using scholarly publications in the field of the protein p53 as a case study, we build a citation network, filter it using citation influence values, and examine the diffusion of topics not only in the field but also in the subfields of p53.",2018-02-01,2-s2.0-85051196814,Journal of the Association for Information Science and Technology,Topic diffusion analysis of a weighted citation network in biomedical literature,"In this study, we propose a framework for detecting topic evolutions in weighted citation networks. Citation networks are important in studying knowledge flows; however, citation network analysis has primarily focused on binary networks in which the individual citation influences of each cited paper in a citing paper are considered identical, even though not all cited papers have a significant influence on the cited publication. Accordingly, it is necessary to build and analyze a citation network comprising scholarly publications that notably impact one another, thus identifying topic evolution in a more precise manner. To measure the strength of citation influence and identify paper topics, we employ a citation influence topic model primarily based on topical inheritance between cited and citing papers. Using scholarly publications in the field of the protein p53 as a case study, we build a citation network, filter it using citation influence values, and examine the diffusion of topics not only in the field but also in the subfields of p53."
95,"Social Network has become a very useful platform for users to share information and make friends with each other. In our daily life, we would like to take friends' advice when we choose products through the Internet. The sequential behaviors also play an important role in making recommendation, we can make use of the sequential factors to mine the relation between users. In order to take advantage of different factors when predicting ratings and enhancing the recommendation accuracy, we propose a novel hierarchical Bayesian model called N-CTR which combines topic model with probabilistic matrix factorization. Our model incorporates not only topic model to mine the latent topic between items and their tags, but also matrix factorization which handles ratings, social network and sequential behaviors. We have conducted experiments on data set hetcrec-2011-Lastfm. Compare with other recommendation algorithms, our method can effectively enhance the recommendation accuracy.",2018-01-30,2-s2.0-85047346516,"Proceedings - 2017 IEEE International Conference on Internet of Things, IEEE Green Computing and Communications, IEEE Cyber, Physical and Social Computing, IEEE Smart Data, iThings-GreenCom-CPSCom-SmartData 2017",Collaborative Topic Regression Based on the Social Network and Sequential Behaviors,"Social Network has become a very useful platform for users to share information and make friends with each other. In our daily life, we would like to take friends' advice when we choose products through the Internet. The sequential behaviors also play an important role in making recommendation, we can make use of the sequential factors to mine the relation between users. In order to take advantage of different factors when predicting ratings and enhancing the recommendation accuracy, we propose a novel hierarchical Bayesian model called N-CTR which combines topic model with probabilistic matrix factorization. Our model incorporates not only topic model to mine the latent topic between items and their tags, but also matrix factorization which handles ratings, social network and sequential behaviors. We have conducted experiments on data set hetcrec-2011-Lastfm. Compare with other recommendation algorithms, our method can effectively enhance the recommendation accuracy."
96,"With the rapid growth of population on social networks, people are confronted with information overload problem. This clearly makes filtering the targeted users a demanding and key research task. Uni-directional social networks are the scenarios where users provide limited follow or not binary features. Related works prefer to utilize these follower-followee relations for recommendation. However, a major problem of these methods is that they assume every follower-followee user pairs are equally likely, and this leads to the coarse user following preferences inferring. Intuitively, a user's adoption of others as followees may be motivated by her interests as well as social connections, hence a good recommender should be able to separate the two situations and take both factors into account for better recommendation results. In this regard, we propose a new user recommendation framework namely UIS-MF in this work. UIS-MF can well capture user preferences by involving both interest and social factors in prediction, and targeted to recommend Top-N followees who have similar interest and close social connection relevant to a target user. Specifically, we first present a unified probabilistic topic model on follower-followee relations, namely UIS-LDA, and it employs Generalized Pólya Urn (GPU) models on mutual-following relations for discovering interest topics and social topics of users. Next we propose a community-based method for user recommendation, it organizes social communities and interest communities based on the estimation of topics obtained from UIS-LDA, and then performs Matrix Factorization (MF) method on each community to generate N most likely followees for individual user. Systematic experiments on Twitter, Sina Weibo and Epinions datasets have not only revealed the significant effect of our UIS-LDA model for the extraction of interest and social topics of users in improving recommending accuracy, but also demonstrated the advantage of our proposed recommendation framework over competitive baselines by large margins.",2018-01-15,2-s2.0-85034955336,Knowledge-Based Systems,Improving user recommendation by extracting social topics and interest topics of users in uni-directional social networks,"With the rapid growth of population on social networks, people are confronted with information overload problem. This clearly makes filtering the targeted users a demanding and key research task. Uni-directional social networks are the scenarios where users provide limited follow or not binary features. Related works prefer to utilize these follower-followee relations for recommendation. However, a major problem of these methods is that they assume every follower-followee user pairs are equally likely, and this leads to the coarse user following preferences inferring. Intuitively, a user's adoption of others as followees may be motivated by her interests as well as social connections, hence a good recommender should be able to separate the two situations and take both factors into account for better recommendation results. In this regard, we propose a new user recommendation framework namely UIS-MF in this work. UIS-MF can well capture user preferences by involving both interest and social factors in prediction, and targeted to recommend Top-N followees who have similar interest and close social connection relevant to a target user. Specifically, we first present a unified probabilistic topic model on follower-followee relations, namely UIS-LDA, and it employs Generalized Pólya Urn (GPU) models on mutual-following relations for discovering interest topics and social topics of users. Next we propose a community-based method for user recommendation, it organizes social communities and interest communities based on the estimation of topics obtained from UIS-LDA, and then performs Matrix Factorization (MF) method on each community to generate N most likely followees for individual user. Systematic experiments on Twitter, Sina Weibo and Epinions datasets have not only revealed the significant effect of our UIS-LDA model for the extraction of interest and social topics of users in improving recommending accuracy, but also demonstrated the advantage of our proposed recommendation framework over competitive baselines by large margins."
97,"Sentiment analysis has important applications in many areas, including marketing, recommendation, and financial analysis. Since topic modeling can discover hidden semantic structures, researchers put forward sentiment analysis models based on topic models. These models have been successfully applied on long texts, but analysis for short text is a challenging task because of the sparsity of features in short texts. We observe that the textual context has been widely considered on text analysis task, but on sentiment analysis area, most sentiment analysis models still lack of consideration and integration of sentimental context. Thus, by taking the speciality of sentiment analysis task and short text into consideration, we propose the sentimental context to enrich the characteristics and improve the performance of sentiment classification over short text. We first put forward the concept of sentimental context, which is extracted from the text body and sentiment lexicon, and then we integrate the sentimental context and propose two sentiment classification models based on word-level and topic-level respectively. We present results on real-world datasets from various sources, validating the effectiveness of the proposed models.",2018-01-12,2-s2.0-85050543659,"Proceedings of 4th International Conference on Behavioral, Economic, and Socio-Cultural Computing, BESC 2017",Sentiment classification of short text using sentimental context,"Sentiment analysis has important applications in many areas, including marketing, recommendation, and financial analysis. Since topic modeling can discover hidden semantic structures, researchers put forward sentiment analysis models based on topic models. These models have been successfully applied on long texts, but analysis for short text is a challenging task because of the sparsity of features in short texts. We observe that the textual context has been widely considered on text analysis task, but on sentiment analysis area, most sentiment analysis models still lack of consideration and integration of sentimental context. Thus, by taking the speciality of sentiment analysis task and short text into consideration, we propose the sentimental context to enrich the characteristics and improve the performance of sentiment classification over short text. We first put forward the concept of sentimental context, which is extracted from the text body and sentiment lexicon, and then we integrate the sentimental context and propose two sentiment classification models based on word-level and topic-level respectively. We present results on real-world datasets from various sources, validating the effectiveness of the proposed models."
98,"Recently, text mining has risen as an advanced technology that analyzes meaningful trends and topics in document collections. Despite its increasing use in various research areas, there have not been previous studies using document collections of international standards. In this paper, we propose the Trend Analysis System for International Standards (TASIS), which automatically performs topic modeling and trend analysis on document collections of the International Telecommunication Union Telecommunication Standardization Sector (ITU-T) Recommendations, based on a latent dirichlet allocation (LDA) algorithm. For providing Web services, the TASIS performs topic modeling by exploiting user-defined parameters, such as the number of topics and iterations, and the results show a list of the documents that each keyword in the topic is included in. The TASIS also describes a TreeMap with the size of the extracted topic as a graphical expression for easier understanding.",2018-01-04,2-s2.0-85048030788,"Proceedings of the 2017 ITU Kaleidoscope Academic Conference: Challenges for a Data-Driven Society, ITU K 2017",TASIS: Trend analysis system for international standards,"Recently, text mining has risen as an advanced technology that analyzes meaningful trends and topics in document collections. Despite its increasing use in various research areas, there have not been previous studies using document collections of international standards. In this paper, we propose the Trend Analysis System for International Standards (TASIS), which automatically performs topic modeling and trend analysis on document collections of the International Telecommunication Union Telecommunication Standardization Sector (ITU-T) Recommendations, based on a latent dirichlet allocation (LDA) algorithm. For providing Web services, the TASIS performs topic modeling by exploiting user-defined parameters, such as the number of topics and iterations, and the results show a list of the documents that each keyword in the topic is included in. The TASIS also describes a TreeMap with the size of the extracted topic as a graphical expression for easier understanding."
100,"Adynamics. research the current landscape High-quality state of a is certain a high-level research scientific landscapes description field and are its of an important tools that allow for more effective research management. This paper presents a novel framework for the mapping of research. It relies on full-text mining and topic modeling to pool data from many sources without relying on any specific taxonomy of scientific fields and areas. The framework is especially useful for scientific fields that are poorly represented in scientometric databases, i.e., Scopus or Web of Science. The high-level algorithm consists of (1) full-text collection from reliable sources; (2) the automatic extraction of research fields using topic modeling; (3) semi-automatic linking to scientometric databases; and (4) a statistical analysis of metrics for the extracted scientific areas. Full-text mining is crucial due to (a) the poor representation of many Russian research areas in systems like Scopus or Web of Science; (b) the poor quality of Russian Science Index data; and (c) the differences between taxonomies used in different data sources. Major advantages of the proposed framework include its data-driven approach, its independence from scientific subjects' taxonomies, and its ability to integrate data from multiple heterogeneous data sources. Furthermore, this framework complements traditional approaches to research mapping using scientometric software like Scopus or Web of Science rather than replacing them. We experimentally evaluated the framework using agricultural science as an example, but the framework is not limited to any particular domain. As a result, we created the first research landscape covering young researchers in agricultural science. Topic modeling yielded six major scientific areas within the field of agriculture. We found that statistically significant differences between these areas exist. This means that a differentiated approach to research management is critical. Further research on this subject includes the application of the framework to other scientific fields and the integration of other collections of research and technical documentation (especially patents).",2018-01-01,2-s2.0-85046454831,Foresight and STI Governance,Mapping the research landscape of agricultural sciences,"Adynamics. research the current landscape High-quality state of a is certain a high-level research scientific landscapes description field and are its of an important tools that allow for more effective research management. This paper presents a novel framework for the mapping of research. It relies on full-text mining and topic modeling to pool data from many sources without relying on any specific taxonomy of scientific fields and areas. The framework is especially useful for scientific fields that are poorly represented in scientometric databases, i.e., Scopus or Web of Science. The high-level algorithm consists of (1) full-text collection from reliable sources; (2) the automatic extraction of research fields using topic modeling; (3) semi-automatic linking to scientometric databases; and (4) a statistical analysis of metrics for the extracted scientific areas. Full-text mining is crucial due to (a) the poor representation of many Russian research areas in systems like Scopus or Web of Science; (b) the poor quality of Russian Science Index data; and (c) the differences between taxonomies used in different data sources. Major advantages of the proposed framework include its data-driven approach, its independence from scientific subjects' taxonomies, and its ability to integrate data from multiple heterogeneous data sources. Furthermore, this framework complements traditional approaches to research mapping using scientometric software like Scopus or Web of Science rather than replacing them. We experimentally evaluated the framework using agricultural science as an example, but the framework is not limited to any particular domain. As a result, we created the first research landscape covering young researchers in agricultural science. Topic modeling yielded six major scientific areas within the field of agriculture. We found that statistically significant differences between these areas exist. This means that a differentiated approach to research management is critical. Further research on this subject includes the application of the framework to other scientific fields and the integration of other collections of research and technical documentation (especially patents)."
101,"Using a case-study based approach, this research contributes to the standardisation versus adaptation debate in global marketing. It analyses the influence of the local culture dimension reflected in consumers' comments in the Facebook platform regarding a new global technological product, the Samsung Galaxy S8/S8+, launched worldwide in 2017. Consumers' comments about this new smartphone were gathered and analysed for three cultural distinct English-speaking countries: Australia, India, and South Africa. The analysis' procedure consisted of a text mining and topic modelling approach, including sentiment classification analysis, to discern and understand consumers' responses to global brand communications. The findings indicate that cultural aspects still play a key role in consumers' reactions to the product in each country, justifying the continued need for marketing strategies that conflate pursuing economies of scale with accounting for the cultural sensitivities of demand at country level. Evidence of consumers attitudes' and behaviours' homogenisation across countries is still limited.",2018-01-01,2-s2.0-85051477258,Journal of Business Research,A cross-cultural case study of consumers' communications about a new technological product,"Using a case-study based approach, this research contributes to the standardisation versus adaptation debate in global marketing. It analyses the influence of the local culture dimension reflected in consumers' comments in the Facebook platform regarding a new global technological product, the Samsung Galaxy S8/S8+, launched worldwide in 2017. Consumers' comments about this new smartphone were gathered and analysed for three cultural distinct English-speaking countries: Australia, India, and South Africa. The analysis' procedure consisted of a text mining and topic modelling approach, including sentiment classification analysis, to discern and understand consumers' responses to global brand communications. The findings indicate that cultural aspects still play a key role in consumers' reactions to the product in each country, justifying the continued need for marketing strategies that conflate pursuing economies of scale with accounting for the cultural sensitivities of demand at country level. Evidence of consumers attitudes' and behaviours' homogenisation across countries is still limited."
102,"Purpose: This paper aims to identify the intellectual structure of four leading hospitality journals over 40 years by applying mixed-method approach, using both machine learning and traditional statistical analyses. Design/methodology/approach: Abstracts from all 4,139 articles published in four top hospitality journals were analyzed using the structured topic modeling and inferential statistics. Topic correlation and community detection were applied to identify strengths of correlations and sub-groups of topics. Trend visualization and regression analysis were used to quantify the effects of the metadata (i.e. year of publication and journal) on topic proportions. Findings: The authors found 50 topics and eight subgroups in the hospitality journals. Different evolutionary patterns in topic popularity were demonstrated, thereby providing the insights for popular research topics over time. The significant differences in topical proportions were found across the four leading hospitality journals, suggesting different foci in research topics in each journal. Research limitations/implications: Combining machine learning techniques with traditional statistics demonstrated potential for discovering valuable insights from big text data in hospitality and tourism research contexts. The findings of this study may serve as a guide to understand the trends in the research field as well as the progress of specific areas or subfields. Originality/value: It is the first attempt to apply topic modeling to academic publications and explore the effects of article metadata with the hospitality literature.",2018-01-01,2-s2.0-85052567701,International Journal of Contemporary Hospitality Management,Toward understanding the topical structure of hospitality literature: Applying machine learning and traditional statistics,"Purpose: This paper aims to identify the intellectual structure of four leading hospitality journals over 40 years by applying mixed-method approach, using both machine learning and traditional statistical analyses. Design/methodology/approach: Abstracts from all 4,139 articles published in four top hospitality journals were analyzed using the structured topic modeling and inferential statistics. Topic correlation and community detection were applied to identify strengths of correlations and sub-groups of topics. Trend visualization and regression analysis were used to quantify the effects of the metadata (i.e. year of publication and journal) on topic proportions. Findings: The authors found 50 topics and eight subgroups in the hospitality journals. Different evolutionary patterns in topic popularity were demonstrated, thereby providing the insights for popular research topics over time. The significant differences in topical proportions were found across the four leading hospitality journals, suggesting different foci in research topics in each journal. Research limitations/implications: Combining machine learning techniques with traditional statistics demonstrated potential for discovering valuable insights from big text data in hospitality and tourism research contexts. The findings of this study may serve as a guide to understand the trends in the research field as well as the progress of specific areas or subfields. Originality/value: It is the first attempt to apply topic modeling to academic publications and explore the effects of article metadata with the hospitality literature."
103,"Purpose: The purpose of this paper is to explore and describe how research on quality management (QM) has evolved historically. The study includes the complete digital archive of three academic journals in the field of QM. Thereby, a unique depiction of how the general outlines of the field as well as trends in research topics have evolved through the years is presented. Design/methodology/approach: The study applies cluster and probabilistic topic modeling to unstructured data from The International Journal of Quality & Reliability Management, The TQM Journal and Total Quality Management & Business Excellence. In addition, trend analysis using support vector machine is performed. Findings: The study identifies six central, perpetual themes of QM research: control, costs, reliability and failure; service quality; TQM – implementation and performance; ISO – certification, standards and systems; Innovation, practices and learning and customers – research and product design. Additionally, historical surges and shifts in research focus are recognized in the study. From these trends, a decrease in interest in TQM and control of quality, costs and processes in favor of service quality, customer satisfaction, Six Sigma, Lean and innovation can be noted during the past decade. The results validate previous findings. Originality/value: Of the identified central themes, innovation, practices and learning appears not to have been documented as a fundamental part of QM research in previous studies. Thus, this theme can be regarded as a new perspective on QM research and thereby on QM.",2018-01-01,2-s2.0-85039788432,International Journal of Quality and Reliability Management,25 years of quality management research – outlines and trends,"Purpose: The purpose of this paper is to explore and describe how research on quality management (QM) has evolved historically. The study includes the complete digital archive of three academic journals in the field of QM. Thereby, a unique depiction of how the general outlines of the field as well as trends in research topics have evolved through the years is presented. Design/methodology/approach: The study applies cluster and probabilistic topic modeling to unstructured data from The International Journal of Quality & Reliability Management, The TQM Journal and Total Quality Management & Business Excellence. In addition, trend analysis using support vector machine is performed. Findings: The study identifies six central, perpetual themes of QM research: control, costs, reliability and failure; service quality; TQM – implementation and performance; ISO – certification, standards and systems; Innovation, practices and learning and customers – research and product design. Additionally, historical surges and shifts in research focus are recognized in the study. From these trends, a decrease in interest in TQM and control of quality, costs and processes in favor of service quality, customer satisfaction, Six Sigma, Lean and innovation can be noted during the past decade. The results validate previous findings. Originality/value: Of the identified central themes, innovation, practices and learning appears not to have been documented as a fundamental part of QM research in previous studies. Thus, this theme can be regarded as a new perspective on QM research and thereby on QM."
104,"Purpose: The increasing competition among higher education institutions (HEI) has led students to conduct a more in-depth analysis to choose where to study abroad. Since students are usually unable to visit each HEIs before making their decision, they are strongly influenced by what is written by former international students (IS) on the internet. HEIs also benefit from such information online. The purpose of this paper is to provide an understanding of the drivers of HEIs success online. Design/methodology/approach: Due to the increasing amount of information published online, HEIs have to use automatic techniques to search for patterns instead of analysing such information manually. The present paper uses text mining (TM) and sentiment analysis (SA) to study online reviews of IS about their HEIs. The paper studied 1938 reviews from 65 different business schools with Association to Advance Collegiate Schools of Business accreditation. Findings: Results show that HEIs may become more attractive online if they financially support students cost of living, provide courses in English, and promote an international environment. Research limitations/implications: Despite the use of a major platform with a broad number of reviews from students around the world, other sources focussed on other types of HEIs may have been used to reinforce the findings in the current paper. Originality/value: The study pioneers the use of TM and SA to highlight topics and sentiments mentioned in online reviews by students attending HEIs, clarifying how such opinions are correlated with satisfaction. Using such information, HEIs’ managers may focus their efforts on promoting international attractiveness of their institutions.",2018-01-01,2-s2.0-85045695822,International Journal of Educational Management,Improving international attractiveness of higher education institutions based on text mining and sentiment analysis,"Purpose: The increasing competition among higher education institutions (HEI) has led students to conduct a more in-depth analysis to choose where to study abroad. Since students are usually unable to visit each HEIs before making their decision, they are strongly influenced by what is written by former international students (IS) on the internet. HEIs also benefit from such information online. The purpose of this paper is to provide an understanding of the drivers of HEIs success online. Design/methodology/approach: Due to the increasing amount of information published online, HEIs have to use automatic techniques to search for patterns instead of analysing such information manually. The present paper uses text mining (TM) and sentiment analysis (SA) to study online reviews of IS about their HEIs. The paper studied 1938 reviews from 65 different business schools with Association to Advance Collegiate Schools of Business accreditation. Findings: Results show that HEIs may become more attractive online if they financially support students cost of living, provide courses in English, and promote an international environment. Research limitations/implications: Despite the use of a major platform with a broad number of reviews from students around the world, other sources focussed on other types of HEIs may have been used to reinforce the findings in the current paper. Originality/value: The study pioneers the use of TM and SA to highlight topics and sentiments mentioned in online reviews by students attending HEIs, clarifying how such opinions are correlated with satisfaction. Using such information, HEIs’ managers may focus their efforts on promoting international attractiveness of their institutions."
105,"This article considers how, and why, “Topic Modelling” tools can be used to analyse historical newspaper archives. While a growing number of media and communication studies projects have applied these techniques to corpuses of born-digital journalism, using the same tools to analyse large-scale collections of historical newspapers requires us to overcome additional technological and methodological challenges. Our discussion is framed around a historical case study examining references to the United States in the 19th Century British Library Newspaper Archive. The article begins by highlighting the problems that researchers of both digital and historical journalism face when attempting to deal with an enormous body of evidence. Next, it argues that Topic Modelling offers one potential solution to these problems by providing a way to “distant read” the archive. The remainder of the article is divided into five experiments that demonstrate how Topic Modelling can be applied to a series of research questions, each of which is applicable to other projects that might make use of newspaper archives. As well as demonstrating the investigative potential of topic modelling, the article also highlights the practical and technological barriers that currently undermine its effectiveness, particularly when it is applied to archives of historical material.",2018-01-01,2-s2.0-85053756164,Digital Journalism,In Search of America: Topic modelling nineteenth-century newspaper archives,"This article considers how, and why, “Topic Modelling” tools can be used to analyse historical newspaper archives. While a growing number of media and communication studies projects have applied these techniques to corpuses of born-digital journalism, using the same tools to analyse large-scale collections of historical newspapers requires us to overcome additional technological and methodological challenges. Our discussion is framed around a historical case study examining references to the United States in the 19th Century British Library Newspaper Archive. The article begins by highlighting the problems that researchers of both digital and historical journalism face when attempting to deal with an enormous body of evidence. Next, it argues that Topic Modelling offers one potential solution to these problems by providing a way to “distant read” the archive. The remainder of the article is divided into five experiments that demonstrate how Topic Modelling can be applied to a series of research questions, each of which is applicable to other projects that might make use of newspaper archives. As well as demonstrating the investigative potential of topic modelling, the article also highlights the practical and technological barriers that currently undermine its effectiveness, particularly when it is applied to archives of historical material."
106,"Purpose: This paper aims to present an automated literature analysis to unveil the drivers for incorporating social media in tourism and hospitality brand strategies. Design/methodology/approach: To gather relevant literature, Google Scholar was queried with “brand”/“branding” and “social media” for articles in ten top-ranked tourism and hospitality journals, resulting in a total of 479 collected articles. The methodology adopted for the analysis is based on text mining and topic modeling procedures. The topics discovered are characterized by terms belonging to a dictionary previously compiled and provide a segmentation of the articles in coherent sets of the literature. Findings: Most of the 213 articles that encompass a strong relation between social media and branding are mentioning mainly brand building stages. A large research gap was found in hospitality and tourism considering that, besides advertising, no topic was discovered related to known brand strategies such as co-branding or franchising. Practical implications: The present analysis concludes that specialized tourism and hospitality literature needs to keep pace with research that is being conducted on a wide range of industries to assess the influence of social media. Originality/value: The automated analysis approach used has no precedent in tourism and hospitality research. By including an innovative topical concept map, it led to identifying and summarizing the topics, providing a clear picture on the findings. This study calls for research by specialized tourism and hospitality publications, eventually leading to special issues on this vibrant subject.",2018-01-01,2-s2.0-85041715546,International Journal of Contemporary Hospitality Management,Brand strategies in social media in hospitality and tourism,"Purpose: This paper aims to present an automated literature analysis to unveil the drivers for incorporating social media in tourism and hospitality brand strategies. Design/methodology/approach: To gather relevant literature, Google Scholar was queried with “brand”/“branding” and “social media” for articles in ten top-ranked tourism and hospitality journals, resulting in a total of 479 collected articles. The methodology adopted for the analysis is based on text mining and topic modeling procedures. The topics discovered are characterized by terms belonging to a dictionary previously compiled and provide a segmentation of the articles in coherent sets of the literature. Findings: Most of the 213 articles that encompass a strong relation between social media and branding are mentioning mainly brand building stages. A large research gap was found in hospitality and tourism considering that, besides advertising, no topic was discovered related to known brand strategies such as co-branding or franchising. Practical implications: The present analysis concludes that specialized tourism and hospitality literature needs to keep pace with research that is being conducted on a wide range of industries to assess the influence of social media. Originality/value: The automated analysis approach used has no precedent in tourism and hospitality research. By including an innovative topical concept map, it led to identifying and summarizing the topics, providing a clear picture on the findings. This study calls for research by specialized tourism and hospitality publications, eventually leading to special issues on this vibrant subject."
107,"Research has emphasized the limitations of qualitative and quantitative approaches to studying organizational phenomena. For example, in-depth interviews are resource-intensive, while questionnaires with closed-ended questions can only measure predefined constructs. With the recent availability of large textual data sets and increased computational power, text mining has become an attractive method that has the potential to mitigate some of these limitations. Thus, we suggest applying topic modeling, a specific text mining technique, as a new and complementary strategy of inquiry to study organizational phenomena. In particular, we outline the potentials of structural topic modeling for organizational research and provide a step-by-step tutorial on how to apply it. Our application example builds on 428,492 reviews of Fortune 500 companies from the online platform Glassdoor, on which employees can evaluate organizations. We demonstrate how structural topic models allow to inductively identify topics that matter to employees and quantify their relationship with employees’ perception of organizational culture. We discuss the advantages and limitations of topic modeling as a research method and outline how future research can apply the technique to study organizational phenomena.",2018-01-01,2-s2.0-85046719447,Organizational Research Methods,Topic Modeling as a Strategy of Inquiry in Organizational Research: A Tutorial With an Application Example on Organizational Culture,"Research has emphasized the limitations of qualitative and quantitative approaches to studying organizational phenomena. For example, in-depth interviews are resource-intensive, while questionnaires with closed-ended questions can only measure predefined constructs. With the recent availability of large textual data sets and increased computational power, text mining has become an attractive method that has the potential to mitigate some of these limitations. Thus, we suggest applying topic modeling, a specific text mining technique, as a new and complementary strategy of inquiry to study organizational phenomena. In particular, we outline the potentials of structural topic modeling for organizational research and provide a step-by-step tutorial on how to apply it. Our application example builds on 428,492 reviews of Fortune 500 companies from the online platform Glassdoor, on which employees can evaluate organizations. We demonstrate how structural topic models allow to inductively identify topics that matter to employees and quantify their relationship with employees’ perception of organizational culture. We discuss the advantages and limitations of topic modeling as a research method and outline how future research can apply the technique to study organizational phenomena."
108,"Assessing the impact of events on the evolution of online public discourse is challenging due to the lack of data prior to the event and appropriate methodologies for capturing the progression of tenor of public discourse, both in terms of their tone and topic. In this article, we introduce a geovisual analytics framework, CarSenToGram, which integrates topic modeling and sentiment analysis with cartograms to identify the changing dynamics of public discourse on a particular topic across space and time. The main novelty of CarSenToGram is coupling comprehensible spatiotemporal overviews of the overall distribution, topical and sentiment patterns with increasing levels of information supported by zoom and filter, and details-on-demand interactions. To demonstrate the utility of CarSenToGram, in this article, we analyze tweets related to immigration the month before and after the 27 January 2017 travel ban in order to reveal insights into one of the defining moments of President Trump’s first year in office. Not only do we find that the travel ban influenced online public discourse and sentiment on immigration, but it also highlighted important partisan divisions within the US.",2018-01-01,2-s2.0-85053527100,Cartography and Geographic Information Science,CarSenToGram: geovisual text analytics for exploring spatiotemporal variation in public discourse on Twitter,"Assessing the impact of events on the evolution of online public discourse is challenging due to the lack of data prior to the event and appropriate methodologies for capturing the progression of tenor of public discourse, both in terms of their tone and topic. In this article, we introduce a geovisual analytics framework, CarSenToGram, which integrates topic modeling and sentiment analysis with cartograms to identify the changing dynamics of public discourse on a particular topic across space and time. The main novelty of CarSenToGram is coupling comprehensible spatiotemporal overviews of the overall distribution, topical and sentiment patterns with increasing levels of information supported by zoom and filter, and details-on-demand interactions. To demonstrate the utility of CarSenToGram, in this article, we analyze tweets related to immigration the month before and after the 27 January 2017 travel ban in order to reveal insights into one of the defining moments of President Trump’s first year in office. Not only do we find that the travel ban influenced online public discourse and sentiment on immigration, but it also highlighted important partisan divisions within the US."
109,"Purpose: The purpose of this study was to explore influences of review-related information on topical proportions and the pattern of word appearances in each topic (topical content) using structural topic model (STM). Design/methodology/approach: For 173,607 Yelp.com reviews written in 2005-2016, STM-based topic modeling was applied with inclusion of covariates in addition to traditional statistical analyses. Findings: Differences in topic prevalence and topical contents were found between certified green and non-certified restaurants. Customers’ recognition in sustainable food topics were changed over time. Research limitations/implications: This study demonstrates the application of STM for the systematic analysis of a large amount of text data. Originality/value: Limited study in the hospitality literature examined the influence of review-level metadata on topic and term estimation. Through topic modeling, customers’ natural responses toward green practices were identified.",2018-01-01,2-s2.0-85053030459,Journal of Hospitality and Tourism Technology,The structural topic model for online review analysis: Comparison between green and non-green restaurants,"Purpose: The purpose of this study was to explore influences of review-related information on topical proportions and the pattern of word appearances in each topic (topical content) using structural topic model (STM). Design/methodology/approach: For 173,607 Yelp.com reviews written in 2005-2016, STM-based topic modeling was applied with inclusion of covariates in addition to traditional statistical analyses. Findings: Differences in topic prevalence and topical contents were found between certified green and non-certified restaurants. Customers’ recognition in sustainable food topics were changed over time. Research limitations/implications: This study demonstrates the application of STM for the systematic analysis of a large amount of text data. Originality/value: Limited study in the hospitality literature examined the influence of review-level metadata on topic and term estimation. Through topic modeling, customers’ natural responses toward green practices were identified."
110,"Based on text mining, this study explored topics in the research domain of knowledge organization. A text corpus consisting of tides and abstracts was generated from 282 articles of the Knowledge Organisation journal for the recent ten years from 2006 to 2015. Term frequency analysis and Latent Dirichlet allocation topic modeling were employed to analyze the collected corpus. Topic modeling uncovered twenty research topics prevailing in the knowledge organization field, including theories and epistemology, classification scheme, domain analysis and ontology, digital archiving, document indexing and retrieval, taxonomy and thesaurus system, metadata and controlled vocabulary, ethical issues, and others. In addition, topic trends over the ten years were examined to identify topics that attracted more discussion in the journal. The top two topics that received increased attention recently were ""ethical issues in knowledge organization"" and ""domain analysis and ontologies."" This study yields insight into a better understanding of the research domain of knowledge organization. Moreover, text mining approaches introduced in this study have methodological implications for domain analysis in knowledge organization.",2018-01-01,2-s2.0-85048822699,Knowledge Organization,Topic analysis of the research domain in knowledge organization: A latent dirichlet allocation approach,"Based on text mining, this study explored topics in the research domain of knowledge organization. A text corpus consisting of tides and abstracts was generated from 282 articles of the Knowledge Organisation journal for the recent ten years from 2006 to 2015. Term frequency analysis and Latent Dirichlet allocation topic modeling were employed to analyze the collected corpus. Topic modeling uncovered twenty research topics prevailing in the knowledge organization field, including theories and epistemology, classification scheme, domain analysis and ontology, digital archiving, document indexing and retrieval, taxonomy and thesaurus system, metadata and controlled vocabulary, ethical issues, and others. In addition, topic trends over the ten years were examined to identify topics that attracted more discussion in the journal. The top two topics that received increased attention recently were ""ethical issues in knowledge organization"" and ""domain analysis and ontologies."" This study yields insight into a better understanding of the research domain of knowledge organization. Moreover, text mining approaches introduced in this study have methodological implications for domain analysis in knowledge organization."
111,"This article reviews Decision Sciences journal articles and metadata to analyze its intellectual tradition. Text analytics is used with probabilistic topic modeling. The topical structure of the journal is reviewed by topic definition and popularity, with correlations. Fifty research topics or themes involving a wide range of quantitative methods and decision-making practice were selected. Functional areas were also examined. The evolution of topics since 1975 is noted. There is clear indication that journal coverage has evolved. In early years, emphasis was on quantitative modeling methods and relevant methodologies. More recently new research areas to include supply chain management, marketing, service management, and health care are more noted. Some topics are highly correlated. We find that this evolution reflects the changes occurring in business and decision-making environment. The article discusses external and internal factors important in shaping the journal's topical trajectory. Unique challenges in analyzing text data are discussed. Latent Dirichlet Allocation, an unsupervised Bayesian approach for statistical topic modeling, is applied to 1,698 research articles from Decision Sciences over the period from 1975 to 2016. This approach is found to be useful to discover journal topical trends. Potential for other applications in decision sciences is discussed.",2018-01-01,2-s2.0-85050506351,Decision Sciences,A Topical Exploration of the Intellectual Development of Decision Sciences 1975-2016,"This article reviews Decision Sciences journal articles and metadata to analyze its intellectual tradition. Text analytics is used with probabilistic topic modeling. The topical structure of the journal is reviewed by topic definition and popularity, with correlations. Fifty research topics or themes involving a wide range of quantitative methods and decision-making practice were selected. Functional areas were also examined. The evolution of topics since 1975 is noted. There is clear indication that journal coverage has evolved. In early years, emphasis was on quantitative modeling methods and relevant methodologies. More recently new research areas to include supply chain management, marketing, service management, and health care are more noted. Some topics are highly correlated. We find that this evolution reflects the changes occurring in business and decision-making environment. The article discusses external and internal factors important in shaping the journal's topical trajectory. Unique challenges in analyzing text data are discussed. Latent Dirichlet Allocation, an unsupervised Bayesian approach for statistical topic modeling, is applied to 1,698 research articles from Decision Sciences over the period from 1975 to 2016. This approach is found to be useful to discover journal topical trends. Potential for other applications in decision sciences is discussed."
112,"Strategic management requires an assessment of a firm's internal and external environments. Our work extends the body of management tools (e.g., SWOT analysis or growth-share matrix) by proposing an automated text mining framework. Here we draw on narrative materials from firms (e.g., financial disclosures) and perform topic modeling so as to identify the key issues faced by an organization. We then quantify the use of language along two dimensions: risk and optimism. This reveals a firm's strengths and weaknesses by identifying business units, activities, and processes subject to risk, while also comparing it with competitors or the market.",2018-01-01,2-s2.0-85047428654,Information and Management,Business analytics for strategic management: Identifying and assessing corporate challenges via topic modeling,"Strategic management requires an assessment of a firm's internal and external environments. Our work extends the body of management tools (e.g., SWOT analysis or growth-share matrix) by proposing an automated text mining framework. Here we draw on narrative materials from firms (e.g., financial disclosures) and perform topic modeling so as to identify the key issues faced by an organization. We then quantify the use of language along two dimensions: risk and optimism. This reveals a firm's strengths and weaknesses by identifying business units, activities, and processes subject to risk, while also comparing it with competitors or the market."
113,"Although investments of R&D by government and firms have enlarged and the amount of patents has increased rapidly, R&D almost fails to commercialize for various reasons. For the purpose of decreasing failure rate of technology commercialization, it is important to identify emerging business based on technology in advance and establish appropriate strategy, leading to surviving at the market. Therefore, this paper aims to explore emerging Research and Business Development (R&BD) areas, and establish a business strategy based on valuable patents by comprehensively analyzing IPRs - patent as well as design and trademark. First, unrevealed but potential R&BD areas are explored by analyzing the relation between patent and trademark through topic modeling and network analysis, which aims to preferentially find potential business opportunities that can be implemented by new technology. Potential R&BD areas are recognized as the hidden link in the network of patents and trademarks. Second, emerging R&BD areas are selected by considering the status of the competition and markets through trademark analysis based on generative topographic mapping (GTM) after finding potential R&BD areas with network analysis from the viewpoint of the applicant for a trademark. Finally, new opportunities and strategies for successful R&BD are suggested by analyzing design patents that are representative of the appearance of a product in detail. The result of this study provides more concrete R&BD strategies within the framework of product and business development, based on relations between IPRs, which can be regarded as an initial study that comprehensively utilizes diverse kinds of IPRs.",2018-01-01,2-s2.0-85047060072,Technological Forecasting and Social Change,Identifying emerging Research and Business Development (R&BD) areas based on topic modeling and visualization with intellectual property right data,"Although investments of R&D by government and firms have enlarged and the amount of patents has increased rapidly, R&D almost fails to commercialize for various reasons. For the purpose of decreasing failure rate of technology commercialization, it is important to identify emerging business based on technology in advance and establish appropriate strategy, leading to surviving at the market. Therefore, this paper aims to explore emerging Research and Business Development (R&BD) areas, and establish a business strategy based on valuable patents by comprehensively analyzing IPRs - patent as well as design and trademark. First, unrevealed but potential R&BD areas are explored by analyzing the relation between patent and trademark through topic modeling and network analysis, which aims to preferentially find potential business opportunities that can be implemented by new technology. Potential R&BD areas are recognized as the hidden link in the network of patents and trademarks. Second, emerging R&BD areas are selected by considering the status of the competition and markets through trademark analysis based on generative topographic mapping (GTM) after finding potential R&BD areas with network analysis from the viewpoint of the applicant for a trademark. Finally, new opportunities and strategies for successful R&BD are suggested by analyzing design patents that are representative of the appearance of a product in detail. The result of this study provides more concrete R&BD strategies within the framework of product and business development, based on relations between IPRs, which can be regarded as an initial study that comprehensively utilizes diverse kinds of IPRs."
114,"Objective: Topic modeling (TM) refers to a group of methods for mathematically identifying latent topics in large corpora of data. Although TM shows promise as a tool for social science research, most researchers lack awareness of the tool's utility. Therefore, this article provides a brief overview of TM's logic and processes, offers a simple example, and suggests several possible uses in social sciences. Methods: Using latent semantic analysis in our example, we analyzed transcripts of the 2016 U.S. presidential debates between Hillary Clinton and Donald Trump. Results: Resulting topics paralleled the most frequent policy-related Internet searches at the time. When divided by candidate, changes in emergent topics reflected individual policy stances, with nuanced differences between the two. Conclusion: Findings underscored the utility of TM to identify thematic patterns embedded in large quantities of text. TM, therefore, represents a valuable addition to the social scientist's methodological tool set.",2018-01-01,2-s2.0-85052960398,Social Science Quarterly,Topic Modeling: Latent Semantic Analysis for the Social Sciences*,"Objective: Topic modeling (TM) refers to a group of methods for mathematically identifying latent topics in large corpora of data. Although TM shows promise as a tool for social science research, most researchers lack awareness of the tool's utility. Therefore, this article provides a brief overview of TM's logic and processes, offers a simple example, and suggests several possible uses in social sciences. Methods: Using latent semantic analysis in our example, we analyzed transcripts of the 2016 U.S. presidential debates between Hillary Clinton and Donald Trump. Results: Resulting topics paralleled the most frequent policy-related Internet searches at the time. When divided by candidate, changes in emergent topics reflected individual policy stances, with nuanced differences between the two. Conclusion: Findings underscored the utility of TM to identify thematic patterns embedded in large quantities of text. TM, therefore, represents a valuable addition to the social scientist's methodological tool set."
115,"Using a probabilistic approach for exploring latent patterns in high-dimensional co-occurrence data, topic models offer researchers a flexible and open framework for soft-clustering large data sets. In recent years, there has been a growing interest among marketing scholars and practitioners to adopt topic models in various marketing application domains. However, to this date, there is no comprehensive overview of this rapidly evolving field. By analyzing a set of 61 published papers along with conceptual contributions, we systematically review this highly heterogeneous area of research. In doing so, we characterize extant contributions employing topic models in marketing along the dimensions data structures and retrieval of input data, implementation and extensions of basic topic models, and model performance evaluation. Our findings confirm that there is considerable progress done in various marketing sub-areas. However, there is still scope for promising future research, in particular with respect to integrating multiple, dynamic data sources, including time-varying covariates and the combination of exploratory topic models with powerful predictive marketing models.",2018-01-01,2-s2.0-85053608300,Journal of Business Economics,Topic modeling in marketing: recent advances and research opportunities,"Using a probabilistic approach for exploring latent patterns in high-dimensional co-occurrence data, topic models offer researchers a flexible and open framework for soft-clustering large data sets. In recent years, there has been a growing interest among marketing scholars and practitioners to adopt topic models in various marketing application domains. However, to this date, there is no comprehensive overview of this rapidly evolving field. By analyzing a set of 61 published papers along with conceptual contributions, we systematically review this highly heterogeneous area of research. In doing so, we characterize extant contributions employing topic models in marketing along the dimensions data structures and retrieval of input data, implementation and extensions of basic topic models, and model performance evaluation. Our findings confirm that there is considerable progress done in various marketing sub-areas. However, there is still scope for promising future research, in particular with respect to integrating multiple, dynamic data sources, including time-varying covariates and the combination of exploratory topic models with powerful predictive marketing models."
116,"Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.",2018-01-01,2-s2.0-85051042447,Research Synthesis Methods,Prioritising references for systematic reviews with RobotAnalyst: A user study,"Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings."
117,"Topic modeling algorithms, such as LDA, find topics, hidden structures, in document corpora in an unsupervised manner. Traditionally, applications of topic modeling over textual data use the bag-of-words model, i.e. only consider words in the documents. In our previous work we developed a framework for mining enriched topic models. We proposed a bag-of-features approach, where a document consists not only of words but also of linked named entities and their related information, such as types or categories. In this work we focused on the feature engineering and selection aspects of enriched topic modeling and evaluated the results based on two measures for assessing the understandability of estimated topics for humans: model precision and topic log odds. In our 10-model experimental setup with 7 pure resource-, 2 hybrid words/resource- and one word-based model, the traditional bag-of-words models were outperformed by 5 pure resource-based models in both measures. These results show that incorporating background knowledge into topic models makes them more understandable for humans.",2018-01-01,2-s2.0-85050623971,Lecture Notes in Business Information Processing,Human perception of enriched topic models,"Topic modeling algorithms, such as LDA, find topics, hidden structures, in document corpora in an unsupervised manner. Traditionally, applications of topic modeling over textual data use the bag-of-words model, i.e. only consider words in the documents. In our previous work we developed a framework for mining enriched topic models. We proposed a bag-of-features approach, where a document consists not only of words but also of linked named entities and their related information, such as types or categories. In this work we focused on the feature engineering and selection aspects of enriched topic modeling and evaluated the results based on two measures for assessing the understandability of estimated topics for humans: model precision and topic log odds. In our 10-model experimental setup with 7 pure resource-, 2 hybrid words/resource- and one word-based model, the traditional bag-of-words models were outperformed by 5 pure resource-based models in both measures. These results show that incorporating background knowledge into topic models makes them more understandable for humans."
118,"The dynamic nature of cities, understood as complex systems with a variety of concurring factors, poses significant challenges to urban analysis for supporting planning processes. This particularly applies to large urban events because their characteristics often contradict daily planning routines. Due to the availability of large amounts of data, social media offer the possibility for fine-scale spatial and temporal analysis in this context, especially regarding public emotions related to varied topics. Thus, this article proposes a combined approach for analyzing large sports events considering event days vs comparison days (before or after the event) and different user groups (residents vs visitors), as well as integrating sentiment analysis and topic extraction. Our results based on various analyses of tweets demonstrate that different spatial and temporal patterns can be identified, clearly distinguishing both residents and visitors, along with positive or negative sentiment. Furthermore, we could assign tweets to specific urban events or extract topics related to the transportation infrastructure. Although the results are potentially able to support urban planning processes of large events, the approach still shows some limitations including well-known biases in social media or shortcomings in identifying the user groups and in the topic modeling approach.",2018-01-01,2-s2.0-85045062619,Urban Planning,#London2012: Towards citizen-contributed urban planning through sentiment analysis of twitter data,"The dynamic nature of cities, understood as complex systems with a variety of concurring factors, poses significant challenges to urban analysis for supporting planning processes. This particularly applies to large urban events because their characteristics often contradict daily planning routines. Due to the availability of large amounts of data, social media offer the possibility for fine-scale spatial and temporal analysis in this context, especially regarding public emotions related to varied topics. Thus, this article proposes a combined approach for analyzing large sports events considering event days vs comparison days (before or after the event) and different user groups (residents vs visitors), as well as integrating sentiment analysis and topic extraction. Our results based on various analyses of tweets demonstrate that different spatial and temporal patterns can be identified, clearly distinguishing both residents and visitors, along with positive or negative sentiment. Furthermore, we could assign tweets to specific urban events or extract topics related to the transportation infrastructure. Although the results are potentially able to support urban planning processes of large events, the approach still shows some limitations including well-known biases in social media or shortcomings in identifying the user groups and in the topic modeling approach."
119,"‘Post-truth politics’, particularly as manifested in ‘fake news’ spread by countermedia, is claimed to be endemic to contemporary populism. I argue that the relationship between knowledge and populism needs a more nuanced analysis. Many have noted that populism valorises ‘common sense’ over expertise. But another populist strategy is counterknowledge, proposing politically charged alternative knowledge authorities in the stead of established ones. I analyse countermedia in Finland, where they have played a part in the rise of right-wing populism, using a combination of computational and interpretive methods. In my data, right-wing populists advocate counterknowledge; they profess belief in truth achievable by inquiry, not by mainstream experts but alternative ones. This is a different knowledge orientation from the valorisation of ‘common sense’, and there is reason to believe it is somewhat specific to contemporary right-wing anti-immigration populism. Populism’s epistemologies are multifaceted but often absolutist, as is populism’s relationship to power and democracy.",2018-01-01,2-s2.0-85052055989,European Journal of Cultural and Political Sociology,Populist knowledge: ‘Post-truth’ repertoires of contesting epistemic authorities,"‘Post-truth politics’, particularly as manifested in ‘fake news’ spread by countermedia, is claimed to be endemic to contemporary populism. I argue that the relationship between knowledge and populism needs a more nuanced analysis. Many have noted that populism valorises ‘common sense’ over expertise. But another populist strategy is counterknowledge, proposing politically charged alternative knowledge authorities in the stead of established ones. I analyse countermedia in Finland, where they have played a part in the rise of right-wing populism, using a combination of computational and interpretive methods. In my data, right-wing populists advocate counterknowledge; they profess belief in truth achievable by inquiry, not by mainstream experts but alternative ones. This is a different knowledge orientation from the valorisation of ‘common sense’, and there is reason to believe it is somewhat specific to contemporary right-wing anti-immigration populism. Populism’s epistemologies are multifaceted but often absolutist, as is populism’s relationship to power and democracy."
120,"Political campaigns mostly run parallel to each other during an election cycle, but intersect when the main candidates face off for televised debates. They offer supporters of these candidates a chance to engage with each other while being exposed to views and opinions different from their own. This study uses a combination of social network analysis and machine learning to examine how the three US presidential debates of 2016 were live tweeted (N = ∼300,000). We find that despite cross-cutting exposure across the ideological divide, people remain highly partisan in terms of who they engage with on Twitter. The issue agendas of Twitter posts during the US presidential debates is set well in advance of the debates themselves; it is highly negative and focused on personality traits of the opposition candidate rather than policy matters. We also detect a shift in the nature of online opinion leadership, with grassroots activists and internet personalities sharing the space with traditional elites such as political leaders and journalists. This shift coincides with the broader anti-establishment turn in the US political climate, as reflected in the early success of Bernie Sanders and the eventual victory of a political outsider like Donald Trump over the seasoned Hillary Clinton.",2018-01-01,2-s2.0-85052091633,Information Communication and Society,Live tweeting live debates: How Twitter reflects and refracts the US political climate in a campaign season,"Political campaigns mostly run parallel to each other during an election cycle, but intersect when the main candidates face off for televised debates. They offer supporters of these candidates a chance to engage with each other while being exposed to views and opinions different from their own. This study uses a combination of social network analysis and machine learning to examine how the three US presidential debates of 2016 were live tweeted (N = ∼300,000). We find that despite cross-cutting exposure across the ideological divide, people remain highly partisan in terms of who they engage with on Twitter. The issue agendas of Twitter posts during the US presidential debates is set well in advance of the debates themselves; it is highly negative and focused on personality traits of the opposition candidate rather than policy matters. We also detect a shift in the nature of online opinion leadership, with grassroots activists and internet personalities sharing the space with traditional elites such as political leaders and journalists. This shift coincides with the broader anti-establishment turn in the US political climate, as reflected in the early success of Bernie Sanders and the eventual victory of a political outsider like Donald Trump over the seasoned Hillary Clinton."
121,"National governments take advantage of collective intelligence when conducting foresight processes. They grasp emerging issues through expert reviews as well as public opinions. It raises national agendas and affects policy-making process. Therefore, by examining policy papers which contain societal issues, we can perceive past, current, and future environments. In this study, we exploit policy research database of Republic of Korea, which is a unique source that automatically collects all policy papers written by national research institutes, to extract latent topics and their trends over 10 years through a probabilistic topic model. Detected topics fairly correspond to expert-selected future drivers in national foresight report, implying that public discourse and policy agenda are coupled. We suggest to utilize open government data and text mining methods for building open foresight framework that various actors exchange their opinions on societal issues.",2018-01-01,2-s2.0-85042008530,Technological Forecasting and Social Change,Horizon scanning in policy research database with a probabilistic topic model,"National governments take advantage of collective intelligence when conducting foresight processes. They grasp emerging issues through expert reviews as well as public opinions. It raises national agendas and affects policy-making process. Therefore, by examining policy papers which contain societal issues, we can perceive past, current, and future environments. In this study, we exploit policy research database of Republic of Korea, which is a unique source that automatically collects all policy papers written by national research institutes, to extract latent topics and their trends over 10 years through a probabilistic topic model. Detected topics fairly correspond to expert-selected future drivers in national foresight report, implying that public discourse and policy agenda are coupled. We suggest to utilize open government data and text mining methods for building open foresight framework that various actors exchange their opinions on societal issues."
122,"Given the research interest on Big Data in Marketing, we present a research literature analysis based on a text mining semi-automated approach with the goal of identifying the main trends in this domain. In particular, the analysis focuses on relevant terms and topics related with five dimensions: Big Data, Marketing, Geographic location of authors’ affiliation (countries and continents), Products, and Sectors. A total of 1560 articles published from 2010 to 2015 were scrutinized. The findings revealed that research is bipartite between technological and research domains, with Big Data publications not clearly aligning cutting edge techniques toward Marketing benefits. Also, few inter-continental co-authored publications were found. Moreover, findings show that research in Big Data applications to Marketing is still in an embryonic stage, thus making it essential to develop more direct efforts toward business for Big Data to thrive in the Marketing arena.",2018-01-01,2-s2.0-85021832314,European Research on Management and Business Economics,Research trends on Big Data in Marketing: A text mining and topic modeling based literature analysis,"Given the research interest on Big Data in Marketing, we present a research literature analysis based on a text mining semi-automated approach with the goal of identifying the main trends in this domain. In particular, the analysis focuses on relevant terms and topics related with five dimensions: Big Data, Marketing, Geographic location of authors’ affiliation (countries and continents), Products, and Sectors. A total of 1560 articles published from 2010 to 2015 were scrutinized. The findings revealed that research is bipartite between technological and research domains, with Big Data publications not clearly aligning cutting edge techniques toward Marketing benefits. Also, few inter-continental co-authored publications were found. Moreover, findings show that research in Big Data applications to Marketing is still in an embryonic stage, thus making it essential to develop more direct efforts toward business for Big Data to thrive in the Marketing arena."
123,"Emotional conversation generation has elicited a wide interest in both academia and industry. However, existing emotional neural conversation systems tend to ignore the necessity to combine topic and emotion in generating responses, possibly leading to a decline in the quality of responses. This paper proposes a topic-enhanced emotional conversation generation model that incorporates emotional factors and topic information into the conversation system, by using two mechanisms. First, we use a Twitter latent Dirichlet allocation (LDA) model to obtain topic words of the input sequences as extra prior information, ensuring the consistency of content between posts and responses for emotional conversation generation. Second, the system uses a dynamic emotional attention mechanism to adaptively acquire content-related and affective information of the input texts and extra topics. The advantage of this study lies in the fact that the presented model can generate abundant emotional responses, with the contents being related and diverse. To demonstrate the effectiveness of our method, we conduct extensive experiments on large-scale Weibo post–response pairs. Experimental results show that our method achieves good performance, even outperforming some existing models.",2018-01-01,2-s2.0-85053828339,Knowledge-Based Systems,Topic-enhanced emotional conversation generation with attention mechanism,"Emotional conversation generation has elicited a wide interest in both academia and industry. However, existing emotional neural conversation systems tend to ignore the necessity to combine topic and emotion in generating responses, possibly leading to a decline in the quality of responses. This paper proposes a topic-enhanced emotional conversation generation model that incorporates emotional factors and topic information into the conversation system, by using two mechanisms. First, we use a Twitter latent Dirichlet allocation (LDA) model to obtain topic words of the input sequences as extra prior information, ensuring the consistency of content between posts and responses for emotional conversation generation. Second, the system uses a dynamic emotional attention mechanism to adaptively acquire content-related and affective information of the input texts and extra topics. The advantage of this study lies in the fact that the presented model can generate abundant emotional responses, with the contents being related and diverse. To demonstrate the effectiveness of our method, we conduct extensive experiments on large-scale Weibo post–response pairs. Experimental results show that our method achieves good performance, even outperforming some existing models."
124,"Generic topics of large-scale document collections can often be divided into more specific subtopics. Topic hierarchies provide a model for such topic relation structure. These models can be especially useful for exploratory search systems. Various approaches to building hierarchical topic models have been proposed so far. However, there is no agreement on a standard approach, largely due to the lack of quality metrics to compare existing models. To bridge this gap we propose automated evaluation metrics which measure the quality of topic-subtopic relations (edges) of a topic hierarchy. We compare automated evaluations with human assessment to validate the proposed metrics. Finally, we show how the proposed metrics can be used to control and to improve the quality of existing hierarchical models.",2018-01-01,2-s2.0-85051263834,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,Quality evaluation and improvement for hierarchical topic modeling,"Generic topics of large-scale document collections can often be divided into more specific subtopics. Topic hierarchies provide a model for such topic relation structure. These models can be especially useful for exploratory search systems. Various approaches to building hierarchical topic models have been proposed so far. However, there is no agreement on a standard approach, largely due to the lack of quality metrics to compare existing models. To bridge this gap we propose automated evaluation metrics which measure the quality of topic-subtopic relations (edges) of a topic hierarchy. We compare automated evaluations with human assessment to validate the proposed metrics. Finally, we show how the proposed metrics can be used to control and to improve the quality of existing hierarchical models."
125,"Debates on climate change, energy and food security concerns have underscored the need for new methods and tools to explore and understand the complexity of these relevant issues. In this study, we used unsupervised probabilistic modeling—latent Dirichlet allocation (LDA)—to examine the changes in social policy debates related to ethanol production in Brazil and its relationship with climate change and food security. We analyze a large amount of data obtained from Brazilian newspapers, government and business documents, and the bulletins of nongovernmental organizations and social movements from 2007 through 2017. The results from the LDA application allowed us to identify key topics, detect novel trends, and follow them through time, in addition to exhibiting the limitations encountered in identifying social actor discourses. To overcome these limitations, we combine LDA and discourse analysis to enable the construction of topics, concepts, and discourses of the actors surrounding the issue of this study. The findings verify the utility of these techniques by emphasizing the themes and discourses of the actors involved and by identifying the determinant positions of the Brazilian actors in the discussions on ethanol production and its competition with food security and the contributions of ethanol as a renewable energy source to mitigate climate change. This finding provides insights for water–energy–food nexus research.",2018-01-01,2-s2.0-85050910893,Energy Research and Social Science,Topic modeling method for analyzing social actor discourses on climate change- energy and food security,"Debates on climate change, energy and food security concerns have underscored the need for new methods and tools to explore and understand the complexity of these relevant issues. In this study, we used unsupervised probabilistic modeling—latent Dirichlet allocation (LDA)—to examine the changes in social policy debates related to ethanol production in Brazil and its relationship with climate change and food security. We analyze a large amount of data obtained from Brazilian newspapers, government and business documents, and the bulletins of nongovernmental organizations and social movements from 2007 through 2017. The results from the LDA application allowed us to identify key topics, detect novel trends, and follow them through time, in addition to exhibiting the limitations encountered in identifying social actor discourses. To overcome these limitations, we combine LDA and discourse analysis to enable the construction of topics, concepts, and discourses of the actors surrounding the issue of this study. The findings verify the utility of these techniques by emphasizing the themes and discourses of the actors involved and by identifying the determinant positions of the Brazilian actors in the discussions on ethanol production and its competition with food security and the contributions of ethanol as a renewable energy source to mitigate climate change. This finding provides insights for water–energy–food nexus research."
126,"Learning topics from short texts has become a critical and fundamental task for understanding the widely-spread streaming social messages, e.g., tweets, snippets and questions/answers. Up to date, there are two distinctive topic learning schemes: generative probabilistic graphical models and geometrically linear algebra approaches, with LDA and NMF being the representative works, respectively. Since these two methods both could uncover the latent topics hidden in the unstructured short texts, some interesting doubts are coming to our minds that which one is better and why? Are there any other more effective extensions? In order to explore valuable insights between LDA and NMF based learning schemes, we comprehensively conduct a series of experiments into two parts. Specifically, the basic LDA and NMF are compared with different experimental settings on several public short text datasets in the first part which would exhibit that NMF tends to perform better than LDA; in the second part, we propose a novel model called “Knowledge-guided Non-negative Matrix Factorization for Better Short Text Topic Mining” (abbreviated as KGNMF), which leverages external knowledge as a semantic regulator with low-rank formalizations, yielding up a time-efficient algorithm. Extensive experiments are conducted on three representative corpora with currently typical short text topic models to demonstrate the effectiveness of our proposed KGNMF. Overall, learning with NMF-based schemes is another effective manner in short text topic mining in addition to the popular LDA-based paradigms.",2018-01-01,2-s2.0-85052838005,Knowledge-Based Systems,Experimental explorations on short text topic mining between LDA and NMF based Schemes,"Learning topics from short texts has become a critical and fundamental task for understanding the widely-spread streaming social messages, e.g., tweets, snippets and questions/answers. Up to date, there are two distinctive topic learning schemes: generative probabilistic graphical models and geometrically linear algebra approaches, with LDA and NMF being the representative works, respectively. Since these two methods both could uncover the latent topics hidden in the unstructured short texts, some interesting doubts are coming to our minds that which one is better and why? Are there any other more effective extensions? In order to explore valuable insights between LDA and NMF based learning schemes, we comprehensively conduct a series of experiments into two parts. Specifically, the basic LDA and NMF are compared with different experimental settings on several public short text datasets in the first part which would exhibit that NMF tends to perform better than LDA; in the second part, we propose a novel model called “Knowledge-guided Non-negative Matrix Factorization for Better Short Text Topic Mining” (abbreviated as KGNMF), which leverages external knowledge as a semantic regulator with low-rank formalizations, yielding up a time-efficient algorithm. Extensive experiments are conducted on three representative corpora with currently typical short text topic models to demonstrate the effectiveness of our proposed KGNMF. Overall, learning with NMF-based schemes is another effective manner in short text topic mining in addition to the popular LDA-based paradigms."
127,"Purpose: Delivering messages and information to potentially interested users is one of the distinguishing applications of online enterprise social network (ESN). The purpose of this paper is to provide insights to better understand the repost preferences of users and provide personalized information service in enterprise social media marketing. Design/methodology/approach: It is accomplished by constructing a target audience identification framework. Repost preference latent Dirichlet allocation (RPLDA) topic model topic model is proposed to understand the mass user online repost preferences toward different contents. A topic-oriented preference metric is proposed to measure the preference degree of individual users. And the function of reposting forecasting is formulated to identify target audience. Findings: The empirical research shows the following: a total of 20 percent of the repost users in ESN represent the key active users who are particularly interested in the latent topic of messages in ESN and fits Pareto distribution; and the target audience identification framework can successfully identify different target key users for messages with different latent topics. Practical implications: The findings should motivate marketing managers to improve enterprise brand by identifying key target audience in ESN and marketing in a way that truthfully reflects personalized preferences. Originality/value: This study runs counter to most current business practices, which tend to use simple popularity to seek important users. Adaptively and dynamically identifying target audience appears to have considerable potential, especially in the rapidly growing area of enterprise social media information service.",2018-01-01,2-s2.0-85053036198,Industrial Management and Data Systems,Identifying target audience on enterprise social network,"Purpose: Delivering messages and information to potentially interested users is one of the distinguishing applications of online enterprise social network (ESN). The purpose of this paper is to provide insights to better understand the repost preferences of users and provide personalized information service in enterprise social media marketing. Design/methodology/approach: It is accomplished by constructing a target audience identification framework. Repost preference latent Dirichlet allocation (RPLDA) topic model topic model is proposed to understand the mass user online repost preferences toward different contents. A topic-oriented preference metric is proposed to measure the preference degree of individual users. And the function of reposting forecasting is formulated to identify target audience. Findings: The empirical research shows the following: a total of 20 percent of the repost users in ESN represent the key active users who are particularly interested in the latent topic of messages in ESN and fits Pareto distribution; and the target audience identification framework can successfully identify different target key users for messages with different latent topics. Practical implications: The findings should motivate marketing managers to improve enterprise brand by identifying key target audience in ESN and marketing in a way that truthfully reflects personalized preferences. Originality/value: This study runs counter to most current business practices, which tend to use simple popularity to seek important users. Adaptively and dynamically identifying target audience appears to have considerable potential, especially in the rapidly growing area of enterprise social media information service."
128,"This explorative article contributes to research on models of “good parenting” by analyzing whether current demands of politics, science and advice literature are discussed in a large German parenting online forum. The data consists of 58,240 user submissions to a digital discussion board. Using this digital data, it can not only be analyzed which topics users discuss but also how they negotiate them. With “topic modeling”, an innovative approach from the computational social sciences (CSS), is combined with qualitative content analysis. The article shows that expert knowledge is picked up by users to justify and reflect on their childrearing practices. By and large, parents refer to expert knowledge in a positive manner, however, some authors are viewed rather critically.",2018-01-01,2-s2.0-85053681476,Zeitschrift fur Familienforschung,Playground chatter on the internet? Models of “good parenting” in a parent online forum Sandkastengespräche im Netz? Leitbilder „guter Erziehung“in einem digitalen Elternforum,"This explorative article contributes to research on models of “good parenting” by analyzing whether current demands of politics, science and advice literature are discussed in a large German parenting online forum. The data consists of 58,240 user submissions to a digital discussion board. Using this digital data, it can not only be analyzed which topics users discuss but also how they negotiate them. With “topic modeling”, an innovative approach from the computational social sciences (CSS), is combined with qualitative content analysis. The article shows that expert knowledge is picked up by users to justify and reflect on their childrearing practices. By and large, parents refer to expert knowledge in a positive manner, however, some authors are viewed rather critically."
129,"The article is devoted to the problem of how to automatically measure the interpretability of topic models. Some new, intra-text, approaches to estimate the interpretability of the topics are proposed. Computational experiments are conducted with the use of text files from “PostNauka”, which is a collection of popular science content.",2018-01-01,2-s2.0-85051254171,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,Intra-text coherence as a measure of topic models' interpretability,"The article is devoted to the problem of how to automatically measure the interpretability of topic models. Some new, intra-text, approaches to estimate the interpretability of the topics are proposed. Computational experiments are conducted with the use of text files from “PostNauka”, which is a collection of popular science content."
130,"Probabilistic topic modeling is a powerful tool of text analysis, that reveals topics as distributions over words and then softly assigns documents to the topics. Even though the aggregated distributions can be good with basic models, a sequential topic representation of each document is often unsatisfactory. This work introduces a method that allows to increase the quality of topical representation of each single text using its segmental structure. Our approach is based on Additive Regularization of Topic Models (ARTM), which is a technique for imposing additional criteria into the model. The proposed method efficiently avoids a bag-of-words assumption by considering the topical connections of words that co-occur in a local segment. We assume, that sequential sentences are topically and semantically coherent, while the number of topics in each particular text fragment is low. We apply our model to topic segmentation task and achieve a better quality than the current state-of-the-art TopicTiling algorithm. In further experiments we demonstrate that the proposed technique reveals an interpretable sequential structure of documents, while keeping a number of topics low, i.e. the sparsity of the model increases. Apart from topic segmentation, the constructed topical text embeddings can be used in any other applications, where the analysis of the document structure is desirable.",2018-01-01,2-s2.0-85051246679,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,Improving topic models with segmental structure of texts,"Probabilistic topic modeling is a powerful tool of text analysis, that reveals topics as distributions over words and then softly assigns documents to the topics. Even though the aggregated distributions can be good with basic models, a sequential topic representation of each document is often unsatisfactory. This work introduces a method that allows to increase the quality of topical representation of each single text using its segmental structure. Our approach is based on Additive Regularization of Topic Models (ARTM), which is a technique for imposing additional criteria into the model. The proposed method efficiently avoids a bag-of-words assumption by considering the topical connections of words that co-occur in a local segment. We assume, that sequential sentences are topically and semantically coherent, while the number of topics in each particular text fragment is low. We apply our model to topic segmentation task and achieve a better quality than the current state-of-the-art TopicTiling algorithm. In further experiments we demonstrate that the proposed technique reveals an interpretable sequential structure of documents, while keeping a number of topics low, i.e. the sparsity of the model increases. Apart from topic segmentation, the constructed topical text embeddings can be used in any other applications, where the analysis of the document structure is desirable."
131,"Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection.",2018-01-01,2-s2.0-85034428503,Decision Support Systems,Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud,"Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection."
132,"This scientometric analysis of the area of ‘smart city(ies)’ research covers 1990–2016, divided into three nine year periods: 1990–1998; 1999–2007; and 2008–2016. The methodology is partly based on the ‘issue management’ approach by Lancaster and Lee (J Assoc Inf Sci Technol 36(6):389–397, 1985) partly on common publication and citation analysis of the set of source documents (n = 4725), the set of their references (n = 27,099) and the set of publications (n = 7863) citing the source documents. Median age analyses are included for the sets of references and citations to the source documents. DIVA-like diagrams (Database Information Visualization and Analysis system) are used to demonstrate the distribution of source documents over document types, time and volume of citations obtained. Social Network Analysis (SNA) is applied to topic modeling of the top-100 central WoS Categories of ‘smart city(ies)’ research and to the set of references. Findings show that the first mention of the concept ‘smart city(ies)’ in publication titles takes place in 1999. The research area demonstrates a strong multidisciplinary nature and an exponential growth of research publications (in WoS) 2008–2016 dominated by China, Italy, USA, Spain and England. The same five countries are also among the most citing and cited countries. Aside from a constantly strong ICT (Information and Communication Technology) and Electrical/Electronic Engineering presence ‘sustainability’ elements (Energy, Transport, Environment) are also vital, in particular during the first and third analysis period. The references from the source documents have more distinct topical clusters than the source documents. Artificial Intelligence (AI) appears as a novel field among the source documents 2008–2016, but disappears from the top-25 list in the citing documents. Instead Economics, Water Resources and Meteorology and Atmospheric Sciences move into the list. Proceedings papers, as in many other engineering and technology based research fields, are the dominant document type (70%) but have small citation impact (0.6 c/p), thus decreasing the overall impact of the area to 3.6 c/p. Journal articles are the most cited type with 76% of all citations received (impact 2008–2016: 7.5 c/p). Most citations to journal articles derive from journal articles themselves (76%).",2018-01-01,2-s2.0-85053430176,Scientometrics,Smart city research 1990–2016,"This scientometric analysis of the area of ‘smart city(ies)’ research covers 1990–2016, divided into three nine year periods: 1990–1998; 1999–2007; and 2008–2016. The methodology is partly based on the ‘issue management’ approach by Lancaster and Lee (J Assoc Inf Sci Technol 36(6):389–397, 1985) partly on common publication and citation analysis of the set of source documents (n = 4725), the set of their references (n = 27,099) and the set of publications (n = 7863) citing the source documents. Median age analyses are included for the sets of references and citations to the source documents. DIVA-like diagrams (Database Information Visualization and Analysis system) are used to demonstrate the distribution of source documents over document types, time and volume of citations obtained. Social Network Analysis (SNA) is applied to topic modeling of the top-100 central WoS Categories of ‘smart city(ies)’ research and to the set of references. Findings show that the first mention of the concept ‘smart city(ies)’ in publication titles takes place in 1999. The research area demonstrates a strong multidisciplinary nature and an exponential growth of research publications (in WoS) 2008–2016 dominated by China, Italy, USA, Spain and England. The same five countries are also among the most citing and cited countries. Aside from a constantly strong ICT (Information and Communication Technology) and Electrical/Electronic Engineering presence ‘sustainability’ elements (Energy, Transport, Environment) are also vital, in particular during the first and third analysis period. The references from the source documents have more distinct topical clusters than the source documents. Artificial Intelligence (AI) appears as a novel field among the source documents 2008–2016, but disappears from the top-25 list in the citing documents. Instead Economics, Water Resources and Meteorology and Atmospheric Sciences move into the list. Proceedings papers, as in many other engineering and technology based research fields, are the dominant document type (70%) but have small citation impact (0.6 c/p), thus decreasing the overall impact of the area to 3.6 c/p. Journal articles are the most cited type with 76% of all citations received (impact 2008–2016: 7.5 c/p). Most citations to journal articles derive from journal articles themselves (76%)."
133,"Course reviews, which is designed as an interactive feedback channel in Massive Open Online Courses, has promoted the generation of large-scale text comments. These data, which contain not only learners' concerns, opinions and feelings toward courses, instructors, and platforms but also learners' interactions (e.g., post, reply), are generally subjective and extremely valuable for online instruction. The purpose of this study is to automatically reveal these potential information from 50 online courses by an improved unified topic model Behavior-Sentiment Topic Mixture, which is validated and effective for detecting frequent topics learners discuss most, topics-oriented sentimental tendency as well as how learners interact with these topics. The results show that learners focus more on the topics about course-related content with positive sentiment, as well as the topics about course logistics and video production with negative sentiment. Moreover, the distributions of behaviors associated with these topics have some differences.",2018-01-01,2-s2.0-85044925781,Journal of Educational Computing Research,Unfolding Sentimental and Behavioral Tendencies of Learners' Concerned Topics From Course Reviews in a MOOC,"Course reviews, which is designed as an interactive feedback channel in Massive Open Online Courses, has promoted the generation of large-scale text comments. These data, which contain not only learners' concerns, opinions and feelings toward courses, instructors, and platforms but also learners' interactions (e.g., post, reply), are generally subjective and extremely valuable for online instruction. The purpose of this study is to automatically reveal these potential information from 50 online courses by an improved unified topic model Behavior-Sentiment Topic Mixture, which is validated and effective for detecting frequent topics learners discuss most, topics-oriented sentimental tendency as well as how learners interact with these topics. The results show that learners focus more on the topics about course-related content with positive sentiment, as well as the topics about course logistics and video production with negative sentiment. Moreover, the distributions of behaviors associated with these topics have some differences."
134,"India and China have launched enormous projects aimed at collecting vital personal information regarding their billion-plus populations and building the world’s biggest data sets in the process. However, both Aadhaar in India and the Social Credit System in China are controversial and raise a plethora of political and ethical concerns. The governments claim that participation in these projects is voluntary, even as they link vital services to citizens registering with these projects. In this study, we analyze how the news media in India and China—crucial data intermediaries that shape public perceptions on data and technological practices—framed these projects since their inception. Topic modeling suggests news coverage in both nations disregards the public interest and focuses largely on how businesses can benefit from them. The media, institutionally and ideologically linked with governments and corporations, show little concern with violations of privacy and mass surveillance that these projects could lead to. We argue that this renders citizens structurally incapable of making a meaningful “choice” about whether or not to participate in such projects. Implications for various stakeholders are discussed.",2018-01-01,2-s2.0-85052117003,Social Science Computer Review,Big Data and the Illusion of Choice: Comparing the Evolution of India’s Aadhaar and China’s Social Credit System as Technosocial Discourses,"India and China have launched enormous projects aimed at collecting vital personal information regarding their billion-plus populations and building the world’s biggest data sets in the process. However, both Aadhaar in India and the Social Credit System in China are controversial and raise a plethora of political and ethical concerns. The governments claim that participation in these projects is voluntary, even as they link vital services to citizens registering with these projects. In this study, we analyze how the news media in India and China—crucial data intermediaries that shape public perceptions on data and technological practices—framed these projects since their inception. Topic modeling suggests news coverage in both nations disregards the public interest and focuses largely on how businesses can benefit from them. The media, institutionally and ideologically linked with governments and corporations, show little concern with violations of privacy and mass surveillance that these projects could lead to. We argue that this renders citizens structurally incapable of making a meaningful “choice” about whether or not to participate in such projects. Implications for various stakeholders are discussed."
135,"Modelling is a good tool for understanding the complex interactions of the agricultural systems; however, few studies have used a combined method of text mining and system dynamics to analyse and simulate the agricultural system. We propose the topic modelling–system dynamics modelling (TM-SDM) analytical framework to extract, synthesise, and dynamically simulate the factors of land management, disaster, pollution, and poverty in the system. Specifically, 1,598 English language publications were collected to analyse the main concerns and conceptual structure of the sustainable development in the case study area based on topic modelling. Historical data from 2000 to 2014 are used for simulation and prediction based on system dynamics modelling, exploring and comparing different policy scenarios. The results show that policies about education and training, sewage treatment, infrastructure construction, and soil erosion control would help poverty reduction significantly. Sewage treatment policy reduces the incidence of human disease and sewage discharge significantly. Education and training policy has the greatest impact on the improvement of poverty reduction and yield of crops, and the environmental protection scenario has the best performance on maintaining the cultivated land. The balanced development scenario benefits both the poverty reduction and environmental protection concurrently. Focussing on a systems approach to improve the sustainability of agriculture, this paper could have a positive impact on the development and implementation of sustainable agricultural systems. The TM-SDM analysis and modelling framework that we proposed has a generic nature, which could be applied to the further research and decision-making on sustainable agriculture and other fields involving complex systems.",2018-01-01,2-s2.0-85053759546,Land Degradation and Development,Modelling environment and poverty factors for sustainable agriculture in the Three Gorges Reservoir Regions of China,"Modelling is a good tool for understanding the complex interactions of the agricultural systems; however, few studies have used a combined method of text mining and system dynamics to analyse and simulate the agricultural system. We propose the topic modelling–system dynamics modelling (TM-SDM) analytical framework to extract, synthesise, and dynamically simulate the factors of land management, disaster, pollution, and poverty in the system. Specifically, 1,598 English language publications were collected to analyse the main concerns and conceptual structure of the sustainable development in the case study area based on topic modelling. Historical data from 2000 to 2014 are used for simulation and prediction based on system dynamics modelling, exploring and comparing different policy scenarios. The results show that policies about education and training, sewage treatment, infrastructure construction, and soil erosion control would help poverty reduction significantly. Sewage treatment policy reduces the incidence of human disease and sewage discharge significantly. Education and training policy has the greatest impact on the improvement of poverty reduction and yield of crops, and the environmental protection scenario has the best performance on maintaining the cultivated land. The balanced development scenario benefits both the poverty reduction and environmental protection concurrently. Focussing on a systems approach to improve the sustainability of agriculture, this paper could have a positive impact on the development and implementation of sustainable agricultural systems. The TM-SDM analysis and modelling framework that we proposed has a generic nature, which could be applied to the further research and decision-making on sustainable agriculture and other fields involving complex systems."
136,"With the explosion of information in our current era, senders of information increasingly need to target their messages to recipients. However, messages within transportation systems, including traffic information and commercial advertisements, tend to be transmitted to all drivers indiscriminately. This is because the information providers (such as other vehicles, roads, facilities, buildings etc.), can hardly recognize the variations within drivers, who should be treated differently as information recipients. As a result of the rapid development of data collection technologies and machine learning techniques in recent years, extraction and recognition of drivers’ unique driving style from actual driving behaviour data become possible. In this paper, two kinds of topic models are investigated: mLDA and mHLDA, to discover distinguishable driving style information with hidden structure from the real-world driving behaviour data. The results show that the proposed models can successfully recognize the differences between driving styles. The study is of great value for providing deep insight into the underlying structure of driving styles and can effectively support the recognition of drivers with different driving styles.",2018-01-01,2-s2.0-85049906355,Transportation Research Part D: Transport and Environment,Recognizing driving styles based on topic models,"With the explosion of information in our current era, senders of information increasingly need to target their messages to recipients. However, messages within transportation systems, including traffic information and commercial advertisements, tend to be transmitted to all drivers indiscriminately. This is because the information providers (such as other vehicles, roads, facilities, buildings etc.), can hardly recognize the variations within drivers, who should be treated differently as information recipients. As a result of the rapid development of data collection technologies and machine learning techniques in recent years, extraction and recognition of drivers’ unique driving style from actual driving behaviour data become possible. In this paper, two kinds of topic models are investigated: mLDA and mHLDA, to discover distinguishable driving style information with hidden structure from the real-world driving behaviour data. The results show that the proposed models can successfully recognize the differences between driving styles. The study is of great value for providing deep insight into the underlying structure of driving styles and can effectively support the recognition of drivers with different driving styles."
137,"Scientific collaboration is vital to many fields, and it is common to see scholars seek out experienced researchers or experts in a domain with whom they can share knowledge, experience, and resources. To explore the diversity of research collaborations, this article performs a temporal analysis on the scientific careers of researchers in the field of computer science. Specifically, we analyze collaborators using 2 indicators: the research topic diversity, measured by the Author-Conference-Topic model and cosine, and the impact diversity, measured by the normalized standard deviation of h-indices. We find that the collaborators of high-impact researchers tend to study diverse research topics and have diverse h-indices. Moreover, by setting PhD graduation as an important milestone in researchers' careers, we examine several indicators related to scientific collaboration and their effects on a career. The results show that collaborating with authoritative authors plays an important role prior to a researcher's PhD graduation, but working with non-authoritative authors carries more weight after PhD graduation.",2018-01-01,2-s2.0-85030177971,Journal of the Association for Information Science and Technology,Understanding success through the diversity of collaborators and the milestone of career,"Scientific collaboration is vital to many fields, and it is common to see scholars seek out experienced researchers or experts in a domain with whom they can share knowledge, experience, and resources. To explore the diversity of research collaborations, this article performs a temporal analysis on the scientific careers of researchers in the field of computer science. Specifically, we analyze collaborators using 2 indicators: the research topic diversity, measured by the Author-Conference-Topic model and cosine, and the impact diversity, measured by the normalized standard deviation of h-indices. We find that the collaborators of high-impact researchers tend to study diverse research topics and have diverse h-indices. Moreover, by setting PhD graduation as an important milestone in researchers' careers, we examine several indicators related to scientific collaboration and their effects on a career. The results show that collaborating with authoritative authors plays an important role prior to a researcher's PhD graduation, but working with non-authoritative authors carries more weight after PhD graduation."
138,"The proceedings contain 83 papers. The topics discussed include: intra-text coherence as a measure of topic models interpretability; improving part-of-speech tagging via multi-task learning and character-level word representations; disambiguation of scope in written English texts; how much does a word weigh? weighting word embeddings for word sense induction; morphological segmentation with sequence to sequence neural network; framework for Russian plagiarism detection using sentence embedding similarity and negative sampling; quality evaluation and improvement for hierarchical topic modeling; semantic analysis with inference: high spots of the football match; using machine translation for automatic genre classification in Arabic; German constructions with modal verbs and their Russian correlates: a supracorpora database project; discourse marker tipa according to the data of Russian national corpus: its origin, semantics and pragmatics; the influence of syntax on prosody: the experimental data from a study of one Russian text; supracorpora database as an instrument of the study of the formal variability of connectives; corpus-based investigation of quotation in Russian sign language; language production and comprehension in face-to-face multichannel communication; learning word embeddings for low resource languages: the case of Buryat; and a database of wordbreaks discursive features in Russian oral speech: the structure, composition and application.",2018-01-01,2-s2.0-85051259893,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,"The proceedings contain 83 papers. The topics discussed include: intra-text coherence as a measure of topic models interpretability; improving part-of-speech tagging via multi-task learning and character-level word representations; disambiguation of scope in written English texts; how much does a word weigh? weighting word embeddings for word sense induction; morphological segmentation with sequence to sequence neural network; framework for Russian plagiarism detection using sentence embedding similarity and negative sampling; quality evaluation and improvement for hierarchical topic modeling; semantic analysis with inference: high spots of the football match; using machine translation for automatic genre classification in Arabic; German constructions with modal verbs and their Russian correlates: a supracorpora database project; discourse marker tipa according to the data of Russian national corpus: its origin, semantics and pragmatics; the influence of syntax on prosody: the experimental data from a study of one Russian text; supracorpora database as an instrument of the study of the formal variability of connectives; corpus-based investigation of quotation in Russian sign language; language production and comprehension in face-to-face multichannel communication; learning word embeddings for low resource languages: the case of Buryat; and a database of wordbreaks discursive features in Russian oral speech: the structure, composition and application."
139,"Over the past decade, anti-vaccination rhetoric has become part of the mainstream discourse regarding the public health practice of childhood vaccination. These utilise social media to foster online spaces that strengthen and popularise anti-vaccination discourses. In this paper, we examine the characteristics of and the discourses present within six popular anti-vaccination Facebook pages. We examine these large-scale datasets using a range of methods, including social network analysis, gender prediction using historical census data, and generative statistical models for topic analysis (Latent Dirichlet allocation). We find that present-day discourses centre around moral outrage and structural oppression by institutional government and the media, suggesting a strong logic of ‘conspiracy-style’ beliefs and thinking. Furthermore, anti-vaccination pages on Facebook reflect a highly ‘feminised’ movement ‒ the vast majority of participants are women. Although anti-vaccination networks on Facebook are large and global in scope, the comment activity sub-networks appear to be ‘small world’. This suggests that social media may have a role in spreading anti-vaccination ideas and making the movement durable on a global scale.",2017-12-23,2-s2.0-85039044264,Information Communication and Society,Mapping the anti-vaccination movement on Facebook,"Over the past decade, anti-vaccination rhetoric has become part of the mainstream discourse regarding the public health practice of childhood vaccination. These utilise social media to foster online spaces that strengthen and popularise anti-vaccination discourses. In this paper, we examine the characteristics of and the discourses present within six popular anti-vaccination Facebook pages. We examine these large-scale datasets using a range of methods, including social network analysis, gender prediction using historical census data, and generative statistical models for topic analysis (Latent Dirichlet allocation). We find that present-day discourses centre around moral outrage and structural oppression by institutional government and the media, suggesting a strong logic of ‘conspiracy-style’ beliefs and thinking. Furthermore, anti-vaccination pages on Facebook reflect a highly ‘feminised’ movement ‒ the vast majority of participants are women. Although anti-vaccination networks on Facebook are large and global in scope, the comment activity sub-networks appear to be ‘small world’. This suggests that social media may have a role in spreading anti-vaccination ideas and making the movement durable on a global scale."
140,"The College of Engineering at the University of Illinois at Urbana-Champaign has recently launched a new B. S. dual degree in Innovation, Leadership, and Engineering Entrepreneurship (ILEE), a second bachelor's degree option for those completing their degree in a traditional engineering discipline. This is unlike many universities where similar degree programs are situated in colleges of business rather than engineering. The first cohort of sixteen students has joined the program in January 2017. In this paper we first report on the stated goals of the new degree, which is meant to combine the technical expertise in the traditional engineering science-focused disciplines with a deeper set of skills in problem-finding, creativity, innovation, leadership, and externalization. The idea is to ensure training in the dual physical and human dimensions and triple aspects of science, design, and leadership that are present in engineering practice. Next we report on the curriculum design as well as the experiential learning pedagogy that is present in most of the courses therein. Core courses include ""Design Thinking/Need-Finding"", ""Creativity, Innovation, Vision"", ""Emotional Intelligence"", ""Innovation and Engineering Design"", and ""Technology Entrepreneurship"". Further, we provide a characterization of the students in the first cohort of undergraduate students to be accepted into the degree program. Drawing on their application materials, we perform text analytics using techniques such as topic modeling under Latent Dirichlet Allocation (LDA) and geometric embedding using the word2vec family of methods, to understand the key motivations for students to pursue this degree. Finally we use these text analytics techniques to make a formal assessment of alignment between the stated goals of the degree program and the key motivations of the students. One particular question is to understand the students' relative level of interest in the three legs of the degree, namely innovation, leadership, and entrepreneurship, so as to predict how level of engagement may vary across courses.",2017-12-12,2-s2.0-85043270664,"Proceedings - Frontiers in Education Conference, FIE","The first cohort in a new innovation, leadership, and engineering entrepreneurship B. S. degree program","The College of Engineering at the University of Illinois at Urbana-Champaign has recently launched a new B. S. dual degree in Innovation, Leadership, and Engineering Entrepreneurship (ILEE), a second bachelor's degree option for those completing their degree in a traditional engineering discipline. This is unlike many universities where similar degree programs are situated in colleges of business rather than engineering. The first cohort of sixteen students has joined the program in January 2017. In this paper we first report on the stated goals of the new degree, which is meant to combine the technical expertise in the traditional engineering science-focused disciplines with a deeper set of skills in problem-finding, creativity, innovation, leadership, and externalization. The idea is to ensure training in the dual physical and human dimensions and triple aspects of science, design, and leadership that are present in engineering practice. Next we report on the curriculum design as well as the experiential learning pedagogy that is present in most of the courses therein. Core courses include ""Design Thinking/Need-Finding"", ""Creativity, Innovation, Vision"", ""Emotional Intelligence"", ""Innovation and Engineering Design"", and ""Technology Entrepreneurship"". Further, we provide a characterization of the students in the first cohort of undergraduate students to be accepted into the degree program. Drawing on their application materials, we perform text analytics using techniques such as topic modeling under Latent Dirichlet Allocation (LDA) and geometric embedding using the word2vec family of methods, to understand the key motivations for students to pursue this degree. Finally we use these text analytics techniques to make a formal assessment of alignment between the stated goals of the degree program and the key motivations of the students. One particular question is to understand the students' relative level of interest in the three legs of the degree, namely innovation, leadership, and entrepreneurship, so as to predict how level of engagement may vary across courses."
141,"The classification of crime into discrete categories entails a massive loss of information. Crimes emerge out of a complex mix of behaviors and situations, yet most of these details cannot be captured by singular crime type labels. This information loss impacts our ability to not only understand the causes of crime, but also how to develop optimal crime prevention strategies. We apply machine learning methods to short narrative text descriptions accompanying crime records with the goal of discovering ecologically more meaningful latent crime classes. We term these latent classes ‘crime topics’ in reference to text-based topic modeling methods that produce them. We use topic distributions to measure clustering among formally recognized crime types. Crime topics replicate broad distinctions between violent and property crime, but also reveal nuances linked to target characteristics, situational conditions and the tools and methods of attack. Formal crime types are not discrete in topic space. Rather, crime types are distributed across a range of crime topics. Similarly, individual crime topics are distributed across a range of formal crime types. Key ecological groups include identity theft, shoplifting, burglary and theft, car crimes and vandalism, criminal threats and confidence crimes, and violent crimes. Though not a replacement for formal legal crime classifications, crime topics provide a unique window into the heterogeneous causal processes underlying crime.",2017-12-01,2-s2.0-85039450419,Crime Science,Crime topic modeling,"The classification of crime into discrete categories entails a massive loss of information. Crimes emerge out of a complex mix of behaviors and situations, yet most of these details cannot be captured by singular crime type labels. This information loss impacts our ability to not only understand the causes of crime, but also how to develop optimal crime prevention strategies. We apply machine learning methods to short narrative text descriptions accompanying crime records with the goal of discovering ecologically more meaningful latent crime classes. We term these latent classes ‘crime topics’ in reference to text-based topic modeling methods that produce them. We use topic distributions to measure clustering among formally recognized crime types. Crime topics replicate broad distinctions between violent and property crime, but also reveal nuances linked to target characteristics, situational conditions and the tools and methods of attack. Formal crime types are not discrete in topic space. Rather, crime types are distributed across a range of crime topics. Similarly, individual crime topics are distributed across a range of formal crime types. Key ecological groups include identity theft, shoplifting, burglary and theft, car crimes and vandalism, criminal threats and confidence crimes, and violent crimes. Though not a replacement for formal legal crime classifications, crime topics provide a unique window into the heterogeneous causal processes underlying crime."
142,"This paper conducts a comparative literature survey of Open Government Data (OGD) and Freedom of Information (FOI), with a view to tracking the central themes in the two civil society campaigns. With seeming similarities and a growing popularity in research, the major themes framing research on the two movements have not clearly emerged. Topic modelling, text mining and document analysis methods are used to extract the themes as well as key named entities. The topics are subsequently labeled and with expert guidance, their semantic meaning are provided. The results indicate that the major theme in FOI research borders on issues relating to disclosure, publishing, access and cost of requests. On the other hand, themes in OGD research have largely centered on technology and related concepts. The approach also helped in determining key similarities and differences in the two campaigns as reported in research.",2017-12-01,2-s2.0-85021670038,International Journal of Information Management,Liberation of public data: Exploring central themes in open government data and freedom of information research,"This paper conducts a comparative literature survey of Open Government Data (OGD) and Freedom of Information (FOI), with a view to tracking the central themes in the two civil society campaigns. With seeming similarities and a growing popularity in research, the major themes framing research on the two movements have not clearly emerged. Topic modelling, text mining and document analysis methods are used to extract the themes as well as key named entities. The topics are subsequently labeled and with expert guidance, their semantic meaning are provided. The results indicate that the major theme in FOI research borders on issues relating to disclosure, publishing, access and cost of requests. On the other hand, themes in OGD research have largely centered on technology and related concepts. The approach also helped in determining key similarities and differences in the two campaigns as reported in research."
143,"In this paper, we analyse the effects of different types of formal collaboration and research topics on research impact of academic articles in the area of agricultural, resource, environmental, and ecological economics. The research impact is measured by the number of times an article has been cited each year since publication. The topics within the area of research are modelled using latent semantic analysis. We distinguish between the effect of institutional, national, and international collaboration. We use statistical models for count data and control for the impacts of journals, publication year, and years since publication. We find that, holding other factors constant, collaboration in the form of co-authorship increases research impact. The effect of inter-institutional collaboration within same country is similar to the effect of collaboration within same institution. However, international collaboration results in additional increase in impact. We find that the topic of a paper substantially influences number of citations and identified which topics are associated with greater impact. The effects of different types of collaboration on citations also vary across topics.",2017-12-01,2-s2.0-85033488356,Scientometrics,"Does academic collaboration equally benefit impact of research across topics? The case of agricultural, resource, environmental and ecological economics","In this paper, we analyse the effects of different types of formal collaboration and research topics on research impact of academic articles in the area of agricultural, resource, environmental, and ecological economics. The research impact is measured by the number of times an article has been cited each year since publication. The topics within the area of research are modelled using latent semantic analysis. We distinguish between the effect of institutional, national, and international collaboration. We use statistical models for count data and control for the impacts of journals, publication year, and years since publication. We find that, holding other factors constant, collaboration in the form of co-authorship increases research impact. The effect of inter-institutional collaboration within same country is similar to the effect of collaboration within same institution. However, international collaboration results in additional increase in impact. We find that the topic of a paper substantially influences number of citations and identified which topics are associated with greater impact. The effects of different types of collaboration on citations also vary across topics."
144,"WeChat is a new generation of mobile instant messaging product, which has over 800 million active accounts and support voice, video, pictures and text messages. China's Ministry of Science and Technology has applied social media to science and technology management. Rui Dong Ruan is one of the official accounts in the WeChat matrix of China's Ministry of Science and Technology. This research proposes an approach which uses Chinese word segmentation and topic model to processing texts of Rui Dong Ruan. The experimental result shows that there is a certain relationship between the content of the articles and the view counts on WeChat, which can make the science and technology publicity more efficient and effective.",2017-11-29,2-s2.0-85043505907,"PICMET 2017 - Portland International Conference on Management of Engineering and Technology: Technology Management for the Interconnected World, Proceedings",Text and data mining of social media in science and technology publicity,"WeChat is a new generation of mobile instant messaging product, which has over 800 million active accounts and support voice, video, pictures and text messages. China's Ministry of Science and Technology has applied social media to science and technology management. Rui Dong Ruan is one of the official accounts in the WeChat matrix of China's Ministry of Science and Technology. This research proposes an approach which uses Chinese word segmentation and topic model to processing texts of Rui Dong Ruan. The experimental result shows that there is a certain relationship between the content of the articles and the view counts on WeChat, which can make the science and technology publicity more efficient and effective."
145,"Bibliometric studies have long used simple search strings, publications count, and word counts to track the emergence of technologies. Novel machine learning methods open new possibilities to study bibliometric data and use algorithmic approaches to uncover emergence of a technology. This study looks at the large and complex dataset of vehicle related patents to uncover emergence indicators. By using machine learning methods this study focuses on if, and to what extent different methods can produce patterns of emergence from the data directly. The data extracted from PATSTAT contains 711296 granted US patent abstracts between the years 1980 and 2014 resulting from a search for ""vehicle"" creating a complex dataset of technologies from automotive to medical applications. Using Latent Dirichlet Allocation and Dynamic Topic Modeling we show different emergence patterns. Finally, we discuss in detail the possibilities of using machine learning approaches to draw emergence dynamics of technologies.",2017-11-29,2-s2.0-85043474004,"PICMET 2017 - Portland International Conference on Management of Engineering and Technology: Technology Management for the Interconnected World, Proceedings",Using machine learning approaches to identify emergence: Case of vehicle related patent data,"Bibliometric studies have long used simple search strings, publications count, and word counts to track the emergence of technologies. Novel machine learning methods open new possibilities to study bibliometric data and use algorithmic approaches to uncover emergence of a technology. This study looks at the large and complex dataset of vehicle related patents to uncover emergence indicators. By using machine learning methods this study focuses on if, and to what extent different methods can produce patterns of emergence from the data directly. The data extracted from PATSTAT contains 711296 granted US patent abstracts between the years 1980 and 2014 resulting from a search for ""vehicle"" creating a complex dataset of technologies from automotive to medical applications. Using Latent Dirichlet Allocation and Dynamic Topic Modeling we show different emergence patterns. Finally, we discuss in detail the possibilities of using machine learning approaches to draw emergence dynamics of technologies."
146,"With the rapid development of e-commerce, recommender systems have been widely studied. Many recommendation algorithms utilize ratings and reviews information. However, as the number of users and items grows, these algorithms face the problems of sparsity and scalability. Those problems make it hard to extract useful information from a highly sparse rating matrix and to apply a trained model to larger datasets. In this paper, we aim at solving the sparsity and scalability problems using rating and review information. Three possible solutions for sparsity and scalability problems are concluded and a novel recommendation model called TCR which combines those three ideas are proposed. Experiments on real-world datasets show that our proposed method has better performance on top-N recommendation and has better scalability compared to the state-of-the-art models.",2017-11-22,2-s2.0-85041663753,"Proceedings - 14th IEEE International Conference on E-Business Engineering, ICEBE 2017 - Including 13th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 207",Optimize Recommendation System with Topic Modeling and Clustering,"With the rapid development of e-commerce, recommender systems have been widely studied. Many recommendation algorithms utilize ratings and reviews information. However, as the number of users and items grows, these algorithms face the problems of sparsity and scalability. Those problems make it hard to extract useful information from a highly sparse rating matrix and to apply a trained model to larger datasets. In this paper, we aim at solving the sparsity and scalability problems using rating and review information. Three possible solutions for sparsity and scalability problems are concluded and a novel recommendation model called TCR which combines those three ideas are proposed. Experiments on real-world datasets show that our proposed method has better performance on top-N recommendation and has better scalability compared to the state-of-the-art models."
147,"The proceedings contain 47 papers. The topics discussed include: pricing mechanism research in duopoly online bicycle-sharing market: a game theory approach; online retailer assortment planning and managing under customer and supplier uncertainty effects using internal and external data; optimize recommendation system with topic modeling and clustering; a multi-view clustering method for community discovery integrating links and tags; development of bulk material management system and research on material balance applications based on business intelligence; LODQL: a language for creation, query, and service generation of linked open data; graph analysis of fog computing systems for Industry 4.0; the effects of consumer perceived different service of trusted third party on trust intention: an empirical study in Australia; role identification to discover potential opportunity information in business process; research on government data publishing based on differential privacy model; an analysis of energy-efficient approaches used for virtual machines and data centres; a parallel FP-growth algorithm based on GPU; indexing for large scale data querying based on spark SQL; a development framework for customer experience management applications: principles and case study; methods of introducing continuous process mining to service management for mobile apps; investment recommendation with total capital value maximization in online P2P lending; a study on the influence of engagement marketing strategy on customer perceived support and willingness to customer engagement; and a blockchain-based supply chain quality management framework.",2017-11-22,2-s2.0-85041646801,"Proceedings - 14th IEEE International Conference on E-Business Engineering, ICEBE 2017 - Including 13th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 207","Proceedings - 14th IEEE International Conference on E-Business Engineering, ICEBE 2017 - Including 13th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 207","The proceedings contain 47 papers. The topics discussed include: pricing mechanism research in duopoly online bicycle-sharing market: a game theory approach; online retailer assortment planning and managing under customer and supplier uncertainty effects using internal and external data; optimize recommendation system with topic modeling and clustering; a multi-view clustering method for community discovery integrating links and tags; development of bulk material management system and research on material balance applications based on business intelligence; LODQL: a language for creation, query, and service generation of linked open data; graph analysis of fog computing systems for Industry 4.0; the effects of consumer perceived different service of trusted third party on trust intention: an empirical study in Australia; role identification to discover potential opportunity information in business process; research on government data publishing based on differential privacy model; an analysis of energy-efficient approaches used for virtual machines and data centres; a parallel FP-growth algorithm based on GPU; indexing for large scale data querying based on spark SQL; a development framework for customer experience management applications: principles and case study; methods of introducing continuous process mining to service management for mobile apps; investment recommendation with total capital value maximization in online P2P lending; a study on the influence of engagement marketing strategy on customer perceived support and willingness to customer engagement; and a blockchain-based supply chain quality management framework."
148,"Communicative ecologies are a tool that can be used to understand the existing use of information and communication tools within a specific community. By using the ecology metaphor to understand the interaction between the technological, discursive, and social layers within the community, this research develops a holistic understanding of citizens' communication surrounding the MobiSAM project. This paper proposes the use of sentiment analysis and topic modelling to understand how citizens are currently using technology for political participation. The paper argues that this rich understanding of current use of technology for participation can support the embedding of further interventions into existing communicative ecologies.",2017-11-08,2-s2.0-85043289855,"2017 IST-Africa Week Conference, IST-Africa 2017",The use of sentiment analysis and topic modelling to understand online communicative ecologies in MobiSAM,"Communicative ecologies are a tool that can be used to understand the existing use of information and communication tools within a specific community. By using the ecology metaphor to understand the interaction between the technological, discursive, and social layers within the community, this research develops a holistic understanding of citizens' communication surrounding the MobiSAM project. This paper proposes the use of sentiment analysis and topic modelling to understand how citizens are currently using technology for political participation. The paper argues that this rich understanding of current use of technology for participation can support the embedding of further interventions into existing communicative ecologies."
149,"Topic modeling is a widely used technique in knowledge discovery and data mining. However, finding the right number of topics in a given text source has remained a challenging issue. In this paper, we study the concept of conceptual stability via nonnegative matrix factorization. Based on this finding, we propose a method to identify the correct number of topics and offer empirical evidence in its favor in terms of classification accuracy and the number of topics that are naturally present in the text sources. Experiments on real-world text corpora demonstrate that the proposed method has outperformed state-of-the-art latent Dirichlet allocation and nonnegative matrix factorization models.",2017-11-06,2-s2.0-85037351048,"International Conference on Information and Knowledge Management, Proceedings",On discovering the number of document topics via conceptual latent space,"Topic modeling is a widely used technique in knowledge discovery and data mining. However, finding the right number of topics in a given text source has remained a challenging issue. In this paper, we study the concept of conceptual stability via nonnegative matrix factorization. Based on this finding, we propose a method to identify the correct number of topics and offer empirical evidence in its favor in terms of classification accuracy and the number of topics that are naturally present in the text sources. Experiments on real-world text corpora demonstrate that the proposed method has outperformed state-of-the-art latent Dirichlet allocation and nonnegative matrix factorization models."
150,"The soaring of social media services has greatly propelled the prevalence of document networks. Rather than a set of plain texts, documents are nodes in graphs. An observable link connects the documents at its two ends, thus it implicitly reflects the semantic association between the document pair. Previous work assumes that only similar documents tend to be connected, which neglects the rich connective patterns in the topological structure. In this paper, we introduce a latent correlation factor to categorize the links into several categories, and each category corresponds to a unique kind of association. By fitting the data, the relational information (e.g., homophily and heterophily) can be comprehensively captured. By resorting to Canonical Correlation Analysis (CCA), we maximize the correlation between all pairs of linked documents. We propose a pure generative model and derive efficient learning algorithms based on the variational EMmethods. Experiments on three different datasets demonstrate that the proposed model is competitive and usually better than the state-of-the-art baselines on both topic modeling and link prediction.",2017-11-06,2-s2.0-85037332333,"International Conference on Information and Knowledge Management, Proceedings",Incorporating the latent link categories in relational topic modeling,"The soaring of social media services has greatly propelled the prevalence of document networks. Rather than a set of plain texts, documents are nodes in graphs. An observable link connects the documents at its two ends, thus it implicitly reflects the semantic association between the document pair. Previous work assumes that only similar documents tend to be connected, which neglects the rich connective patterns in the topological structure. In this paper, we introduce a latent correlation factor to categorize the links into several categories, and each category corresponds to a unique kind of association. By fitting the data, the relational information (e.g., homophily and heterophily) can be comprehensively captured. By resorting to Canonical Correlation Analysis (CCA), we maximize the correlation between all pairs of linked documents. We propose a pure generative model and derive efficient learning algorithms based on the variational EMmethods. Experiments on three different datasets demonstrate that the proposed model is competitive and usually better than the state-of-the-art baselines on both topic modeling and link prediction."
151,"People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification.",2017-11-06,2-s2.0-85037361799,"International Conference on Information and Knowledge Management, Proceedings",An enhanced topic modeling approach to multiple stance identification,"People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification."
152,"Linking multiple news streams based on the reported events and analyzing the streams' temporal publishing patterns are two very important tasks for information analysis, discovering newsworthy stories, studying the event evolution, and detecting untrustworthy sources of information. In this paper, we propose techniques for cross-linking news streams based on the reported events with the purpose of analyzing the temporal dependencies among streams. Our research tackles two main issues: (1) how news streams are connected as reporting an event or the evolution of the same event and (2) how timely the newswires report related events using different publishing platforms. Our approach is based on dynamic topic modeling for detecting and tracking events over the timeline and on clustering news according to the events. We leverage the event-based clustering to link news across different streams and present two scoring functions for ranking the streams based on their timeliness in publishing news about a specific event.",2017-11-06,2-s2.0-85037356525,"International Conference on Information and Knowledge Management, Proceedings",Linking news across multiple streams for timeliness analysis,"Linking multiple news streams based on the reported events and analyzing the streams' temporal publishing patterns are two very important tasks for information analysis, discovering newsworthy stories, studying the event evolution, and detecting untrustworthy sources of information. In this paper, we propose techniques for cross-linking news streams based on the reported events with the purpose of analyzing the temporal dependencies among streams. Our research tackles two main issues: (1) how news streams are connected as reporting an event or the evolution of the same event and (2) how timely the newswires report related events using different publishing platforms. Our approach is based on dynamic topic modeling for detecting and tracking events over the timeline and on clustering news according to the events. We leverage the event-based clustering to link news across different streams and present two scoring functions for ranking the streams based on their timeliness in publishing news about a specific event."
153,"We propose Topic Anchoring-based Review Summarization (TARS), a two-step extractive summarization method, which creates review summaries from the sentences that represent the most important aspects of a review. In the first step, the proposed method utilizes Topic Aspect Sentiment Model (TASM), a novel sentiment-topic model, to identify aspects of sentiment-specific topics in a collection of reviews. The output of TASM is utilized in the second step of TARS to rank review sentences based on how representative of the most important review aspects their words are. Qualitative and quantitative evaluation of review summaries using two collections indicate the effectiveness of structuring review summaries around aspects of sentiment-specific topics.",2017-11-06,2-s2.0-85037356952,"International Conference on Information and Knowledge Management, Proceedings",Sentence retrieval with sentiment-specific topical anchoring for review summarization,"We propose Topic Anchoring-based Review Summarization (TARS), a two-step extractive summarization method, which creates review summaries from the sentences that represent the most important aspects of a review. In the first step, the proposed method utilizes Topic Aspect Sentiment Model (TASM), a novel sentiment-topic model, to identify aspects of sentiment-specific topics in a collection of reviews. The output of TASM is utilized in the second step of TARS to rank review sentences based on how representative of the most important review aspects their words are. Qualitative and quantitative evaluation of review summaries using two collections indicate the effectiveness of structuring review summaries around aspects of sentiment-specific topics."
154,"Exploratory analysis of a text corpus is an important task that can be aided by informative visualization. One spatially-oriented form of document visualization is a scatterplot, whereby every document is associated with a coordinate, and relationships among documents can be perceived through their spatial distances. Semantic visualization further infuses the visualization space with latent semantics, by incorporating a topic model that has a representation in the visualization space, allowing users to also perceive relationships between documents and topics spatially.We illustrate how a semantic visualization system called SemVis could be used to navigate a text corpus interactively and topically via browsing and searching.",2017-11-06,2-s2.0-85037377634,"International Conference on Information and Knowledge Management, Proceedings",SemVis: Semantic visualization for interactive topical analysis,"Exploratory analysis of a text corpus is an important task that can be aided by informative visualization. One spatially-oriented form of document visualization is a scatterplot, whereby every document is associated with a coordinate, and relationships among documents can be perceived through their spatial distances. Semantic visualization further infuses the visualization space with latent semantics, by incorporating a topic model that has a representation in the visualization space, allowing users to also perceive relationships between documents and topics spatially.We illustrate how a semantic visualization system called SemVis could be used to navigate a text corpus interactively and topically via browsing and searching."
155,"Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics.",2017-11-06,2-s2.0-85037334875,"International Conference on Information and Knowledge Management, Proceedings",A study of feature construction for text-based forecasting of time series variables,"Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics."
156,"Internet information services have been greatly improved profiting from the growing performance of interest mining technology. Visual perceptual behaviours, a newhotspot of mining user's interests, have resulted in great gains in some typical Internet information services, e.g., information retrieval and recommendation. It is validated that combining the subjective visual perceptual behaviours with the objective contents can significantly improve these services' performance. However, the existing methods usually treat the contents and visual perceptual behaviours as two independent parts in the calculating process. The gain of visual perceptual behaviours has not been fully exploited. In this paper, we mainly aim at improving the gain of visual perceptual behaviour for text recommendation, by integrating the objective contents with subjective visual perceptual behaviours. We investigate the correlation between user's reading interests and records of real-time interaction on texts, and then design a real-time visual perceptual behaviour based method for text recommendation, which is able to: (1) build a joint interest model, called ViP-LDA (Visual Perceptual LDA), by integrating the user's visual perceptual behaviours into topic model; (2) make more accurate text recommendation based on ViP-LDA with feedback adjustment. Several experiments on a real data set are implemented to demonstrate the effectiveness of our method.",2017-11-06,2-s2.0-85037372352,"International Conference on Information and Knowledge Management, Proceedings",Improving the gain of visual perceptual behaviour on topic modeling for text recommendation,"Internet information services have been greatly improved profiting from the growing performance of interest mining technology. Visual perceptual behaviours, a newhotspot of mining user's interests, have resulted in great gains in some typical Internet information services, e.g., information retrieval and recommendation. It is validated that combining the subjective visual perceptual behaviours with the objective contents can significantly improve these services' performance. However, the existing methods usually treat the contents and visual perceptual behaviours as two independent parts in the calculating process. The gain of visual perceptual behaviours has not been fully exploited. In this paper, we mainly aim at improving the gain of visual perceptual behaviour for text recommendation, by integrating the objective contents with subjective visual perceptual behaviours. We investigate the correlation between user's reading interests and records of real-time interaction on texts, and then design a real-time visual perceptual behaviour based method for text recommendation, which is able to: (1) build a joint interest model, called ViP-LDA (Visual Perceptual LDA), by integrating the user's visual perceptual behaviours into topic model; (2) make more accurate text recommendation based on ViP-LDA with feedback adjustment. Several experiments on a real data set are implemented to demonstrate the effectiveness of our method."
157,"Personalized recommendation of items frequently faces scenarios where we have sparse observations on users' adoption of items. In the literature, there are two promising directions. One is to connect sparse items through similarity in content. The other is to connect sparse users through similarity in social relations. We seek to integrate both types of information, in addition to the adoption information, within a single integrated model. Our proposed method models item content via a topic model, and user communities via an autoencoder model, while bridging a user's community-based preference to her topic-based preference. Experiments on public real-life data showcase the utility of the model, particularly when there is significant compatibility between communities and topics.",2017-11-06,2-s2.0-85037356079,"International Conference on Information and Knowledge Management, Proceedings",Collaborative topic regression with denoising autoencoder for content and community co-representation,"Personalized recommendation of items frequently faces scenarios where we have sparse observations on users' adoption of items. In the literature, there are two promising directions. One is to connect sparse items through similarity in content. The other is to connect sparse users through similarity in social relations. We seek to integrate both types of information, in addition to the adoption information, within a single integrated model. Our proposed method models item content via a topic model, and user communities via an autoencoder model, while bridging a user's community-based preference to her topic-based preference. Experiments on public real-life data showcase the utility of the model, particularly when there is significant compatibility between communities and topics."
158,"Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines.",2017-11-06,2-s2.0-85037352373,"International Conference on Information and Knowledge Management, Proceedings",Joint topic-semantic-aware social recommendation for online voting,"Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines."
159,"Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA.",2017-11-06,2-s2.0-85037341070,"International Conference on Information and Knowledge Management, Proceedings",A topic model based on poisson decomposition,"Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA."
160,"Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that ""one-hot"" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.",2017-11-06,2-s2.0-85037353960,"International Conference on Information and Knowledge Management, Proceedings",RATE: Overcoming noise and sparsity of textual features in real-time location estimation,"Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that ""one-hot"" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression."
161,"Social media platforms such as weblogs and social networking sites provide Internet users with an unprecedented means to express their opinions and debate on a wide range of issues. Concurrently with their growing importance in public communication, social media platforms may foster echo chambers and filter bubbles: homophily and content personalization lead users to be increasingly exposed to conforming opinions.There is therefore a need for unbiased systems able to identify and provide access to varied viewpoints. To address this task, we propose in this paper a novel unsupervised topic model, the Social Network Viewpoint Discovery Model (SNVDM). Given a specific issue (e.g., U.S. policy) as well as the text and social interactions from the users discussing this issue on a social networking site, SNVDM jointly identifies the issue's topics, the users' viewpoints, and the discourse pertaining to the different topics and viewpoints. In order to overcome the potential sparsity of the social network (i.e., some users interact with only a few other users), we propose an extension to SNVDM based on the Generalized pólya Urn sampling scheme (SNVDM-GPU) to leverage ""acquaintances of acquaintances"" relationships. We benchmark the different proposed models against three baselines, namely TAM, SN-LDA, and VODUM, on a viewpoint clustering task using two real-world datasets. We thereby provide evidence that our model SNVDM and its extension SNVDM-GPU significantly outperform state-of-the-art baselines, and we show that utilizing social interactions greatly improves viewpoint clustering performance.",2017-11-06,2-s2.0-85037336749,"International Conference on Information and Knowledge Management, Proceedings",Users are known by the company they keep: Topic models for viewpoint discovery in social networks,"Social media platforms such as weblogs and social networking sites provide Internet users with an unprecedented means to express their opinions and debate on a wide range of issues. Concurrently with their growing importance in public communication, social media platforms may foster echo chambers and filter bubbles: homophily and content personalization lead users to be increasingly exposed to conforming opinions.There is therefore a need for unbiased systems able to identify and provide access to varied viewpoints. To address this task, we propose in this paper a novel unsupervised topic model, the Social Network Viewpoint Discovery Model (SNVDM). Given a specific issue (e.g., U.S. policy) as well as the text and social interactions from the users discussing this issue on a social networking site, SNVDM jointly identifies the issue's topics, the users' viewpoints, and the discourse pertaining to the different topics and viewpoints. In order to overcome the potential sparsity of the social network (i.e., some users interact with only a few other users), we propose an extension to SNVDM based on the Generalized pólya Urn sampling scheme (SNVDM-GPU) to leverage ""acquaintances of acquaintances"" relationships. We benchmark the different proposed models against three baselines, namely TAM, SN-LDA, and VODUM, on a viewpoint clustering task using two real-world datasets. We thereby provide evidence that our model SNVDM and its extension SNVDM-GPU significantly outperform state-of-the-art baselines, and we show that utilizing social interactions greatly improves viewpoint clustering performance."
162,"This study aims to identify both where technology transfer research originated and where it is going. A quantitative approach was adopted in this study to observe the trends from an objective perspective. To do this, longitudinal bibliographic data of journal papers describing technology transfer from 1980 to 2015 are collected. Topic modeling and co-authorship network analyses are then applied to classify topics and identify an evolution of research groups. First, the principal transfer agent is changed from governmental organizations to universities, as technology donors, while industry plays the role of technology recipients. Second, major technology fields that researchers have focused on follow socially attractive interests. Third, the scope of focus gradually moves from national level research or international transfers to organizational level research. In addition, technology transfer research seems to change from a technology transfer application to a dynamic technology transfer process. In addition, six topics are identified and further discussed to understand future research directions. The research findings are expected to help us understand research trends in technology transfer and, thus, are expected to provide valuable insights to researchers in this field and policy makers who are in charge of developing policies to support technology transfer.",2017-11-03,2-s2.0-85032817031,Journal of Technology Transfer,Where technology transfer research originated and where it is going: a quantitative analysis of literature published between 1980 and 2015,"This study aims to identify both where technology transfer research originated and where it is going. A quantitative approach was adopted in this study to observe the trends from an objective perspective. To do this, longitudinal bibliographic data of journal papers describing technology transfer from 1980 to 2015 are collected. Topic modeling and co-authorship network analyses are then applied to classify topics and identify an evolution of research groups. First, the principal transfer agent is changed from governmental organizations to universities, as technology donors, while industry plays the role of technology recipients. Second, major technology fields that researchers have focused on follow socially attractive interests. Third, the scope of focus gradually moves from national level research or international transfers to organizational level research. In addition, technology transfer research seems to change from a technology transfer application to a dynamic technology transfer process. In addition, six topics are identified and further discussed to understand future research directions. The research findings are expected to help us understand research trends in technology transfer and, thus, are expected to provide valuable insights to researchers in this field and policy makers who are in charge of developing policies to support technology transfer."
163,"BACKGROUND Automated text analysis is widely used across the social sciences, yet the application of these methods has largely proceeded independently of qualitative analysis. OBJECTIVE This paper explores the advantages of applying automated text analysis to augment traditional qualitative methods in demography. Computational text analysis does not replace close reading or subjective theorizing, but it can provide a complementary set of tools that we believe will be appealing for qualitative demographers. METHODS We apply topic modeling to text data from the Malawi Journals Project as a case study. RESULTS We examine three common issues that demographers face in analyzing qualitative data: large samples, the challenge of comparing qualitative data across external categories, and making data analysis transparent and readily accessible to other scholars. We discuss ways that new tools from machine learning and computer science might help qualitative scholars to address these issues. CONCLUSIONS We believe that there is great promise in mixed-method approaches to analyzing text. New methods that allow better access to data and new ways to approach qualitative data are likely to be fertile ground for research. CONTRIBUTIONS No research, to our knowledge, has used automated text analysis to take an explicitly mixed-method approach to the analysis of textual data. We develop a framework that allows qualitative researchers to do so.",2017-11-02,2-s2.0-85041073313,Demographic Research,A mixed-methods framework for analyzing text data: Integrating computational techniques with qualitative methods in demogra,"BACKGROUND Automated text analysis is widely used across the social sciences, yet the application of these methods has largely proceeded independently of qualitative analysis. OBJECTIVE This paper explores the advantages of applying automated text analysis to augment traditional qualitative methods in demography. Computational text analysis does not replace close reading or subjective theorizing, but it can provide a complementary set of tools that we believe will be appealing for qualitative demographers. METHODS We apply topic modeling to text data from the Malawi Journals Project as a case study. RESULTS We examine three common issues that demographers face in analyzing qualitative data: large samples, the challenge of comparing qualitative data across external categories, and making data analysis transparent and readily accessible to other scholars. We discuss ways that new tools from machine learning and computer science might help qualitative scholars to address these issues. CONCLUSIONS We believe that there is great promise in mixed-method approaches to analyzing text. New methods that allow better access to data and new ways to approach qualitative data are likely to be fertile ground for research. CONTRIBUTIONS No research, to our knowledge, has used automated text analysis to take an explicitly mixed-method approach to the analysis of textual data. We develop a framework that allows qualitative researchers to do so."
164,"During the past two decades, the focus of marketing has moved from the tactics of persuasion to the strategies of value cocreation. After moving toward cognitive science and corporate strategies in the early 2000s, marketing research returned to its traditional domains of consumer psychologies and customer management. While conscientious consumers are gradually restraining themselves from selfish indulgence, marketers have refocused on a new set of values that encompass mental, experiential, and societal well-being. In this regard, we adopt an unprecedented approach by incorporating topic modeling with social network analysis. The results show that, in terms of topic heterogeneity, the most impactful journals are the most diverse, whereas each runner-up has a unique focus. Among the journals, we detect two major co-authorship communities, and among the topics, we detect three. Further, we find that the communities of the most cited papers are composed of heterogeneous clusters of similar topics. The pivots within, and the bridges between, these communities are also reported. In the spirit of collaborative research, our topic model and network analysis are shared via online collaboration and visualization platforms that readers can use to explore our models interactively and to download the dataset for further studies.",2017-11-01,2-s2.0-85030455391,Journal of Interactive Marketing,"Popular Research Topics in Marketing Journals, 1995–2014","During the past two decades, the focus of marketing has moved from the tactics of persuasion to the strategies of value cocreation. After moving toward cognitive science and corporate strategies in the early 2000s, marketing research returned to its traditional domains of consumer psychologies and customer management. While conscientious consumers are gradually restraining themselves from selfish indulgence, marketers have refocused on a new set of values that encompass mental, experiential, and societal well-being. In this regard, we adopt an unprecedented approach by incorporating topic modeling with social network analysis. The results show that, in terms of topic heterogeneity, the most impactful journals are the most diverse, whereas each runner-up has a unique focus. Among the journals, we detect two major co-authorship communities, and among the topics, we detect three. Further, we find that the communities of the most cited papers are composed of heterogeneous clusters of similar topics. The pivots within, and the bridges between, these communities are also reported. In the spirit of collaborative research, our topic model and network analysis are shared via online collaboration and visualization platforms that readers can use to explore our models interactively and to download the dataset for further studies."
165,"In recent decades, analyzing the sentiments in online customer reviews has become important to many businesses and researchers. However, insufficient amount of labeled training corpus is a bottleneck for machine learning approaches. Self-training is one of the promising semi-supervised techniques which does not require large amounts of labeled data. However, self-training also suffers from an incorrect labeling problem along with insufficient amount of labeled data. This study proposed a semi-supervised learning framework that adds only confidently predicted data to the training corpus in order to enrich the initial classifier in self-training. The experimental results indicate that the proposed method performed better than self-training.",2017-11-01,2-s2.0-85030475861,Electronic Commerce Research and Applications,Sentiment labeling for extending initial labeled data to improve semi-supervised sentiment classification,"In recent decades, analyzing the sentiments in online customer reviews has become important to many businesses and researchers. However, insufficient amount of labeled training corpus is a bottleneck for machine learning approaches. Self-training is one of the promising semi-supervised techniques which does not require large amounts of labeled data. However, self-training also suffers from an incorrect labeling problem along with insufficient amount of labeled data. This study proposed a semi-supervised learning framework that adds only confidently predicted data to the training corpus in order to enrich the initial classifier in self-training. The experimental results indicate that the proposed method performed better than self-training."
166,"Science policy is increasingly shifting towards an emphasis in societal problems or grand challenges. As a result, new evaluative tools are needed to help assess not only the knowledge production side of research programmes or organisations, but also the articulation of research agendas with societal needs. In this paper, we present an exploratory investigation of science supply and societal needs on the grand challenge of obesity – an emerging health problem with enormous social costs. We illustrate a potential approach that uses topic modelling to explore: (a) how scientific publications can be used to describe existing priorities in science production; (b) how policy records (in this case here questions posed in the European parliament) can be used as an instance of mapping discourse of social needs; (c) how the comparison between the two may show (mis)alignments between societal concerns and scientific outputs. While this is a technical exercise, we propose that this type of mapping methods can be useful to domain experts for informing strategic planning and evaluation in funding agencies.",2017-11-01,2-s2.0-85031015037,Journal of Informetrics,Improving fitness: Mapping research priorities against societal needs on obesity,"Science policy is increasingly shifting towards an emphasis in societal problems or grand challenges. As a result, new evaluative tools are needed to help assess not only the knowledge production side of research programmes or organisations, but also the articulation of research agendas with societal needs. In this paper, we present an exploratory investigation of science supply and societal needs on the grand challenge of obesity – an emerging health problem with enormous social costs. We illustrate a potential approach that uses topic modelling to explore: (a) how scientific publications can be used to describe existing priorities in science production; (b) how policy records (in this case here questions posed in the European parliament) can be used as an instance of mapping discourse of social needs; (c) how the comparison between the two may show (mis)alignments between societal concerns and scientific outputs. While this is a technical exercise, we propose that this type of mapping methods can be useful to domain experts for informing strategic planning and evaluation in funding agencies."
167,"The increasing use of the Internet for many purposes is creating big data, many of which are generated from social media. These big data potentially could assist in obtaining valuable administrative information and even explore new social phenomena. Traditional ways of collecting data, such as questionnaire surveys, are time-consuming and costly. Therefore, the use of social media affords the opportunity to extract information that might be of benefit to the construction industry in a responsive and inexpensive manner. To this end, this paper explores whether information and knowledge that would be valuable in the construction domain can be generated by analyzing social media data. Twitter was selected for an initial trial analysis because of its wide usage in the United States. Because they represent a majority of the construction users in Twitter, the following four user clusters were selected and analyzed: construction workers, construction companies, construction unions, and construction media. For each user identified in the four clusters, the 3,200 most recent Twitter messages were collected, which were analyzed from the following aspects: sentiment analysis, topic modeling, link analysis, geolocation analysis, and timeline analysis. Different data-analysis methods were used for the specific themes, such as Stanford Natural Language Processing (StanfordNLP) for sentiment analysis. The detailed findings, benefits, and barriers to incorporating social media data analytics in the construction industry, as well as future research directions, are discussed in this paper. For example, the sentiment analysis results indicated that construction workers tend to have a higher proportion of negative messages compared to the other clusters, which may prompt more attention to emotional guidance and understanding by construction companies and the public. This paper benefits academia by testing an alternative way of studying the construction population, which could help decision makers gain a better understanding of real-world situations in the construction industry.",2017-11-01,2-s2.0-85029504535,Journal of Management in Engineering,Social Media Data Analytics for the U.S. Construction Industry: Preliminary Study on Twitter,"The increasing use of the Internet for many purposes is creating big data, many of which are generated from social media. These big data potentially could assist in obtaining valuable administrative information and even explore new social phenomena. Traditional ways of collecting data, such as questionnaire surveys, are time-consuming and costly. Therefore, the use of social media affords the opportunity to extract information that might be of benefit to the construction industry in a responsive and inexpensive manner. To this end, this paper explores whether information and knowledge that would be valuable in the construction domain can be generated by analyzing social media data. Twitter was selected for an initial trial analysis because of its wide usage in the United States. Because they represent a majority of the construction users in Twitter, the following four user clusters were selected and analyzed: construction workers, construction companies, construction unions, and construction media. For each user identified in the four clusters, the 3,200 most recent Twitter messages were collected, which were analyzed from the following aspects: sentiment analysis, topic modeling, link analysis, geolocation analysis, and timeline analysis. Different data-analysis methods were used for the specific themes, such as Stanford Natural Language Processing (StanfordNLP) for sentiment analysis. The detailed findings, benefits, and barriers to incorporating social media data analytics in the construction industry, as well as future research directions, are discussed in this paper. For example, the sentiment analysis results indicated that construction workers tend to have a higher proportion of negative messages compared to the other clusters, which may prompt more attention to emotional guidance and understanding by construction companies and the public. This paper benefits academia by testing an alternative way of studying the construction population, which could help decision makers gain a better understanding of real-world situations in the construction industry."
168,"Mass media have long provided general publics with science news. New media such as Twitter have entered this system and provide an additional platform for the dissemination of science information. Based on automated collection and analysis of >900 news articles and 70,000 tweets, this study explores the online communication of current science news. Topic modeling (latent Dirichlet allocation) was used to extract five broad themes of science reporting: space missions, the US government shutdown, cancer research, Nobel Prizes, and climate change. Using content and network analysis, Twitter was found to extend public science communication by providing additional voices and contextualizations of science issues. It serves a recommender role by linking to web resources, connecting users, and directing users’ attention. This article suggests that microblogging adds a new and relevant layer to the public communication of science.",2017-11-01,2-s2.0-85031399273,Public Understanding of Science,Microblogging as an extension of science reporting,"Mass media have long provided general publics with science news. New media such as Twitter have entered this system and provide an additional platform for the dissemination of science information. Based on automated collection and analysis of >900 news articles and 70,000 tweets, this study explores the online communication of current science news. Topic modeling (latent Dirichlet allocation) was used to extract five broad themes of science reporting: space missions, the US government shutdown, cancer research, Nobel Prizes, and climate change. Using content and network analysis, Twitter was found to extend public science communication by providing additional voices and contextualizations of science issues. It serves a recommender role by linking to web resources, connecting users, and directing users’ attention. This article suggests that microblogging adds a new and relevant layer to the public communication of science."
169,"In recent years, social media has become a significant resource for improving healthcare and mental health. Mental health forums are online communities where people express their issues, and seek help from moderators and other users. In such forums, there are often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self-harm. Moderators need to respond to these severe posts in a timely manner to prevent potential self-harm. However, the large volume of daily posted content makes it difficult for the moderators to locate and respond to these critical posts. We propose an approach for triaging user content into four severity categories that are defined based on an indication of self-harm ideation. Our models are based on a feature-rich classification framework, which includes lexical, psycholinguistic, contextual, and topic modeling features. Our approaches improve over the state of the art in triaging the content severity in mental health forums by large margins (up to 17% improvement over the F-1 scores). Furthermore, using our proposed model, we analyze the mental state of users and we show that overall, long-term users of the forum demonstrate decreased severity of risk over time. Our analysis on the interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for the moderators to provide timely response to the users in need.",2017-11-01,2-s2.0-85030230974,Journal of the Association for Information Science and Technology,Triaging content severity in online mental health forums,"In recent years, social media has become a significant resource for improving healthcare and mental health. Mental health forums are online communities where people express their issues, and seek help from moderators and other users. In such forums, there are often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self-harm. Moderators need to respond to these severe posts in a timely manner to prevent potential self-harm. However, the large volume of daily posted content makes it difficult for the moderators to locate and respond to these critical posts. We propose an approach for triaging user content into four severity categories that are defined based on an indication of self-harm ideation. Our models are based on a feature-rich classification framework, which includes lexical, psycholinguistic, contextual, and topic modeling features. Our approaches improve over the state of the art in triaging the content severity in mental health forums by large margins (up to 17% improvement over the F-1 scores). Furthermore, using our proposed model, we analyze the mental state of users and we show that overall, long-term users of the forum demonstrate decreased severity of risk over time. Our analysis on the interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for the moderators to provide timely response to the users in need."
170,"Blog search engines and general search engines automatically crawl web pages from the Internet and produce search results for users. One difference between the two is that blog search engines focus on posts and ignore the rest of the pages. Obviously, the pages indexed by the general search engine are always greater than the posts. This feature allows bloggers to focus only on the posts they are interested in, rather than other types of pages. The other difference is that posts involve more time-related issues compared to general pages. For the general pages, the general search engine often can only show the last update time. However, for the post, the blog search engine can display various possible times. For some often updated posts, the time factor can help bloggers find information more efficiently. In this paper, we first use some well-known semantic analysis models to analyze the performance of the blog search. Next, we consider the time relationship between posts to further improve its performance. Finally, we provide some experiments to simulate various possible scenarios to confirm the effectiveness of this relationship. The contributions of this paper are twofold. One is that we build a high-performance system that considers the importance of blog topics at different times. The other is that we consider the time relationship between posts, which can rank the relevant blog topics based on the popularity of the posts.",2017-11-01,2-s2.0-85028695681,Information Processing and Management,An effective LDA-based time topic model to improve blog search performance,"Blog search engines and general search engines automatically crawl web pages from the Internet and produce search results for users. One difference between the two is that blog search engines focus on posts and ignore the rest of the pages. Obviously, the pages indexed by the general search engine are always greater than the posts. This feature allows bloggers to focus only on the posts they are interested in, rather than other types of pages. The other difference is that posts involve more time-related issues compared to general pages. For the general pages, the general search engine often can only show the last update time. However, for the post, the blog search engine can display various possible times. For some often updated posts, the time factor can help bloggers find information more efficiently. In this paper, we first use some well-known semantic analysis models to analyze the performance of the blog search. Next, we consider the time relationship between posts to further improve its performance. Finally, we provide some experiments to simulate various possible scenarios to confirm the effectiveness of this relationship. The contributions of this paper are twofold. One is that we build a high-performance system that considers the importance of blog topics at different times. The other is that we consider the time relationship between posts, which can rank the relevant blog topics based on the popularity of the posts."
171,"This work presents a framework for collecting, processing and mining geo-located tweets in order to extract meaningful and actionable knowledge in the context of smart cities. We collected and characterized more than 9M tweets from the two biggest cities in Brazil, Rio de Janeiro and Sao Paulo. We performed topic modeling using the Latent Dirichlet Allocation model to produce an unsupervised distribution of semantic topics over the stream of geo-located tweets as well as a distribution of words over those topics. We manually labeled and aggregated similar topics obtaining a total of 29 different topics across both cities. Results showed similarities in the majority of topics for both cities, reflecting similar interests and concerns among the population of Rio de Janeiro and Sao Paulo. Nevertheless, some specific topics are more predominant in one of the cities.",2017-10-30,2-s2.0-85039940161,"2017 International Smart Cities Conference, ISC2 2017",Characterizing geo-located tweets in brazilian megacities,"This work presents a framework for collecting, processing and mining geo-located tweets in order to extract meaningful and actionable knowledge in the context of smart cities. We collected and characterized more than 9M tweets from the two biggest cities in Brazil, Rio de Janeiro and Sao Paulo. We performed topic modeling using the Latent Dirichlet Allocation model to produce an unsupervised distribution of semantic topics over the stream of geo-located tweets as well as a distribution of words over those topics. We manually labeled and aggregated similar topics obtaining a total of 29 different topics across both cities. Results showed similarities in the majority of topics for both cities, reflecting similar interests and concerns among the population of Rio de Janeiro and Sao Paulo. Nevertheless, some specific topics are more predominant in one of the cities."
172,"One important pedagogical approach is to motivate students actively learn and discover new knowledge by using contemporary instructional design methods. This is in sharp contrast to another approach that forces students to take a passive rote-learning strategy. To promote active learning, CityU Hong Kong adopted the so-called discovery-enriched curriculum (DEC) pedagogical approach to enhance student learning at the tertiary education setting. While various curriculum design and instructional methods have been explored in recent years, an effective way to assess students' novel discoveries and creativity remains a great challenge. This paper investigates into a relatively new text analytics computational approach to facilitate instructors of identifying and assessing the concrete evidences of students' achievements under a DEC-based pedagogical approach. In particular, we illustrate a topic modeling-based text mining method which can extract the hidden intents and creative ideas of students embedded in their project work. According to our best knowledge, this is one of the few successful research work of applying a topic modeling method to enhance the assessment of students' achievements under a DEC pedagogical approach. The practical application and implications of our work is that educational practitioners can apply the proposed computational method to facilitate the assessment of students' novel discoveries.",2017-10-18,2-s2.0-85039908491,"Ubi-Media 2017 - Proceedings of the 10th International Conference on Ubi-Media Computing and Workshops with the 4th International Workshop on Advanced E-Learning and the 1st International Workshop on Multimedia and IoT: Networks, Systems and Applications",Discovery-Enriched curriculum: A text mining approach for assessing students' discoveries,"One important pedagogical approach is to motivate students actively learn and discover new knowledge by using contemporary instructional design methods. This is in sharp contrast to another approach that forces students to take a passive rote-learning strategy. To promote active learning, CityU Hong Kong adopted the so-called discovery-enriched curriculum (DEC) pedagogical approach to enhance student learning at the tertiary education setting. While various curriculum design and instructional methods have been explored in recent years, an effective way to assess students' novel discoveries and creativity remains a great challenge. This paper investigates into a relatively new text analytics computational approach to facilitate instructors of identifying and assessing the concrete evidences of students' achievements under a DEC-based pedagogical approach. In particular, we illustrate a topic modeling-based text mining method which can extract the hidden intents and creative ideas of students embedded in their project work. According to our best knowledge, this is one of the few successful research work of applying a topic modeling method to enhance the assessment of students' achievements under a DEC pedagogical approach. The practical application and implications of our work is that educational practitioners can apply the proposed computational method to facilitate the assessment of students' novel discoveries."
173,"The development of the Internet and mobile devices enabled the emergence of travel and hospitality review sites, leading to a large number of customer opinion posts. While such comments may influence future demand of the targeted hotels, they can also be used by hotel managers to improve customer experience. In this article, sentiment classification of an eco-hotel is assessed through a text mining approach using several different sources of customer reviews. The latent Dirichlet allocation modeling algorithm is applied to gather relevant topics that characterize a given hospitality issue by a sentiment. Several findings were unveiled including that hotel food generates ordinary positive sentiments, while hospitality generates both ordinary and strong positive feelings. Such results are valuable for hospitality management, validating the proposed approach.",2017-10-03,2-s2.0-85018173060,Journal of Hospitality Marketing and Management,Sentiment Classification of Consumer-Generated Online Reviews Using Topic Modeling,"The development of the Internet and mobile devices enabled the emergence of travel and hospitality review sites, leading to a large number of customer opinion posts. While such comments may influence future demand of the targeted hotels, they can also be used by hotel managers to improve customer experience. In this article, sentiment classification of an eco-hotel is assessed through a text mining approach using several different sources of customer reviews. The latent Dirichlet allocation modeling algorithm is applied to gather relevant topics that characterize a given hospitality issue by a sentiment. Several findings were unveiled including that hotel food generates ordinary positive sentiments, while hospitality generates both ordinary and strong positive feelings. Such results are valuable for hospitality management, validating the proposed approach."
174,"Classifying requirements into functional requirements (FR) and non-functional ones (NFR) is an important task in requirements engineering. However, automated classification of requirements written in natural language is not straightforward, due to the variability of natural language and the absence of a controlled vocabulary. This paper investigates how automated classification of requirements into FR and NFR can be improved and how well several machine learning approaches work in this context. We contribute an approach for preprocessing requirements that standardizes and normalizes requirements before applying classification algorithms. Further, we report on how well several existing machine learning methods perform for automated classification of NFRs into sub-categories such as usability, availability, or performance. Our study is performed on 625 requirements provided by the OpenScience tera-PROMISE repository. We found that our preprocessing improved the performance of an existing classification method. We further found significant differences in the performance of approaches such as Latent Dirichlet Allocation, Biterm Topic Modeling, or Naïve Bayes for the sub-classification of NFRs.",2017-09-22,2-s2.0-85032825139,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",What Works Better? A Study of Classifying Requirements,"Classifying requirements into functional requirements (FR) and non-functional ones (NFR) is an important task in requirements engineering. However, automated classification of requirements written in natural language is not straightforward, due to the variability of natural language and the absence of a controlled vocabulary. This paper investigates how automated classification of requirements into FR and NFR can be improved and how well several machine learning approaches work in this context. We contribute an approach for preprocessing requirements that standardizes and normalizes requirements before applying classification algorithms. Further, we report on how well several existing machine learning methods perform for automated classification of NFRs into sub-categories such as usability, availability, or performance. Our study is performed on 625 requirements provided by the OpenScience tera-PROMISE repository. We found that our preprocessing improved the performance of an existing classification method. We further found significant differences in the performance of approaches such as Latent Dirichlet Allocation, Biterm Topic Modeling, or Naïve Bayes for the sub-classification of NFRs."
175,"Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number-in the range of thousands per day for popular applications-a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements.",2017-09-22,2-s2.0-85032809108,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",A Little Bird Told Me: Mining Tweets for Requirements and Software Evolution,"Twitter is one of the most popular social networks. Previous research found that users employ Twitter to communicate about software applications via short messages, commonly referred to as tweets, and that these tweets can be useful for requirements engineering and software evolution. However, due to their large number-in the range of thousands per day for popular applications-a manual analysis is unfeasible.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to specific attributes, such as content category, sentiment and number of retweets. We ran our approach on 68,108 collected tweets from three software applications and compared its results against software practitioners' judgement. Our results show that ALERTme is an effective approach for filtering, summarizing and ranking tweets about software applications. ALERTme enables the exploitation of Twitter as a feedback channel for information relevant to software evolution, including end-user requirements."
176,"Due to the popularity of social networks, such as microblogs and Twitter, a vast amount of short text data is created every day. Much recent research in short text becomes increasingly significant, such as topic inference for short text. Biterm topic model (BTM) benefits from the word co-occurrence patterns of the corpus, which makes it perform better than conventional topic models in uncovering latent semantic relevance for short text. However, BTM resorts to Gibbs sampling to infer topics, which is very time consuming, especially for large-scale datasets or when the number of topics is extremely large. It requires O(K) operations per sample for K topics, where K denotes the number of topics in the corpus. In this paper, we propose an acceleration algorithm of BTM, FastBTM, using an efficient sampling method for BTM, which converges much faster than BTM without degrading topic quality. FastBTM is based on Metropolis-Hastings and alias method, both of which have been widely adopted in Latent Dirichlet Allocation (LDA) model and achieved outstanding speedup. Our FastBTM can effectively reduce the sampling complexity of biterm topic model from O(K) to O(1) amortized time. We carry out a number of experiments on three datasets including two short text datasets, Tweets2011 Collection dataset and Yahoo! Answers dataset, and one long document dataset, Enron dataset. Our experimental results show that when the number of topics K increases, the gap in running time speed between FastBTM and BTM gets especially larger. In addition, our FastBTM is effective for both short text datasets and long document datasets.",2017-09-15,2-s2.0-85020799830,Knowledge-Based Systems,FastBTM: Reducing the sampling time for biterm topic model,"Due to the popularity of social networks, such as microblogs and Twitter, a vast amount of short text data is created every day. Much recent research in short text becomes increasingly significant, such as topic inference for short text. Biterm topic model (BTM) benefits from the word co-occurrence patterns of the corpus, which makes it perform better than conventional topic models in uncovering latent semantic relevance for short text. However, BTM resorts to Gibbs sampling to infer topics, which is very time consuming, especially for large-scale datasets or when the number of topics is extremely large. It requires O(K) operations per sample for K topics, where K denotes the number of topics in the corpus. In this paper, we propose an acceleration algorithm of BTM, FastBTM, using an efficient sampling method for BTM, which converges much faster than BTM without degrading topic quality. FastBTM is based on Metropolis-Hastings and alias method, both of which have been widely adopted in Latent Dirichlet Allocation (LDA) model and achieved outstanding speedup. Our FastBTM can effectively reduce the sampling complexity of biterm topic model from O(K) to O(1) amortized time. We carry out a number of experiments on three datasets including two short text datasets, Tweets2011 Collection dataset and Yahoo! Answers dataset, and one long document dataset, Enron dataset. Our experimental results show that when the number of topics K increases, the gap in running time speed between FastBTM and BTM gets especially larger. In addition, our FastBTM is effective for both short text datasets and long document datasets."
177,"Qualitative geographic information systems (GIS) has progressed in meaningful ways since early calls for a qualitative GIS in the 1990s. From participatory methods to the invention of the participatory geoweb and finally to geospatial social media sources, the amount of information available to nonquantitative GIScientists has grown tremendously. Recently, researchers have advanced qualitative GIS by taking advantage of new data sources, like Twitter, to illustrate the occurrence of various phenomena in the data set geospatially. At the same time, computer scientists in the field of natural language processing have built increasingly sophisticated methods for digesting and analyzing large text-based data sources. In this article, the authors implement one of these methods, topic modeling, and create a visualization method to illustrate the results in a visually comparative way, directly onto the map canvas. The method is a step toward making the advances in natural language processing available to all GIScientists. The article discusses the ways in which geography plays an important part in understanding the results presented from the model and visualization, including issues of place and space.",2017-09-03,2-s2.0-85017102034,Annals of the American Association of Geographers,Area-Based Topic Modeling and Visualization of Social Media for Qualitative GIS,"Qualitative geographic information systems (GIS) has progressed in meaningful ways since early calls for a qualitative GIS in the 1990s. From participatory methods to the invention of the participatory geoweb and finally to geospatial social media sources, the amount of information available to nonquantitative GIScientists has grown tremendously. Recently, researchers have advanced qualitative GIS by taking advantage of new data sources, like Twitter, to illustrate the occurrence of various phenomena in the data set geospatially. At the same time, computer scientists in the field of natural language processing have built increasingly sophisticated methods for digesting and analyzing large text-based data sources. In this article, the authors implement one of these methods, topic modeling, and create a visualization method to illustrate the results in a visually comparative way, directly onto the map canvas. The method is a step toward making the advances in natural language processing available to all GIScientists. The article discusses the ways in which geography plays an important part in understanding the results presented from the model and visualization, including issues of place and space."
178,"Previous research has acknowledged the use of social media in political communication by right-wing populist parties and politicians. Less is known, however, about its pivotal role for right-wing social movements which rely on personalized messages to mobilize supporters and challenge the mainstream party system. This paper analyzes online political communication by the right-wing populist movement Pegida and German political parties. We investigate to which extent parties attract supporters of Pegida, to which extent they address topics similar to Pegida and whether their topic use has become more similar over a period of almost two years. The empirical analysis is based on Facebook posts by main accounts and individual representatives of these political groups. We first show that there are considerable overlaps in the audiences of Pegida and the new challenger in the party system, AfD. Then we use topic models to characterize topic use by party and surveyed crowdworkers to which extent they perceive the identified topics as populist communication. The results show that while Pegida and AfD talk about rather unique topics and smaller parties engage to varying degrees with the topics populists emphasize, the two governing parties CDU and SPD clearly deemphasize those. Overall, the findings indicate that the considerable attention devoted to populist actors and shifts in public opinion due to the refugee crisis have left only moderate marks in political communication within the mainstream party system.",2017-09-02,2-s2.0-85019766952,Information Communication and Society,When populists become popular: comparing Facebook use by the right-wing movement Pegida and German political parties,"Previous research has acknowledged the use of social media in political communication by right-wing populist parties and politicians. Less is known, however, about its pivotal role for right-wing social movements which rely on personalized messages to mobilize supporters and challenge the mainstream party system. This paper analyzes online political communication by the right-wing populist movement Pegida and German political parties. We investigate to which extent parties attract supporters of Pegida, to which extent they address topics similar to Pegida and whether their topic use has become more similar over a period of almost two years. The empirical analysis is based on Facebook posts by main accounts and individual representatives of these political groups. We first show that there are considerable overlaps in the audiences of Pegida and the new challenger in the party system, AfD. Then we use topic models to characterize topic use by party and surveyed crowdworkers to which extent they perceive the identified topics as populist communication. The results show that while Pegida and AfD talk about rather unique topics and smaller parties engage to varying degrees with the topics populists emphasize, the two governing parties CDU and SPD clearly deemphasize those. Overall, the findings indicate that the considerable attention devoted to populist actors and shifts in public opinion due to the refugee crisis have left only moderate marks in political communication within the mainstream party system."
179,"In order to measure ideology, political scientists heavily rely on the so-called left-right scale. Left and right are, however, abstract political concepts and may trigger different associations among respondents. If these associations vary systematically with other variables this may induce bias in the empirical study of ideology. We illustrate this problem using a unique survey that asked respondents open-ended questions regarding the meanings they attribute to the concepts “left” and “right”. We assess and categorize this textual data using topic modeling techniques. Our analysis shows that variation in respondents’ associations is systematically related to their self-placement on the left-right scale and also to variables such as education and respondents’ cultural background (East vs. West Germany). Our findings indicate that the interpersonal comparability of the left-right scale across individuals is impaired. More generally, our study suggests that we need more research on how respondents interpret various abstract concepts that we regularly use in survey questions.",2017-09-01,2-s2.0-84991106875,Political Behavior,Is the Left-Right Scale a Valid Measure of Ideology?: Individual-Level Variation in Associations with “Left” and “Right” and Left-Right Self-Placement,"In order to measure ideology, political scientists heavily rely on the so-called left-right scale. Left and right are, however, abstract political concepts and may trigger different associations among respondents. If these associations vary systematically with other variables this may induce bias in the empirical study of ideology. We illustrate this problem using a unique survey that asked respondents open-ended questions regarding the meanings they attribute to the concepts “left” and “right”. We assess and categorize this textual data using topic modeling techniques. Our analysis shows that variation in respondents’ associations is systematically related to their self-placement on the left-right scale and also to variables such as education and respondents’ cultural background (East vs. West Germany). Our findings indicate that the interpersonal comparability of the left-right scale across individuals is impaired. More generally, our study suggests that we need more research on how respondents interpret various abstract concepts that we regularly use in survey questions."
180,"Various researchers have already engaged in using auxiliary side information within recommender applications to improve the quality and accuracy of recommendations. This side information has either been in the form of structured information such as product specifications and user demographic information or unstructured information such as product reviews. The abundance of unstructured information compared to structured information entices the use of such unstructured information in the recommendation process. Existing works that employ unstructured content have been confined to standard text modeling technique such as the use of frequency measures or topic modeling techniques. In this paper, we propose to model unstructured content about both products and users through the exploitation of word embedding techniques. More specifically, we propose to learn both user and product representations from any type of unstructured textual contents available in different external information sources using recurrent neural networks. We then apply our learnt product and user representations on two recommendation frameworks based on matrix factorization and link prediction to enhance the recommendation task. Experimental results on four datasets constructed from the Rotten Tomatoes website (movie review aggregator database) have shown the effectiveness of our proposed approach in different real-world situations compared to the state of the art.",2017-09-01,2-s2.0-85028733908,Electronic Commerce Research and Applications,Embedding unstructured side information in product recommendation,"Various researchers have already engaged in using auxiliary side information within recommender applications to improve the quality and accuracy of recommendations. This side information has either been in the form of structured information such as product specifications and user demographic information or unstructured information such as product reviews. The abundance of unstructured information compared to structured information entices the use of such unstructured information in the recommendation process. Existing works that employ unstructured content have been confined to standard text modeling technique such as the use of frequency measures or topic modeling techniques. In this paper, we propose to model unstructured content about both products and users through the exploitation of word embedding techniques. More specifically, we propose to learn both user and product representations from any type of unstructured textual contents available in different external information sources using recurrent neural networks. We then apply our learnt product and user representations on two recommendation frameworks based on matrix factorization and link prediction to enhance the recommendation task. Experimental results on four datasets constructed from the Rotten Tomatoes website (movie review aggregator database) have shown the effectiveness of our proposed approach in different real-world situations compared to the state of the art."
181,"This paper offers an overview of the bibliometric study of the domain of library and information science (LIS), with the aim of giving a multidisciplinary perspective of the topical boundaries and the main areas and research tendencies. Based on a retrospective and selective search, we have obtained the bibliographical references (title and abstract) of academic production on LIS in the database LISA in the period 1978–2014, which runs to 92,705 documents. In the context of the statistical technique of topic modeling, we apply latent Dirichlet allocation, in order to identify the main topics and categories in the corpus of documents analyzed. The quantitative results reveal the existence of 19 important topics, which can be grouped together into four main areas: processes, information technology, library and specific areas of information application.",2017-09-01,2-s2.0-85021116285,Scientometrics,Mapping the evolution of library and information science (1978–2014) using topic modeling on LISA,"This paper offers an overview of the bibliometric study of the domain of library and information science (LIS), with the aim of giving a multidisciplinary perspective of the topical boundaries and the main areas and research tendencies. Based on a retrospective and selective search, we have obtained the bibliographical references (title and abstract) of academic production on LIS in the database LISA in the period 1978–2014, which runs to 92,705 documents. In the context of the statistical technique of topic modeling, we apply latent Dirichlet allocation, in order to identify the main topics and categories in the corpus of documents analyzed. The quantitative results reveal the existence of 19 important topics, which can be grouped together into four main areas: processes, information technology, library and specific areas of information application."
182,"User-generated content becomes an alternative or supplementary source for public opinion studies. Nevertheless, the question whether modern methods of data processing can completely or partially replace opinion surveys remains unclear. The purpose of this article is to show methodological possibilities of topic modeling and opinion mining based on large text dataset analysis. The article provides a comparison between the analysis of YouTube user comments and the results of the public opinion survey devoted to the Chaika documentary produced by the Anti-Corruption Foundation. The study reveals that the analysis of opinions collected from the Internet cannot completely replace opinion surveys; however, it provides a broader context for interpreting the opinions and their more detailed assessments. Additionally, it can be used to improve the questionnaire structure and the quality of questions.",2017-09-01,2-s2.0-85037028041,Monitoring Obshchestvennogo Mneniya: Ekonomicheskie i Sotsial'nye Peremeny,Mining opinions on the internet: Can the text analysis methods replace public opinion polls?,"User-generated content becomes an alternative or supplementary source for public opinion studies. Nevertheless, the question whether modern methods of data processing can completely or partially replace opinion surveys remains unclear. The purpose of this article is to show methodological possibilities of topic modeling and opinion mining based on large text dataset analysis. The article provides a comparison between the analysis of YouTube user comments and the results of the public opinion survey devoted to the Chaika documentary produced by the Anti-Corruption Foundation. The study reveals that the analysis of opinions collected from the Internet cannot completely replace opinion surveys; however, it provides a broader context for interpreting the opinions and their more detailed assessments. Additionally, it can be used to improve the questionnaire structure and the quality of questions."
183,"Topic modeling is a common tool for understanding large bodies of text, but is typically provided as a “take it or leave it” proposition. Incorporating human knowledge in unsupervised learning is a promising approach to create high-quality topic models. Existing interactive systems and modeling algorithms support a wide range of refinement operations to express feedback. However, these systems’ interactions are primarily driven by algorithmic convenience, ignoring users who may lack expertise in topic modeling. To better understand how non-expert users understand, assess, and refine topics, we conducted two user studies—an in-person interview study and an online crowdsourced study. These studies demonstrate a disconnect between what non-expert users want and the complex, low-level operations that current interactive systems support. In particular, our findings include: (1) analysis of how non-expert users perceive topic models; (2) characterization of primary refinement operations expected by non-expert users and ordered by relative preference; (3) further evidence of the benefits of supporting users in directly refining a topic model; (4) design implications for future human-in-the-loop topic modeling interfaces.",2017-09-01,2-s2.0-85016065365,International Journal of Human Computer Studies,"The human touch: How non-expert users perceive, interpret, and fix topic models","Topic modeling is a common tool for understanding large bodies of text, but is typically provided as a “take it or leave it” proposition. Incorporating human knowledge in unsupervised learning is a promising approach to create high-quality topic models. Existing interactive systems and modeling algorithms support a wide range of refinement operations to express feedback. However, these systems’ interactions are primarily driven by algorithmic convenience, ignoring users who may lack expertise in topic modeling. To better understand how non-expert users understand, assess, and refine topics, we conducted two user studies—an in-person interview study and an online crowdsourced study. These studies demonstrate a disconnect between what non-expert users want and the complex, low-level operations that current interactive systems support. In particular, our findings include: (1) analysis of how non-expert users perceive topic models; (2) characterization of primary refinement operations expected by non-expert users and ordered by relative preference; (3) further evidence of the benefits of supporting users in directly refining a topic model; (4) design implications for future human-in-the-loop topic modeling interfaces."
184,"Energy is fundamental to human existence, and its ubiquity has allowed energy to enter cultural consciousness in a manner that is reflected in the stories that we tell ourselves and others. Modern society runs on fossil fuels like oil and coal, two resources that are frequently discussed in part due to their contribution to both positive and negative outcomes. This research uses a digital ecocritical approach to explore a corpus of 60 narratives, both fictional and nonfictional, published between 2002 and 2016 by US authors. We combine text mining methods, including sentiment analysis and topic modeling, with selected manual review of texts to posit that American narratives often depict oil as new and exciting, with associated dangers seen as tragic but thrilling. Appalachian coal, by contrast, is portrayed nostalgically, depicted as a nearly familial presence that has betrayed its communities and no longer represents security and prosperity. Thus, while oil is hypothetical and exciting, coal is real and disappointing. Latent cultural attitudes about these and other resources can provide insights as to how Americans view ongoing deployment of energy infrastructure. Further, understanding cultural context can help direct attention to issues of high significance to communities experiencing energy development.",2017-09-01,2-s2.0-85019662946,Energy Research and Social Science,Villainous or valiant? Depictions of oil and coal in American fiction and nonfiction narratives,"Energy is fundamental to human existence, and its ubiquity has allowed energy to enter cultural consciousness in a manner that is reflected in the stories that we tell ourselves and others. Modern society runs on fossil fuels like oil and coal, two resources that are frequently discussed in part due to their contribution to both positive and negative outcomes. This research uses a digital ecocritical approach to explore a corpus of 60 narratives, both fictional and nonfictional, published between 2002 and 2016 by US authors. We combine text mining methods, including sentiment analysis and topic modeling, with selected manual review of texts to posit that American narratives often depict oil as new and exciting, with associated dangers seen as tragic but thrilling. Appalachian coal, by contrast, is portrayed nostalgically, depicted as a nearly familial presence that has betrayed its communities and no longer represents security and prosperity. Thus, while oil is hypothetical and exciting, coal is real and disappointing. Latent cultural attitudes about these and other resources can provide insights as to how Americans view ongoing deployment of energy infrastructure. Further, understanding cultural context can help direct attention to issues of high significance to communities experiencing energy development."
185,"Tackling climate change requires both policy and individual action. Mobilizing such action can be made more optimal with knowledge about how the public views climate change solutions and what they think needs to be done in the face of climate change. Yet most public opinion research to date uses either closed questions about agreement with various pre-determined statements (such as views on science, worry, and support for given policy options) or use open-ended questions eliciting generic associations with climate change. This article uses an open-ended survey question in a probability-based Internet survey panel in Norway, analyzing 4634 textual responses to the question of “what should be done” about climate change. Using structural topic modeling (STM), we induce seven topics: Transportation, energy transition, attribution of climate change, emission reduction, the international dimension, lifestyle/consumption and government measures. We find that Norwegians strongly emphasize mitigation over adaptation, as few responses mention the latter topic. Also, men seem to externalize the solutions to climate change, emphasizing energy policies, the international dimension, and discussions about the causes of climate change, while women to a larger extent understand climate action as an issue involving individual behavior, calling for better public transportation and lifestyle changes. Overall, our results suggest a willingness to accept stronger mitigation action, but also that central and local governments need to facilitate low-carbon choices, bridging policy and individual action to mitigate climate change.",2017-09-01,2-s2.0-85026536586,Global Environmental Change,Citizens’ preferences for tackling climate change. Quantitative and qualitative analyses of their freely formulated solutions,"Tackling climate change requires both policy and individual action. Mobilizing such action can be made more optimal with knowledge about how the public views climate change solutions and what they think needs to be done in the face of climate change. Yet most public opinion research to date uses either closed questions about agreement with various pre-determined statements (such as views on science, worry, and support for given policy options) or use open-ended questions eliciting generic associations with climate change. This article uses an open-ended survey question in a probability-based Internet survey panel in Norway, analyzing 4634 textual responses to the question of “what should be done” about climate change. Using structural topic modeling (STM), we induce seven topics: Transportation, energy transition, attribution of climate change, emission reduction, the international dimension, lifestyle/consumption and government measures. We find that Norwegians strongly emphasize mitigation over adaptation, as few responses mention the latter topic. Also, men seem to externalize the solutions to climate change, emphasizing energy policies, the international dimension, and discussions about the causes of climate change, while women to a larger extent understand climate action as an issue involving individual behavior, calling for better public transportation and lifestyle changes. Overall, our results suggest a willingness to accept stronger mitigation action, but also that central and local governments need to facilitate low-carbon choices, bridging policy and individual action to mitigate climate change."
186,"This article proposes a reflexive approach on the scientific production in the field of game studies in recent years. It relies on a sociology of science perspective to answer the question: What are game studies really about? Relying on scientometric and lexicometric tools, we analyze the metadata and content of a corpus of articles from the journals Games Studies and Games & Culture and of Digital Games Research Association (DiGRA) proceedings. We show that published researches have been studying only a limited set of game genres and that they especially focus on online games. We then expose the different ways game studies are talking about games through a topic model analysis of our corpus. We test two hypotheses to explain the concentration of research on singular objects: path dependence and trading zone. We describe integrative properties of the focus on common objects but stress also the scientific limits met by this tendency.",2017-09-01,2-s2.0-85027579475,Games and Culture,What We Know About Games: A Scientometric Approach to Game Studies in the 2000s,"This article proposes a reflexive approach on the scientific production in the field of game studies in recent years. It relies on a sociology of science perspective to answer the question: What are game studies really about? Relying on scientometric and lexicometric tools, we analyze the metadata and content of a corpus of articles from the journals Games Studies and Games & Culture and of Digital Games Research Association (DiGRA) proceedings. We show that published researches have been studying only a limited set of game genres and that they especially focus on online games. We then expose the different ways game studies are talking about games through a topic model analysis of our corpus. We test two hypotheses to explain the concentration of research on singular objects: path dependence and trading zone. We describe integrative properties of the focus on common objects but stress also the scientific limits met by this tendency."
187,"Microblogs, such as Twitter, are a way for users to express their opinions or share pieces of interesting news by posting relatively short messages (corpus) compared with the regular blogs. The volume of corpus updates that users receive daily is overwhelming. Also, as information diffuses from one user to another, some topics become of interest to only small groups of users, thus do not become widely adopted, and could fade away quickly. This paper proposes a framework to enhance user's interaction and experience in social networks. It first introduces a model that provides better subscription to the user through a dynamic personalized recommendation system that provides the user with the most important tweets. This paper also presents TrendFusion, an innovative model used to enhance the suggestions provided by the social media to the users. It analyzes, predicts the localized diffusion of trends in social networks, and recommends the most interesting trends to the user. Our performance evaluation demonstrates the effectiveness of the proposed recommendation system and shows that it improves the precision and recall of identifying important tweets by up to 36% and 80%, respectively. Results also show that TrendFusion accurately predicts places in which a trend will appear, with 98% recall and 80% precision.",2017-09-01,2-s2.0-85028908881,IEEE Transactions on Computational Social Systems,Personalized Recommendation for Online Social Networks Information: Personal Preferences and Location-Based Community Trends,"Microblogs, such as Twitter, are a way for users to express their opinions or share pieces of interesting news by posting relatively short messages (corpus) compared with the regular blogs. The volume of corpus updates that users receive daily is overwhelming. Also, as information diffuses from one user to another, some topics become of interest to only small groups of users, thus do not become widely adopted, and could fade away quickly. This paper proposes a framework to enhance user's interaction and experience in social networks. It first introduces a model that provides better subscription to the user through a dynamic personalized recommendation system that provides the user with the most important tweets. This paper also presents TrendFusion, an innovative model used to enhance the suggestions provided by the social media to the users. It analyzes, predicts the localized diffusion of trends in social networks, and recommends the most interesting trends to the user. Our performance evaluation demonstrates the effectiveness of the proposed recommendation system and shows that it improves the precision and recall of identifying important tweets by up to 36% and 80%, respectively. Results also show that TrendFusion accurately predicts places in which a trend will appear, with 98% recall and 80% precision."
188,"Big data is one of the key transformative factors which increasingly influences all aspects of modern life. Although this transformation brings vast opportunities it also generates novel challenges, not the least of which is organizing and searching this data deluge. The field of medicinal chemistry is not different: more and more data are being generated, for instance, by technologies such as DNA encoded libraries, peptide libraries, text mining of large literature corpora, and new in silico enumeration methods. Handling those huge sets of molecules effectively is quite challenging and requires compromises that often come at the expense of the interpretability of the results. In order to find an intuitive and meaningful approach to organizing large molecular data sets, we adopted a probabilistic framework called ""topic modeling"" from the text-mining field. Here we present the first chemistry-related implementation of this method, which allows large molecule sets to be assigned to ""chemical topics"" and investigating the relationships between those. In this first study, we thoroughly evaluate this novel method in different experiments and discuss both its disadvantages and advantages. We show very promising results in reproducing human-assigned concepts using the approach to identify and retrieve chemical series from sets of molecules. We have also created an intuitive visualization of the chemical topics output by the algorithm. This is a huge benefit compared to other unsupervised machine-learning methods, like clustering, which are commonly used to group sets of molecules. Finally, we applied the new method to the 1.6 million molecules of the ChEMBL22 data set to test its robustness and efficiency. In about 1 h we built a 100-topic model of this large data set in which we could identify interesting topics like ""proteins"", ""DNA"", or ""steroids"". Along with this publication we provide our data sets and an open-source implementation of the new method (CheTo) which will be part of an upcoming version of the open-source cheminformatics toolkit RDKit.",2017-08-28,2-s2.0-85028567881,Journal of Chemical Information and Modeling,Chemical Topic Modeling: Exploring Molecular Data Sets Using a Common Text-Mining Approach,"Big data is one of the key transformative factors which increasingly influences all aspects of modern life. Although this transformation brings vast opportunities it also generates novel challenges, not the least of which is organizing and searching this data deluge. The field of medicinal chemistry is not different: more and more data are being generated, for instance, by technologies such as DNA encoded libraries, peptide libraries, text mining of large literature corpora, and new in silico enumeration methods. Handling those huge sets of molecules effectively is quite challenging and requires compromises that often come at the expense of the interpretability of the results. In order to find an intuitive and meaningful approach to organizing large molecular data sets, we adopted a probabilistic framework called ""topic modeling"" from the text-mining field. Here we present the first chemistry-related implementation of this method, which allows large molecule sets to be assigned to ""chemical topics"" and investigating the relationships between those. In this first study, we thoroughly evaluate this novel method in different experiments and discuss both its disadvantages and advantages. We show very promising results in reproducing human-assigned concepts using the approach to identify and retrieve chemical series from sets of molecules. We have also created an intuitive visualization of the chemical topics output by the algorithm. This is a huge benefit compared to other unsupervised machine-learning methods, like clustering, which are commonly used to group sets of molecules. Finally, we applied the new method to the 1.6 million molecules of the ChEMBL22 data set to test its robustness and efficiency. In about 1 h we built a 100-topic model of this large data set in which we could identify interesting topics like ""proteins"", ""DNA"", or ""steroids"". Along with this publication we provide our data sets and an open-source implementation of the new method (CheTo) which will be part of an upcoming version of the open-source cheminformatics toolkit RDKit."
189,"Students' reflective writings are useful not only for students themselves but also teachers. It is important for teachers to know which concepts were understood well by students and which concepts were not, to continuously improve their classes. However, it is difficult for teachers to thoroughly read the journals of more than one hundred students. In this paper, we propose a novel method to extract common topics and students' common impressions against them from students' journals. Weekly keywords are discovered from journals by scoring noun words with a measure based on TF-IDF term weighting scheme, and then we analyze co-occurrence relationships between extracted keywords and adjectives. We employs nonnegative matrix factorization, one of the topic modeling techniques, to discover the hidden impression topics from the co-occurrence relationships. As a case study, we applied our method on students' journals of the course 'Information Science' held in our university. Our experimental results show that conceptual keywords are successfully extracted, and four significant impression topics are identified. We conclude that our analysis method can be used to collectively understand the impressions of students from journal texts.",2017-08-03,2-s2.0-85030260591,"Proceedings - IEEE 17th International Conference on Advanced Learning Technologies, ICALT 2017",Revealing Hidden Impression Topics in Students' Journals Based on Nonnegative Matrix Factorization,"Students' reflective writings are useful not only for students themselves but also teachers. It is important for teachers to know which concepts were understood well by students and which concepts were not, to continuously improve their classes. However, it is difficult for teachers to thoroughly read the journals of more than one hundred students. In this paper, we propose a novel method to extract common topics and students' common impressions against them from students' journals. Weekly keywords are discovered from journals by scoring noun words with a measure based on TF-IDF term weighting scheme, and then we analyze co-occurrence relationships between extracted keywords and adjectives. We employs nonnegative matrix factorization, one of the topic modeling techniques, to discover the hidden impression topics from the co-occurrence relationships. As a case study, we applied our method on students' journals of the course 'Information Science' held in our university. Our experimental results show that conceptual keywords are successfully extracted, and four significant impression topics are identified. We conclude that our analysis method can be used to collectively understand the impressions of students from journal texts."
190,"This paper introduces topic modelling, a machine learning technique that automatically identifies 'topics' in a given corpus. The paper illustrates its use in the exploration of a corpus of academic English. It first offers the intuitive explanation of the underlying mechanism of topic modelling and describes the procedure for building a model, including the decisions involved in the model-building process. The paper then explores the model. A topic in topic models is characterised by a set of co-occurring words, and we will demonstrate that such topics bring us rich insights into the nature of a corpus. As exemplary tasks, this paper identifies the prominent topics in different parts of papers, investigates the chronological change of a journal, and reveals different types of papers in the journal. The paper further compares topic modelling to two more traditional techniques in corpus linguistics, semantic annotation and keywords analysis, and highlights the strengths of topic modelling.We believe that topic modelling is particularly useful in the initial exploration of a corpus.",2017-08-01,2-s2.0-85026661755,Corpora,'What is this corpus about?': Using topic modelling to explore a specialised corpus,"This paper introduces topic modelling, a machine learning technique that automatically identifies 'topics' in a given corpus. The paper illustrates its use in the exploration of a corpus of academic English. It first offers the intuitive explanation of the underlying mechanism of topic modelling and describes the procedure for building a model, including the decisions involved in the model-building process. The paper then explores the model. A topic in topic models is characterised by a set of co-occurring words, and we will demonstrate that such topics bring us rich insights into the nature of a corpus. As exemplary tasks, this paper identifies the prominent topics in different parts of papers, investigates the chronological change of a journal, and reveals different types of papers in the journal. The paper further compares topic modelling to two more traditional techniques in corpus linguistics, semantic annotation and keywords analysis, and highlights the strengths of topic modelling.We believe that topic modelling is particularly useful in the initial exploration of a corpus."
191,"Many applications require semantic understanding of short texts, and inferring discriminative and coherent latent topics is a critical and fundamental task in these applications. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Recent studies show that the Dirichlet Multinomial Mixture (DMM) model is effective for topic inference over short texts by assuming that each piece of short text is generated by a single topic. However, DMM has two main limitations. First, even though it seems reasonable to assume that each short text has only one topic because of its shortness, the definition of ""shortness"" is subjective and the length of the short texts is dataset dependent. That is, the single-topic assumption may be too strong for some datasets. To address this limitation, we propose to model the topic number as a Poisson distribution, allowing each short text to be associated with a small number of topics (e.g., one to three topics). This model is named PDMM. Second, DMM (and also PDMM) does not have access to background knowledge (e.g., semantic relations between words) when modeling short texts.When a human being interprets a piece of short text, the understanding is not solely based on its content words, but also their semantic relations. Recent advances in word embeddings offer effective learning of word semantic relations from a large corpus. Such auxiliary word embeddings enable us to address the second limitation. To this end, we propose to promote the semantically related words under the same topic during the sampling process, by using the generalized Polya urn (GPU) model. Through the GPU model, background knowledge about word semantic relations learned from millions of external documents can be easily exploited to improve topic modeling for short texts. By directly extending the PDMM model with the GPU model, we propose two more effective topic models for short texts, named GPU-DMM and GPU-PDMM. Through extensive experiments on two real-world short text collections in two languages, we demonstrate that PDMM achieves better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to better accuracy in a text classification task, as an indirect evaluation. Both GPU-DMM and GPU-PDMM further improve topic coherence and text classification accuracy. GPUPDMM outperforms GPU-DMM at the price of higher computational costs.",2017-08-01,2-s2.0-85028533702,ACM Transactions on Information Systems,Enhancing topic modeling for short texts with auxiliary word embeddings,"Many applications require semantic understanding of short texts, and inferring discriminative and coherent latent topics is a critical and fundamental task in these applications. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Recent studies show that the Dirichlet Multinomial Mixture (DMM) model is effective for topic inference over short texts by assuming that each piece of short text is generated by a single topic. However, DMM has two main limitations. First, even though it seems reasonable to assume that each short text has only one topic because of its shortness, the definition of ""shortness"" is subjective and the length of the short texts is dataset dependent. That is, the single-topic assumption may be too strong for some datasets. To address this limitation, we propose to model the topic number as a Poisson distribution, allowing each short text to be associated with a small number of topics (e.g., one to three topics). This model is named PDMM. Second, DMM (and also PDMM) does not have access to background knowledge (e.g., semantic relations between words) when modeling short texts.When a human being interprets a piece of short text, the understanding is not solely based on its content words, but also their semantic relations. Recent advances in word embeddings offer effective learning of word semantic relations from a large corpus. Such auxiliary word embeddings enable us to address the second limitation. To this end, we propose to promote the semantically related words under the same topic during the sampling process, by using the generalized Polya urn (GPU) model. Through the GPU model, background knowledge about word semantic relations learned from millions of external documents can be easily exploited to improve topic modeling for short texts. By directly extending the PDMM model with the GPU model, we propose two more effective topic models for short texts, named GPU-DMM and GPU-PDMM. Through extensive experiments on two real-world short text collections in two languages, we demonstrate that PDMM achieves better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to better accuracy in a text classification task, as an indirect evaluation. Both GPU-DMM and GPU-PDMM further improve topic coherence and text classification accuracy. GPUPDMM outperforms GPU-DMM at the price of higher computational costs."
192,"The dynamics of knowledge transfer is an important topic for engineering managers. In this paper, we study knowledge boundaries - barriers to knowledge transfer - in groups of experts, using topic modeling, a natural language processing technique, applied to transcript data from the U.S. Food and Drug Administration's Circulatory Systems Advisory Panel. As predicted by prior theory, we find that knowledge boundaries emerge as the group faces increasingly challenging problems. Beyond this theory, we find that knowledge boundaries cease to structure communications between communities of practice when the group's expert ability is insufficient to solve its task, such as in the presence of high novelty. We conjecture that the amount of expert knowledge that the group can collectively bring to bear is a determining factor in boundary formation. This implies that some of the factors underlying knowledge boundary formation may aid - rather than hinder - knowledge aggregation. We briefly explore this conjecture using qualitative exploration of several relevant meetings. Finally, we discuss the implications of these results for organizations attempting to leverage their expertise given the state of their collective knowledge.",2017-08-01,2-s2.0-85018501372,IEEE Transactions on Engineering Management,The Emergence and Collapse of Knowledge Boundaries,"The dynamics of knowledge transfer is an important topic for engineering managers. In this paper, we study knowledge boundaries - barriers to knowledge transfer - in groups of experts, using topic modeling, a natural language processing technique, applied to transcript data from the U.S. Food and Drug Administration's Circulatory Systems Advisory Panel. As predicted by prior theory, we find that knowledge boundaries emerge as the group faces increasingly challenging problems. Beyond this theory, we find that knowledge boundaries cease to structure communications between communities of practice when the group's expert ability is insufficient to solve its task, such as in the presence of high novelty. We conjecture that the amount of expert knowledge that the group can collectively bring to bear is a determining factor in boundary formation. This implies that some of the factors underlying knowledge boundary formation may aid - rather than hinder - knowledge aggregation. We briefly explore this conjecture using qualitative exploration of several relevant meetings. Finally, we discuss the implications of these results for organizations attempting to leverage their expertise given the state of their collective knowledge."
193,"With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals.",2017-08-01,2-s2.0-85028542544,ACM Transactions on Information Systems,Mining exploratory behavior to improve mobile app recommendations,"With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals."
194,"Science and technology (S&T) linkages have been studied extensively using patent and scientific publication databases. Existing methods used to track S&T linkages, such as analysis of non-patent literature (NPL) or author-inventor matching offer a narrow window for industry level analysis of the data. This paper examines the application of a machine learning algorithm, namely Latent Dirichlet Allocation, to detect the semantic relationship between patent and scientific publication corpus. The case of 'Taxol', a cancer drug, is used to illustrate the performance of the unsupervised algorithm in clustering documents with similar topics. In total 26 475 documents retrieved from the Europe PMC database was used a sample for the analysis. Qualitative analysis of the clusters shows that the topic clustering algorithm is valuable approach in detection of patent and publication linkage.",2017-07-31,2-s2.0-85028575629,"2017 IEEE Technology and Engineering Management Society Conference, TEMSCON 2017",A topic model analysis of science and technology linkages: A case study in pharmaceutical industry,"Science and technology (S&T) linkages have been studied extensively using patent and scientific publication databases. Existing methods used to track S&T linkages, such as analysis of non-patent literature (NPL) or author-inventor matching offer a narrow window for industry level analysis of the data. This paper examines the application of a machine learning algorithm, namely Latent Dirichlet Allocation, to detect the semantic relationship between patent and scientific publication corpus. The case of 'Taxol', a cancer drug, is used to illustrate the performance of the unsupervised algorithm in clustering documents with similar topics. In total 26 475 documents retrieved from the Europe PMC database was used a sample for the analysis. Qualitative analysis of the clusters shows that the topic clustering algorithm is valuable approach in detection of patent and publication linkage."
195,"Technology assessment and planning requires that we can reliably, but indirectly, measure knowledge embedded in the organization. Operationalizing knowledge embedded into companies is increasingly challenging but also more and more relevant in the current cross-disciplinary and complex technological environment. Existing approaches for operationalizing company knowledge are based on patent data and analyzing patent classifications. These approaches have, however, significant limitations. In this study, knowledge depth and breadth is studied using full-text patent data from seven large telecommunication companies totaling 157,718 patents. The data was analyzed with Latent Dirichlet Allocation, an unsupervised learning method. The results are quantified using a technological diversity metric, showing temporal changes in companies knowledge. The result show how the operationalization of company knowledge is independent of patent count and that companies have their specific trajectory of knowledge development. The approach offers a novel method of analyzing the knowledge trajectory of a company, compared to existing patent classification based methods.",2017-07-31,2-s2.0-85028575610,"2017 IEEE Technology and Engineering Management Society Conference, TEMSCON 2017",Topic modelling approach to knowledge depth and breadth: Analyzing trajectories of technological knowledge,"Technology assessment and planning requires that we can reliably, but indirectly, measure knowledge embedded in the organization. Operationalizing knowledge embedded into companies is increasingly challenging but also more and more relevant in the current cross-disciplinary and complex technological environment. Existing approaches for operationalizing company knowledge are based on patent data and analyzing patent classifications. These approaches have, however, significant limitations. In this study, knowledge depth and breadth is studied using full-text patent data from seven large telecommunication companies totaling 157,718 patents. The data was analyzed with Latent Dirichlet Allocation, an unsupervised learning method. The results are quantified using a technological diversity metric, showing temporal changes in companies knowledge. The result show how the operationalization of company knowledge is independent of patent count and that companies have their specific trajectory of knowledge development. The approach offers a novel method of analyzing the knowledge trajectory of a company, compared to existing patent classification based methods."
196,"This paper examines the social media strategies of candidates seeking their party’s nomination for the 2016 U.S. presidential election. We use textual analysis to understand what candidates focused on. We assess eight themes covered in Twitter posts. For example, Clinton focused on GUN CONTROL, while Sanders focused on climate change. Using Facebook data, we introduce a topic modeling approach, latent Dirichlet allocation, to the political marketing literature. This allows us to uncover what topics the candidates focus on without researcher intervention and, using a dynamic model, show how this changes over time. We note that Clinton’s focus on Trump increases toward the end of the primary campaign.",2017-07-29,2-s2.0-85026518584,Journal of Political Marketing,Understanding the social media strategies of U.S. primary candidates,"This paper examines the social media strategies of candidates seeking their party’s nomination for the 2016 U.S. presidential election. We use textual analysis to understand what candidates focused on. We assess eight themes covered in Twitter posts. For example, Clinton focused on GUN CONTROL, while Sanders focused on climate change. Using Facebook data, we introduce a topic modeling approach, latent Dirichlet allocation, to the political marketing literature. This allows us to uncover what topics the candidates focus on without researcher intervention and, using a dynamic model, show how this changes over time. We note that Clinton’s focus on Trump increases toward the end of the primary campaign."
197,"As more and more companies become aware of the benefits of collecting and analyzing data, hiring employee with data analytics expertise is a key issue faced by HR practitioners. Although previous research empirically highlighted the differences of knowledge and skill requirements between big data (BD) and business intelligence (BI) in English-speaking countries, limited similar study is conducted in China. By analyzing and interpreting the topic modeling results with dataset extracted from online job recruitment website Zhaopin.com, this exploratory study reveals that (1) the demand for BD competencies is far bigger (nearly six times) than the demand for BI competencies in China job market; (2) for Chinese employer, hard skills, especially those advanced analytic and programming skills, are much emphasized when HR searching for qualified BD applicants; (3) given the large volume and unstructured nature of big data, BD investments are currently much more human-capital-intensive than BI projects are. Our findings not only enhance the understanding of similarities and differences in BI and BD areas of competency but also provide guidance for organizations to choose suitable HR decisions.",2017-07-28,2-s2.0-85028619831,"14th International Conference on Services Systems and Services Management, ICSSSM 2017 - Proceedings",Are big data talents different from business intelligence expertise?: Evidence from text mining using job recruitment advertisements,"As more and more companies become aware of the benefits of collecting and analyzing data, hiring employee with data analytics expertise is a key issue faced by HR practitioners. Although previous research empirically highlighted the differences of knowledge and skill requirements between big data (BD) and business intelligence (BI) in English-speaking countries, limited similar study is conducted in China. By analyzing and interpreting the topic modeling results with dataset extracted from online job recruitment website Zhaopin.com, this exploratory study reveals that (1) the demand for BD competencies is far bigger (nearly six times) than the demand for BI competencies in China job market; (2) for Chinese employer, hard skills, especially those advanced analytic and programming skills, are much emphasized when HR searching for qualified BD applicants; (3) given the large volume and unstructured nature of big data, BD investments are currently much more human-capital-intensive than BI projects are. Our findings not only enhance the understanding of similarities and differences in BI and BD areas of competency but also provide guidance for organizations to choose suitable HR decisions."
198,"Social tags are user-defined keywords associated with online content that reflect consumers' perceptions of various objects, including products and brands. This research presents a new approach for harvesting rich, qualitative information on brands from user-generated social tags. The authors first compare their proposed approach with conventional techniques such as brand concept maps and text mining. They highlight the added value of their approach that results from the unconstrained, open-ended, and synoptic nature of consumer-generated content contained within social tags. The authors then apply existing text-mining and data-reduction methods to analyze disaggregate-level social tagging data for marketing research and demonstrate how marketers can utilize the information in social tags by extracting key representative topics, monitoring common dynamic trends, and understanding heterogeneous perceptions of a brand.",2017-07-01,2-s2.0-85023206466,Journal of Marketing,Harvesting brand information from social Tags,"Social tags are user-defined keywords associated with online content that reflect consumers' perceptions of various objects, including products and brands. This research presents a new approach for harvesting rich, qualitative information on brands from user-generated social tags. The authors first compare their proposed approach with conventional techniques such as brand concept maps and text mining. They highlight the added value of their approach that results from the unconstrained, open-ended, and synoptic nature of consumer-generated content contained within social tags. The authors then apply existing text-mining and data-reduction methods to analyze disaggregate-level social tagging data for marketing research and demonstrate how marketers can utilize the information in social tags by extracting key representative topics, monitoring common dynamic trends, and understanding heterogeneous perceptions of a brand."
199,"In this article we employ bibliometric techniques to examine the authorial, disciplinary, bibliographical, and thematic profiles of the Journal DADOS, from its foundation to the present (1966-2015). The database was amassed using two different methods, the periodical’s most recent output was scraped from the website SciELO in XML format while the older issues had to be scanned and its bibliography extracted by painstakingly copying and pasting each entry. We employed two techniques to analyze the material: Correspondence Analysis, to determine the patterns of co-citations and thus to chart the bibliographical map of the periodical, and Topic Modeling, to identify the main themes and approaches in the corpus. The results show that Dados privileged themes and debates that were most relevant in each period, in a constant attempt to bring together methodological rigor and public relevance.",2017-07-01,2-s2.0-85041008149,Dados,Fifty years of revista DADOS: A bibliometric study of its disciplinary and thematic profiles Les 50 Ans de la Revue DADOS: Une Analyse Bibliométrique de son Profil Disciplinaire et Thématique 50 Anos da Revista DADOS: Uma Análise Bibliométrica do seu Perfil Disciplinar e Temático 50 Años de la Revista DADOS: Un Análisis Bibliométrico de su Perfil Disciplinario y Temático,"In this article we employ bibliometric techniques to examine the authorial, disciplinary, bibliographical, and thematic profiles of the Journal DADOS, from its foundation to the present (1966-2015). The database was amassed using two different methods, the periodical’s most recent output was scraped from the website SciELO in XML format while the older issues had to be scanned and its bibliography extracted by painstakingly copying and pasting each entry. We employed two techniques to analyze the material: Correspondence Analysis, to determine the patterns of co-citations and thus to chart the bibliographical map of the periodical, and Topic Modeling, to identify the main themes and approaches in the corpus. The results show that Dados privileged themes and debates that were most relevant in each period, in a constant attempt to bring together methodological rigor and public relevance."
200,"With the overflowing of Short Message Service (SMS) spam nowadays, many traditional text classification algorithms are used for SMS spam filtering. Nevertheless, because the content of SMS spam messages are miscellaneous and distinct from general text files, such as more shorter, usually including mass of abbreviations, symbols, variant words and distort or deform sentences, the traditional classifiers aren't fit for the task of SMS spam filtering. In this paper, the authors propose a Short Message Biterm Topic Model (SM-BTM) which can be used to automatically learn latent semantic features from SMS spam corpus for the task of SMS spam filtering. The SM-BTM is based on the probability of topic model theory and Biterm Topic Model (BTM). The experiments in this work show the proposed model SM-BTM can acquire higher quality of topic features than the original BTM, and is more suitable for identifying the miscellaneous SMS spam.",2017-07-01,2-s2.0-85019101927,International Journal of Business Data Communications and Networking,Bi-term topic model for SMS classification,"With the overflowing of Short Message Service (SMS) spam nowadays, many traditional text classification algorithms are used for SMS spam filtering. Nevertheless, because the content of SMS spam messages are miscellaneous and distinct from general text files, such as more shorter, usually including mass of abbreviations, symbols, variant words and distort or deform sentences, the traditional classifiers aren't fit for the task of SMS spam filtering. In this paper, the authors propose a Short Message Biterm Topic Model (SM-BTM) which can be used to automatically learn latent semantic features from SMS spam corpus for the task of SMS spam filtering. The SM-BTM is based on the probability of topic model theory and Biterm Topic Model (BTM). The experiments in this work show the proposed model SM-BTM can acquire higher quality of topic features than the original BTM, and is more suitable for identifying the miscellaneous SMS spam."
201,"Tracking how discussion topics evolve in social media and where these topics are discussed geographically over time has the potential to provide useful information for many different purposes. In crisis management, knowing a specific topic's current geographical location could provide vital information to where, or even which, resources should be allocated. This paper describes an attempt to track online discussions geographically over time. A distributed geo-aware streaming latent Dirichlet allocation model was developed for the purpose of recognizing topics' locations in unstructured text. To evaluate the model it has been implemented and used for automatic discovery and geographical tracking of election topics during parts of the 2016 American presidential primary elections. It was shown that the locations correlated with the actual election locations, and that the model provides a better geolocation classification compared to using a keyword-based approach.",2017-07-01,2-s2.0-85020801622,Decision Support Systems,Tracking geographical locations using a geo-aware topic model for analyzing social media data,"Tracking how discussion topics evolve in social media and where these topics are discussed geographically over time has the potential to provide useful information for many different purposes. In crisis management, knowing a specific topic's current geographical location could provide vital information to where, or even which, resources should be allocated. This paper describes an attempt to track online discussions geographically over time. A distributed geo-aware streaming latent Dirichlet allocation model was developed for the purpose of recognizing topics' locations in unstructured text. To evaluate the model it has been implemented and used for automatic discovery and geographical tracking of election topics during parts of the 2016 American presidential primary elections. It was shown that the locations correlated with the actual election locations, and that the model provides a better geolocation classification compared to using a keyword-based approach."
202,"Analyzing large quantities of real-world textual data has the potential to provide new insights for researchers. However, such data present challenges for both human and computational methods, requiring a diverse range of specialist skills, often shared across a number of individuals. In this paper we use the analysis of a real-world data set as our case study, and use this exploration as a demonstration of our “insight workflow,” which we present for use and adaptation by other researchers. The data we use are impact case study documents collected as part of the UK Research Excellence Framework (REF), consisting of 6,679 documents and 6.25 million words; the analysis was commissioned by the Higher Education Funding Council for England (published as report HEFCE 2015). In our exploration and analysis we used a variety of techniques, ranging from keyword in context and frequency information to more sophisticated methods (topic modeling), with these automated techniques providing an empirical point of entry for in-depth and intensive human analysis. We present the 60 topics to demonstrate the output of our methods, and illustrate how the variety of analysis techniques can be combined to provide insights. We note potential limitations and propose future work.",2017-07-01,2-s2.0-85018308848,Journal of the Association for Information Science and Technology,Insight workflow: Systematically combining human and computational methods to explore textual data,"Analyzing large quantities of real-world textual data has the potential to provide new insights for researchers. However, such data present challenges for both human and computational methods, requiring a diverse range of specialist skills, often shared across a number of individuals. In this paper we use the analysis of a real-world data set as our case study, and use this exploration as a demonstration of our “insight workflow,” which we present for use and adaptation by other researchers. The data we use are impact case study documents collected as part of the UK Research Excellence Framework (REF), consisting of 6,679 documents and 6.25 million words; the analysis was commissioned by the Higher Education Funding Council for England (published as report HEFCE 2015). In our exploration and analysis we used a variety of techniques, ranging from keyword in context and frequency information to more sophisticated methods (topic modeling), with these automated techniques providing an empirical point of entry for in-depth and intensive human analysis. We present the 60 topics to demonstrate the output of our methods, and illustrate how the variety of analysis techniques can be combined to provide insights. We note potential limitations and propose future work."
203,"In universities worldwide, instructors may spend a significant amount of time reviewing homework and group projects submitted by their students. Web-based technologies, like Google Docs, have provided a platform for students to write documents collaboratively. Currently, those platforms provide limited information on the individual contribution made by each student. Previous studies have focused on the quantitative aspects of individuals' contribution in collaborative writing, while the quality aspect has received less attention. In this paper, we propose a new model to measure not only quantitative input but also the quality of the content that has been contributed to a document written collaboratively in Spanish language. Based on topics-modeling techniques, we use an adaptive non-negative matrix factorization (NMF) model to extract topics from the content of the document, and grade higher students making those contributions. Using Google documents submitted by students to the academic system of our university as part of their projects, experimental results show that compared to other baseline methods such as edits or words count, our model provide a better approximation to the scores given by human reviewers. Therefore, our model can be used as part of an automatic grading subsystem within the academic system, to provide a baseline score of students' contribution in collaborative documents. This will allow instructors to reduce their workload associated with revision and grading of documents and focus their time on more relevant tasks.",2017-06-29,2-s2.0-85026836526,"2017 4th International Conference on eDemocracy and eGovernment, ICEDEG 2017",Measuring contribution in collaborative writing: An adaptive NMF topic modelling approach,"In universities worldwide, instructors may spend a significant amount of time reviewing homework and group projects submitted by their students. Web-based technologies, like Google Docs, have provided a platform for students to write documents collaboratively. Currently, those platforms provide limited information on the individual contribution made by each student. Previous studies have focused on the quantitative aspects of individuals' contribution in collaborative writing, while the quality aspect has received less attention. In this paper, we propose a new model to measure not only quantitative input but also the quality of the content that has been contributed to a document written collaboratively in Spanish language. Based on topics-modeling techniques, we use an adaptive non-negative matrix factorization (NMF) model to extract topics from the content of the document, and grade higher students making those contributions. Using Google documents submitted by students to the academic system of our university as part of their projects, experimental results show that compared to other baseline methods such as edits or words count, our model provide a better approximation to the scores given by human reviewers. Therefore, our model can be used as part of an automatic grading subsystem within the academic system, to provide a baseline score of students' contribution in collaborative documents. This will allow instructors to reduce their workload associated with revision and grading of documents and focus their time on more relevant tasks."
204,The proceedings contain 47 papers. The topics discussed include: quality evaluation of government website; design and implementation of a wireless sensor network to detect forest fires; learning analytic in a smart classroom to improve the e-education; e-collaborating with sigma. the measurement of the impact of social actions for democratizing information; increasing public value through co-creation of open knowledge; data mining model in the discovery of trends and patterns of intruder attacks on the data network as a public-sector innovation; measuring contribution in collaborative writing: an adaptive NMF topic modeling approach; use of embedded markup for semantic annotations in e-government and e-education websites; and how online lurking contributes value to e-participation. A conceptual approach to evaluating the role of lurkers in e-participation.,2017-06-29,2-s2.0-85026821414,"2017 4th International Conference on eDemocracy and eGovernment, ICEDEG 2017","2017 4th International Conference on eDemocracy and eGovernment, ICEDEG 2017",The proceedings contain 47 papers. The topics discussed include: quality evaluation of government website; design and implementation of a wireless sensor network to detect forest fires; learning analytic in a smart classroom to improve the e-education; e-collaborating with sigma. the measurement of the impact of social actions for democratizing information; increasing public value through co-creation of open knowledge; data mining model in the discovery of trends and patterns of intruder attacks on the data network as a public-sector innovation; measuring contribution in collaborative writing: an adaptive NMF topic modeling approach; use of embedded markup for semantic annotations in e-government and e-education websites; and how online lurking contributes value to e-participation. A conceptual approach to evaluating the role of lurkers in e-participation.
205,"In order to generate user's information needs from a collection of documents, many term-based and pattern-based approaches have been used in Information Filtering. In these approaches, the documents in the collection are all about one topic. However, user's interests can be diverse and the documents in the collection often involve multiple topics. Topic modeling is useful for the area of machine learning and text mining. It generates models to discover the hidden multiple topics in a collection of documents and each of these topics are presented by distribution of words. But its effectiveness in information filtering has not been so well explored. Patterns are always thought to be more discriminative than single terms for describing documents. The major challenge found in frequent pattern mining is a large number of result patterns. As the minimum threshold becomes lower, an exponentially large number of patterns are generated. To deal with the above mentioned limitations and problems, in this paper, a novel information filtering model, EFITM (Enhanced Frequent Itemsets based on Topic Model) model is proposed. Experimental results using the CRANFIELD dataset for the task of information filtering show that the proposed model outperforms over state-of-the-art models.",2017-06-27,2-s2.0-85030652848,"Proceedings - 16th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2017",Enhanced frequent itemsets based on topic modeling in information filtering,"In order to generate user's information needs from a collection of documents, many term-based and pattern-based approaches have been used in Information Filtering. In these approaches, the documents in the collection are all about one topic. However, user's interests can be diverse and the documents in the collection often involve multiple topics. Topic modeling is useful for the area of machine learning and text mining. It generates models to discover the hidden multiple topics in a collection of documents and each of these topics are presented by distribution of words. But its effectiveness in information filtering has not been so well explored. Patterns are always thought to be more discriminative than single terms for describing documents. The major challenge found in frequent pattern mining is a large number of result patterns. As the minimum threshold becomes lower, an exponentially large number of patterns are generated. To deal with the above mentioned limitations and problems, in this paper, a novel information filtering model, EFITM (Enhanced Frequent Itemsets based on Topic Model) model is proposed. Experimental results using the CRANFIELD dataset for the task of information filtering show that the proposed model outperforms over state-of-the-art models."
206,"This study traces how Facebook-promoted internet.org/Free Basics, despite initial acclaim, was eventually rejected in India – and how net neutrality came to be codified in the process. Topic modeling of articles (N = 1752) published over two-and-a-half years in 100 media outlets pinpoints the critical junctures in time at which the public discourse changed its trajectory. Critical discourse analysis of different phases of the discourse then identifies the causal factors and contingent conditions that produced the new policy. The study advances an understanding of technologies as social constructs and technological change as a social process, shaped by the dynamic interaction of a complex array of social actors coming together at critical junctures. It also draws attention to how discourse, produced by social actors in contingent conditions, recursively shapes the dominant ideology and structures these interactions. In addition, the study demonstrates how algorithmic and interpretive research techniques can be combined for longitudinal analysis of textual data sets.",2017-06-27,2-s2.0-85021261754,Information Communication and Society,"Facing up to Facebook: how digital activism, independent regulation, and mass media foiled a neoliberal threat to net neutrality","This study traces how Facebook-promoted internet.org/Free Basics, despite initial acclaim, was eventually rejected in India – and how net neutrality came to be codified in the process. Topic modeling of articles (N = 1752) published over two-and-a-half years in 100 media outlets pinpoints the critical junctures in time at which the public discourse changed its trajectory. Critical discourse analysis of different phases of the discourse then identifies the causal factors and contingent conditions that produced the new policy. The study advances an understanding of technologies as social constructs and technological change as a social process, shaped by the dynamic interaction of a complex array of social actors coming together at critical junctures. It also draws attention to how discourse, produced by social actors in contingent conditions, recursively shapes the dominant ideology and structures these interactions. In addition, the study demonstrates how algorithmic and interpretive research techniques can be combined for longitudinal analysis of textual data sets."
207,"PM2.5 is one of the major indicators of ambient air quality which has become a focus of public attention. Urban PM2.5 can be measured by air quality monitoring stations which are costly and not sufficiently installed in a city. In this paper, we aim to infer the PM2.5 information at the place where there is no air quality monitoring station. As PM2.5 concentration varies over time and space domains, we propose a joint topic model to jointly model the spatial and temporal patterns of PM2.5. Numerical results suggest that the proposed model achieves better inference based on five related datasets compared with traditional methods.",2017-06-27,2-s2.0-85030647222,"Proceedings - 16th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2017",A spatial-temporal model to improve PM2.5 inference,"PM2.5 is one of the major indicators of ambient air quality which has become a focus of public attention. Urban PM2.5 can be measured by air quality monitoring stations which are costly and not sufficiently installed in a city. In this paper, we aim to infer the PM2.5 information at the place where there is no air quality monitoring station. As PM2.5 concentration varies over time and space domains, we propose a joint topic model to jointly model the spatial and temporal patterns of PM2.5. Numerical results suggest that the proposed model achieves better inference based on five related datasets compared with traditional methods."
208,"The multidimensional nature of digital libraries evaluation domain poses several challenges to the research communities that intend to assess criteria, methods, products and tools, and also practice them. The amount of scientific production that is published in the domain hinders and disorientates the interested researchers. These researchers need guidance to exploit effectively the considerable amount of data and the diversity of methods, as well as to identify new research goals and develop their plans for future studies. This paper proposes a methodological pathway to investigate the core topics that structure the digital library evaluation domain and their impact. Further to the exploration of these topical entities, this study investigates also the researchers that contribute substantially to key topics, their communities and their relationships. The proposed methodology exploits topic modeling and network analysis in combination with citation and altmetrics analysis on a corpus consisting of the digital library evaluation papers presented in JCDL, ECDL/TDPL and ICADL conferences in the period 2001–2013.",2017-06-24,2-s2.0-85021249747,International Journal on Digital Libraries,Discovering the structure and impact of the digital library evaluation domain,"The multidimensional nature of digital libraries evaluation domain poses several challenges to the research communities that intend to assess criteria, methods, products and tools, and also practice them. The amount of scientific production that is published in the domain hinders and disorientates the interested researchers. These researchers need guidance to exploit effectively the considerable amount of data and the diversity of methods, as well as to identify new research goals and develop their plans for future studies. This paper proposes a methodological pathway to investigate the core topics that structure the digital library evaluation domain and their impact. Further to the exploration of these topical entities, this study investigates also the researchers that contribute substantially to key topics, their communities and their relationships. The proposed methodology exploits topic modeling and network analysis in combination with citation and altmetrics analysis on a corpus consisting of the digital library evaluation papers presented in JCDL, ECDL/TDPL and ICADL conferences in the period 2001–2013."
210,"Named entity disambiguation (NED) refers to the task of mapping entity mentions in running texts to the correct entries in a specific knowledge base (e.g., Wikipedia). Although there has been a lot of work on NED for long and formal texts like Wikipedia and news, the task is not well studied for questions in community question answering (CQA). The challenges of the task include little context for mentions in questions, lack of ground truth for learning, and language gaps between CQA and knowledge bases. To overcome these problems, we propose a topic modelling approach to NED for questions. Our model performs learning in an unsupervised manner, but can take advantage of weak supervision signals estimated from the metadata of CQA and knowledge bases. The signals can enrich the context of mentions in questions, and bridge the language gaps between CQA and knowledge bases. Besides these advantages, our model simulates people's behavior in CQA and thus is intuitively interpretable. We conduct experiments on both Chinese and English CQA data. The experimental results show that our method can significantly outperform state-of-the-art methods when we apply them to questions in CQA.",2017-06-15,2-s2.0-85018918452,Knowledge-Based Systems,Named entity disambiguation for questions in community question answering,"Named entity disambiguation (NED) refers to the task of mapping entity mentions in running texts to the correct entries in a specific knowledge base (e.g., Wikipedia). Although there has been a lot of work on NED for long and formal texts like Wikipedia and news, the task is not well studied for questions in community question answering (CQA). The challenges of the task include little context for mentions in questions, lack of ground truth for learning, and language gaps between CQA and knowledge bases. To overcome these problems, we propose a topic modelling approach to NED for questions. Our model performs learning in an unsupervised manner, but can take advantage of weak supervision signals estimated from the metadata of CQA and knowledge bases. The signals can enrich the context of mentions in questions, and bridge the language gaps between CQA and knowledge bases. Besides these advantages, our model simulates people's behavior in CQA and thus is intuitively interpretable. We conduct experiments on both Chinese and English CQA data. The experimental results show that our method can significantly outperform state-of-the-art methods when we apply them to questions in CQA."
211,"The popularity of online forums provides a good opportunity to learn user interests which can be used in many business scenarios, such as product or news recommendation. There exist many approaches to infer forum topics and users’ interests. Among them, Author-Topic (AT) like models are most popular. But a thread in online forum is composed of a root post and some response posts which may be relevant or irrelevant to the root post. So the assumption of AT that response posts are generated from user's interest topics is not comprehensive. In this paper, we distinguish user's serious and unserious interest topics and argue that the topic of a relevant response post is jointly determined by its author's serious interest topics and the topics of its root post, while the topic of irrelevant response post is only determined by its author's unserious interest topics. Based on these assumptions, we propose Forum-LDA to model the generative process of root post, relevant and irrelevant response posts jointly. Therefore, our model can not only learn more coherent topics and serious interests, but also identify unserious users who publish many irrelevant posts. Extensive experiments on real forum dataset demonstrate the advantages of our model in tasks such as user interest and unserious user discovery.",2017-06-15,2-s2.0-85017548366,Knowledge-Based Systems,Forum latent Dirichlet allocation for user interest discovery,"The popularity of online forums provides a good opportunity to learn user interests which can be used in many business scenarios, such as product or news recommendation. There exist many approaches to infer forum topics and users’ interests. Among them, Author-Topic (AT) like models are most popular. But a thread in online forum is composed of a root post and some response posts which may be relevant or irrelevant to the root post. So the assumption of AT that response posts are generated from user's interest topics is not comprehensive. In this paper, we distinguish user's serious and unserious interest topics and argue that the topic of a relevant response post is jointly determined by its author's serious interest topics and the topics of its root post, while the topic of irrelevant response post is only determined by its author's unserious interest topics. Based on these assumptions, we propose Forum-LDA to model the generative process of root post, relevant and irrelevant response posts jointly. Therefore, our model can not only learn more coherent topics and serious interests, but also identify unserious users who publish many irrelevant posts. Extensive experiments on real forum dataset demonstrate the advantages of our model in tasks such as user interest and unserious user discovery."
212,"The U.S Government has been the target for cyberattacks from all over the world. Just recently, former President Obama accused the Russian government of the leaking emails to Wikileaks and declared that the U.S. might be forced to respond. While Russia denied involvement, it is clear that the U.S. has to take some defensive measures to protect its data infrastructure. Insider threats have been the cause of other sensitive information leaks too, including the infamous Edward Snowden incident. Most of the recent leaks were in the form of text. Due to the nature of text data, security classifications are assigned manually. In an adversarial environment, insiders can leak texts through E-mail, printers, or any untrusted channels. The optimal defense is to automatically detect the unstructured text security class and enforce the appropriate protection mechanism without degrading services or daily tasks. Unfortunately, existing Data Leak Prevention (DLP) systems are not well suited for detecting unstructured texts. In this paper, we compare two recent approaches in the literature for text security classification, evaluating them on actual sensitive text data from the WikiLeaks dataset.",2017-06-07,2-s2.0-85022207246,"2017 IEEE International Symposium on Technologies for Homeland Security, HST 2017",Automated U.S diplomatic cables security classification: Topic model pruning vs. classification based on clusters,"The U.S Government has been the target for cyberattacks from all over the world. Just recently, former President Obama accused the Russian government of the leaking emails to Wikileaks and declared that the U.S. might be forced to respond. While Russia denied involvement, it is clear that the U.S. has to take some defensive measures to protect its data infrastructure. Insider threats have been the cause of other sensitive information leaks too, including the infamous Edward Snowden incident. Most of the recent leaks were in the form of text. Due to the nature of text data, security classifications are assigned manually. In an adversarial environment, insiders can leak texts through E-mail, printers, or any untrusted channels. The optimal defense is to automatically detect the unstructured text security class and enforce the appropriate protection mechanism without degrading services or daily tasks. Unfortunately, existing Data Leak Prevention (DLP) systems are not well suited for detecting unstructured texts. In this paper, we compare two recent approaches in the literature for text security classification, evaluating them on actual sensitive text data from the WikiLeaks dataset."
213,"The study of technological forecasting is an important part of patent analysis. Although fitting models can provide a rough tendency of a technical area, the trend of the detailed content within the area remains hidden. It is also difficult to reveal the trend of specific topics using keyword-based text mining techniques, since it is very hard to track the temporal patterns of a single keyword that generally represents a technological concept. To overcome these limitations, this research proposes a topic-based technological forecasting approach, to uncover the trends of specific topics underlying massive patent claims using topic modelling. A topic annual weight matrix and a sequence of topic-based trend coefficients are generated to quantitatively estimate the developing trends of the discovered topics, and evaluate to what degree various topics have contributed to the patenting activities of the whole area. To demonstrate the effectiveness of the approach, we present a case study using 13,910 utility patents that were published during the years 2000 to 2014, owned by Australian assignees, in the United States Patent and Trademark Office (USPTO). The results indicate that the proposed approach is effective for estimating the temporal patterns and forecast the future trends of the latent topics underlying massive claims. The topic-based knowledge and the corresponding trend analysis provided by the approach can be used to facilitate further technological decisions or opportunity discovery.",2017-06-01,2-s2.0-85015677572,Technological Forecasting and Social Change,Topic-based technological forecasting based on patent data: A case study of Australian patents from 2000 to 2014,"The study of technological forecasting is an important part of patent analysis. Although fitting models can provide a rough tendency of a technical area, the trend of the detailed content within the area remains hidden. It is also difficult to reveal the trend of specific topics using keyword-based text mining techniques, since it is very hard to track the temporal patterns of a single keyword that generally represents a technological concept. To overcome these limitations, this research proposes a topic-based technological forecasting approach, to uncover the trends of specific topics underlying massive patent claims using topic modelling. A topic annual weight matrix and a sequence of topic-based trend coefficients are generated to quantitatively estimate the developing trends of the discovered topics, and evaluate to what degree various topics have contributed to the patenting activities of the whole area. To demonstrate the effectiveness of the approach, we present a case study using 13,910 utility patents that were published during the years 2000 to 2014, owned by Australian assignees, in the United States Patent and Trademark Office (USPTO). The results indicate that the proposed approach is effective for estimating the temporal patterns and forecast the future trends of the latent topics underlying massive claims. The topic-based knowledge and the corresponding trend analysis provided by the approach can be used to facilitate further technological decisions or opportunity discovery."
214,"This study examines trends in academic research on personal information privacy. Using Scopus DB, we extracted 2356 documents covering journal articles, reviews, book chapters, conference papers, and working papers published between 1972 and August 2015. Latent Dirichlet allocation (LDA) is applied to the abstracts of those extracted documents to identify topics. Topics discovered from all documents focus mainly on technology, and the findings indicate that algorithms, Facebook privacy, and online social networks have become prominent topics. In contrast, it was observed that journal articles put more emphasis on both the e-business and healthcare. These results identify a research gap in the area of personal information privacy and offer a direction for future research.",2017-06-01,2-s2.0-85016304065,Computers and Security,Analyzing research trends in personal information privacy using topic modeling,"This study examines trends in academic research on personal information privacy. Using Scopus DB, we extracted 2356 documents covering journal articles, reviews, book chapters, conference papers, and working papers published between 1972 and August 2015. Latent Dirichlet allocation (LDA) is applied to the abstracts of those extracted documents to identify topics. Topics discovered from all documents focus mainly on technology, and the findings indicate that algorithms, Facebook privacy, and online social networks have become prominent topics. In contrast, it was observed that journal articles put more emphasis on both the e-business and healthcare. These results identify a research gap in the area of personal information privacy and offer a direction for future research."
215,"Historians in the twenty-first century are faced with an abundance of digitized material. Large-scale scanning efforts have made millions of textual primary sources available to historians at the click of a mouse. This paper discusses using topic modeling, a probability algorithm, to extract themes and discourses in large corpuses of digitized text. Mind and Body, a monthly publication about physical education, which ran from 1894 until 1936, was often at the very center of developments in the field of physical education. Major developments such as professionalization; the battle between the American, German, and Swedish exercise systems; and shifts in ideas about the role of exercise in American life were discussed and debated within the pages of Mind and Body. Applying distant-reading methodologies to such a central text AIDS scholars in identifying themes, discourses, and points for further examination.",2017-06-01,2-s2.0-85028690898,Journal of Sport History,Mining mind and body: Approaches and considerations for using topic modeling to identify discourses in digitized publications,"Historians in the twenty-first century are faced with an abundance of digitized material. Large-scale scanning efforts have made millions of textual primary sources available to historians at the click of a mouse. This paper discusses using topic modeling, a probability algorithm, to extract themes and discourses in large corpuses of digitized text. Mind and Body, a monthly publication about physical education, which ran from 1894 until 1936, was often at the very center of developments in the field of physical education. Major developments such as professionalization; the battle between the American, German, and Swedish exercise systems; and shifts in ideas about the role of exercise in American life were discussed and debated within the pages of Mind and Body. Applying distant-reading methodologies to such a central text AIDS scholars in identifying themes, discourses, and points for further examination."
216,"This study has proposed a topic based competitive keywords suggestion method called TCK to enhance search engine advertising. On the basis of query logs, the method explores the indirect associations between keywords and extracts the hidden topic information to identify competitive keywords. It can help advertisers not only broaden the choices of keywords but also carry out a competitive strategy for search engine advertising. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed method. Results prove that the proposed method performs better than existing keyword suggestion methods, contributing greatly to the keyword suggestion advertising market.",2017-06-01,2-s2.0-85008235246,Information and Management,Finding competitive keywords from query logs to enhance search engine advertising,"This study has proposed a topic based competitive keywords suggestion method called TCK to enhance search engine advertising. On the basis of query logs, the method explores the indirect associations between keywords and extracts the hidden topic information to identify competitive keywords. It can help advertisers not only broaden the choices of keywords but also carry out a competitive strategy for search engine advertising. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed method. Results prove that the proposed method performs better than existing keyword suggestion methods, contributing greatly to the keyword suggestion advertising market."
217,"Topic model is a hot research topic which is attracting attentions from many fields. Recently, several studies have applied topic model to ASR (audio scene recognition). Among these studies, most of them use the document-word co-occurrence matrix for topic analysis. In this work, we propose a new ASR algorithm based on audio events and topic model, which uses the document-event co-occurrence matrix for topic analysis. Our work is based on the hypothesis that: for an audio document, compared with its word distribution, its event distribution is more in line with humans’ way of thinking, and then the topic distribution obtained based on the document-event co-occurrence matrix can represent the audio document better. The contribution of this work lies in that: (1) we propose an ASR algorithm which uses document-event co-occurrence matrix for topic analysis. Compared with the current studies which use document-word co-occurrence matrix for topic analysis, the proposed algorithm can extract the topic distribution which can express the audio documents better, and then can get better recognition results; (2) we propose a much easier method to obtain the document-event co-occurrence matrix; (3) we propose a method to weight the event distribution of audio documents; this weighting method can emphasize the audio events that are important in reflecting the unique topics of the audio documents, and can suppress the audio events that are common to many topics. Experimental results on two public datasets verify the effectiveness of the proposed ASR algorithm, and also verify the necessity and effectiveness of the proposed weighting method. The innovative ideas in this work are not limited to ASR, but can be extended to many other fields, such as the video classification etc.",2017-06-01,2-s2.0-85017444988,Knowledge-Based Systems,Audio scene recognition based on audio events and topic model,"Topic model is a hot research topic which is attracting attentions from many fields. Recently, several studies have applied topic model to ASR (audio scene recognition). Among these studies, most of them use the document-word co-occurrence matrix for topic analysis. In this work, we propose a new ASR algorithm based on audio events and topic model, which uses the document-event co-occurrence matrix for topic analysis. Our work is based on the hypothesis that: for an audio document, compared with its word distribution, its event distribution is more in line with humans’ way of thinking, and then the topic distribution obtained based on the document-event co-occurrence matrix can represent the audio document better. The contribution of this work lies in that: (1) we propose an ASR algorithm which uses document-event co-occurrence matrix for topic analysis. Compared with the current studies which use document-word co-occurrence matrix for topic analysis, the proposed algorithm can extract the topic distribution which can express the audio documents better, and then can get better recognition results; (2) we propose a much easier method to obtain the document-event co-occurrence matrix; (3) we propose a method to weight the event distribution of audio documents; this weighting method can emphasize the audio events that are important in reflecting the unique topics of the audio documents, and can suppress the audio events that are common to many topics. Experimental results on two public datasets verify the effectiveness of the proposed ASR algorithm, and also verify the necessity and effectiveness of the proposed weighting method. The innovative ideas in this work are not limited to ASR, but can be extended to many other fields, such as the video classification etc."
218,"Researchers in information science and related areas have developed various methods for analyzing textual data, such as survey responses. This article describes the application of analysis methods from two distinct fields, one method from interpretive social science and one method from statistical machine learning, to the same survey data. The results show that the two analyses produce some similar and some complementary insights about the phenomenon of interest, in this case, nonuse of social media. We compare both the processes of conducting these analyses and the results they produce to derive insights about each method's unique advantages and drawbacks, as well as the broader roles that these methods play in the respective fields where they are often used. These insights allow us to make more informed decisions about the tradeoffs in choosing different methods for analyzing textual data. Furthermore, this comparison suggests ways that such methods might be combined in novel and compelling ways.",2017-06-01,2-s2.0-85018953289,Journal of the Association for Information Science and Technology,Comparing grounded theory and topic modeling: Extreme divergence or unlikely convergence?,"Researchers in information science and related areas have developed various methods for analyzing textual data, such as survey responses. This article describes the application of analysis methods from two distinct fields, one method from interpretive social science and one method from statistical machine learning, to the same survey data. The results show that the two analyses produce some similar and some complementary insights about the phenomenon of interest, in this case, nonuse of social media. We compare both the processes of conducting these analyses and the results they produce to derive insights about each method's unique advantages and drawbacks, as well as the broader roles that these methods play in the respective fields where they are often used. These insights allow us to make more informed decisions about the tradeoffs in choosing different methods for analyzing textual data. Furthermore, this comparison suggests ways that such methods might be combined in novel and compelling ways."
219,"With the rapid development of the Internet and its applications, growing volumes of documents increasingly become interconnected to form large-scale document networks. Accordingly, topic modeling in a network of documents has been attracting continuous research attention. Most of the existing network-based topic models assume that topics in a document are influenced by its directly linked neighbouring documents in a document network and overlook the potential influence from indirectly linked ones. The existing work also has not carefully modeled variations of such influence among neighboring documents. Recognizing these modeling limitations, this paper introduces a novel Local Context-Aware LDA Model (LC-LDA), which is capable of observing a local context comprising a rich collection of documents that may directly or indirectly influence the topic distributions of a target document. The proposed model can also differentiate the respective influence of each document in the local context on the target document according to both structural and temporal relationships between the two documents. The proposed model is extensively evaluated through multiple document clustering and classification tasks conducted over several large-scale document sets. Evaluation results clearly and consistently demonstrate the effectiveness and superiority of the new model with respect to several state-of-the-art peer models.",2017-06-01,2-s2.0-85017541037,Journal of the Association for Information Science and Technology,A local context-aware LDA model for topic modeling in a document network,"With the rapid development of the Internet and its applications, growing volumes of documents increasingly become interconnected to form large-scale document networks. Accordingly, topic modeling in a network of documents has been attracting continuous research attention. Most of the existing network-based topic models assume that topics in a document are influenced by its directly linked neighbouring documents in a document network and overlook the potential influence from indirectly linked ones. The existing work also has not carefully modeled variations of such influence among neighboring documents. Recognizing these modeling limitations, this paper introduces a novel Local Context-Aware LDA Model (LC-LDA), which is capable of observing a local context comprising a rich collection of documents that may directly or indirectly influence the topic distributions of a target document. The proposed model can also differentiate the respective influence of each document in the local context on the target document according to both structural and temporal relationships between the two documents. The proposed model is extensively evaluated through multiple document clustering and classification tasks conducted over several large-scale document sets. Evaluation results clearly and consistently demonstrate the effectiveness and superiority of the new model with respect to several state-of-the-art peer models."
220,"Development of the Brahmaputra River, which links China, India and Bangladesh, has been hindered by significant challenges, particularly political challenges. News reports can mirror the perceptions of political actors, but are, owing to the complexity of the issue, complicated and unstructured. We present a comparative content analysis of the overall framing in news reports of the Brahmaputra River development from major English news media. A structural topic model is established to discover latent topics in the corpus of 1,569 news articles published in 34 countries or regions. We find that politics, including domestic and international politics, dominates the news narratives. Environmental issues, such as glacier status and climate change impacts, are secondarily discussed. Technology and economy issues are less frequently presented in the media coverage. Advantages of upstream countries and dependences of downstream countries are reflected in news reporting and explicitly emerge in the structural topic model. These findings and implications are important for promoting mutual understanding and cooperation among riparian countries in developing the Brahmaputra River. The proposed approach is expected to be widely used as a methodological strategy in future water policy studies.",2017-06-01,2-s2.0-85021183396,Water Policy,Framing the Brahmaputra River hydropower development: Different concerns in riparian and international media reporting,"Development of the Brahmaputra River, which links China, India and Bangladesh, has been hindered by significant challenges, particularly political challenges. News reports can mirror the perceptions of political actors, but are, owing to the complexity of the issue, complicated and unstructured. We present a comparative content analysis of the overall framing in news reports of the Brahmaputra River development from major English news media. A structural topic model is established to discover latent topics in the corpus of 1,569 news articles published in 34 countries or regions. We find that politics, including domestic and international politics, dominates the news narratives. Environmental issues, such as glacier status and climate change impacts, are secondarily discussed. Technology and economy issues are less frequently presented in the media coverage. Advantages of upstream countries and dependences of downstream countries are reflected in news reporting and explicitly emerge in the structural topic model. These findings and implications are important for promoting mutual understanding and cooperation among riparian countries in developing the Brahmaputra River. The proposed approach is expected to be widely used as a methodological strategy in future water policy studies."
221,"General news recommendations are important but have received limited attention because of the difficulties of measuring public interest. In public search engines, the objects of search terms reflect the issues that interest or concern search engine users. Because of the popularity of search engines, search indexes have become a new measure for describing public interest trends. With the help of a public search index provided by search engines, we construct a news topic search feature and a news object search feature. These features measure the public attention on key elements of the news. In the experiment, we compare various feature models with machine learning algorithms with respect to financial news recommendations. The results demonstrate that the topic search features perform best compared with other feature models. This research contributes to both the feature generation and news recommendation domains.",2017-06-01,2-s2.0-85018997514,Journal of Information Science,A search index-enhanced feature model for news recommendation,"General news recommendations are important but have received limited attention because of the difficulties of measuring public interest. In public search engines, the objects of search terms reflect the issues that interest or concern search engine users. Because of the popularity of search engines, search indexes have become a new measure for describing public interest trends. With the help of a public search index provided by search engines, we construct a news topic search feature and a news object search feature. These features measure the public attention on key elements of the news. In the experiment, we compare various feature models with machine learning algorithms with respect to financial news recommendations. The results demonstrate that the topic search features perform best compared with other feature models. This research contributes to both the feature generation and news recommendation domains."
222,"Journal rankings, frequently determined by the journal impact factor or similar indices, are quantitative measures for evaluating a journal's performance in its discipline, which is presently a major research thrust in the bibliometrics field. Recently, text mining was adopted to augment journal ranking-based evaluation with the content analysis of a discipline taking a time-variant factor into consideration. However, previous studies focused mainly on a silo analysis of a discipline using either citation-or content-oriented approaches, and no attempt was made to analyze topical journal ranking and its change over time in a seamless and integrated manner. To address this issue, we propose a journal-time-topic model, an extension of Dirichlet multinomial regression, which we applied to the field of bioinformatics to understand journal contribution to topics in a field and the shift of topic trends. The journal-time-topic model allows us to identify which journals are the major leaders in what topics and the manner in which their topical focus. It also helps reveal an interesting distinct pattern in the journal impact factor of high- and low-ranked journals. The study results shed a new light for understanding topic specific journal rankings and shifts in journals' concentration on a subject.",2017-06-01,2-s2.0-85019012575,Journal of the Association for Information Science and Technology,Ensemble analysis of topical journal ranking in bioinformatics,"Journal rankings, frequently determined by the journal impact factor or similar indices, are quantitative measures for evaluating a journal's performance in its discipline, which is presently a major research thrust in the bibliometrics field. Recently, text mining was adopted to augment journal ranking-based evaluation with the content analysis of a discipline taking a time-variant factor into consideration. However, previous studies focused mainly on a silo analysis of a discipline using either citation-or content-oriented approaches, and no attempt was made to analyze topical journal ranking and its change over time in a seamless and integrated manner. To address this issue, we propose a journal-time-topic model, an extension of Dirichlet multinomial regression, which we applied to the field of bioinformatics to understand journal contribution to topics in a field and the shift of topic trends. The journal-time-topic model allows us to identify which journals are the major leaders in what topics and the manner in which their topical focus. It also helps reveal an interesting distinct pattern in the journal impact factor of high- and low-ranked journals. The study results shed a new light for understanding topic specific journal rankings and shifts in journals' concentration on a subject."
223,"Rapid advancements in internet and social media technologies have made “information overload” a rampant and widespread problem. Complex subjects, histories, or issues break down into branches, side stories, and intertwining narratives; a “topic evolution map” can assist in joining together and clarifying these disparate parts of an unfamiliar territory. This paper reviews the extant research on topic evolution map based on text and cross-media corpora over the past decade. We first define a series of necessary terms, then go on to describe the traditional topic evolution map per 1) topic evolution over time, based on the probabilistic generative model, and 2) topic evolution from a non-probabilistic perspective. Next, we discuss the current state of research on topic evolution map based on the cross-media corpus, including some open questions and possible future research directions. The main contribution of this review is in its construction of an evolution map that can be used to visualize and integrate the extant studies on topic modeling – specifically in regards to cross-media research.",2017-05-15,2-s2.0-85015644584,Knowledge-Based Systems,A survey on trends of cross-media topic evolution map,"Rapid advancements in internet and social media technologies have made “information overload” a rampant and widespread problem. Complex subjects, histories, or issues break down into branches, side stories, and intertwining narratives; a “topic evolution map” can assist in joining together and clarifying these disparate parts of an unfamiliar territory. This paper reviews the extant research on topic evolution map based on text and cross-media corpora over the past decade. We first define a series of necessary terms, then go on to describe the traditional topic evolution map per 1) topic evolution over time, based on the probabilistic generative model, and 2) topic evolution from a non-probabilistic perspective. Next, we discuss the current state of research on topic evolution map based on the cross-media corpus, including some open questions and possible future research directions. The main contribution of this review is in its construction of an evolution map that can be used to visualize and integrate the extant studies on topic modeling – specifically in regards to cross-media research."
224,"Following a recent worldwide boom in the democratization of knowledge, crowdsourcing and Participatory GIS, heritage practice increasingly draws on crowdsourced geographical data. In this paper, I discuss a public crowdsourcing of twentieth century conflict heritage in Finland, launched by state-owned broadcasting company Yleisradio. Here emphasis is on analysing the user behaviour and incentives, which can inform analogous future initiatives. Many of the public entries mirror local perspectives on conflict heritage: pride of personally important loci and self-satisfaction appear to be important incentives for taking part. Finally, I summarize themes that other heritage crowdsourcing organizers could apply to their work.",2017-05-04,2-s2.0-85041019695,Journal of Community Archaeology and Heritage,Crowdsourcing cultural heritage: public participation and conflict legacy in Finland,"Following a recent worldwide boom in the democratization of knowledge, crowdsourcing and Participatory GIS, heritage practice increasingly draws on crowdsourced geographical data. In this paper, I discuss a public crowdsourcing of twentieth century conflict heritage in Finland, launched by state-owned broadcasting company Yleisradio. Here emphasis is on analysing the user behaviour and incentives, which can inform analogous future initiatives. Many of the public entries mirror local perspectives on conflict heritage: pride of personally important loci and self-satisfaction appear to be important incentives for taking part. Finally, I summarize themes that other heritage crowdsourcing organizers could apply to their work."
225,"This study investigates how computational overhead for topic model training may be reduced by selectively removing terms from the vocabulary of text corpora being modeled. We compare the impact of removing singly occurring terms, the top 0.5%, 1% and 5% most frequently occurring terms and both top 0.5% most frequent and singly occurring terms, along with changes in the number of topics modeled (10, 20, 30, 40, 50, 100) using three datasets. Four outcome measures are compared. The removal of singly occurring terms has little impact on outcomes for all of the measures tested. Document discriminative capacity, as measured by the document space density, is reduced by the removal of frequently occurring terms, but increases with higher numbers of topics. Vocabulary size does not greatly influence entropy, but entropy is affected by the number of topics. Finally, topic similarity, as measured by pairwise topic similarity and Jensen-Shannon divergence, decreases with the removal of frequent terms. The findings have implications for information science research in information retrieval and informetrics that makes use of topic modeling.",2017-05-01,2-s2.0-85010915116,Information Processing and Management,Vocabulary size and its effect on topic representation,"This study investigates how computational overhead for topic model training may be reduced by selectively removing terms from the vocabulary of text corpora being modeled. We compare the impact of removing singly occurring terms, the top 0.5%, 1% and 5% most frequently occurring terms and both top 0.5% most frequent and singly occurring terms, along with changes in the number of topics modeled (10, 20, 30, 40, 50, 100) using three datasets. Four outcome measures are compared. The removal of singly occurring terms has little impact on outcomes for all of the measures tested. Document discriminative capacity, as measured by the document space density, is reduced by the removal of frequently occurring terms, but increases with higher numbers of topics. Vocabulary size does not greatly influence entropy, but entropy is affected by the number of topics. Finally, topic similarity, as measured by pairwise topic similarity and Jensen-Shannon divergence, decreases with the removal of frequent terms. The findings have implications for information science research in information retrieval and informetrics that makes use of topic modeling."
226,"Induced by “big data,” “topic modeling” has become an attractive alternative to mapping co-words in terms of co-occurrences and co-absences using network techniques. Does topic modeling provide an alternative for co-word mapping in research practices using moderately sized document collections? We return to the word/document matrix using first a single text with a strong argument (“The Leiden Manifesto”) and then upscale to a sample of moderate size (n = 687) to study the pros and cons of the two approaches in terms of the resulting possibilities for making semantic maps that can serve an argument. The results from co-word mapping (using two different routines) versus topic modeling are significantly uncorrelated. Whereas components in the co-word maps can easily be designated, the topic models provide sets of words that are very differently organized. In these samples, the topic models seem to reveal similarities other than semantic ones (e.g., linguistic ones). In other words, topic modeling does not replace co-word mapping in small and medium-sized sets; but the paper leaves open the possibility that topic modeling would work well for the semantic mapping of large sets.",2017-04-01,2-s2.0-85014591604,Journal of the Association for Information Science and Technology,"Co-word maps and topic modeling: A comparison using small and medium-sized corpora (N < 1,000)","Induced by “big data,” “topic modeling” has become an attractive alternative to mapping co-words in terms of co-occurrences and co-absences using network techniques. Does topic modeling provide an alternative for co-word mapping in research practices using moderately sized document collections? We return to the word/document matrix using first a single text with a strong argument (“The Leiden Manifesto”) and then upscale to a sample of moderate size (n = 687) to study the pros and cons of the two approaches in terms of the resulting possibilities for making semantic maps that can serve an argument. The results from co-word mapping (using two different routines) versus topic modeling are significantly uncorrelated. Whereas components in the co-word maps can easily be designated, the topic models provide sets of words that are very differently organized. In these samples, the topic models seem to reveal similarities other than semantic ones (e.g., linguistic ones). In other words, topic modeling does not replace co-word mapping in small and medium-sized sets; but the paper leaves open the possibility that topic modeling would work well for the semantic mapping of large sets."
227,"Purpose: The purpose of this study is to describe the underlying topics and the topic evolution in the 50-year history of educational leadership research literature. Method: We used automated text data mining with probabilistic latent topic models to examine the full text of the entire publication history of all 1,539 articles published in Educational Administration Quarterly (EAQ) from 1965 to 2014. Given the computationally intensive data analysis required by probabilistic topic models, relying on high-performance computing, we used a 10-fold cross-validation to estimate the model in which we categorized each article in each year into one of 19 latent topics and illustrated the rise and fall of topics over the EAQ’s 50-year history. Findings: Our model identified a total of 19 topics from the 1965 to 2014 EAQ corpus. Among them, five topics—inequity and social justice, female leadership, school leadership preparation and development, trust, and teaching and instructional leadership—gained research attention over the 50-year time period, whereas the research interest appears to have declined for the topic of epistemology of educational leadership since the 2000s. Other topics waxed and waned over the past five decades. Implications: This study maps the temporal terrain of topics in the educational leadership field over the past 50 years and sheds new light on the development and current status of the central topics in educational leadership research literature. More important, the panoramic view of topical landscape provides a unique backdrop as scholars contemplate the future of educational leadership research.",2017-04-01,2-s2.0-85014635355,Educational Administration Quarterly,Automated Text Data Mining Analysis of Five Decades of Educational Leadership Research Literature: Probabilistic Topic Modeling of EAQ Articles From 1965 to 2014,"Purpose: The purpose of this study is to describe the underlying topics and the topic evolution in the 50-year history of educational leadership research literature. Method: We used automated text data mining with probabilistic latent topic models to examine the full text of the entire publication history of all 1,539 articles published in Educational Administration Quarterly (EAQ) from 1965 to 2014. Given the computationally intensive data analysis required by probabilistic topic models, relying on high-performance computing, we used a 10-fold cross-validation to estimate the model in which we categorized each article in each year into one of 19 latent topics and illustrated the rise and fall of topics over the EAQ’s 50-year history. Findings: Our model identified a total of 19 topics from the 1965 to 2014 EAQ corpus. Among them, five topics—inequity and social justice, female leadership, school leadership preparation and development, trust, and teaching and instructional leadership—gained research attention over the 50-year time period, whereas the research interest appears to have declined for the topic of epistemology of educational leadership since the 2000s. Other topics waxed and waned over the past five decades. Implications: This study maps the temporal terrain of topics in the educational leadership field over the past 50 years and sheds new light on the development and current status of the central topics in educational leadership research literature. More important, the panoramic view of topical landscape provides a unique backdrop as scholars contemplate the future of educational leadership research."
228,"With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers' news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods.",2017-04-01,2-s2.0-85026465032,ACM Transactions on Information Systems,Learning to align comments to news topics,"With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers' news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods."
229,"We consider the problem of search result diversification for streams of short texts. Diversifying search results in short text streams is more challenging than in the case of long documents, as it is difficult to capture the latent topics of short documents. To capture the changes of topics and the probabilities of documents for a given query at a specific time in a short text stream, we propose a dynamic Dirichlet multinomial mixture topic model, called D2M3, as well as a Gibbs sampling algorithm for the inference. We also propose a streaming diversification algorithm, SDA, that integrates the information captured by D2M3 with our proposed modified version of the PM-2 (Proportionality-based diversification Method - second version) diversification algorithm. We conduct experiments on a Twitter dataset and find that SDA statistically significantly outperforms state-of-the-art non-streaming retrieval methods, plain streaming retrieval methods, as well as streaming diversification methods that use other dynamic topic models.",2017-04-01,2-s2.0-85026456548,ACM Transactions on Information Systems,Search result diversification in short text streams,"We consider the problem of search result diversification for streams of short texts. Diversifying search results in short text streams is more challenging than in the case of long documents, as it is difficult to capture the latent topics of short documents. To capture the changes of topics and the probabilities of documents for a given query at a specific time in a short text stream, we propose a dynamic Dirichlet multinomial mixture topic model, called D2M3, as well as a Gibbs sampling algorithm for the inference. We also propose a streaming diversification algorithm, SDA, that integrates the information captured by D2M3 with our proposed modified version of the PM-2 (Proportionality-based diversification Method - second version) diversification algorithm. We conduct experiments on a Twitter dataset and find that SDA statistically significantly outperforms state-of-the-art non-streaming retrieval methods, plain streaming retrieval methods, as well as streaming diversification methods that use other dynamic topic models."
230,"Topic models, such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA), have shown impressive success in many fields. Recently, multi-view learning via probabilistic latent semantic analysis (MVPLSA), is also designed for multi-view topic modeling. These approaches are instances of generative model, whereas they all ignore the manifold structure of data distribution, which is generally useful for preserving the nonlinear information. In this paper, we propose a novel multiple graph regularized generative model to exploit the manifold structure in multiple views. Specifically, we construct a nearest neighbor graph for each view to encode its corresponding manifold information. A multiple graph ensemble regularization framework is proposed to learn the optimal intrinsic manifold. Then, the manifold regularization term is incorporated into a multi-view topic model, resulting in a unified objective function. The solutions are derived based on the Expectation Maximization optimization framework. Experimental results on real-world multi-view data sets demonstrate the effectiveness of our approach.",2017-04-01,2-s2.0-85011601755,Knowledge-Based Systems,Multi-view learning via multiple graph regularized generative model,"Topic models, such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA), have shown impressive success in many fields. Recently, multi-view learning via probabilistic latent semantic analysis (MVPLSA), is also designed for multi-view topic modeling. These approaches are instances of generative model, whereas they all ignore the manifold structure of data distribution, which is generally useful for preserving the nonlinear information. In this paper, we propose a novel multiple graph regularized generative model to exploit the manifold structure in multiple views. Specifically, we construct a nearest neighbor graph for each view to encode its corresponding manifold information. A multiple graph ensemble regularization framework is proposed to learn the optimal intrinsic manifold. Then, the manifold regularization term is incorporated into a multi-view topic model, resulting in a unified objective function. The solutions are derived based on the Expectation Maximization optimization framework. Experimental results on real-world multi-view data sets demonstrate the effectiveness of our approach."
231,"Transportation research is a key area in both science and engineering. In this paper, we present an empirical analysis of 17,163 articles published in 22 leading transportation journals from 1990 to 2015. We apply a latent Dirichlet allocation (LDA) model on article abstracts to infer 50 key topics. We show that those characterized topics are both representative and meaningful, mostly corresponding to established sub-fields in transportation research. These identified fields reveal a research landscape for transportation. Based on the results of LDA, we quantify the similarity of journals and countries/regions in terms of their aggregated topic distributions. By measuring the variation of topic distributions over time, we find some general research trends, such as topics on sustainability, travel behavior and non-motorized mobility are becoming increasingly popular over time. We also carry out this temporal analysis for each journal, observing a high degree of consistency for most journals. However, some interesting anomaly, such as special issues on particular topics, are detected from temporal variation as well. By quantifying the temporal trends at the country/region level, we find that countries/regions display clearly distinguishable patterns, suggesting that research communities in different regions tend to focus on different sub-fields. Our results could benefit different parties in the academic community—including researchers, journal editors and funding agencies—in terms of identifying promising research topics/projects, seeking for candidate journals for a submission, and realigning focus for journal development.",2017-04-01,2-s2.0-85010808868,Transportation Research Part C: Emerging Technologies,Discovering themes and trends in transportation research using topic modeling,"Transportation research is a key area in both science and engineering. In this paper, we present an empirical analysis of 17,163 articles published in 22 leading transportation journals from 1990 to 2015. We apply a latent Dirichlet allocation (LDA) model on article abstracts to infer 50 key topics. We show that those characterized topics are both representative and meaningful, mostly corresponding to established sub-fields in transportation research. These identified fields reveal a research landscape for transportation. Based on the results of LDA, we quantify the similarity of journals and countries/regions in terms of their aggregated topic distributions. By measuring the variation of topic distributions over time, we find some general research trends, such as topics on sustainability, travel behavior and non-motorized mobility are becoming increasingly popular over time. We also carry out this temporal analysis for each journal, observing a high degree of consistency for most journals. However, some interesting anomaly, such as special issues on particular topics, are detected from temporal variation as well. By quantifying the temporal trends at the country/region level, we find that countries/regions display clearly distinguishable patterns, suggesting that research communities in different regions tend to focus on different sub-fields. Our results could benefit different parties in the academic community—including researchers, journal editors and funding agencies—in terms of identifying promising research topics/projects, seeking for candidate journals for a submission, and realigning focus for journal development."
232,"Augmented reality has recently achieved a rapid growth through its applications in various industries, including education and entertainment. Despite the growing attraction of augmented reality, trend analyses in this emerging technology have relied on qualitative literature review, failing to provide comprehensive competitive intelligence analysis using objective data. Therefore, tracing industrial competition trends in augmented reality will provide technology experts with a better understanding of evolving competition trends and insights for further technology and sustainable business planning. In this paper, we apply a topic modeling approach to 3595 patents related to augmented reality technology to identify technology subjects and their knowledge stocks, thereby analyzing industrial competitive intelligence in light of technology subject and firm levels. As a result, we were able to obtain some findings from an inventional viewpoint: technological development of augmented reality will soon enter a mature stage, technologies of infrastructural requirements have been a focal subject since 2001, and several software firms and camera manufacturing firms have dominated the recent development of augmented reality.",2017-03-25,2-s2.0-85017394173,Sustainability (Switzerland),Competitive intelligence analysis of augmented reality technology using patent information,"Augmented reality has recently achieved a rapid growth through its applications in various industries, including education and entertainment. Despite the growing attraction of augmented reality, trend analyses in this emerging technology have relied on qualitative literature review, failing to provide comprehensive competitive intelligence analysis using objective data. Therefore, tracing industrial competition trends in augmented reality will provide technology experts with a better understanding of evolving competition trends and insights for further technology and sustainable business planning. In this paper, we apply a topic modeling approach to 3595 patents related to augmented reality technology to identify technology subjects and their knowledge stocks, thereby analyzing industrial competitive intelligence in light of technology subject and firm levels. As a result, we were able to obtain some findings from an inventional viewpoint: technological development of augmented reality will soon enter a mature stage, technologies of infrastructural requirements have been a focal subject since 2001, and several software firms and camera manufacturing firms have dominated the recent development of augmented reality."
233,"The Philippines is a country that is often plagued by typhoons. In times of this disaster, many people turn to social media such as Twitter for information, making it useful. We take advantage of data present in tweets to get some insights, through discovered topics, on how they reflect the behavior of Filipinos during typhoons. Thus, we present a framework that uses Biterm Topic Modelling (BTM) to make sense of typhoon-related tweets. We focused on tweets collected from February 2013 to November 2014 of Twitter users from Metro Manila. Data preprocessing was applied to remove noisy and irrelevant data, like stop words and punctuations. We then conducted experiments using BTM for topic modelling and open coding for evaluating the results. Results revealed different Filipino behaviors during a typhoon such as determination to rise up after the typhoon, voicing out concerns, and using word play. Future work could experiment on selecting the appropriate number of words per topic model.",2017-03-10,2-s2.0-85017219344,"Proceedings of the 2016 International Conference on Asian Language Processing, IALP 2016",Using Topic Modelling to make sense of typhoon-related tweets,"The Philippines is a country that is often plagued by typhoons. In times of this disaster, many people turn to social media such as Twitter for information, making it useful. We take advantage of data present in tweets to get some insights, through discovered topics, on how they reflect the behavior of Filipinos during typhoons. Thus, we present a framework that uses Biterm Topic Modelling (BTM) to make sense of typhoon-related tweets. We focused on tweets collected from February 2013 to November 2014 of Twitter users from Metro Manila. Data preprocessing was applied to remove noisy and irrelevant data, like stop words and punctuations. We then conducted experiments using BTM for topic modelling and open coding for evaluating the results. Results revealed different Filipino behaviors during a typhoon such as determination to rise up after the typhoon, voicing out concerns, and using word play. Future work could experiment on selecting the appropriate number of words per topic model."
234,"The ease with which data can be created, copied, modified, and deleted over the Internet has made it increasingly difficult to determine the source of web data. Data provenance, which provides information about the origin and lineage of a dataset, assists in determining its genuineness and trustworthiness. Several data provenance techniques record provenance when the data is created or modified. However, many existing datasets have no recorded provenance. Provenance Reconstruction techniques attempt to generate an approximate provenance in these datasets. Current reconstruction techniques require timing metadata to reconstruct provenance. In thats paper, we improve our multi-funneling technique, which combines existing techniques, including topic modeling, longest common subsequence, and genetic algorithm to achieve higher accuracy in reconstructing provenance without requiring timing metadata. In addition, we introduce novel funnels that are customized to the provided datasets, which further boosts precision and recall rates. We evaluated our approach with various experiments and compare the results of our approach with existing techniques. Finally, we present lessons learned, including the applicability of our approach to other datasets.",2017-03-03,2-s2.0-85016785136,"Proceedings of the 2016 IEEE 12th International Conference on e-Science, e-Science 2016",Improving data provenance reconstruction via a multi-level funneling approach,"The ease with which data can be created, copied, modified, and deleted over the Internet has made it increasingly difficult to determine the source of web data. Data provenance, which provides information about the origin and lineage of a dataset, assists in determining its genuineness and trustworthiness. Several data provenance techniques record provenance when the data is created or modified. However, many existing datasets have no recorded provenance. Provenance Reconstruction techniques attempt to generate an approximate provenance in these datasets. Current reconstruction techniques require timing metadata to reconstruct provenance. In thats paper, we improve our multi-funneling technique, which combines existing techniques, including topic modeling, longest common subsequence, and genetic algorithm to achieve higher accuracy in reconstructing provenance without requiring timing metadata. In addition, we introduce novel funnels that are customized to the provided datasets, which further boosts precision and recall rates. We evaluated our approach with various experiments and compare the results of our approach with existing techniques. Finally, we present lessons learned, including the applicability of our approach to other datasets."
235,"Autonomous vehicles (AVs) have emerged as a transformative technology with the potential to both fundamentally improve lives in cities but also to exacerbate suburban sprawl, vehicle miles traveled and the associated greenhouse gas emissions. Are communities willing to adopt best practices that can lead to early adoption of more sustainable outcomes? This paper presents innovative means to analyze social preferences, demand for AVs, and the potential to resolve community concerns with integrated solutions. We discuss our comprehensive analysis of unstructured and structured data from a survey on AVs that was conducted by the Atlanta Regional Commission in 2015. We used topic modeling to synthesize the “topics” from 1540 comments. The topics captured Atlanta residents' concerns and suggestions about implementing AVs. Further, sentiment analysis revealed people's attitudes on the topics. Accordingly, we proposed an integration of AVs and transit-oriented development (TOD: the development of compact and mixed-use communities around high quality mass transit services within a 10-min walking distance). The second type of data is people's responses to multiple-choice questions about AVs and TOD, which we call structured data. Using latent-class analysis, we identified heterogeneity in preferences for AVs and TOD. More Atlanta residents are willing to live in transit-oriented communities than traditional automobile-dependent ones if AVs save time and improve productivity. This finding portends the future success of combining AVs with TOD and reaping the sustainable benefits of this transformative technology.",2017-03-01,2-s2.0-85010310683,Cities,Data-enabled public preferences inform integration of autonomous vehicles with transit-oriented development in Atlanta,"Autonomous vehicles (AVs) have emerged as a transformative technology with the potential to both fundamentally improve lives in cities but also to exacerbate suburban sprawl, vehicle miles traveled and the associated greenhouse gas emissions. Are communities willing to adopt best practices that can lead to early adoption of more sustainable outcomes? This paper presents innovative means to analyze social preferences, demand for AVs, and the potential to resolve community concerns with integrated solutions. We discuss our comprehensive analysis of unstructured and structured data from a survey on AVs that was conducted by the Atlanta Regional Commission in 2015. We used topic modeling to synthesize the “topics” from 1540 comments. The topics captured Atlanta residents' concerns and suggestions about implementing AVs. Further, sentiment analysis revealed people's attitudes on the topics. Accordingly, we proposed an integration of AVs and transit-oriented development (TOD: the development of compact and mixed-use communities around high quality mass transit services within a 10-min walking distance). The second type of data is people's responses to multiple-choice questions about AVs and TOD, which we call structured data. Using latent-class analysis, we identified heterogeneity in preferences for AVs and TOD. More Atlanta residents are willing to live in transit-oriented communities than traditional automobile-dependent ones if AVs save time and improve productivity. This finding portends the future success of combining AVs with TOD and reaping the sustainable benefits of this transformative technology."
236,"Typography is overlooked in knowledge maps (KM) and information retrieval (IR), and some deficiencies in these systems can potentially be improved by encoding information into font attributes. A review of font use across domains is used to itemize font attributes and information visualization theory is used to characterize each attribute. Tasks associated with KM and IR, such as skimming, opinion analysis, character analysis, topic modelling and sentiment analysis can be aided through the use of novel representations using font attributes such as skim formatting, proportional encoding, textual stem and leaf plots and multi-attribute labels.",2017-03-01,2-s2.0-84957632805,International Journal on Digital Libraries,"Font attributes enrich knowledge maps and information retrieval: Skim formatting, proportional encoding, text stem and leaf plots, and multi-attribute labels","Typography is overlooked in knowledge maps (KM) and information retrieval (IR), and some deficiencies in these systems can potentially be improved by encoding information into font attributes. A review of font use across domains is used to itemize font attributes and information visualization theory is used to characterize each attribute. Tasks associated with KM and IR, such as skimming, opinion analysis, character analysis, topic modelling and sentiment analysis can be aided through the use of novel representations using font attributes such as skim formatting, proportional encoding, textual stem and leaf plots and multi-attribute labels."
237,"Topic modeling is one of the more promising quantitative procedures for exploring semantic structures. The creators of the corresponding algorithms use large text sets to investigate hidden thematic connections which cannot be perceived by the eye alone. We have, by contrast, tested a medium-sized corpus of novellas that can be explored using both individual readings and statistical procedures. We were motivated by a previously little considered observation: The scholars who have been able to implement statistical procedures for literary corpus analysis in a pertinent way were all extraordinarily familiar with their respective corpora. Because we were also very familiar with our set, it was possible for us to order the topics being studied according to text-relevant themes. Using the keyword falcon topics, we describe another type of Topics, ones which seem to reflect the special character of individual texts.",2017-03-01,2-s2.0-85018523899,Lili - Zeitschrift fur Literaturwissenschaft und Linguistik,Falcon topics: On some problems of topic modeling of literary texts Falkentopics: Über einige Probleme beim Topic Modeling literarischer Texte,"Topic modeling is one of the more promising quantitative procedures for exploring semantic structures. The creators of the corresponding algorithms use large text sets to investigate hidden thematic connections which cannot be perceived by the eye alone. We have, by contrast, tested a medium-sized corpus of novellas that can be explored using both individual readings and statistical procedures. We were motivated by a previously little considered observation: The scholars who have been able to implement statistical procedures for literary corpus analysis in a pertinent way were all extraordinarily familiar with their respective corpora. Because we were also very familiar with our set, it was possible for us to order the topics being studied according to text-relevant themes. Using the keyword falcon topics, we describe another type of Topics, ones which seem to reflect the special character of individual texts."
238,"Although climate change and energy are intricately linked, their explicit connection is not always prominent in public discourse and the media. Disruptive extreme weather events, including hurricanes, focus public attention in new and different ways offering a unique window of opportunity to analyze how a focusing event influences public discourse. Media coverage of extreme weather events simultaneously shapes and reflects public discourse on climate issues. Here, we analyze climate and energy newspaper coverage of Hurricanes Katrina (2005) and Sandy (2012) using topic models, mathematical techniques used to discover abstract topics within a set of documents. Our results demonstrate that post-Katrina media coverage does not contain a climate change topic, and the energy topic is limited to discussion of energy prices, markets, and the economy with almost no explicit linkages made between energy and climate change. In contrast, post-Sandy media coverage does contain a prominent climate change topic, a distinct energy topic, as well as integrated representation of climate change and energy, indicating a shift in climate and energy reporting between Hurricane Katrina and Hurricane Sandy.",2017-03-01,2-s2.0-85012009139,Journal of Environmental Studies and Sciences,Transitions in climate and energy discourse between Hurricanes Katrina and Sandy,"Although climate change and energy are intricately linked, their explicit connection is not always prominent in public discourse and the media. Disruptive extreme weather events, including hurricanes, focus public attention in new and different ways offering a unique window of opportunity to analyze how a focusing event influences public discourse. Media coverage of extreme weather events simultaneously shapes and reflects public discourse on climate issues. Here, we analyze climate and energy newspaper coverage of Hurricanes Katrina (2005) and Sandy (2012) using topic models, mathematical techniques used to discover abstract topics within a set of documents. Our results demonstrate that post-Katrina media coverage does not contain a climate change topic, and the energy topic is limited to discussion of energy prices, markets, and the economy with almost no explicit linkages made between energy and climate change. In contrast, post-Sandy media coverage does contain a prominent climate change topic, a distinct energy topic, as well as integrated representation of climate change and energy, indicating a shift in climate and energy reporting between Hurricane Katrina and Hurricane Sandy."
239,"This paper focuses on the contribution of temporal relations inference and distributional semantic models to the event ordering task. Our system automatically builds ordered timelines of events from different written texts in English by performing first temporal clustering and then semantic clustering. In order to determine temporal compatibility, an inference from the temporal relationships between events-automatically extracted from a Temporal Information Processing system-is applied. Regarding semantic compatibility between events, we analyze two different distributional semantic models: LDA Topic modeling and Word2Vec word embeddings. Both semantic models together with the temporal inference have been evaluated within the framework of SemEval 2015 Task 4 Track B. Experiments show that, using both models, the current State of the Art is improved, showing significant advance in the Cross-Document Event Ordering task.",2017-03-01,2-s2.0-85016982190,Procesamiento de Lenguaje Natural,Cross-document event ordering through temporal relation inference and distributional semantic models,"This paper focuses on the contribution of temporal relations inference and distributional semantic models to the event ordering task. Our system automatically builds ordered timelines of events from different written texts in English by performing first temporal clustering and then semantic clustering. In order to determine temporal compatibility, an inference from the temporal relationships between events-automatically extracted from a Temporal Information Processing system-is applied. Regarding semantic compatibility between events, we analyze two different distributional semantic models: LDA Topic modeling and Word2Vec word embeddings. Both semantic models together with the temporal inference have been evaluated within the framework of SemEval 2015 Task 4 Track B. Experiments show that, using both models, the current State of the Art is improved, showing significant advance in the Cross-Document Event Ordering task."
240,"We propose a principled approach for learning parameters in Bayesian networks from incomplete datasets, where the examples of a dataset are subject to equivalence constraints. These equivalence constraints arise from datasets where examples are tied together, in that we may not know the value of a particular variable, but whatever that value is, we know it must be the same across different examples. We formalize the problem by defining the notion of a constrained dataset and a corresponding constrained likelihood that we seek to optimize. We further propose a new learning algorithm that can effectively learn more accurate Bayesian networks using equivalence constraints, which we demonstrate empirically. Moreover, we highlight how our general approach can be brought to bear on more specialized learning tasks, such as those in semi-supervised clustering and topic modeling, where more domain-specific approaches were previously developed.",2017-03-01,2-s2.0-84930896894,Artificial Intelligence,Learning Bayesian network parameters under equivalence constraints,"We propose a principled approach for learning parameters in Bayesian networks from incomplete datasets, where the examples of a dataset are subject to equivalence constraints. These equivalence constraints arise from datasets where examples are tied together, in that we may not know the value of a particular variable, but whatever that value is, we know it must be the same across different examples. We formalize the problem by defining the notion of a constrained dataset and a corresponding constrained likelihood that we seek to optimize. We further propose a new learning algorithm that can effectively learn more accurate Bayesian networks using equivalence constraints, which we demonstrate empirically. Moreover, we highlight how our general approach can be brought to bear on more specialized learning tasks, such as those in semi-supervised clustering and topic modeling, where more domain-specific approaches were previously developed."
241,"For over a decade now, due to the introduction of UTF-8 encoding, the digitization of Hindi content has increased rapidly because of which Hindi-music has accomplished popularity on the web. The focus is to identify the emotion, a person is experiencing while listening to a song track. The aim of this research work is to analyze the lyrics of Hindi-language based songs, in order to detect the mood of the listener. We used unigram and term-frequency as the main features. The songs were reduced to a level where only relevant words will be used for mood-detection. We employ unsupervised machine learning namely topic-modeling (Latent Dirichlet Allocation model) for mining the mood out of every song in the corpus. We created our own dataset of 1900 songs consisting of Bollywood tracks, bhajans (spiritual prayers) and ghazals. A mood taxonomy is used to distinguish songs into Happy or Sad. Data is applied to LDA model to discover the hidden emotions within each song. At the end of experimentation, we compare the results with manually pre-annotated dataset for validation purpose and observe good results.",2017-02-15,2-s2.0-85015836876,"2016 International Conference on Information Technology, InCITe 2016 - The Next Generation IT Summit on the Theme - Internet of Things: Connect your Worlds",Music mood classification based on lyrical analysis of Hindi songs using Latent Dirichlet Allocation,"For over a decade now, due to the introduction of UTF-8 encoding, the digitization of Hindi content has increased rapidly because of which Hindi-music has accomplished popularity on the web. The focus is to identify the emotion, a person is experiencing while listening to a song track. The aim of this research work is to analyze the lyrics of Hindi-language based songs, in order to detect the mood of the listener. We used unigram and term-frequency as the main features. The songs were reduced to a level where only relevant words will be used for mood-detection. We employ unsupervised machine learning namely topic-modeling (Latent Dirichlet Allocation model) for mining the mood out of every song in the corpus. We created our own dataset of 1900 songs consisting of Bollywood tracks, bhajans (spiritual prayers) and ghazals. A mood taxonomy is used to distinguish songs into Happy or Sad. Data is applied to LDA model to discover the hidden emotions within each song. At the end of experimentation, we compare the results with manually pre-annotated dataset for validation purpose and observe good results."
242,"Patent data has been an obvious choice for analysis leading to strategic technology intelligence, yet, the recent proliferation of machine learning text analysis methods is changing the status of traditional patent data analysis methods and approaches. This article discusses the benefits and constraints of machine learning approaches in industry level patent analysis, and to this end offers a demonstration of unsupervised learning based analysis of the leading telecommunication firms between 2001 and 2014 based on about 160,000 USPTO full-text patents. Data were classified using full-text descriptions with Latent Dirichlet Allocation, and latent patterns emerging through the unsupervised learning process were modelled by company and year to create an overall view of patenting within the industry, and to forecast future trends. Our results demonstrate company-specific differences in their knowledge profiles, as well as show the evolution of the knowledge profiles of industry leaders from hardware to software focussed technology strategies. The results cast also light on the dynamics of emerging and declining knowledge areas in the telecommunication industry. Our results prompt a consideration of the current status of established approaches to patent landscaping, such as key-word or technology classifications and other approaches relying on semantic labelling, in the context of novel machine learning approaches. Finally, we discuss implications for policy makers, and, in particular, for strategic management in firms.",2017-02-01,2-s2.0-84992118302,Technological Forecasting and Social Change,Firms' knowledge profiles: Mapping patent data with unsupervised learning,"Patent data has been an obvious choice for analysis leading to strategic technology intelligence, yet, the recent proliferation of machine learning text analysis methods is changing the status of traditional patent data analysis methods and approaches. This article discusses the benefits and constraints of machine learning approaches in industry level patent analysis, and to this end offers a demonstration of unsupervised learning based analysis of the leading telecommunication firms between 2001 and 2014 based on about 160,000 USPTO full-text patents. Data were classified using full-text descriptions with Latent Dirichlet Allocation, and latent patterns emerging through the unsupervised learning process were modelled by company and year to create an overall view of patenting within the industry, and to forecast future trends. Our results demonstrate company-specific differences in their knowledge profiles, as well as show the evolution of the knowledge profiles of industry leaders from hardware to software focussed technology strategies. The results cast also light on the dynamics of emerging and declining knowledge areas in the telecommunication industry. Our results prompt a consideration of the current status of established approaches to patent landscaping, such as key-word or technology classifications and other approaches relying on semantic labelling, in the context of novel machine learning approaches. Finally, we discuss implications for policy makers, and, in particular, for strategic management in firms."
243,"metaknowledge is a full-featured Python package for computational research in information science, network analysis, and science of science. It is optimized to scale efficiently for analyzing very large datasets, and is designed to integrate well with reproducible and open research workflows. It currently accepts raw data from the Web of Science, Scopus, PubMed, ProQuest Dissertations and Theses, and select funding agencies. It processes these raw data inputs and outputs a variety of datasets for quantitative analysis, including time series methods, Standard and Multi Reference Publication Year Spectroscopy, computational text analysis (e.g. topic modeling, burst analysis), and network analysis (including multi-mode, multi-level, and longitudinal networks). This article motivates the use of metaknowledge and explains its design and core functionality.",2017-02-01,2-s2.0-85007370730,Journal of Informetrics,"Introducing metaknowledge: Software for computational research in information science, network analysis, and science of science","metaknowledge is a full-featured Python package for computational research in information science, network analysis, and science of science. It is optimized to scale efficiently for analyzing very large datasets, and is designed to integrate well with reproducible and open research workflows. It currently accepts raw data from the Web of Science, Scopus, PubMed, ProQuest Dissertations and Theses, and select funding agencies. It processes these raw data inputs and outputs a variety of datasets for quantitative analysis, including time series methods, Standard and Multi Reference Publication Year Spectroscopy, computational text analysis (e.g. topic modeling, burst analysis), and network analysis (including multi-mode, multi-level, and longitudinal networks). This article motivates the use of metaknowledge and explains its design and core functionality."
244,"Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decision-making process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and long-term consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior.",2017-02-01,2-s2.0-85002991742,Cognition,Exploration and exploitation of Victorian science in Darwin's reading notebooks,"Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decision-making process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and long-term consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior."
245,"In order to manage and organize information on the web, we propose a novel web page classification strategy integrating topic model and SVM. We use topic model to harness the implicit information on web pages for feature extraction. Accuracy of the strategy is 84.15%, 2.23% superior to the traditional classification strategy based on CHI.",2017-01-03,2-s2.0-85014125872,"IEEE/ACM BESC 2016 - Proceedings of 2016 International Conference on Behavioral, Economic, Socio - Cultural Computing",Towards effective web page classification,"In order to manage and organize information on the web, we propose a novel web page classification strategy integrating topic model and SVM. We use topic model to harness the implicit information on web pages for feature extraction. Accuracy of the strategy is 84.15%, 2.23% superior to the traditional classification strategy based on CHI."
246,"We present the key features of topic modeling based on Latent Dirichlet Allocation (LDA), and demonstrate its application by analyzing Organization Research Methods articles since its inception. Our analysis, based on 421 ORM articles reveals 15 topics, which are quite similar to other, more human intensive review exercises.",2017-01-01,2-s2.0-85041782746,"2017 Annual Meeting of the Academy of Management, AOM 2017",Topic models as a novel approach to identify themes in content analysis: The example of organizational research methods,"We present the key features of topic modeling based on Latent Dirichlet Allocation (LDA), and demonstrate its application by analyzing Organization Research Methods articles since its inception. Our analysis, based on 421 ORM articles reveals 15 topics, which are quite similar to other, more human intensive review exercises."
247,"We examine the emergence of an organizational form, charter schools, in Oakland, California. We link field-level logics to organizational founding identities using topic modeling. We find corporate and community founding actors create distinct and consistent identities, whereas more peripheral founders indulge in more unique identity construction. We see the settlement of the form into a stable ecosystem with multiple identity codes rather than driving toward a single organizational identity. The variety of identities that emerge do not always map onto field-level logics. This has implications for the conditions under which organizational innovation and experimentation within a new form may develop.",2017-01-01,2-s2.0-85016287001,Research in the Sociology of Organizations,A patchwork of identities: Emergence of charter schools as a new organizational form,"We examine the emergence of an organizational form, charter schools, in Oakland, California. We link field-level logics to organizational founding identities using topic modeling. We find corporate and community founding actors create distinct and consistent identities, whereas more peripheral founders indulge in more unique identity construction. We see the settlement of the form into a stable ecosystem with multiple identity codes rather than driving toward a single organizational identity. The variety of identities that emerge do not always map onto field-level logics. This has implications for the conditions under which organizational innovation and experimentation within a new form may develop."
248,"Purpose: The purpose of this paper is to explore and describe research presented in the International Journal of Quality & Reliability Management (IJQRM), thereby creating an increased understanding of how the areas of research have evolved through the years. An additional purpose is to show how text mining methodology can be used as a tool for exploration and description of research publications. Design/methodology/approach: The study applies text mining methodologies to explore and describe the digital library of IJQRM from 1984 up to 2014. To structure and condense the data, k-means clustering and probabilistic topic modeling with latent Dirichlet allocation is applied. The data set consists of research paper abstracts. Findings: The results support the suggestion of the occurrence of trends, fads and fashion in research publications. Research on quality function deployment (QFD) and reliability management are noted to be on the downturn whereas research on Six Sigma with a focus on lean, innovation, performance and improvement on the rise. Furthermore, the study confirms IJQRM as a scientific journal with quality and reliability management as primary areas of coverage, accompanied by specific topics such as total quality management, service quality, process management, ISO, QFD and Six Sigma. The study also gives an insight into how text mining can be used as a way to efficiently explore and describe large quantities of research paper abstracts. Research limitations/implications: The study focuses on abstracts of research papers, thus topics and categories that could be identified via other journal publications, such as book reviews; general reviews; secondary articles; editorials; guest editorials; awards for excellence (notifications); introductions or summaries from conferences; notes from the publisher; and articles without an abstract, are excluded. Originality/value: There do not seem to be any prior text mining studies that apply cluster modeling and probabilistic topic modeling to research article abstracts in the IJQRM. This study therefore offers a unique perspective on the journal’s content.",2017-01-01,2-s2.0-85027337496,International Journal of Quality and Reliability Management,Exploring research on quality and reliability management through text mining methodology,"Purpose: The purpose of this paper is to explore and describe research presented in the International Journal of Quality & Reliability Management (IJQRM), thereby creating an increased understanding of how the areas of research have evolved through the years. An additional purpose is to show how text mining methodology can be used as a tool for exploration and description of research publications. Design/methodology/approach: The study applies text mining methodologies to explore and describe the digital library of IJQRM from 1984 up to 2014. To structure and condense the data, k-means clustering and probabilistic topic modeling with latent Dirichlet allocation is applied. The data set consists of research paper abstracts. Findings: The results support the suggestion of the occurrence of trends, fads and fashion in research publications. Research on quality function deployment (QFD) and reliability management are noted to be on the downturn whereas research on Six Sigma with a focus on lean, innovation, performance and improvement on the rise. Furthermore, the study confirms IJQRM as a scientific journal with quality and reliability management as primary areas of coverage, accompanied by specific topics such as total quality management, service quality, process management, ISO, QFD and Six Sigma. The study also gives an insight into how text mining can be used as a way to efficiently explore and describe large quantities of research paper abstracts. Research limitations/implications: The study focuses on abstracts of research papers, thus topics and categories that could be identified via other journal publications, such as book reviews; general reviews; secondary articles; editorials; guest editorials; awards for excellence (notifications); introductions or summaries from conferences; notes from the publisher; and articles without an abstract, are excluded. Originality/value: There do not seem to be any prior text mining studies that apply cluster modeling and probabilistic topic modeling to research article abstracts in the IJQRM. This study therefore offers a unique perspective on the journal’s content."
249,"Purpose: Competitor analysis is a key component in operations management. Most business decisions are rooted in the analysis of rival products inferred from market structure. Relative to more traditional competitor analysis methods, the purpose of this paper is to provide operations managers with an innovative tool to monitor a firm’s market position and competitors in real time at higher resolution and lower cost than more traditional competitor analysis methods. Design/methodology/approach: The authors combine the techniques of Web Crawler, Natural Language Processing and Machine Learning algorithms with data visualization to develop a big data competitor-analysis system that informs operations managers about competitors and meaningful relationships among them. The authors illustrate the approach using the fitness mobile app business. Findings: The study shows that the system supports operational decision making both descriptively and prescriptively. In particular, the innovative probabilistic topic modeling algorithm combined with conventional multidimensional scaling, product feature comparison and market structure analyses reveal an app’s position in relation to its peers. The authors also develop a user segment overlapping index based on user’s social media data. The authors combine this new index with the product functionality similarity index to map indirect and direct competitors with and without user lock-in. Originality/value: The approach improves on previous approaches by fully automating information extraction from multiple online sources. The authors believe this is the first system of its kind. With limited human intervention, the methodology can easily be adapted to different settings, giving quicker, more reliable real-time results. The approach is also cost effective for market analysis projects covering different data sources.",2017-01-01,2-s2.0-85021349194,Business Process Management Journal,Automated competitor analysis using big data analytics: Evidence from the fitness mobile app business,"Purpose: Competitor analysis is a key component in operations management. Most business decisions are rooted in the analysis of rival products inferred from market structure. Relative to more traditional competitor analysis methods, the purpose of this paper is to provide operations managers with an innovative tool to monitor a firm’s market position and competitors in real time at higher resolution and lower cost than more traditional competitor analysis methods. Design/methodology/approach: The authors combine the techniques of Web Crawler, Natural Language Processing and Machine Learning algorithms with data visualization to develop a big data competitor-analysis system that informs operations managers about competitors and meaningful relationships among them. The authors illustrate the approach using the fitness mobile app business. Findings: The study shows that the system supports operational decision making both descriptively and prescriptively. In particular, the innovative probabilistic topic modeling algorithm combined with conventional multidimensional scaling, product feature comparison and market structure analyses reveal an app’s position in relation to its peers. The authors also develop a user segment overlapping index based on user’s social media data. The authors combine this new index with the product functionality similarity index to map indirect and direct competitors with and without user lock-in. Originality/value: The approach improves on previous approaches by fully automating information extraction from multiple online sources. The authors believe this is the first system of its kind. With limited human intervention, the methodology can easily be adapted to different settings, giving quicker, more reliable real-time results. The approach is also cost effective for market analysis projects covering different data sources."
250,"This study analyzes the political agenda of the European Parliament (EP) plenary, how it has evolved over time, and the manner in which Members of the European Parliament (MEPs) have reacted to external and internal stimuli when making plenary speeches. To unveil the plenary agenda and detect latent themes in legislative speeches over time, MEP speech content is analyzed using a new dynamic topic modeling method based on two layers of Non-negative Matrix Factorization (NMF). This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 1999 to 2014. Our findings suggest that two-layer NMF is a valuable alternative to existing dynamic topic modeling approaches found in the literature, and can unveil niche topics and associated vocabularies not captured by existing methods. Substantively, our findings suggest that the political agenda of the EP evolves significantly over time and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro Crisis. MEP contributions to the plenary agenda are also found to be impacted upon by voting behavior and the committee structure of the Parliament.",2017-01-01,2-s2.0-85015165471,Political Analysis,Exploring the political agenda of the european parliament using a dynamic topic modeling approach,"This study analyzes the political agenda of the European Parliament (EP) plenary, how it has evolved over time, and the manner in which Members of the European Parliament (MEPs) have reacted to external and internal stimuli when making plenary speeches. To unveil the plenary agenda and detect latent themes in legislative speeches over time, MEP speech content is analyzed using a new dynamic topic modeling method based on two layers of Non-negative Matrix Factorization (NMF). This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 1999 to 2014. Our findings suggest that two-layer NMF is a valuable alternative to existing dynamic topic modeling approaches found in the literature, and can unveil niche topics and associated vocabularies not captured by existing methods. Substantively, our findings suggest that the political agenda of the EP evolves significantly over time and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro Crisis. MEP contributions to the plenary agenda are also found to be impacted upon by voting behavior and the committee structure of the Parliament."
251,"Part of speech (POS) taggers and dependency parsers tend to work well on homogeneous datasets but their performance suffers on datasets containing data from different genres. In our current work, we investigate how to create POS tagging and dependency parsing experts for heterogeneous data by employing topic modeling. We create topic models (using Latent Dirichlet Allocation) to determine genres from a heterogeneous dataset and then train an expert for each of the genres. Our results show that the topic modeling experts reach substantial improvements when compared to the general versions. For dependency parsing, the improvement reaches 2 percent points over the full training baseline when we use two topics.",2017-01-01,2-s2.0-85021637907,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference",Creating POS tagging & dependency parsing experts via topic modeling,"Part of speech (POS) taggers and dependency parsers tend to work well on homogeneous datasets but their performance suffers on datasets containing data from different genres. In our current work, we investigate how to create POS tagging and dependency parsing experts for heterogeneous data by employing topic modeling. We create topic models (using Latent Dirichlet Allocation) to determine genres from a heterogeneous dataset and then train an expert for each of the genres. Our results show that the topic modeling experts reach substantial improvements when compared to the general versions. For dependency parsing, the improvement reaches 2 percent points over the full training baseline when we use two topics."
252,"Research topics, as indicators of the profession's development, are central to the evaluation of academic practices in communication research. To investigate the main topics in our field, we trace the development of research topics since the 1930s by evaluating more than 15,000 articles from 19 academic journals based on an automated content analysis. Topic modeling reveals a high diversity from the early years on. Only a few journals show the tendency to focus on one topic only, whereas most outlets cover a broad variety and thus represent the field as a whole. Although our discipline is strongly interconnected with the changing media landscape, results show that communication research is characterized by high consistency. Although they have not provoked a revolutionary change, Internet and social media have become the most monitored media, parallel to-not displacing-classic media such as newspapers and TV.",2017-01-01,2-s2.0-85042143013,International Journal of Communication,What communication scholars write about: An analysis of 80 years of research in high-impact journals,"Research topics, as indicators of the profession's development, are central to the evaluation of academic practices in communication research. To investigate the main topics in our field, we trace the development of research topics since the 1930s by evaluating more than 15,000 articles from 19 academic journals based on an automated content analysis. Topic modeling reveals a high diversity from the early years on. Only a few journals show the tendency to focus on one topic only, whereas most outlets cover a broad variety and thus represent the field as a whole. Although our discipline is strongly interconnected with the changing media landscape, results show that communication research is characterized by high consistency. Although they have not provoked a revolutionary change, Internet and social media have become the most monitored media, parallel to-not displacing-classic media such as newspapers and TV."
253,"Radical social movements are broadly engaged in, and dedicated to, promoting change in their social environment. In their corresponding efforts to call attention to various causes, communicate with like-minded groups, and mobilize support for their activities, radical social movements also produce an enormous amount of text. These texts, like radical social movements themselves, are often (i) densely connected and (ii) highly variable in advocated protest activities. Given a corpus of radical social movement texts, can one uncover the underlying network structure of the radical activist groups involved in this movement? If so, can one then also identify which groups (and which subnetworks) are more prone to radical versus mainstream protest activities? Using a large corpus of British radical environmentalist texts (1992–2003), we seek to answer these questions through a novel integration of network discovery and unsupervised topic modeling. In doing so, we apply classic network descriptives (e.g., centrality measures) and more modern statistical models (e.g., exponential random graph models) to carefully parse apart these questions. Our findings provide a number of revealing insights into the networks and nature of radical environmentalists and their texts.",2017-01-01,2-s2.0-85041625544,Sociological Methods and Research,Using Radical Environmentalist Texts to Uncover Network Structure and Network Features,"Radical social movements are broadly engaged in, and dedicated to, promoting change in their social environment. In their corresponding efforts to call attention to various causes, communicate with like-minded groups, and mobilize support for their activities, radical social movements also produce an enormous amount of text. These texts, like radical social movements themselves, are often (i) densely connected and (ii) highly variable in advocated protest activities. Given a corpus of radical social movement texts, can one uncover the underlying network structure of the radical activist groups involved in this movement? If so, can one then also identify which groups (and which subnetworks) are more prone to radical versus mainstream protest activities? Using a large corpus of British radical environmentalist texts (1992–2003), we seek to answer these questions through a novel integration of network discovery and unsupervised topic modeling. In doing so, we apply classic network descriptives (e.g., centrality measures) and more modern statistical models (e.g., exponential random graph models) to carefully parse apart these questions. Our findings provide a number of revealing insights into the networks and nature of radical environmentalists and their texts."
254,"Security analysts have not systematically studied visual discourses, even if they apparently play a prominent role in current propaganda efforts. The article intends to address this disciplinary insufficiency by introducing an inter-scientific approach to analysing large visual data samples. The article illustrates the method by applying it to the dataset comprised of the IS's visual stills. To achieve this goal, the article will first introduce an archive compiled by utilising the knowledge of IS's content dissemination strategies. Second, the article addresses narrative techniques used to effectively convey the message of an alternative worldview. Finally, the text introduces a computational method based on probabilistic topic modelling. Reflecting the results of its application, the article argues that with the growing complexity of the resulting topic sets there is an increase in the probability that the propagandist effort in question represents an attempt at a systematic articulation of a holistic socio-political order alternative to the status quo.",2017-01-01,2-s2.0-85037158661,Central European Journal of International and Security Studies,Establishing the complexity of the Islamic state's visual propaganda,"Security analysts have not systematically studied visual discourses, even if they apparently play a prominent role in current propaganda efforts. The article intends to address this disciplinary insufficiency by introducing an inter-scientific approach to analysing large visual data samples. The article illustrates the method by applying it to the dataset comprised of the IS's visual stills. To achieve this goal, the article will first introduce an archive compiled by utilising the knowledge of IS's content dissemination strategies. Second, the article addresses narrative techniques used to effectively convey the message of an alternative worldview. Finally, the text introduces a computational method based on probabilistic topic modelling. Reflecting the results of its application, the article argues that with the growing complexity of the resulting topic sets there is an increase in the probability that the propagandist effort in question represents an attempt at a systematic articulation of a holistic socio-political order alternative to the status quo."
255,"Preferences of employers have critical roles to determine job-matching processes between the university graduates and the employers. Some researches show that the employers expect self-directedness to the graduates, but these results are only based on the sample of very large companies. This study investigates what types of ability employers demand from graduates, with a special focus on the differences among sizes of companies. Our data are collected from job-search website for new graduates in 2016, and consist of 20,859 companies including large and small companies. We analyze the descriptions obtained from the companies using topic modeling. The results show that large companies and small ones expect different abilities to the graduates. Large companies tend to expect self-directedness and entrepreneurial spirit to graduates. This is consistent with previous studies. By contrast, small companies tend to demand discipline and aggressiveness. These findings suggest that previous studies have overlooked the trends of small companies.",2017-01-01,2-s2.0-85045194006,Sociological Theory and Methods,What types of ability do employers expect to graduates?,"Preferences of employers have critical roles to determine job-matching processes between the university graduates and the employers. Some researches show that the employers expect self-directedness to the graduates, but these results are only based on the sample of very large companies. This study investigates what types of ability employers demand from graduates, with a special focus on the differences among sizes of companies. Our data are collected from job-search website for new graduates in 2016, and consist of 20,859 companies including large and small companies. We analyze the descriptions obtained from the companies using topic modeling. The results show that large companies and small ones expect different abilities to the graduates. Large companies tend to expect self-directedness and entrepreneurial spirit to graduates. This is consistent with previous studies. By contrast, small companies tend to demand discipline and aggressiveness. These findings suggest that previous studies have overlooked the trends of small companies."
256,"Interactive topic models are powerful tools for understanding large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms-an approach we call ""Tandem Anchors"". We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches.",2017-01-01,2-s2.0-85040918264,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",Tandem anchoring: A multiword anchor approach for interactive topic modeling,"Interactive topic models are powerful tools for understanding large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms-an approach we call ""Tandem Anchors"". We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches."
257,"Purpose: The purpose of this paper is automatic classification of TV series reviews based on generic categories. Design/methodology/approach: What the authors mainly applied is using surrogate instead of specific roles or actors’ name in reviews to make reviews more generic. Besides, feature selection techniques and different kinds of classifiers are incorporated. Findings: With roles’ and actors’ names replaced by generic tags, the experimental result showed that it can generalize well to agnostic TV series as compared with reviews keeping the original names. Research limitations/implications: The model presented in this paper must be built on top of an already existed knowledge base like Baidu Encyclopedia. Such database takes lots of work. Practical implications: Like in digital information supply chain, if reviews are part of the information to be transported or exchanged, then the model presented in this paper can help automatically identify individual review according to different requirements and help the information sharing. Originality/value: One originality is that the authors proposed the surrogate-based approach to make reviews more generic. Besides, they also built a review data set of hot Chinese TV series, which includes eight generic category labels for each review.",2017-01-01,2-s2.0-85021717653,Information Discovery and Delivery,A surrogate-based generic classifier for Chinese TV series reviews,"Purpose: The purpose of this paper is automatic classification of TV series reviews based on generic categories. Design/methodology/approach: What the authors mainly applied is using surrogate instead of specific roles or actors’ name in reviews to make reviews more generic. Besides, feature selection techniques and different kinds of classifiers are incorporated. Findings: With roles’ and actors’ names replaced by generic tags, the experimental result showed that it can generalize well to agnostic TV series as compared with reviews keeping the original names. Research limitations/implications: The model presented in this paper must be built on top of an already existed knowledge base like Baidu Encyclopedia. Such database takes lots of work. Practical implications: Like in digital information supply chain, if reviews are part of the information to be transported or exchanged, then the model presented in this paper can help automatically identify individual review according to different requirements and help the information sharing. Originality/value: One originality is that the authors proposed the surrogate-based approach to make reviews more generic. Besides, they also built a review data set of hot Chinese TV series, which includes eight generic category labels for each review."
258,"Measuring the impact and productivity of an author is an important, yet a challenging task. Most of the existing methods for ranking or indexing of authors are based on simple parameters such as publication counts, citation counts and their combinations. These methods are topic independent, hence ignoring the intra-field differences. This study introduces a specific method for indexing of researchers to measure their productivity in a given field of interest, believing that an author can be interested in more than one fields and can have different level of expertise in all these fields. This paper proposes Domain Specific Index (DSI), a novel method for indexing of authors with respect to their fields of interest. Latent Dirichlet Allocation (LDA) is applied to capture the latent topics within text corpora. DSI calculates the standing of an author in all topics of his or her interest by considering topic based citations instead of using overall citations like traditional methods. The citations received by a multi-authored paper are divided among all its co-authors on the basis of their topic probability in that particular field. Results show that instead of giving credit of received citations equally to all co-authors of a paper, if a weight is given with respect to their level of interest in that field, more specific authors in that field will be ranked as top authors.",2017-01-01,2-s2.0-85016460620,Malaysian Journal of Library and Information Science,Indexing of authors according to their domain of expertise,"Measuring the impact and productivity of an author is an important, yet a challenging task. Most of the existing methods for ranking or indexing of authors are based on simple parameters such as publication counts, citation counts and their combinations. These methods are topic independent, hence ignoring the intra-field differences. This study introduces a specific method for indexing of researchers to measure their productivity in a given field of interest, believing that an author can be interested in more than one fields and can have different level of expertise in all these fields. This paper proposes Domain Specific Index (DSI), a novel method for indexing of authors with respect to their fields of interest. Latent Dirichlet Allocation (LDA) is applied to capture the latent topics within text corpora. DSI calculates the standing of an author in all topics of his or her interest by considering topic based citations instead of using overall citations like traditional methods. The citations received by a multi-authored paper are divided among all its co-authors on the basis of their topic probability in that particular field. Results show that instead of giving credit of received citations equally to all co-authors of a paper, if a weight is given with respect to their level of interest in that field, more specific authors in that field will be ranked as top authors."
259,"The potential of social media to give insight into the dynamic evolution of public conversations, and into their reactive and constitutive role in political activities, has to date been underdeveloped. While topic modeling can give static insight into the structure of a conversation, and keyword volume tracking can show how engagement with a specific idea varies over time, there is need for a method of analysis able to understand how conversations about societal values evolve and react to events in the world by incorporating new ideas and relating them to existing themes. In this article, we propose a method for analyzing social media messages that formalizes the structure of public conversations and allows the sociologist to study the evolution of public discourse in a rigorous, replicable, and data-driven fashion. This approach may be useful to those studying the social construction of meaning, the origins of factionalism and internecine conflict, or boundary-setting and group-identification exercises and has potential implications.",2017-01-01,2-s2.0-85041326168,Sociological Methods and Research,Beyond Keywords: Tracking the Evolution of Conversational Clusters in Social Media,"The potential of social media to give insight into the dynamic evolution of public conversations, and into their reactive and constitutive role in political activities, has to date been underdeveloped. While topic modeling can give static insight into the structure of a conversation, and keyword volume tracking can show how engagement with a specific idea varies over time, there is need for a method of analysis able to understand how conversations about societal values evolve and react to events in the world by incorporating new ideas and relating them to existing themes. In this article, we propose a method for analyzing social media messages that formalizes the structure of public conversations and allows the sociologist to study the evolution of public discourse in a rigorous, replicable, and data-driven fashion. This approach may be useful to those studying the social construction of meaning, the origins of factionalism and internecine conflict, or boundary-setting and group-identification exercises and has potential implications."
260,"Communication in social media is increasingly being found to reproduce or even reinforce ethnic prejudice and hostility toward migrants. In Russia of the 2010s, with its world's second largest immigrant population, polls have detected high levels of hostility of the Russian population toward migranty (migrants), a label attached to resettlers from Central Asia and the Caucasus. We tested the online hostility hypothesis by using the data of 363,000 posts from the Russian-language LiveJournal. We applied data mining, regression analysis, and selective interpretative reading to map bloggers' attitudes toward migranty, among other ethnicities and nations. Our findings significantly alter the picture drawn from the polls: Migranty neither provoke the biggest amount of discussion nor experience the worst treatment in Russian blogs, in which Americans take the lead. Furthermore, Central Asians and North Caucasians are treated very differently.",2017-01-01,2-s2.0-85047969630,International Journal of Communication,Who's bad? Attitudes toward resettlers from the post-soviet south versus other nations in the Russian blogosphere,"Communication in social media is increasingly being found to reproduce or even reinforce ethnic prejudice and hostility toward migrants. In Russia of the 2010s, with its world's second largest immigrant population, polls have detected high levels of hostility of the Russian population toward migranty (migrants), a label attached to resettlers from Central Asia and the Caucasus. We tested the online hostility hypothesis by using the data of 363,000 posts from the Russian-language LiveJournal. We applied data mining, regression analysis, and selective interpretative reading to map bloggers' attitudes toward migranty, among other ethnicities and nations. Our findings significantly alter the picture drawn from the polls: Migranty neither provoke the biggest amount of discussion nor experience the worst treatment in Russian blogs, in which Americans take the lead. Furthermore, Central Asians and North Caucasians are treated very differently."
261,"This paper presents HBIN-LBD, a novel literature-based discovery (LBD) method that exploits the lexico-citation structures within the heterogeneous bibliographic information network (HBIN) graphs. Unlike other existing LBD methods, HBIN-LBD harnesses the metapath features found in HBIN graphs for discovering the latent associations between scientific papers published in otherwise disconnected research areas. Further, this paper investigates the effects of incorporating semantic and topic modeling components into the proposed models. Using time-sliced historical bibliographic data, we demonstrate the performance of our method by reconstructing two LBD hypotheses: the Fish Oil and Raynaud's Syndrome hypothesis and the Migraine and Magnesium hypothesis. The proposed method is capable of predicting the future co-citation links between research papers of these previously disconnected research areas with up to 88.86% accuracy and 0.89 F-measure.",2017-01-01,2-s2.0-84995910778,Knowledge-Based Systems,Learning the heterogeneous bibliographic information network for literature-based discovery,"This paper presents HBIN-LBD, a novel literature-based discovery (LBD) method that exploits the lexico-citation structures within the heterogeneous bibliographic information network (HBIN) graphs. Unlike other existing LBD methods, HBIN-LBD harnesses the metapath features found in HBIN graphs for discovering the latent associations between scientific papers published in otherwise disconnected research areas. Further, this paper investigates the effects of incorporating semantic and topic modeling components into the proposed models. Using time-sliced historical bibliographic data, we demonstrate the performance of our method by reconstructing two LBD hypotheses: the Fish Oil and Raynaud's Syndrome hypothesis and the Migraine and Magnesium hypothesis. The proposed method is capable of predicting the future co-citation links between research papers of these previously disconnected research areas with up to 88.86% accuracy and 0.89 F-measure."
262,"Due to the construction of infrastructure of wired and wireless networks and raid development of the speed, digital based data, which is no longer manageable with general technology, is increasing explosively and its form and quantity are tremendous. In accordance with this, the application plan of previously unused data and the area of value creation through this are gradually widened. Especially, the importance of text data analysis, which represents the public's opinion such as social network services (social media) and online product reviews, is magnified. Like this, the development of comprehension and prediction solutions of customer needs that utilize the reviews is estimated to optimize the values of all the future industries and technologies and is expected to become the base of upcoming economic effect creation. In other words, seeing the hidden value through data can suggest pending issues that enhance the competitiveness of the company. Especially, as extensive amount of data is created in games, it is deemed as a promising business that can expect high growth through future data analysis. However, the research on the opinion analysis based on the importance of game reviews and texts is unsatisfactory. Therefore, in this research, we would like to look into utilization of review data within a game and examine text data analysis technique and application plan based on the existing preceding research. In addition, through opinion mining, we have tried to investigate what are some major keywords per topic within a game and compare what are the characteristics of each game that can be inferred through comparison of two analytical techniques. Moreover, we wanted to suggest practical implications for creating economic value such as game sales and system improvement through utilization of review data in the game industry in the future.",2017-01-01,2-s2.0-85032572196,International Journal of Applied Business and Economic Research,A study of analyzing STEAM game review data using text mining,"Due to the construction of infrastructure of wired and wireless networks and raid development of the speed, digital based data, which is no longer manageable with general technology, is increasing explosively and its form and quantity are tremendous. In accordance with this, the application plan of previously unused data and the area of value creation through this are gradually widened. Especially, the importance of text data analysis, which represents the public's opinion such as social network services (social media) and online product reviews, is magnified. Like this, the development of comprehension and prediction solutions of customer needs that utilize the reviews is estimated to optimize the values of all the future industries and technologies and is expected to become the base of upcoming economic effect creation. In other words, seeing the hidden value through data can suggest pending issues that enhance the competitiveness of the company. Especially, as extensive amount of data is created in games, it is deemed as a promising business that can expect high growth through future data analysis. However, the research on the opinion analysis based on the importance of game reviews and texts is unsatisfactory. Therefore, in this research, we would like to look into utilization of review data within a game and examine text data analysis technique and application plan based on the existing preceding research. In addition, through opinion mining, we have tried to investigate what are some major keywords per topic within a game and compare what are the characteristics of each game that can be inferred through comparison of two analytical techniques. Moreover, we wanted to suggest practical implications for creating economic value such as game sales and system improvement through utilization of review data in the game industry in the future."
263,"Students attending lectures in universities suffer from a weak structural awareness on lecture content. According to learning theories, structural awareness is a relevant factor to association and comprehension of new learning inputs. We synthesize semantic structures from non annotated lecture slides using Topic Modeling algorithms to identify relevant terms and relate them in force-directed graphs. The synthesized graphs provide a structural overview on the topic distribution and relations of non annotated sequential lecture slides.",2017-01-01,2-s2.0-85023194392,CSEDU 2017 - Proceedings of the 9th International Conference on Computer Supported Education,Autonomous semantic structuring of lecture topics synthesis of knowledge models,"Students attending lectures in universities suffer from a weak structural awareness on lecture content. According to learning theories, structural awareness is a relevant factor to association and comprehension of new learning inputs. We synthesize semantic structures from non annotated lecture slides using Topic Modeling algorithms to identify relevant terms and relate them in force-directed graphs. The synthesized graphs provide a structural overview on the topic distribution and relations of non annotated sequential lecture slides."
264,This paper uses topic modeling and statistical analysis of keywords within early American scientific journals in order to better understand the professionalization of American science in the late 19,2017-01-01,2-s2.0-85040789792,Proceedings of the Association for Information Science and Technology,Textual analysis and the history of scholarly communication,This paper uses topic modeling and statistical analysis of keywords within early American scientific journals in order to better understand the professionalization of American science in the late 19
265,"This poster presents preliminary findings from a textual analysis of social media posts created by public libraries. We collected a text corpus, consisting of 3,622 Facebook posts containing more than seven words from 151 public libraries. We conducted a term frequency analysis and latent dirichlet allocation (LDA) topic modeling to explore the content of Facebook posts. The results revealed that terms related to library programming and library event announcements appeared frequently. The LDA topic modeling identified 25 topics underlying the corpus. We observed various topics were posted on public library Facebook Pages while library programming and event related topics were most common.",2017-01-01,2-s2.0-85040778133,Proceedings of the Association for Information Science and Technology,Content analysis of facebook posts in public libraries based on textual analysis,"This poster presents preliminary findings from a textual analysis of social media posts created by public libraries. We collected a text corpus, consisting of 3,622 Facebook posts containing more than seven words from 151 public libraries. We conducted a term frequency analysis and latent dirichlet allocation (LDA) topic modeling to explore the content of Facebook posts. The results revealed that terms related to library programming and library event announcements appeared frequently. The LDA topic modeling identified 25 topics underlying the corpus. We observed various topics were posted on public library Facebook Pages while library programming and event related topics were most common."
266,"Social media data have recently attracted considerable attention as an emerging voice of the customer as it has rapidly become a channel for exchanging and storing customer-generated, large-scale, and unregulated voices about products. Although product planning studies using social media data have used systematic methods for product planning, their methods have limitations, such as the difficulty of identifying latent product features due to the use of only term-level analysis and insufficient consideration of opportunity potential analysis of the identified features. Therefore, an opportunity mining approach is proposed in this study to identify product opportunities based on topic modeling and sentiment analysis of social media data. For a multifunctional product, this approach can identify latent product topics discussed by product customers in social media using topic modeling, thereby quantifying the importance of each product topic. Next, the satisfaction level of each product topic is evaluated using sentiment analysis. Finally, the opportunity value and improvement direction of each product topic from a customer-centered view are identified by an opportunity algorithm based on product topics' importance and satisfaction. We expect that our approach for product planning will contribute to the systematic identification of product opportunities from large-scale customer-generated social media data and will be used as a real-time monitoring tool for changing customer needs analysis in rapidly evolving product environments.",2017-01-01,2-s2.0-85031108967,International Journal of Information Management,Social media mining for product planning: A product opportunity mining approach based on topic modeling and sentiment analysis,"Social media data have recently attracted considerable attention as an emerging voice of the customer as it has rapidly become a channel for exchanging and storing customer-generated, large-scale, and unregulated voices about products. Although product planning studies using social media data have used systematic methods for product planning, their methods have limitations, such as the difficulty of identifying latent product features due to the use of only term-level analysis and insufficient consideration of opportunity potential analysis of the identified features. Therefore, an opportunity mining approach is proposed in this study to identify product opportunities based on topic modeling and sentiment analysis of social media data. For a multifunctional product, this approach can identify latent product topics discussed by product customers in social media using topic modeling, thereby quantifying the importance of each product topic. Next, the satisfaction level of each product topic is evaluated using sentiment analysis. Finally, the opportunity value and improvement direction of each product topic from a customer-centered view are identified by an opportunity algorithm based on product topics' importance and satisfaction. We expect that our approach for product planning will contribute to the systematic identification of product opportunities from large-scale customer-generated social media data and will be used as a real-time monitoring tool for changing customer needs analysis in rapidly evolving product environments."
267,"Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation (LDA). However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach (ISLDA) where certain predefined sets of keywords (that define the topics researchers are interested in) are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis.",2017-01-01,2-s2.0-85011601372,Journal of Information Science,Topic modelling for qualitative studies,"Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation (LDA). However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach (ISLDA) where certain predefined sets of keywords (that define the topics researchers are interested in) are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis."
268,"The growing popularity of social media networks such as Twitter, Facebook, Instagram and so forth is taking the Internet sphere to a higher level, creating a huge volume of social network-generated data, including tweets. As a result, all these data and information can be easily or exclusively found on the Internet by the public. However, little is known yet about how to turn this data into useful knowledge and the collection of the data has not been sufficiently researched, especially in the online retail industry. In an effort to help companies in the online retail industry to utilise the data, this paper explores social network-generated contents on Twitter sites through topics identification that can subsequently facilitate companies to improve their services and performances. This paper describes a study based on data gathered on the Twitter sites of the leading UK retailers: Amazon, Argos and Tesco, during the periods of Black Friday, Boxing Day and Christmas in the UK. We chose Twitter as a platform because, whilst other previous works addressed online reviews and website contents in their studies, microblogs are just starting to be explored due to their characters' limitations and informal content making it harder to interpret. In this paper, we infer topics from short-Text tweets posted by customers mentioning brand names by employing a machine learning approach. We then analyse the tweets to determine the primary issues or topics focused by customers regarding these online retail brands by interpreting the themes of the specific keywords set represented. The topics which emerge based on the collection of tweet posts contribute to various aspects of companies' operations, such as delivery, customer service and product performance. We believe that this study derives some implications and insights for online retail brand companies in improving their service provisions, especially in customer service management. Insights on such topics can be beneficial in numerous ways, such as marketing, innovation and public image.",2017-01-01,2-s2.0-85039840879,"Proceedings of the 11th European Conference on Information Systems Management, ECISM 2017",Mining social network content of online retail brands: A machine learning approach,"The growing popularity of social media networks such as Twitter, Facebook, Instagram and so forth is taking the Internet sphere to a higher level, creating a huge volume of social network-generated data, including tweets. As a result, all these data and information can be easily or exclusively found on the Internet by the public. However, little is known yet about how to turn this data into useful knowledge and the collection of the data has not been sufficiently researched, especially in the online retail industry. In an effort to help companies in the online retail industry to utilise the data, this paper explores social network-generated contents on Twitter sites through topics identification that can subsequently facilitate companies to improve their services and performances. This paper describes a study based on data gathered on the Twitter sites of the leading UK retailers: Amazon, Argos and Tesco, during the periods of Black Friday, Boxing Day and Christmas in the UK. We chose Twitter as a platform because, whilst other previous works addressed online reviews and website contents in their studies, microblogs are just starting to be explored due to their characters' limitations and informal content making it harder to interpret. In this paper, we infer topics from short-Text tweets posted by customers mentioning brand names by employing a machine learning approach. We then analyse the tweets to determine the primary issues or topics focused by customers regarding these online retail brands by interpreting the themes of the specific keywords set represented. The topics which emerge based on the collection of tweet posts contribute to various aspects of companies' operations, such as delivery, customer service and product performance. We believe that this study derives some implications and insights for online retail brand companies in improving their service provisions, especially in customer service management. Insights on such topics can be beneficial in numerous ways, such as marketing, innovation and public image."
269,"The Aviation Safety Reporting System includes over a million confidential reports describing safety incidents. Natural language processing techniques allow for relatively rapid and largely automated analysis of large collections of text data. Meaningful interpretation of the results and further investigations by subject matter experts can follow. This article describes the application of structural topic modeling to Aviation Safety Reporting System data. Results reveal that the application is able to identify known issues. The method also has the potential to identify previously unknown connections that may warrant further, more manual, study. Results reported here highlight the importance of fuel pump, tank, and landing gear issues and the relative insignificance of smoke and fire issues for private aircraft. The results also uncovered evidence of the prominence of the Quiet Bridge Visual and Tip Toe Visual approach paths at San Francisco International Airport in safety incident reports.",2017-01-01,2-s2.0-85030982166,12th USA/Europe Air Traffic Management R and D Seminar,Topics and trends in incident reports using structural topic modeling to explore aviation safety reporting system data,"The Aviation Safety Reporting System includes over a million confidential reports describing safety incidents. Natural language processing techniques allow for relatively rapid and largely automated analysis of large collections of text data. Meaningful interpretation of the results and further investigations by subject matter experts can follow. This article describes the application of structural topic modeling to Aviation Safety Reporting System data. Results reveal that the application is able to identify known issues. The method also has the potential to identify previously unknown connections that may warrant further, more manual, study. Results reported here highlight the importance of fuel pump, tank, and landing gear issues and the relative insignificance of smoke and fire issues for private aircraft. The results also uncovered evidence of the prominence of the Quiet Bridge Visual and Tip Toe Visual approach paths at San Francisco International Airport in safety incident reports."
270,"Disputed authorship and text transfer are notorious problems in the textual transmission of Sanskrit, especially for large anonymous texts such as the Mahābhārata. Stratification methods for such texts have so far mainly relied on manuscriptology, higher textual criticism, and scattered historical evidence. This paper introduces a quantitative method for text stratification that uses frequent linguistic features for inducing authorial layers in Sanskrit texts. The proposed method is tested with texts whose authorial composition is known, and then applied to the Bhīşmaparvan of the Mahābhārata.",2017-01-01,2-s2.0-85017008991,Indo-Iranian Journal,Stratifying the Mahābhārata,"Disputed authorship and text transfer are notorious problems in the textual transmission of Sanskrit, especially for large anonymous texts such as the Mahābhārata. Stratification methods for such texts have so far mainly relied on manuscriptology, higher textual criticism, and scattered historical evidence. This paper introduces a quantitative method for text stratification that uses frequent linguistic features for inducing authorial layers in Sanskrit texts. The proposed method is tested with texts whose authorial composition is known, and then applied to the Bhīşmaparvan of the Mahābhārata."
271,"Topical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents. The ranking procedure consists of running PageRank K times, where K is the number of topics used in the LDA model. In this paper, we propose a modification of TPR, called Salience Rank. Salience Rank only needs to run PageRank once and extracts comparable or better keyphrases on benchmark datasets. In addition to quality and efficiency benefits, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity.",2017-01-01,2-s2.0-85040619681,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",Salience rank: Efficient keyphrase extraction with topic modeling,"Topical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents. The ranking procedure consists of running PageRank K times, where K is the number of topics used in the LDA model. In this paper, we propose a modification of TPR, called Salience Rank. Salience Rank only needs to run PageRank once and extracts comparable or better keyphrases on benchmark datasets. In addition to quality and efficiency benefits, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity."
272,"In modern day software development environments, analysis and understanding of the emerging industry needs is of strategic importance for a more effective software engineering (SE) education that is innovative and responsive to changing industry needs. Considering the demand for well-trained software engineers in the near future, an empirical study was performed on SE job postings in order to identify the emerging needs and trends in the software industry. The methodology of this study was based on semantic topic analysis implemented by latent Dirichlet allocation (LDA), a probabilistic generative approach for topic modeling. The findings of the study indicated that, the software industry has a wide spectrum in terms of professional roles, responsibilities (in-demand skills) and combinations of programming languages. Each of the professional roles is profoundly based on specific skill sets that reflect the dynamics of the software industry. Also, the topics discovered by LDA highlighted a broad range of the characteristics of the SE, such as contemporary trends, demands, skills, tools, platforms, methodologies, and technologies that indicate the level of progress in this dynamic field. In light of these findings, an innovative academic curriculum for SE education can be designed consistent with the emerging needs and trends in the software industry. In this regard, the findings can provide valuable implications for the industry, academia, and SE community to close the gap between the industry needs and the current SE education.",2017-01-01,2-s2.0-85021934602,International Journal of Engineering Education,Analysis of software engineering industry needs and trends: Implications for education,"In modern day software development environments, analysis and understanding of the emerging industry needs is of strategic importance for a more effective software engineering (SE) education that is innovative and responsive to changing industry needs. Considering the demand for well-trained software engineers in the near future, an empirical study was performed on SE job postings in order to identify the emerging needs and trends in the software industry. The methodology of this study was based on semantic topic analysis implemented by latent Dirichlet allocation (LDA), a probabilistic generative approach for topic modeling. The findings of the study indicated that, the software industry has a wide spectrum in terms of professional roles, responsibilities (in-demand skills) and combinations of programming languages. Each of the professional roles is profoundly based on specific skill sets that reflect the dynamics of the software industry. Also, the topics discovered by LDA highlighted a broad range of the characteristics of the SE, such as contemporary trends, demands, skills, tools, platforms, methodologies, and technologies that indicate the level of progress in this dynamic field. In light of these findings, an innovative academic curriculum for SE education can be designed consistent with the emerging needs and trends in the software industry. In this regard, the findings can provide valuable implications for the industry, academia, and SE community to close the gap between the industry needs and the current SE education."
273,"Social media based digital epidemiology has the potential to support faster response and deeper understanding of public health related threats. This study proposes a new framework to analyze unstructured health related textual data via Twitter users' post (tweets) to characterize the negative health sentiments and nonhealth related concerns in relations to the corpus of negative sentiments regarding diet, diabetes, exercise and obesity (DDEO). Through the collection of six million Tweets for one month, this study identified the prominent topics of users as it relates to the negative sentiments. Our proposed framework uses two text mining methods, sentiment analysis and topic modeling, to discover negative topics. The negative sentiments of Twitter users support the literature narratives and the many morbidity issues that are associated with DDEO and the linkage between obesity and diabetes. The framework offers a potential method to understand the publics' opinions and sentiments regarding DDEO. More importantly, this research provides new opportunities for computational social scientists, medical experts and public health professionals to collectively address DDEOrelated issues.",2017-01-01,2-s2.0-85040762565,Proceedings of the Association for Information Science and Technology,"Computational content analysis of negative tweets for obesity, diet, diabetes, and exercise","Social media based digital epidemiology has the potential to support faster response and deeper understanding of public health related threats. This study proposes a new framework to analyze unstructured health related textual data via Twitter users' post (tweets) to characterize the negative health sentiments and nonhealth related concerns in relations to the corpus of negative sentiments regarding diet, diabetes, exercise and obesity (DDEO). Through the collection of six million Tweets for one month, this study identified the prominent topics of users as it relates to the negative sentiments. Our proposed framework uses two text mining methods, sentiment analysis and topic modeling, to discover negative topics. The negative sentiments of Twitter users support the literature narratives and the many morbidity issues that are associated with DDEO and the linkage between obesity and diabetes. The framework offers a potential method to understand the publics' opinions and sentiments regarding DDEO. More importantly, this research provides new opportunities for computational social scientists, medical experts and public health professionals to collectively address DDEOrelated issues."
274,"Aspect extraction is a task to abstract the common properties of objects from corpora discussing them, such as reviews of products. Recent work on aspect extraction is leveraging the hierarchical relationship between products and their categories. However, such effort focuses on the aspects of child categories but ignores those from parent categories. Hence, we propose an LDA-based generative topic model inducing the two-layer categorical information (CAT-LDA), to balance the aspects of both a parent category and its child categories. Our hypothesis is that child categories inherit aspects from parent categories, controlled by the hierarchy between them. Experimental results on 5 categories of Amazon.com products show that both common aspects of parent category and the individual aspects of subcategories can be extracted to align well with the common sense. We further evaluate the manually extracted aspects of 16 products, resulting in an average hit rate of 79.10%.",2017-01-01,2-s2.0-85021756678,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference",Aspect extraction from product reviews using category hierarchy information,"Aspect extraction is a task to abstract the common properties of objects from corpora discussing them, such as reviews of products. Recent work on aspect extraction is leveraging the hierarchical relationship between products and their categories. However, such effort focuses on the aspects of child categories but ignores those from parent categories. Hence, we propose an LDA-based generative topic model inducing the two-layer categorical information (CAT-LDA), to balance the aspects of both a parent category and its child categories. Our hypothesis is that child categories inherit aspects from parent categories, controlled by the hierarchy between them. Experimental results on 5 categories of Amazon.com products show that both common aspects of parent category and the individual aspects of subcategories can be extracted to align well with the common sense. We further evaluate the manually extracted aspects of 16 products, resulting in an average hit rate of 79.10%."
275,"Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.",2017-01-01,2-s2.0-85040908597,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)",Topically driven neural language model,"Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics."
276,"Purpose-The information needs of the users of literature database systems often come from the task at hand, which is short term and can be represented as a small number of articles. Previous works on recommending articles to satisfy users' short-term interests have utilized article content, usage logs, and more recently, coauthorship networks. The usefulness of coauthorship has been demonstrated by some research works, which, however, tend to adopt a simple coauthorship network that records only the strength of coauthorships. The purpose of this paper is to enhance the effectiveness of coauthorship-based recommendation by incorporating scholars' collaboration topics into the coauthorship network. Design/methodology/approach-The authors propose a latent Dirichlet allocation (LDA)-coauthorshipnetwork- based method that integrates topic information into the links of the coauthorship networks using LDA, and a task-focused technique is developed for recommending literature articles. Findings-The experimental results using information systems journal articles show that the proposed method is more effective than the previous coauthorship network-based method over all scenarios examined. The authors further develop a hybrid method that combines the results of content-based and LDAcoauthorship- network-based recommendations. The resulting hybrid method achieves greater or comparable recommendation effectiveness under all scenarios when compared to the content-based method. Originality/value-This paper makes two contributions. The authors first show that topic model is indeed useful and can be incorporated into the construction of coaurthoship-network to improve literature recommendation. The authors subsequently demonstrate that coauthorship-network-based and contentbased recommendations are complementary in their hit article rank distributions, and then devise a hybrid recommendation method to further improve the effectiveness of literature recommendation.",2017-01-01,2-s2.0-85019679456,Online Information Review,Coauthorship network-based literature recommendation with topic model,"Purpose-The information needs of the users of literature database systems often come from the task at hand, which is short term and can be represented as a small number of articles. Previous works on recommending articles to satisfy users' short-term interests have utilized article content, usage logs, and more recently, coauthorship networks. The usefulness of coauthorship has been demonstrated by some research works, which, however, tend to adopt a simple coauthorship network that records only the strength of coauthorships. The purpose of this paper is to enhance the effectiveness of coauthorship-based recommendation by incorporating scholars' collaboration topics into the coauthorship network. Design/methodology/approach-The authors propose a latent Dirichlet allocation (LDA)-coauthorshipnetwork- based method that integrates topic information into the links of the coauthorship networks using LDA, and a task-focused technique is developed for recommending literature articles. Findings-The experimental results using information systems journal articles show that the proposed method is more effective than the previous coauthorship network-based method over all scenarios examined. The authors further develop a hybrid method that combines the results of content-based and LDAcoauthorship- network-based recommendations. The resulting hybrid method achieves greater or comparable recommendation effectiveness under all scenarios when compared to the content-based method. Originality/value-This paper makes two contributions. The authors first show that topic model is indeed useful and can be incorporated into the construction of coaurthoship-network to improve literature recommendation. The authors subsequently demonstrate that coauthorship-network-based and contentbased recommendations are complementary in their hit article rank distributions, and then devise a hybrid recommendation method to further improve the effectiveness of literature recommendation."
277,"Computational models of place are a key component of spatial information theory and play an increasing role in research ranging from spatial search to transportation studies. One method to arrive at such models is to extract knowledge from user-generated content e.g., from texts, tags, trajectories, pictures, and so forth. Over the last years, topic modeling techniques such as latent Dirichlet allocation (LDA) have been studied to reveal linguistic patterns that characterize places and their types. Intuitively, people are more likely to describe places such as Yosemite National Park in terms of hiking, nature, and camping than cocktail or dancing. The geo-indicativeness of non-georeferenced text does not only apply to place instances but also place types, e.g., state parks. While different parks will vary greatly with respect to their landscape and thus human descriptions, the distribution of topics common to all parks will differ significantly from other types of places, e.g., night clubs. This aggregation of topics to the type level creates thematic signatures that can be used for place categorization, data cleansing and conflation, semantic search, and so on. To make full use of these signatures, however, requires a better understanding of their intra-type variability as regional differences effect the predictive power of the signatures. Intuitively, the topic composition for place types such as store and office should be less effected by regional differences than the topic composition for types such as monument and mountain. In this work, we approach this regional variability hypothesis by attempting to prove that all place types are aspatial with respect to their thematic signatures. We reject this hypothesis by comparing the signature similarities of 316 place types between major cities in the U.S. We then select the most and least varying place types and compare them to thematic signatures from regions outside of the U.S. Finally, we explore the effects of LDA topic resolution on differences between and within place types.",2017-01-01,2-s2.0-85017497795,Lecture Notes in Geoinformation and Cartography,The effect of regional variation and resolution on geosocial thematic signatures for points of interest,"Computational models of place are a key component of spatial information theory and play an increasing role in research ranging from spatial search to transportation studies. One method to arrive at such models is to extract knowledge from user-generated content e.g., from texts, tags, trajectories, pictures, and so forth. Over the last years, topic modeling techniques such as latent Dirichlet allocation (LDA) have been studied to reveal linguistic patterns that characterize places and their types. Intuitively, people are more likely to describe places such as Yosemite National Park in terms of hiking, nature, and camping than cocktail or dancing. The geo-indicativeness of non-georeferenced text does not only apply to place instances but also place types, e.g., state parks. While different parks will vary greatly with respect to their landscape and thus human descriptions, the distribution of topics common to all parks will differ significantly from other types of places, e.g., night clubs. This aggregation of topics to the type level creates thematic signatures that can be used for place categorization, data cleansing and conflation, semantic search, and so on. To make full use of these signatures, however, requires a better understanding of their intra-type variability as regional differences effect the predictive power of the signatures. Intuitively, the topic composition for place types such as store and office should be less effected by regional differences than the topic composition for types such as monument and mountain. In this work, we approach this regional variability hypothesis by attempting to prove that all place types are aspatial with respect to their thematic signatures. We reject this hypothesis by comparing the signature similarities of 316 place types between major cities in the U.S. We then select the most and least varying place types and compare them to thematic signatures from regions outside of the U.S. Finally, we explore the effects of LDA topic resolution on differences between and within place types."
278,"Purpose: Analyzing the sentiment orientation of each product aspect/feature might be sufficient to assist the customer to make purchase/usage decisions, but such level of information obtained by sentiment analysis is not detailed enough to assist the company in making product improvement or design decisions. Therefore, this paper aims to propose a novel method to extract more detailed information of the product. Design/methodology/approach: This paper proposed to use a set of trivial lexical-Part-of-Speech patterns to prepare candidate corpus and then adopted a topic model to find the optimal number of topics and get the words distributions in each topic. Finally, combined a priori analysis and compactness rules, the authors found out the expected strong rules in each topic, which make up the final problems. Findings: Experimental results on a real-life data set from Xiaomi forum showed the proposed method can extract the product problems effectively. The authors also explained the errors of experiment, which suggested the direction for future research. Originality/value: This paper proposed a novel method to obtain information of product problems in detail, which will be useful to assist companies to improve their product performance.",2017-01-01,2-s2.0-85016275195,Kybernetes,Mining product problems from online feedback of Chinese users,"Purpose: Analyzing the sentiment orientation of each product aspect/feature might be sufficient to assist the customer to make purchase/usage decisions, but such level of information obtained by sentiment analysis is not detailed enough to assist the company in making product improvement or design decisions. Therefore, this paper aims to propose a novel method to extract more detailed information of the product. Design/methodology/approach: This paper proposed to use a set of trivial lexical-Part-of-Speech patterns to prepare candidate corpus and then adopted a topic model to find the optimal number of topics and get the words distributions in each topic. Finally, combined a priori analysis and compactness rules, the authors found out the expected strong rules in each topic, which make up the final problems. Findings: Experimental results on a real-life data set from Xiaomi forum showed the proposed method can extract the product problems effectively. The authors also explained the errors of experiment, which suggested the direction for future research. Originality/value: This paper proposed a novel method to obtain information of product problems in detail, which will be useful to assist companies to improve their product performance."
279,"In this study, we propose a framework for detecting topic evolutions in weighted citation networks. Citation networks are important in studying knowledge flows; however, citation network analysis has primarily focused on binary networks in which the individual citation influences of each cited paper in a citing paper are considered identical, even though not all cited papers have a significant influence on the cited publication. Accordingly, it is necessary to build and analyze a citation network comprising scholarly publications that notably impact one another, thus identifying topic evolution in a more precise manner. To measure the strength of citation influence and identify paper topics, we employ a citation influence topic model primarily based on topical inheritance between cited and citing papers. Using scholarly publications in the field of the protein p53 as a case study, we build a citation network, filter it using citation influence values, and examine the diffusion of topics not only in the field but also in the subfields of p53.",2017-01-01,2-s2.0-85032227196,Journal of the Association for Information Science and Technology,Topic diffusion analysis of a weighted citation network in biomedical literature,"In this study, we propose a framework for detecting topic evolutions in weighted citation networks. Citation networks are important in studying knowledge flows; however, citation network analysis has primarily focused on binary networks in which the individual citation influences of each cited paper in a citing paper are considered identical, even though not all cited papers have a significant influence on the cited publication. Accordingly, it is necessary to build and analyze a citation network comprising scholarly publications that notably impact one another, thus identifying topic evolution in a more precise manner. To measure the strength of citation influence and identify paper topics, we employ a citation influence topic model primarily based on topical inheritance between cited and citing papers. Using scholarly publications in the field of the protein p53 as a case study, we build a citation network, filter it using citation influence values, and examine the diffusion of topics not only in the field but also in the subfields of p53."
280,"This article presents an automated method of semantic analysis of patent array, used for patent examination and designed to reduce the time that an expert spends for the prior-art search and for the decision making if the patent should be granted or not. This method is part of a three steps methodology, consists of preprocessing step, statistical analysis and semantic analysis. The method includes: the decomposition of the complex sentence into several small sentences, methods of morphological and semantic analysis of the text, the semantic network simplification, the comparison of semantic networks based on an algorithm Levenstein.",2016-12-14,2-s2.0-85013191440,"IISA 2016 - 7th International Conference on Information, Intelligence, Systems and Applications",Automated methods of patent array analysis,"This article presents an automated method of semantic analysis of patent array, used for patent examination and designed to reduce the time that an expert spends for the prior-art search and for the decision making if the patent should be granted or not. This method is part of a three steps methodology, consists of preprocessing step, statistical analysis and semantic analysis. The method includes: the decomposition of the complex sentence into several small sentences, methods of morphological and semantic analysis of the text, the semantic network simplification, the comparison of semantic networks based on an algorithm Levenstein."
281,"This paper addresses the problem of topic clustering, through the utilization of a novel genetic algorithm approach which is highly scalable on large volumes of textual data, by introducing a centroid-based encoding scheme. The proposed topic clustering method is anchored on the Latent Dirichlet Allocation (LDA) probabilistic topic modeling framework, aiming at identifying cluster formations that are optimal in terms of semantic coherence. Our work focuses on reformulating the clustering problem as a discrete optimization problem within the n-dimensional standard simplex since all the LDA-based data patterns correspond to n-valued probability distribution vectors. The novelty of our proposed genetic algorithm approach lies primarily upon the adaptation of the centroid-based encoding scheme, in the sense that cluster assignments are implicitly extracted by assigning each data point to the nearest cluster center. Experimentation was conducted on a large corpus of twitter posts, particularly relating to the UBER transportation network. The obtained topic clustering results indicate significant improvement in extracting semantically focused groups of documents when compared against traditional clustering algorithms, such as the k-means. The clustering superiority of our proposed genetic algorithm is also justified by measuring the intra- and inter-cluster semantic distances of the obtained cluster formations.",2016-12-14,2-s2.0-85013192571,"IISA 2016 - 7th International Conference on Information, Intelligence, Systems and Applications",A genetic algorithm approach for topic clustering: A centroid-based encoding scheme,"This paper addresses the problem of topic clustering, through the utilization of a novel genetic algorithm approach which is highly scalable on large volumes of textual data, by introducing a centroid-based encoding scheme. The proposed topic clustering method is anchored on the Latent Dirichlet Allocation (LDA) probabilistic topic modeling framework, aiming at identifying cluster formations that are optimal in terms of semantic coherence. Our work focuses on reformulating the clustering problem as a discrete optimization problem within the n-dimensional standard simplex since all the LDA-based data patterns correspond to n-valued probability distribution vectors. The novelty of our proposed genetic algorithm approach lies primarily upon the adaptation of the centroid-based encoding scheme, in the sense that cluster assignments are implicitly extracted by assigning each data point to the nearest cluster center. Experimentation was conducted on a large corpus of twitter posts, particularly relating to the UBER transportation network. The obtained topic clustering results indicate significant improvement in extracting semantically focused groups of documents when compared against traditional clustering algorithms, such as the k-means. The clustering superiority of our proposed genetic algorithm is also justified by measuring the intra- and inter-cluster semantic distances of the obtained cluster formations."
282,"Online physician reviews have recently gained an increasing attention because they can have a significant impact on patients' choice of physician. A large number of patients consult these reviews before choosing their physician. Despite that the helpfulness of product reviews has been widely investigated in the marketing domain, little is known about the helpfulness of online physician reviews. This study aims to analyzes factors that influence the helpfulness of online physician reviews. It uses review ratings, linguistic, psychological and semantic features as input to classify these reviews into helpful or unhelpful categories. The review data have been collected from RateMDs.com. The results demonstrate a significant impact of review ratings on the helpfulness of online physician reviews. The findings reveal differences between the reviews of the product and physician domains, which can have significant implications for the design of physician review websites.",2016-12-06,2-s2.0-85010298779,"Proceedings - 2016 IEEE International Conference on Healthcare Informatics, ICHI 2016",Predicting the Helpfulness of Online Physician Reviews,"Online physician reviews have recently gained an increasing attention because they can have a significant impact on patients' choice of physician. A large number of patients consult these reviews before choosing their physician. Despite that the helpfulness of product reviews has been widely investigated in the marketing domain, little is known about the helpfulness of online physician reviews. This study aims to analyzes factors that influence the helpfulness of online physician reviews. It uses review ratings, linguistic, psychological and semantic features as input to classify these reviews into helpful or unhelpful categories. The review data have been collected from RateMDs.com. The results demonstrate a significant impact of review ratings on the helpfulness of online physician reviews. The findings reveal differences between the reviews of the product and physician domains, which can have significant implications for the design of physician review websites."
283,"The development of early literacy skills has been critically linked to a child’s later academic success. In particular, repeated studies have shown that reading aloud to children and providing opportunities for them to discuss the stories that they hear is of utmost importance to later academic success. CloudPrimer is a tablet-based interactive reading primer that aims to foster early literacy skills by supporting parents in shared reading with their children through user-targeted discussion topic suggestions. The tablet application records discussions between parents and children as they read a story and, in combination with a common sense knowledge base, leverages this information to produce suggestions. Because of the unique challenges presented by our application, the suggestion generation method relies on a novel topic modeling method that is based on semantic graph topology. We conducted a user study in which we compared how delivering suggestions generated by our approach compares to expert-crafted suggestions. Our results show that our system can successfully improve engagement and parent–child reading practices in the absence of a literacy expert’s tutoring.",2016-12-01,2-s2.0-84979210769,User Modeling and User-Adapted Interaction,Fostering parent–child dialog through automated discussion suggestions,"The development of early literacy skills has been critically linked to a child’s later academic success. In particular, repeated studies have shown that reading aloud to children and providing opportunities for them to discuss the stories that they hear is of utmost importance to later academic success. CloudPrimer is a tablet-based interactive reading primer that aims to foster early literacy skills by supporting parents in shared reading with their children through user-targeted discussion topic suggestions. The tablet application records discussions between parents and children as they read a story and, in combination with a common sense knowledge base, leverages this information to produce suggestions. Because of the unique challenges presented by our application, the suggestion generation method relies on a novel topic modeling method that is based on semantic graph topology. We conducted a user study in which we compared how delivering suggestions generated by our approach compares to expert-crafted suggestions. Our results show that our system can successfully improve engagement and parent–child reading practices in the absence of a literacy expert’s tutoring."
284,"Social network analysis (SNA) is an analytical technique rapidly gaining popularity within archaeology for its applicability to a wide variety of issues relating to past communities. Using the 20-year project at Çatalhöyük, Turkey, as a case study, I demonstrate how SNA can be helpfully used to understand knowledge production in archaeology. Balancing network visualization and computation with contextual knowledge, combining SNA with topic modeling, and concentrating on social structures all work to provide a diachronic view of how information flows between disparate research teams at Çatalhöyük as well as the social structures and specific individuals promoting this flow. SNA has the undeniable potential to provide new perspectives on how dispersed datasets are assembled to produce archaeological knowledge, illustrating the value of retaining focus on the social conditions of scientific practice even as significant insights are being derived from instead investigating objects and ontology.",2016-12-01,2-s2.0-84944699348,Journal of Archaeological Method and Theory,"Tracing Teams, Texts, and Topics: Applying Social Network Analysis to Understand Archaeological Knowledge Production at Çatalhöyük","Social network analysis (SNA) is an analytical technique rapidly gaining popularity within archaeology for its applicability to a wide variety of issues relating to past communities. Using the 20-year project at Çatalhöyük, Turkey, as a case study, I demonstrate how SNA can be helpfully used to understand knowledge production in archaeology. Balancing network visualization and computation with contextual knowledge, combining SNA with topic modeling, and concentrating on social structures all work to provide a diachronic view of how information flows between disparate research teams at Çatalhöyük as well as the social structures and specific individuals promoting this flow. SNA has the undeniable potential to provide new perspectives on how dispersed datasets are assembled to produce archaeological knowledge, illustrating the value of retaining focus on the social conditions of scientific practice even as significant insights are being derived from instead investigating objects and ontology."
285,"Communication studies depend on information and communication technology (ICT) and the behavior of people using the technology. ICT enables individuals to transfer information quickly via various media. Social changes are occurring rapidly and their studies are growing in number. Thus, a tool to extract knowledge to comprehend the quickly changing dynamics of communication studies is required. We propose a subject–method topic network analysis method that integrates topic modeling analysis and network analysis to understand the state of communication studies. Our analysis focuses on the relationships between topics classified as subjects and methods. From the relationships, we examine the societal and perspective changes relative to emerging media technologies. We apply our method to all papers listed in the Journal Citation Reports Social Science Citation Index as communication studies between 1990 and 2014. The study results allow us to identify popular subjects, methods, and subject–method pairs in proportion and relation.",2016-12-01,2-s2.0-84989177746,Scientometrics,Subject–method topic network analysis in communication studies,"Communication studies depend on information and communication technology (ICT) and the behavior of people using the technology. ICT enables individuals to transfer information quickly via various media. Social changes are occurring rapidly and their studies are growing in number. Thus, a tool to extract knowledge to comprehend the quickly changing dynamics of communication studies is required. We propose a subject–method topic network analysis method that integrates topic modeling analysis and network analysis to understand the state of communication studies. Our analysis focuses on the relationships between topics classified as subjects and methods. From the relationships, we examine the societal and perspective changes relative to emerging media technologies. We apply our method to all papers listed in the Journal Citation Reports Social Science Citation Index as communication studies between 1990 and 2014. The study results allow us to identify popular subjects, methods, and subject–method pairs in proportion and relation."
286,"An important part of an organisation's mission is protecting its information assets from inside or outside threats. As the information environment has become more diverse and inclusive, security concern has shifted from information assets resided in the organisation to information assets and networked devices exposed to broader cyberspace, such as cloud or Internet of things environment and mobile Internet. Organisations have to keep up with the knowledge and trends in information security and cyber-security to safeguard their information assets. Knowledge mapping will aid in this sort of knowledge management process. Mandatory standards and government regulations help industries establish best practices in cyber-security. Knowledge mapping and scientometric analysis across disciplines also provide a tracking system to notify researchers and practitioners should the new solutions and technology facilitating threat detection emerge. While various topics in information security and cyber-security have been extensively investigated in academia, identifying salient themes and development trajectories in information security and cyber-security research is relatively unexplored. This study employs scientometric analysis and topic modelling to develop knowledge maps that visualise core concepts associated with information security and cyber-security research over time and across disciplines. With scientometric analysis and knowledge mapping using topic models, this study identifies the commonality, difference, and relationship between information security and cyber-security research domains. This approach could gain insights into how these research areas have evolved and might be improved concerning learning and teaching cyber-security. The proposed approach to developing the knowledge map may be extended to other research areas.",2016-12-01,2-s2.0-85007569594,Journal of Information and Knowledge Management,The synergy of scientometric analysis and knowledge mapping with topic models: Modelling the development trajectories of information security and cyber-security research,"An important part of an organisation's mission is protecting its information assets from inside or outside threats. As the information environment has become more diverse and inclusive, security concern has shifted from information assets resided in the organisation to information assets and networked devices exposed to broader cyberspace, such as cloud or Internet of things environment and mobile Internet. Organisations have to keep up with the knowledge and trends in information security and cyber-security to safeguard their information assets. Knowledge mapping will aid in this sort of knowledge management process. Mandatory standards and government regulations help industries establish best practices in cyber-security. Knowledge mapping and scientometric analysis across disciplines also provide a tracking system to notify researchers and practitioners should the new solutions and technology facilitating threat detection emerge. While various topics in information security and cyber-security have been extensively investigated in academia, identifying salient themes and development trajectories in information security and cyber-security research is relatively unexplored. This study employs scientometric analysis and topic modelling to develop knowledge maps that visualise core concepts associated with information security and cyber-security research over time and across disciplines. With scientometric analysis and knowledge mapping using topic models, this study identifies the commonality, difference, and relationship between information security and cyber-security research domains. This approach could gain insights into how these research areas have evolved and might be improved concerning learning and teaching cyber-security. The proposed approach to developing the knowledge map may be extended to other research areas."
287,"A literature review is a central part of any research project, allowing the existing research to be mapped and new research questions to be posited. However, due to the limitations of human data processing, the literature review can suffer from an inability to handle large volumes of research articles. The computational literature review (CLR) is proposed here as a complementary part of a wider literature review process. The CLR automates some of the analysis of research articles with analyses of impact (citations), structure (co-authorship networks) and content (topic modeling of abstracts). A contribution of the paper is to demonstrate how the content of abstracts can be analyzed automatically to provide a set of research topics within a literature corpus. The CLR software can be used to support three use cases: (1) analysis of the literature for a research area, (2) analysis and ranking of journals, and (3) analysis and ranking of individual scholars and research teams. The working of the CLR software is illustrated through application to the technology acceptance model (TAM) using a set of 3,386 articles. The CLR is an open source offering, developed in the statistical programming language R, and made freely available to researchers to use and develop further.",2016-12-01,2-s2.0-84988029364,International Journal of Information Management,A computational literature review of the technology acceptance model,"A literature review is a central part of any research project, allowing the existing research to be mapped and new research questions to be posited. However, due to the limitations of human data processing, the literature review can suffer from an inability to handle large volumes of research articles. The computational literature review (CLR) is proposed here as a complementary part of a wider literature review process. The CLR automates some of the analysis of research articles with analyses of impact (citations), structure (co-authorship networks) and content (topic modeling of abstracts). A contribution of the paper is to demonstrate how the content of abstracts can be analyzed automatically to provide a set of research topics within a literature corpus. The CLR software can be used to support three use cases: (1) analysis of the literature for a research area, (2) analysis and ranking of journals, and (3) analysis and ranking of individual scholars and research teams. The working of the CLR software is illustrated through application to the technology acceptance model (TAM) using a set of 3,386 articles. The CLR is an open source offering, developed in the statistical programming language R, and made freely available to researchers to use and develop further."
288,"The problem of detecting events from content published on microblogs has garnered much interest in recent times. In this paper, we address the questions of what happens after the outbreak of an event in terms of how the event gradually progresses and attains each of its milestones, and how it eventually dissipates. We propose a model based approach to capture the gradual unfolding of an event over time. This enables the model to automatically produce entire timeline trajectories of events from the time of their outbreak to their disappearance. We apply our model on the Twitter messages collected about Ebola during the 2014 outbreak and obtain the progression timelines of several events that occurred during the outbreak. We also compare our model to several existing topic modeling and event detection baselines in literature to demonstrate its efficiency.",2016-11-21,2-s2.0-85006791349,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",From event detection to storytelling on microblogs,"The problem of detecting events from content published on microblogs has garnered much interest in recent times. In this paper, we address the questions of what happens after the outbreak of an event in terms of how the event gradually progresses and attains each of its milestones, and how it eventually dissipates. We propose a model based approach to capture the gradual unfolding of an event over time. This enables the model to automatically produce entire timeline trajectories of events from the time of their outbreak to their disappearance. We apply our model on the Twitter messages collected about Ebola during the 2014 outbreak and obtain the progression timelines of several events that occurred during the outbreak. We also compare our model to several existing topic modeling and event detection baselines in literature to demonstrate its efficiency."
289,"Community question answering(CQA) websites such as Yahoo! Answers and Stack Overflow provide a new way of asking and answering questions which are not well served by general web search engines. Due to the huge volume and ever-increasing number of questions, not all new questions can get fully answered in required time. Therefore, it is of great significance to design some effective strategies of recommending experts for new questions. In this paper, we propose a novel personalized recommendation method for routing new questions to a group of experts. Different from prior work which only considers the topic modeling or the link structure, we aim at recommending new questions to more appropriate experts by considering both of these two factors. Moreover, we design a new strategy of network construction with the personalization fully considered. The comparison experiments are conducted with Stack Overflow data and the experimental results demonstrate that the proposed method improves the recommendation performance over other methods in expert recommendation.",2016-11-21,2-s2.0-85006711833,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",Personalized recommendation for new questions in community question answering,"Community question answering(CQA) websites such as Yahoo! Answers and Stack Overflow provide a new way of asking and answering questions which are not well served by general web search engines. Due to the huge volume and ever-increasing number of questions, not all new questions can get fully answered in required time. Therefore, it is of great significance to design some effective strategies of recommending experts for new questions. In this paper, we propose a novel personalized recommendation method for routing new questions to a group of experts. Different from prior work which only considers the topic modeling or the link structure, we aim at recommending new questions to more appropriate experts by considering both of these two factors. Moreover, we design a new strategy of network construction with the personalization fully considered. The comparison experiments are conducted with Stack Overflow data and the experimental results demonstrate that the proposed method improves the recommendation performance over other methods in expert recommendation."
290,"Social network is a hot topic of interest for the researchers in the field of computer science in recent years. The vast amount of data generated by these social networks play a very important role in information diffusion. Social network data are generated by its users. So, user's behavior and activities are being investigated by the researchers to get a logical view of social network platform. This research proposed a novel retweet prediction model which considers difference in user's behavior as an author (as reflected in the tweets) and a retweeter (as reflected in the retweets) and do the prediction accordingly. The proposed retweet prediction strategy taking this difference into consideration, gave better prediction accuracy than the conventional strategy. The findings of this research explains that in social networks, some users show different behavior indifferent roles and these differences may have impact on future research.",2016-11-21,2-s2.0-85006804221,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",Retweet prediction considering user's difference as an author and retweeter,"Social network is a hot topic of interest for the researchers in the field of computer science in recent years. The vast amount of data generated by these social networks play a very important role in information diffusion. Social network data are generated by its users. So, user's behavior and activities are being investigated by the researchers to get a logical view of social network platform. This research proposed a novel retweet prediction model which considers difference in user's behavior as an author (as reflected in the tweets) and a retweeter (as reflected in the retweets) and do the prediction accordingly. The proposed retweet prediction strategy taking this difference into consideration, gave better prediction accuracy than the conventional strategy. The findings of this research explains that in social networks, some users show different behavior indifferent roles and these differences may have impact on future research."
291,"Social networking sites (SNS), such as Facebook and Twitter, are important spaces for political engagement. SNS have become common elements in political participation, campaigns, and elections. However, little is known about the dynamics between candidate posts and commentator sentiment in response to those posts on SNS. This study enriches computational political science by studying the 2016 U.S. elections and how candidates and commentators engage on Facebook. This paper also examines how online activity might be connected to offline activity and vice versa. We extracted 9,700 Facebook posts by five presidential candidates (Hillary Clinton, Donald Trump, Bernie Sanders, Ted Cruz, and John Kasich) from their official Facebook pages and 12,050,595 comments on those posts. We employed topic modeling, sentiment analysis, and trends detection using wavelet transforms to discover topics, trends, and reactions. Our findings suggest that Republican candidates are more likely to share information on controversial events that have taken place during the election cycle, while Democratic candidates focus on social policy issues. As expected, commentators on Republican candidate pages express negative sentiments toward current public policies as they seldom support decisions made by the Obama administration, while commentators on democratic candidate pages are more likely to express support for continuation or advancement of existing policies. However, the significance (strong/weak) and nature (positive/negative) of sentiments varied between candidates within political parties based on perceived credibility of the candidate's degree of credibility on a given issue. Additionally, we explored correlation between online trends of comments/sentiment and offline events. When analyzing the trend patterns, we found that changes in online trends are driven by three factors: 1) popular post, 2) offline debates, and 3) candidates dropping out of the race.",2016-11-21,2-s2.0-85006815884,"Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016",An analysis of sentiments on facebook during the 2016 U.S. presidential election,"Social networking sites (SNS), such as Facebook and Twitter, are important spaces for political engagement. SNS have become common elements in political participation, campaigns, and elections. However, little is known about the dynamics between candidate posts and commentator sentiment in response to those posts on SNS. This study enriches computational political science by studying the 2016 U.S. elections and how candidates and commentators engage on Facebook. This paper also examines how online activity might be connected to offline activity and vice versa. We extracted 9,700 Facebook posts by five presidential candidates (Hillary Clinton, Donald Trump, Bernie Sanders, Ted Cruz, and John Kasich) from their official Facebook pages and 12,050,595 comments on those posts. We employed topic modeling, sentiment analysis, and trends detection using wavelet transforms to discover topics, trends, and reactions. Our findings suggest that Republican candidates are more likely to share information on controversial events that have taken place during the election cycle, while Democratic candidates focus on social policy issues. As expected, commentators on Republican candidate pages express negative sentiments toward current public policies as they seldom support decisions made by the Obama administration, while commentators on democratic candidate pages are more likely to express support for continuation or advancement of existing policies. However, the significance (strong/weak) and nature (positive/negative) of sentiments varied between candidates within political parties based on perceived credibility of the candidate's degree of credibility on a given issue. Additionally, we explored correlation between online trends of comments/sentiment and offline events. When analyzing the trend patterns, we found that changes in online trends are driven by three factors: 1) popular post, 2) offline debates, and 3) candidates dropping out of the race."
292,"Clinical Pathway is ubiquitous and plays an essential role in clinical workflow management. The combination of topic modeling and process mining is an efficient approach to get a non-static and topic-based process model. Topic modeling is used to group the activities of each clinical day into the latent topics, and process mining is used to generate a concise workflow model based on these topics. However, because of the specificity of clinical data, it usually suffers from the performance of topic modeling. In this paper, we take an important clinical practice, all the same activities in one clinical day tend to represent the same clinical goal, into account to enhance the effectiveness of topic modeling. The experiments on real data show significant performance gains of our approach.",2016-11-18,2-s2.0-85006384900,"2016 IEEE 18th International Conference on e-Health Networking, Applications and Services, Healthcom 2016",Summarizing patient daily activities for clinical pathway mining,"Clinical Pathway is ubiquitous and plays an essential role in clinical workflow management. The combination of topic modeling and process mining is an efficient approach to get a non-static and topic-based process model. Topic modeling is used to group the activities of each clinical day into the latent topics, and process mining is used to generate a concise workflow model based on these topics. However, because of the specificity of clinical data, it usually suffers from the performance of topic modeling. In this paper, we take an important clinical practice, all the same activities in one clinical day tend to represent the same clinical goal, into account to enhance the effectiveness of topic modeling. The experiments on real data show significant performance gains of our approach."
293,"Previous literature has considered the relevance of Twitter to journalism, for example as a tool for reporters to collect information and for organizations to disseminate news to the public. We consider the reciprocal perspective, carrying out a survey of news media-related content within Twitter. Using a random sample of 1.8 billion tweets over four months in 2014, we look at the distribution of activity across news media and the relative dominance of certain news organizations in terms of relative share of content, the Twitter behavior of news media, the hashtags used in news content versus Twitter as a whole, and the proportion of Twitter activity that is news media-related. We find a small but consistent proportion of Twitter is news media-related (0.8 percent by volume); that news media-related tweets focus on a different set of hashtags than Twitter as a whole, with some hashtags such as those of countries of conflict (Arab Spring countries, Ukraine) reaching over 15 percent of tweets being news media-related; and we find that news organizations’ accounts, across all major organizations, largely use Twitter as a professionalized, one-way communication medium to promote their own reporting. Using Latent Dirichlet Allocation topic modeling, we also examine how the proportion of news content varies across topics within 100,000 #Egypt tweets, finding that the relative proportion of news media-related tweets varies vastly across different subtopics. Over-time analysis reveals that news media were among the earliest adopters of certain #Egypt subtopics, providing a necessary (although not sufficient) condition for influence.",2016-11-16,2-s2.0-84965053749,Digital Journalism,A Macroscopic Analysis of News Content in Twitter,"Previous literature has considered the relevance of Twitter to journalism, for example as a tool for reporters to collect information and for organizations to disseminate news to the public. We consider the reciprocal perspective, carrying out a survey of news media-related content within Twitter. Using a random sample of 1.8 billion tweets over four months in 2014, we look at the distribution of activity across news media and the relative dominance of certain news organizations in terms of relative share of content, the Twitter behavior of news media, the hashtags used in news content versus Twitter as a whole, and the proportion of Twitter activity that is news media-related. We find a small but consistent proportion of Twitter is news media-related (0.8 percent by volume); that news media-related tweets focus on a different set of hashtags than Twitter as a whole, with some hashtags such as those of countries of conflict (Arab Spring countries, Ukraine) reaching over 15 percent of tweets being news media-related; and we find that news organizations’ accounts, across all major organizations, largely use Twitter as a professionalized, one-way communication medium to promote their own reporting. Using Latent Dirichlet Allocation topic modeling, we also examine how the proportion of news content varies across topics within 100,000 #Egypt tweets, finding that the relative proportion of news media-related tweets varies vastly across different subtopics. Over-time analysis reveals that news media were among the earliest adopters of certain #Egypt subtopics, providing a necessary (although not sufficient) condition for influence."
294,"Although well-renowned universities attempt to differentiate themselves from other universities, little research has been undertaken on the principal themes involved in the concept of the world-class university (WCU) as presented in speeches by members of WCUs. These discourses are a key tool in universities’ attempt to shape the competitive framework of higher education through legitimacy. We study the presidents’ discourses from 100 leading universities to identify the themes and emergent discourse of these universities’ communities. Applying topic modeling methodology to the speeches’ corpus, we find seven communities (Worldwide-the four regions, American-from different states, Flagship, Education concerned, Some Chinese universities, Central European universities and Challengers) and four main themes arising from WCUs’ discourses (Research universities within international rankings, Stakeholders and leadership, Mission and values, and Education). Our preliminary findings suggest that leading universities are working to adopt the WCU label based on their salient characteristics.",2016-11-01,2-s2.0-84962476742,Higher Education Research and Development,Legitimating the world-class university concept through the discourse of elite universities’ presidents†       ,"Although well-renowned universities attempt to differentiate themselves from other universities, little research has been undertaken on the principal themes involved in the concept of the world-class university (WCU) as presented in speeches by members of WCUs. These discourses are a key tool in universities’ attempt to shape the competitive framework of higher education through legitimacy. We study the presidents’ discourses from 100 leading universities to identify the themes and emergent discourse of these universities’ communities. Applying topic modeling methodology to the speeches’ corpus, we find seven communities (Worldwide-the four regions, American-from different states, Flagship, Education concerned, Some Chinese universities, Central European universities and Challengers) and four main themes arising from WCUs’ discourses (Research universities within international rankings, Stakeholders and leadership, Mission and values, and Education). Our preliminary findings suggest that leading universities are working to adopt the WCU label based on their salient characteristics."
295,"During the three decades since its inception in 1984, the JPIM has shaped the evolution of innovation research as a scientific field. It helped create a topic landscape that is not only more diverse and rich in insights, but also more complex and fragmented in structure than ever before. We seek to map this landscape and identify salient development trajectories over time. In contrast to prior citation-based studies covering the first two decades of JPIM research, we benefit from recent advances in natural language processing and rely on a topic modeling algorithm to extract 57 distinct topics and the corresponding most common words, terms, and phrases from the entire full-text corpus of 1008 JPIM articles published between 1984 and 2013. Estimating the development trajectory of each topic based on yearly publication counts in JPIM allows us to identify “hot,” “cold,” “revival,” “evergreen,” and “wall-flower” topics. We map these topics onto the Product Development and Management Association (PDMA) Body of Knowledge categories and discover that these categories differ significantly not only in terms of their internal topic diversity and relative prevalence, but also—and arguably more importantly—in terms of their publication and citation trajectories over time. For instance, the PDMA category “Codevelopment and Alliances” exhibits only moderate topic diversity (7 out of 57 topics) and prevalence in JPIM (161 out of 1008 articles). That said, it is among the most dynamic categories featuring two evergreen topic (“Users and Innovation” and “Tools and Systems for Technology Transfer”) and three hot topics (“Open Innovation,” “Alliances and Cooperation,” and “Networks and Network Structure”) as well as a sharply growing annual number of citations received. Our findings are likely to be of interest to all those who are keen to (re)discover JPIM's topic landscape in search of hidden structures and development trajectories.",2016-11-01,2-s2.0-84949255004,Journal of Product Innovation Management,"Mapping the Topic Landscape of JPIM, 1984–2013: In Search of Hidden Structures and Development Trajectories","During the three decades since its inception in 1984, the JPIM has shaped the evolution of innovation research as a scientific field. It helped create a topic landscape that is not only more diverse and rich in insights, but also more complex and fragmented in structure than ever before. We seek to map this landscape and identify salient development trajectories over time. In contrast to prior citation-based studies covering the first two decades of JPIM research, we benefit from recent advances in natural language processing and rely on a topic modeling algorithm to extract 57 distinct topics and the corresponding most common words, terms, and phrases from the entire full-text corpus of 1008 JPIM articles published between 1984 and 2013. Estimating the development trajectory of each topic based on yearly publication counts in JPIM allows us to identify “hot,” “cold,” “revival,” “evergreen,” and “wall-flower” topics. We map these topics onto the Product Development and Management Association (PDMA) Body of Knowledge categories and discover that these categories differ significantly not only in terms of their internal topic diversity and relative prevalence, but also—and arguably more importantly—in terms of their publication and citation trajectories over time. For instance, the PDMA category “Codevelopment and Alliances” exhibits only moderate topic diversity (7 out of 57 topics) and prevalence in JPIM (161 out of 1008 articles). That said, it is among the most dynamic categories featuring two evergreen topic (“Users and Innovation” and “Tools and Systems for Technology Transfer”) and three hot topics (“Open Innovation,” “Alliances and Cooperation,” and “Networks and Network Structure”) as well as a sharply growing annual number of citations received. Our findings are likely to be of interest to all those who are keen to (re)discover JPIM's topic landscape in search of hidden structures and development trajectories."
296,"Creating links among online encyclopedia articles in different languages is crucial in the construction and integration of large multilingual knowledge bases. Most research to date has focused on linking among different language versions of Wikipedia, yet other large online encyclopedias in a variety of languages exist. In this work, we present a cross-language article-linking method using a bilingual topic model and translation features based on an SVM model to link articles in English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China. To evaluate our approach, we compile data sets from Baidu Baike articles and their corresponding English Wikipedia articles. The evaluation results show that our approach achieves at most 0.8158 in MRR, outperforming the baseline system by 0.1328 (+19.44%) in MRR. Our method does not heavily depend on linguistic characteristics, and it can be easily extended to generate cross-language article links among different online encyclopedias in other languages.",2016-11-01,2-s2.0-84990943170,Knowledge-Based Systems,Cross-language article linking with different knowledge bases using bilingual topic model and translation features,"Creating links among online encyclopedia articles in different languages is crucial in the construction and integration of large multilingual knowledge bases. Most research to date has focused on linking among different language versions of Wikipedia, yet other large online encyclopedias in a variety of languages exist. In this work, we present a cross-language article-linking method using a bilingual topic model and translation features based on an SVM model to link articles in English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China. To evaluate our approach, we compile data sets from Baidu Baike articles and their corresponding English Wikipedia articles. The evaluation results show that our approach achieves at most 0.8158 in MRR, outperforming the baseline system by 0.1328 (+19.44%) in MRR. Our method does not heavily depend on linguistic characteristics, and it can be easily extended to generate cross-language article links among different online encyclopedias in other languages."
297,"Public opinion becomes increasingly salient in the ex post evaluation stage of large infrastructure projects which have significant impacts to the environment and the society. However, traditional survey methods are inefficient in collection and assessment of the public opinion due to its large quantity and diversity. Recently, Social media platforms provide a rich data source for monitoring and assessing the public opinion on controversial infrastructure projects. This paper proposes an assessment framework to transform unstructured online public opinions on large infrastructure projects into sentimental and topical indicators for enhancing practices of ex post evaluation and public participation. The framework uses web crawlers to collect online comments related to a large infrastructure project and employs two natural language processing technologies, including sentiment analysis and topic modeling, with spatio-temporal analysis, to transform these comments into indicators for assessing online public opinion on the project. Based on the framework, we investigate the online public opinion of the Three Gorges Project on China's largest microblogging site, namely, Weibo. Assessment results present spatial-temporal distributions of post intensity and sentiment polarity, reveals major topics with different sentiments and summarizes managerial implications, for ex post evaluation of the world's largest hydropower project. The proposed assessment framework is expected to be widely applied as a methodological strategy to assess public opinion in the ex post evaluation stage of large infrastructure projects.",2016-11-01,2-s2.0-84978284976,Environmental Impact Assessment Review,Assessment of online public opinions on large infrastructure projects: A case study of the Three Gorges Project in China,"Public opinion becomes increasingly salient in the ex post evaluation stage of large infrastructure projects which have significant impacts to the environment and the society. However, traditional survey methods are inefficient in collection and assessment of the public opinion due to its large quantity and diversity. Recently, Social media platforms provide a rich data source for monitoring and assessing the public opinion on controversial infrastructure projects. This paper proposes an assessment framework to transform unstructured online public opinions on large infrastructure projects into sentimental and topical indicators for enhancing practices of ex post evaluation and public participation. The framework uses web crawlers to collect online comments related to a large infrastructure project and employs two natural language processing technologies, including sentiment analysis and topic modeling, with spatio-temporal analysis, to transform these comments into indicators for assessing online public opinion on the project. Based on the framework, we investigate the online public opinion of the Three Gorges Project on China's largest microblogging site, namely, Weibo. Assessment results present spatial-temporal distributions of post intensity and sentiment polarity, reveals major topics with different sentiments and summarizes managerial implications, for ex post evaluation of the world's largest hydropower project. The proposed assessment framework is expected to be widely applied as a methodological strategy to assess public opinion in the ex post evaluation stage of large infrastructure projects."
298,"In the era of the Social Web, crowdfunding has become an increasingly more important channel for entrepreneurs to raise funds from the crowd to support their startup projects. Previous studies examined various factors such as project goals, project durations, and categories of projects that might influence the outcomes of the fund raising campaigns. However, textual information of projects has rarely been studied for analyzing crowdfunding successes. The main contribution of our research work is the design of a novel text analytics-based framework that can extract latent semantics from the textual descriptions of projects to predict the fund raising outcomes of these projects. More specifically, we develop the Domain-Constraint Latent Dirichlet Allocation (DC-LDA) topic model for effective extraction of topical features from texts. Based on two real-world crowdfunding datasets, our experimental results reveal that the proposed framework outperforms a classical LDA-based method in predicting fund raising success by an average of 11% in terms of F",2016-11-01,2-s2.0-84993965442,Decision Support Systems,The determinants of crowdfunding success: A semantic text analytics approach,"In the era of the Social Web, crowdfunding has become an increasingly more important channel for entrepreneurs to raise funds from the crowd to support their startup projects. Previous studies examined various factors such as project goals, project durations, and categories of projects that might influence the outcomes of the fund raising campaigns. However, textual information of projects has rarely been studied for analyzing crowdfunding successes. The main contribution of our research work is the design of a novel text analytics-based framework that can extract latent semantics from the textual descriptions of projects to predict the fund raising outcomes of these projects. More specifically, we develop the Domain-Constraint Latent Dirichlet Allocation (DC-LDA) topic model for effective extraction of topical features from texts. Based on two real-world crowdfunding datasets, our experimental results reveal that the proposed framework outperforms a classical LDA-based method in predicting fund raising success by an average of 11% in terms of F"
299,"As the fields of digital humanities and digital history have grown in scale and visibility since the 1990s, legal history has largely remained on the margins of those fields. The move to make material available online in the first decade of the web featured only a small number of legal history projects: Famous Trials; Anglo-American Legal Tradition; The Proceedings of the Old Bailey Online, 1674-1913. Early efforts to construct hypertext narratives and scholarship also included some works of legal history: ""Hearsay of the Sun: Photography, Identity and the Law of Evidence in Nineteenth-Century Courts,"" in Hypertext Scholarship in American Studies; Who Killed William Robinson? and Gilded Age Plains City: The Great Sheedy Murder Trial and the Booster Ethos of Lincoln, Nebraska. In the second decade of the web, the focus shifted from distributing material to exploring it using digital tools. The presence of digital history grew at the meetings of organizations of historians ranging from the American Historical Association to the Urban History Association, but not at the American Society for Legal History conferences, the annual meetings of the Law and Society Association, or the British Legal History Conference. Only a few Anglo-American legal historians took up computational tools for sorting and visualizing sources such as data mining, text mining, and topic modeling; network analysis; and mapping. Paul Craven and Douglas Hay's Master and Servant project text mined a comprehensive database of 2,000 statutes and 1,200,000 words to explore similarities and influence among statutes. Data Mining with Criminal Intent mined and visualized the words in trial records using structured data from The Proceedings of the Old Bailey Online, 1674-1913. Locating London's Past, a project that mapped resources relating to the early modern and eighteenth century city, and also made use of the Old Bailey records. Digital Harlem mapped crime in the context of everyday life in the 1920s. Only in the past few years has more digital legal history using computational tools begun to appear, and like many of the projects discussed in this special issue, most remain at a preliminary stage. This article seeks to bring into focus the constraints, possibilities, and choices that shape digital legal history, in order to create a context for the work in this special issue, and to promote discussion of what it means to do legal history in the digital age.",2016-11-01,2-s2.0-84986586569,Law and History Review,Searching for Anglo-American Digital Legal History,"As the fields of digital humanities and digital history have grown in scale and visibility since the 1990s, legal history has largely remained on the margins of those fields. The move to make material available online in the first decade of the web featured only a small number of legal history projects: Famous Trials; Anglo-American Legal Tradition; The Proceedings of the Old Bailey Online, 1674-1913. Early efforts to construct hypertext narratives and scholarship also included some works of legal history: ""Hearsay of the Sun: Photography, Identity and the Law of Evidence in Nineteenth-Century Courts,"" in Hypertext Scholarship in American Studies; Who Killed William Robinson? and Gilded Age Plains City: The Great Sheedy Murder Trial and the Booster Ethos of Lincoln, Nebraska. In the second decade of the web, the focus shifted from distributing material to exploring it using digital tools. The presence of digital history grew at the meetings of organizations of historians ranging from the American Historical Association to the Urban History Association, but not at the American Society for Legal History conferences, the annual meetings of the Law and Society Association, or the British Legal History Conference. Only a few Anglo-American legal historians took up computational tools for sorting and visualizing sources such as data mining, text mining, and topic modeling; network analysis; and mapping. Paul Craven and Douglas Hay's Master and Servant project text mined a comprehensive database of 2,000 statutes and 1,200,000 words to explore similarities and influence among statutes. Data Mining with Criminal Intent mined and visualized the words in trial records using structured data from The Proceedings of the Old Bailey Online, 1674-1913. Locating London's Past, a project that mapped resources relating to the early modern and eighteenth century city, and also made use of the Old Bailey records. Digital Harlem mapped crime in the context of everyday life in the 1920s. Only in the past few years has more digital legal history using computational tools begun to appear, and like many of the projects discussed in this special issue, most remain at a preliminary stage. This article seeks to bring into focus the constraints, possibilities, and choices that shape digital legal history, in order to create a context for the work in this special issue, and to promote discussion of what it means to do legal history in the digital age."
300,"To help people make choices and take decisions is an important function of recommender system. Reviews and comments that are written online by users after watching movies, reading books, listening to music or purchasing a specific item, are important sources of user generated data, which can be utilized for decision making using recommendations given to user by the recommender system. When the available data in one domain is sparse, data (content and ratings) from other domains can be used to make cross domain recommendations. In this paper we have used this content information as well as ratings of both domains where there is no user-item overlap between the given domains for cross domain recommendations. User generated content (reviews and comments) crawled from web requires topic modeling to discover the latent thematic structure in the corpora of both domains. Since the topics in both domains are dissimilar, we have compared various approaches based on semantic space of corpus and knowledge based methods for finding cross domain recommendations. Experimental results show improvement in precision in recommendations over existing approach based on semantic clustering.",2016-10-27,2-s2.0-84997354407,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",On using reviews and comments for cross domain recommendations and decision making,"To help people make choices and take decisions is an important function of recommender system. Reviews and comments that are written online by users after watching movies, reading books, listening to music or purchasing a specific item, are important sources of user generated data, which can be utilized for decision making using recommendations given to user by the recommender system. When the available data in one domain is sparse, data (content and ratings) from other domains can be used to make cross domain recommendations. In this paper we have used this content information as well as ratings of both domains where there is no user-item overlap between the given domains for cross domain recommendations. User generated content (reviews and comments) crawled from web requires topic modeling to discover the latent thematic structure in the corpora of both domains. Since the topics in both domains are dissimilar, we have compared various approaches based on semantic space of corpus and knowledge based methods for finding cross domain recommendations. Experimental results show improvement in precision in recommendations over existing approach based on semantic clustering."
301,"Bug localization is the process of identifying the elements of source code that require modification to fix the bug. By automating the task of bug localization efficiently, the cost of software can also be reduced. For performing bug localization, many Information Retrieval models have been used in past. In this paper, bug localization has been performed using Pachinko Allocation Model (PAM). PAM is also an IR model, which falls under the category of topic models and has not been used for locating bugs yet. This paper describes the proposed PAM based approach for bug localization at file level. The PAM based approach is compared with LDA based approach and it has been shown that PAM based bug localization performs better as compared to LDA based bug localization. For evaluating the performance of PAM and LDA based approaches, the datasets downloaded from two open source projects i.e. Rhino and ModeShape have been used.",2016-10-27,2-s2.0-84997113348,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",Software bug localization using Pachinko Allocation Model,"Bug localization is the process of identifying the elements of source code that require modification to fix the bug. By automating the task of bug localization efficiently, the cost of software can also be reduced. For performing bug localization, many Information Retrieval models have been used in past. In this paper, bug localization has been performed using Pachinko Allocation Model (PAM). PAM is also an IR model, which falls under the category of topic models and has not been used for locating bugs yet. This paper describes the proposed PAM based approach for bug localization at file level. The PAM based approach is compared with LDA based approach and it has been shown that PAM based bug localization performs better as compared to LDA based bug localization. For evaluating the performance of PAM and LDA based approaches, the datasets downloaded from two open source projects i.e. Rhino and ModeShape have been used."
302,"Tracking public opinion in social media provides important information to enterprises or governments during a decision making process. In addition, identifying and extracting the causes of sentiment spikes allows interested parties to redesign and adjust strategies with the aim to attract more positive sentiments. In this paper, we focus on the problem of tracking sentiment towards different entities, detecting sentiment spikes and on the problem of extracting and ranking the causes of a sentiment spike. Our approach combines LDA topic model with Relative Entropy. The former is used for extracting the topics discussed in the time window before the sentiment spike. The latter allows to rank the detected topics based on their contribution to the sentiment spike.",2016-10-24,2-s2.0-84996490661,"International Conference on Information and Knowledge Management, Proceedings",Explaining sentiment spikes in twitter,"Tracking public opinion in social media provides important information to enterprises or governments during a decision making process. In addition, identifying and extracting the causes of sentiment spikes allows interested parties to redesign and adjust strategies with the aim to attract more positive sentiments. In this paper, we focus on the problem of tracking sentiment towards different entities, detecting sentiment spikes and on the problem of extracting and ranking the causes of a sentiment spike. Our approach combines LDA topic model with Relative Entropy. The former is used for extracting the topics discussed in the time window before the sentiment spike. The latter allows to rank the detected topics based on their contribution to the sentiment spike."
303,"While social data is being widely used in various applications such as sentiment analysis and trend prediction, its sheer size also presents great challenges for storing, sharing and processing such data. These challenges can be addressed by data summarization which transforms the original dataset into a smaller, yet still useful, subset. Existing methods find such subsets with objective functions based on data properties such as representativeness or informativeness but do not exploit social contexts, which are distinct characteristics of social data. Further, till date very little work has focused on topic preserving data summarization, despite the abundant work on topic modeling. This is a challenging task for two reasons. First, since topic model is based on latent variables, existing methods are not well-suited to capture latent topics. Second, it is difficult to find such social contexts that provide valuable information for building effective topic-preserving summarization model. To tackle these challenges, in this paper, we focus on exploiting social contexts to summarize social data while preserving topics in the original dataset. We take Twitter data as a case study. Through analyzing Twitter data, we discover two social contexts which are important for topic generation and dissemination, namely (i) CrowdExp topic score that captures the influence of both the crowd and the expert users in Twitter and (ii) Retweet topic score that captures the influence of Twitter users' actions. We conduct extensive experiments on two real-world Twitter datasets using two applications. The experimental results show that, by leveraging social contexts, our proposed solution can enhance topic-preserving data summarization and improve application performance by up to 18%.",2016-10-24,2-s2.0-84996567089,"International Conference on Information and Knowledge Management, Proceedings",Data summarization with social contexts,"While social data is being widely used in various applications such as sentiment analysis and trend prediction, its sheer size also presents great challenges for storing, sharing and processing such data. These challenges can be addressed by data summarization which transforms the original dataset into a smaller, yet still useful, subset. Existing methods find such subsets with objective functions based on data properties such as representativeness or informativeness but do not exploit social contexts, which are distinct characteristics of social data. Further, till date very little work has focused on topic preserving data summarization, despite the abundant work on topic modeling. This is a challenging task for two reasons. First, since topic model is based on latent variables, existing methods are not well-suited to capture latent topics. Second, it is difficult to find such social contexts that provide valuable information for building effective topic-preserving summarization model. To tackle these challenges, in this paper, we focus on exploiting social contexts to summarize social data while preserving topics in the original dataset. We take Twitter data as a case study. Through analyzing Twitter data, we discover two social contexts which are important for topic generation and dissemination, namely (i) CrowdExp topic score that captures the influence of both the crowd and the expert users in Twitter and (ii) Retweet topic score that captures the influence of Twitter users' actions. We conduct extensive experiments on two real-world Twitter datasets using two applications. The experimental results show that, by leveraging social contexts, our proposed solution can enhance topic-preserving data summarization and improve application performance by up to 18%."
304,"Spatial event detection is an important and challenging problem. Unlike traditional event detection that focuses on the timing of global urgent event, the task of spatial event detection is to detect the spatial regions (e.g. clusters of neighboring cities) where urgent events occur. In this paper, we focus on the problem of spatial event detection using textual information in social media. We observe that, when a spatial event occurs, the topics relevant to the event are often discussed more coherently in cities near the event location than those far away. In order to capture this pattern, we propose a new method called Graph Topic Scan Statistic (Graph-TSS) that corresponds to a generalized log-likelihood ratio test based on topic modeling. We first demonstrate that the detection of spatial event regions under Graph-TSS is NP-hard due to a reduction from classical node-weighted prize-collecting Steiner tree problem (NW-PCST). We then design an efficient algorithm that approximately maximizes the graph topic scan statistic over spatial regions of arbitrary form. As a case study, we consider three applications using Twitter data, including Argentina civil unrest event detection, Chile earthquake detection, and United States influenza disease outbreak detection. Empirical evidence demonstrates that the proposed Graph-TSS performs superior over state-of-the-art methods on both running time and accuracy.",2016-10-24,2-s2.0-84996588067,"International Conference on Information and Knowledge Management, Proceedings",Graph topic scan statistic for spatial event detection,"Spatial event detection is an important and challenging problem. Unlike traditional event detection that focuses on the timing of global urgent event, the task of spatial event detection is to detect the spatial regions (e.g. clusters of neighboring cities) where urgent events occur. In this paper, we focus on the problem of spatial event detection using textual information in social media. We observe that, when a spatial event occurs, the topics relevant to the event are often discussed more coherently in cities near the event location than those far away. In order to capture this pattern, we propose a new method called Graph Topic Scan Statistic (Graph-TSS) that corresponds to a generalized log-likelihood ratio test based on topic modeling. We first demonstrate that the detection of spatial event regions under Graph-TSS is NP-hard due to a reduction from classical node-weighted prize-collecting Steiner tree problem (NW-PCST). We then design an efficient algorithm that approximately maximizes the graph topic scan statistic over spatial regions of arbitrary form. As a case study, we consider three applications using Twitter data, including Argentina civil unrest event detection, Chile earthquake detection, and United States influenza disease outbreak detection. Empirical evidence demonstrates that the proposed Graph-TSS performs superior over state-of-the-art methods on both running time and accuracy."
305,"Developing text classifiers often requires a large number of labeled documents as training examples. However, manually labeling documents is costly and time-consuming. Recently, a few methods have been proposed to label documents by using a small set of relevant keywords for each category, known as dataless text classification. In this paper, we propose a Seed-Guided Topic Model (named STM) for the dataless text classification task. Given a collection of unla-beled documents, and for each category a small set of seed words that are relevant to the semantic meaning of the category, the STM predicts the category labels of the documents through topic influence. STM models two kinds of topics: category-topics and general-topics. Each category-topic is associated with one specific category, representing its semantic meaning. The general-topics capture the global semantic information underlying the whole document collection. STM assumes that each document is associated with a single category-topic and a mixture of general-topics. A novelty of the model is that STM learns the topics by exploiting the explicit word co-occurrence patterns between the seed words and regular words (i.e., non-seed words) in the document collection. A document is then labeled, or classified, based on its posterior category-topic assignment. Experiments on two widely used datasets show that STM consistently outperforms the state-of-the-art dataless text classifiers. In some tasks, STM can also achieve comparable or even better classification accuracy than the state-of-the-art supervised learning solutions. Our experimental results further show that STM is insensitive to the tuning parameters. Stable performance with little variation can be achieved in a broad range of parameter settings, making it a desired choice for real applications.",2016-10-24,2-s2.0-84996598610,"International Conference on Information and Knowledge Management, Proceedings",Effective document labeling with very few seed words: A topic modeling approach,"Developing text classifiers often requires a large number of labeled documents as training examples. However, manually labeling documents is costly and time-consuming. Recently, a few methods have been proposed to label documents by using a small set of relevant keywords for each category, known as dataless text classification. In this paper, we propose a Seed-Guided Topic Model (named STM) for the dataless text classification task. Given a collection of unla-beled documents, and for each category a small set of seed words that are relevant to the semantic meaning of the category, the STM predicts the category labels of the documents through topic influence. STM models two kinds of topics: category-topics and general-topics. Each category-topic is associated with one specific category, representing its semantic meaning. The general-topics capture the global semantic information underlying the whole document collection. STM assumes that each document is associated with a single category-topic and a mixture of general-topics. A novelty of the model is that STM learns the topics by exploiting the explicit word co-occurrence patterns between the seed words and regular words (i.e., non-seed words) in the document collection. A document is then labeled, or classified, based on its posterior category-topic assignment. Experiments on two widely used datasets show that STM consistently outperforms the state-of-the-art dataless text classifiers. In some tasks, STM can also achieve comparable or even better classification accuracy than the state-of-the-art supervised learning solutions. Our experimental results further show that STM is insensitive to the tuning parameters. Stable performance with little variation can be achieved in a broad range of parameter settings, making it a desired choice for real applications."
306,"A viewpoint is a triple consisting of an entity, a topic related to this entity and sentiment towards this topic. In time-aware multi-viewpoint summarization one monitors viewpoints for a running topic and selects a small set of informative documents. In this paper, we focus on time-aware multi-viewpoint summarization of multilingual social text streams. Viewpoint drift, ambiguous entities and multilingual text make this a challenging task. Our approach includes three core ingredients: dynamic viewpoint modeling, cross-language viewpoint alignment, and, finally, multi-viewpoint summarization. Specifically, we propose a dynamic latent factor model to explicitly characterize a set of viewpoints through which entities, topics and sentiment labels during a time interval are derived jointly; we connect viewpoints in different languages by using an entity-based semantic similarity measure; and we employ an update viewpoint summarization strategy to generate a time-aware summary to reflect viewpoints. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method for time-aware multi-viewpoint summarization of multilingual social text streams.",2016-10-24,2-s2.0-84996524181,"International Conference on Information and Knowledge Management, Proceedings",Time-aware multi-viewpoint summarization of multilingual social text streams,"A viewpoint is a triple consisting of an entity, a topic related to this entity and sentiment towards this topic. In time-aware multi-viewpoint summarization one monitors viewpoints for a running topic and selects a small set of informative documents. In this paper, we focus on time-aware multi-viewpoint summarization of multilingual social text streams. Viewpoint drift, ambiguous entities and multilingual text make this a challenging task. Our approach includes three core ingredients: dynamic viewpoint modeling, cross-language viewpoint alignment, and, finally, multi-viewpoint summarization. Specifically, we propose a dynamic latent factor model to explicitly characterize a set of viewpoints through which entities, topics and sentiment labels during a time interval are derived jointly; we connect viewpoints in different languages by using an entity-based semantic similarity measure; and we employ an update viewpoint summarization strategy to generate a time-aware summary to reflect viewpoints. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed method for time-aware multi-viewpoint summarization of multilingual social text streams."
307,"Recommendation for user generated content sites has gained significant attention. To satisfy the niche tastes of users, product recommendation poses more challenges due to the data sparsity issue. This work is motivated by a real world online video recommendation problem, where the click records database suffers from s-parseness of video inventory and video tags. Targeting the long tail phenomena of user behavior and sparsity of item features, we propose a personalized compound recommendation framework for online video recommendation called Dirichlet mixture probit model for information scarcity (DPIS). Assuming that each record is generated from a representation of user preferences, DPIS is a probit classifier utilizing record topical clustering on the user part for recommendation. As demonstrated by the real-world application, the proposed DPIS achieves better performance than traditional methods.",2016-10-24,2-s2.0-84996548923,"International Conference on Information and Knowledge Management, Proceedings",Scarce feature topic mining for video recommendation,"Recommendation for user generated content sites has gained significant attention. To satisfy the niche tastes of users, product recommendation poses more challenges due to the data sparsity issue. This work is motivated by a real world online video recommendation problem, where the click records database suffers from s-parseness of video inventory and video tags. Targeting the long tail phenomena of user behavior and sparsity of item features, we propose a personalized compound recommendation framework for online video recommendation called Dirichlet mixture probit model for information scarcity (DPIS). Assuming that each record is generated from a representation of user preferences, DPIS is a probit classifier utilizing record topical clustering on the user part for recommendation. As demonstrated by the real-world application, the proposed DPIS achieves better performance than traditional methods."
308,"With the soaring popularity of online social media like Twitter, analyzing short text has emerged as an increasingly important task which is challenging to classical topic models. as topic sparsity exists in short text. Topic sparsity refers to the observation that individual document usually concentrates on several salient topics, which may be rare in entire corpus. Understanding this sparse topical structure of short text has been recognized as the key ingredient for mining user-generated Web content and social medium, which are featured in the form of extremely short posts and discussions. However, the existing sparsity-enhanced topic models all assume over-complicated generative process, which severely limits their scalability and makes them unable to automatically infer the number of topics from data. In this paper, we propose a probabilistic Bayesian topic model, namely Sparse Dirichlet mixture Topic Model (S-parseDTM), based on Indian Buffet Process (IBP) prior, and infer our model on the large text corpora through a novel inference procedure called stochastic variational-Gibbs inference. Unlike prior work, the proposed approach is able to achieve exact sparse topical structure of large short text collections, and automatically identify the number of topics with a good balance between completeness and homogeneity of topic coherence. Experiments on different genres of large text corpora demonstrate that our approach outperforms various existing sparse topic models. The improvement is significant on large-scale collections of short text.",2016-10-24,2-s2.0-84996522100,"International Conference on Information and Knowledge Management, Proceedings",Understanding sparse topical structure of short text via stochastic variational-gibbs inference,"With the soaring popularity of online social media like Twitter, analyzing short text has emerged as an increasingly important task which is challenging to classical topic models. as topic sparsity exists in short text. Topic sparsity refers to the observation that individual document usually concentrates on several salient topics, which may be rare in entire corpus. Understanding this sparse topical structure of short text has been recognized as the key ingredient for mining user-generated Web content and social medium, which are featured in the form of extremely short posts and discussions. However, the existing sparsity-enhanced topic models all assume over-complicated generative process, which severely limits their scalability and makes them unable to automatically infer the number of topics from data. In this paper, we propose a probabilistic Bayesian topic model, namely Sparse Dirichlet mixture Topic Model (S-parseDTM), based on Indian Buffet Process (IBP) prior, and infer our model on the large text corpora through a novel inference procedure called stochastic variational-Gibbs inference. Unlike prior work, the proposed approach is able to achieve exact sparse topical structure of large short text collections, and automatically identify the number of topics with a good balance between completeness and homogeneity of topic coherence. Experiments on different genres of large text corpora demonstrate that our approach outperforms various existing sparse topic models. The improvement is significant on large-scale collections of short text."
309,"Mining topics in short texts (e.g. tweets, instant messages) can help people grasp essential information and understand key contents, and is widely used in many applications related to social media and text analysis. The sparsity and noise of short texts often restrict the performance of traditional topic models like LDA. Recently proposed Biterm Topic Model (BTM) which models word co-occurrence patterns directly, is revealed effective for topic detection in short texts. However, BTM has two main drawbacks. It needs to manually specify topic number, which is difficult to accurately determine when facing new corpora. Besides, BTM assumes that two words in same term should belong to the same topic, which is often too strong as it does not differentiate two types of words (i.e. general words and topical words). To tackle these problems, in this paper, we propose a non-parametric topic model npCTM with the above distinction. Our model incorporates the Chinese restaurant process (CRP) into the BTM model to determine topic number automatically. Our model also distinguishes general words from topical words by jointly considering the distribution of these two word types for each word as well as word coherence information as prior knowledge. We carry out experimental studies on real-world twitter dataset. The results demonstrate the effectiveness of our method to discover coherent topics compared with the baseline methods.",2016-10-24,2-s2.0-84996598809,"International Conference on Information and Knowledge Management, Proceedings",A non-parametric topic model for short texts incorporating word coherence knowledge,"Mining topics in short texts (e.g. tweets, instant messages) can help people grasp essential information and understand key contents, and is widely used in many applications related to social media and text analysis. The sparsity and noise of short texts often restrict the performance of traditional topic models like LDA. Recently proposed Biterm Topic Model (BTM) which models word co-occurrence patterns directly, is revealed effective for topic detection in short texts. However, BTM has two main drawbacks. It needs to manually specify topic number, which is difficult to accurately determine when facing new corpora. Besides, BTM assumes that two words in same term should belong to the same topic, which is often too strong as it does not differentiate two types of words (i.e. general words and topical words). To tackle these problems, in this paper, we propose a non-parametric topic model npCTM with the above distinction. Our model incorporates the Chinese restaurant process (CRP) into the BTM model to determine topic number automatically. Our model also distinguishes general words from topical words by jointly considering the distribution of these two word types for each word as well as word coherence information as prior knowledge. We carry out experimental studies on real-world twitter dataset. The results demonstrate the effectiveness of our method to discover coherent topics compared with the baseline methods."
310,"The newly emerging location-based social networks (LBSN) such as Tinder and Momo extends social interaction from friends to strangers, providing novel experiences of making new friends. Familiar strangers refer to the strangers who meet frequently in daily life and may share common interests; thus they may be good candidates for friend recommendation. In this paper, we study the problem of discovering familiar strangers, specifically, public transportation trip companions, and their common interests. We collect 5.7 million transaction records of smart cards from about 3.02 million people in the city of Beijing, China. We first analyze this dataset and reveal the temporal and spatial characteristics of passenger encounter behaviors. Then we propose a stability metric to measure hidden friend relations. This metric facilitates us to employ community detection techniques to capture the communities of trip companions. Further, we infer common interests of each community using a topic model, i.e., LDA4HFC (Latent Dirichlet Allocation for Hidden Friend Communities) model. Such topics for communities help to understand how hidden friend clusters are formed. We evaluate our method using large-scale and real-world datasets, consisting of two-week smart card records and 901,855 Point of Interests (POIs) in Beijing. The results show that our method outperforms three baseline methods with higher recommendation accuracy. Moreover, our case study demonstrates that the discovered topics interpret the communities very well.",2016-10-24,2-s2.0-84996527566,"International Conference on Information and Knowledge Management, Proceedings",Who are my familiar strangers? Revealing hidden friend relations and common interests from smart card data,"The newly emerging location-based social networks (LBSN) such as Tinder and Momo extends social interaction from friends to strangers, providing novel experiences of making new friends. Familiar strangers refer to the strangers who meet frequently in daily life and may share common interests; thus they may be good candidates for friend recommendation. In this paper, we study the problem of discovering familiar strangers, specifically, public transportation trip companions, and their common interests. We collect 5.7 million transaction records of smart cards from about 3.02 million people in the city of Beijing, China. We first analyze this dataset and reveal the temporal and spatial characteristics of passenger encounter behaviors. Then we propose a stability metric to measure hidden friend relations. This metric facilitates us to employ community detection techniques to capture the communities of trip companions. Further, we infer common interests of each community using a topic model, i.e., LDA4HFC (Latent Dirichlet Allocation for Hidden Friend Communities) model. Such topics for communities help to understand how hidden friend clusters are formed. We evaluate our method using large-scale and real-world datasets, consisting of two-week smart card records and 901,855 Point of Interests (POIs) in Beijing. The results show that our method outperforms three baseline methods with higher recommendation accuracy. Moreover, our case study demonstrates that the discovered topics interpret the communities very well."
311,"The study described in the paper aims at multi-faceted characterization of active community question anwering (CQA) users who provide answers to health-related questions. The study employs various research techniques-both qualitative (surveys) and quantitative. With two online surveys we get insights into 1. perception of online health-related information and its use by patients by medical professionals and 2. motivtion of most active CQA answerers, a significant share of which apparently constitute users with medical education. In the second series of experiments we apply topic modeling to a yearly collection of questions and answers from a popular Russian CQA servce in order to find users focused on a particular topic. Further, we attempt to find users with professional medical backround based on the lexis of their answers. The obtained results provide a beter understanding of motivation and backround of CQA users and can be used for the improvement of CQA services, as well as for solving problems such as CQA content quality evaluation, expert search, and question routing, etc.",2016-10-05,2-s2.0-84994410846,"Proceedings of the International FRUCT Conference on Intelligence, Social Media and Web, ISMW FRUCT 2016",Does everybody lie? characterizing answerers in health-related CQA,"The study described in the paper aims at multi-faceted characterization of active community question anwering (CQA) users who provide answers to health-related questions. The study employs various research techniques-both qualitative (surveys) and quantitative. With two online surveys we get insights into 1. perception of online health-related information and its use by patients by medical professionals and 2. motivtion of most active CQA answerers, a significant share of which apparently constitute users with medical education. In the second series of experiments we apply topic modeling to a yearly collection of questions and answers from a popular Russian CQA servce in order to find users focused on a particular topic. Further, we attempt to find users with professional medical backround based on the lexis of their answers. The obtained results provide a beter understanding of motivation and backround of CQA users and can be used for the improvement of CQA services, as well as for solving problems such as CQA content quality evaluation, expert search, and question routing, etc."
312,"E-petitioning technology platforms elicit the participation of citizens in the policy-making process but at the same time create large volumes of unstructured textual data that are difficult to analyze. Fortunately, computational tools can assist policy analysts in uncovering latent patterns from these large textual datasets. This study uses such computational tools to explore e-petitions, viewing them as persuasive texts with linguistic and semantic features that may be related to the popularity of petitions, as indexed by the number of signatures they attract. Using We the People website data, we analyzed linguistic features, such as extremity and repetition, and semantic features, such as named entities and topics, to determine whether and to what extent they are related to petition popularity. The results show that each block of variables independently explains statistically significant variation in signature accumulation, and that 1) language extremity is persistently and negatively associated with petition popularity, 2) petitions with many names tend not to become popular, and 3) petition popularity is associated with petitions that include topics familiar to the public or about important social events. We believe explorations along these lines will yield useful strategies to address the wicked problem of too much text data and to facilitate the enhancement of public participation in policy-making.",2016-10-01,2-s2.0-84995917582,Government Information Quarterly,E-petition popularity: Do linguistic and semantic factors matter?,"E-petitioning technology platforms elicit the participation of citizens in the policy-making process but at the same time create large volumes of unstructured textual data that are difficult to analyze. Fortunately, computational tools can assist policy analysts in uncovering latent patterns from these large textual datasets. This study uses such computational tools to explore e-petitions, viewing them as persuasive texts with linguistic and semantic features that may be related to the popularity of petitions, as indexed by the number of signatures they attract. Using We the People website data, we analyzed linguistic features, such as extremity and repetition, and semantic features, such as named entities and topics, to determine whether and to what extent they are related to petition popularity. The results show that each block of variables independently explains statistically significant variation in signature accumulation, and that 1) language extremity is persistently and negatively associated with petition popularity, 2) petitions with many names tend not to become popular, and 3) petition popularity is associated with petitions that include topics familiar to the public or about important social events. We believe explorations along these lines will yield useful strategies to address the wicked problem of too much text data and to facilitate the enhancement of public participation in policy-making."
313,"This study focuses on the improvement of prospective teachers’ science generic skills in learning for earth and space sciences subject using a laboratory-based model. Research methods were Research and Development used mixed methods design, i.e. Embedded Experimental models. The results showed that the structure of the learning using laboratory-based model consists of four stages, namely engagement, exploration, explanation, and elabo­ration; with details on description of natural phenomena, the identification of questions, linking concepts with practical topic; modeling, training, exploration, interpretation of data, conclude, and communicate the proce­dures and results of the investigation. Moreover, the ability of science generic skills of teacher candidates was low at 52.50%. In addition, the profile of lecturer performance in learning reached 55%. Also, the condition of the learning environment is generally sufficient in supporting the development of laboratory activities, for example: a comfortable laboratory room for conducting the practicum. Next, 75% student agreed that for understand the material; it needs a practice in lab. As many as 70% of students agreed that practicum will increase students’ un­derstanding, although students also agreed that clarification is still required by lecturers (80%). Furthermore, 69% of the students found the materials of Earth and Space Sciences ismore attractive than other materials, although 78% said material of Earth and Space Sciences is fairly complicated and abstract. The interest of students to the material of Earth and Space Sciences is because these materials can foster curiosity (88%) as well as providing benefits to their daily life (90%). Finally, three out of four students agreed that the science generic skills can de­velop the ability of scientific work and critical thinking.",2016-10-01,2-s2.0-85031786148,Jurnal Pendidikan IPA Indonesia,The development of laboratory-based earth and space science learning model to improve science generic skills of pre-service teachers,"This study focuses on the improvement of prospective teachers’ science generic skills in learning for earth and space sciences subject using a laboratory-based model. Research methods were Research and Development used mixed methods design, i.e. Embedded Experimental models. The results showed that the structure of the learning using laboratory-based model consists of four stages, namely engagement, exploration, explanation, and elabo­ration; with details on description of natural phenomena, the identification of questions, linking concepts with practical topic; modeling, training, exploration, interpretation of data, conclude, and communicate the proce­dures and results of the investigation. Moreover, the ability of science generic skills of teacher candidates was low at 52.50%. In addition, the profile of lecturer performance in learning reached 55%. Also, the condition of the learning environment is generally sufficient in supporting the development of laboratory activities, for example: a comfortable laboratory room for conducting the practicum. Next, 75% student agreed that for understand the material; it needs a practice in lab. As many as 70% of students agreed that practicum will increase students’ un­derstanding, although students also agreed that clarification is still required by lecturers (80%). Furthermore, 69% of the students found the materials of Earth and Space Sciences ismore attractive than other materials, although 78% said material of Earth and Space Sciences is fairly complicated and abstract. The interest of students to the material of Earth and Space Sciences is because these materials can foster curiosity (88%) as well as providing benefits to their daily life (90%). Finally, three out of four students agreed that the science generic skills can de­velop the ability of scientific work and critical thinking."
314,"The past few years have witnessed millions of credit/debit cards flowing through the underground economy and ultimately causing significant financial loss. Examining key underground economy sellers has both practical and academic significance for cybercrime forensics and criminology research. Drawing on social media analytics, we have developed the AZSecure text mining system for identifying and profiling key sellers. The system identifies sellers using sentiment analysis of customer reviews and profiles sellers using topic modeling of advertisements. We evaluated the AZSecure system on eight international underground economy forums. The system significantly outperformed all benchmark machine-learning methods on identifying advertisement threads, classifying customer review sentiments, and profiling seller characteristics, with an average F-measure of about 80 percent to 90 percent. In our case study, we identified the famous carder, Rescator, who was affiliated with the Target breach, and captured important seller characteristics in terms of product type, payment options, and contact channels. Our research leverages social media analytics to probe into the underground economy in order to help law enforcement target key sellers and prevent future fraud. It also contributes to our understanding of the use of information technology in detecting deception in online systems.",2016-10-01,2-s2.0-85012186740,Journal of Management Information Systems,Identifying and Profiling Key Sellers in Cyber Carding Community: AZSecure Text Mining System,"The past few years have witnessed millions of credit/debit cards flowing through the underground economy and ultimately causing significant financial loss. Examining key underground economy sellers has both practical and academic significance for cybercrime forensics and criminology research. Drawing on social media analytics, we have developed the AZSecure text mining system for identifying and profiling key sellers. The system identifies sellers using sentiment analysis of customer reviews and profiles sellers using topic modeling of advertisements. We evaluated the AZSecure system on eight international underground economy forums. The system significantly outperformed all benchmark machine-learning methods on identifying advertisement threads, classifying customer review sentiments, and profiling seller characteristics, with an average F-measure of about 80 percent to 90 percent. In our case study, we identified the famous carder, Rescator, who was affiliated with the Target breach, and captured important seller characteristics in terms of product type, payment options, and contact channels. Our research leverages social media analytics to probe into the underground economy in order to help law enforcement target key sellers and prevent future fraud. It also contributes to our understanding of the use of information technology in detecting deception in online systems."
315,"The delineation of coordinates is fundamental for the cartography of science, and accurate and credible classification of scientific knowledge presents a persistent challenge in this regard. We present a map of Finnish science based on unsupervised-learning classification, and discuss the advantages and disadvantages of this approach vis-à-vis those generated by human reasoning. We conclude that from theoretical and practical perspectives there exist several challenges for human reasoning-based classification frameworks of scientific knowledge, as they typically try to fit new-to-the-world knowledge into historical models of scientific knowledge, and cannot easily be deployed for new large-scale data sets. Automated classification schemes, in contrast, generate classification models only from the available text corpus, thereby identifying credibly novel bodies of knowledge. They also lend themselves to versatile large-scale data analysis, and enable a range of Big Data possibilities. However, we also argue that it is neither possible nor fruitful to declare one or another method a superior approach in terms of realism to classify scientific knowledge, and we believe that the merits of each approach are dependent on the practical objectives of analysis.",2016-10-01,2-s2.0-84987643700,Journal of the Association for Information Science and Technology,Map of science with topic modeling: Comparison of unsupervised learning and human-assigned subject classification,"The delineation of coordinates is fundamental for the cartography of science, and accurate and credible classification of scientific knowledge presents a persistent challenge in this regard. We present a map of Finnish science based on unsupervised-learning classification, and discuss the advantages and disadvantages of this approach vis-à-vis those generated by human reasoning. We conclude that from theoretical and practical perspectives there exist several challenges for human reasoning-based classification frameworks of scientific knowledge, as they typically try to fit new-to-the-world knowledge into historical models of scientific knowledge, and cannot easily be deployed for new large-scale data sets. Automated classification schemes, in contrast, generate classification models only from the available text corpus, thereby identifying credibly novel bodies of knowledge. They also lend themselves to versatile large-scale data analysis, and enable a range of Big Data possibilities. However, we also argue that it is neither possible nor fruitful to declare one or another method a superior approach in terms of realism to classify scientific knowledge, and we believe that the merits of each approach are dependent on the practical objectives of analysis."
316,"This article combines topic modeling and critical discourse analysis to examine patterns of representation around the words Muslim and Islam in a 105 million word corpus of a large Swedish Internet forum from 2000 to 2013. Despite the increased importance of social media in the (re)production of discursive power in society, this is the first study of its kind. The analysis shows that Muslims are portrayed in the forum as a homogeneous outgroup that is embroiled in conflict, violence and extremism: characteristics that are described as emanating from Islam as a religion. These patterns are strikingly similar to – but often more extreme versions of – those previously found in analysis of traditional media. This indicates that, in this case, the internet forum seems to serve as an “online amplifier” that reflects and reinforces existing discourses in traditional media, which is likely to result in even stronger polarizing effects on public discourses.",2016-09-01,2-s2.0-84979537039,"Discourse, Context and Media",Muslims in social media discourse: Combining topic modeling and critical discourse analysis,"This article combines topic modeling and critical discourse analysis to examine patterns of representation around the words Muslim and Islam in a 105 million word corpus of a large Swedish Internet forum from 2000 to 2013. Despite the increased importance of social media in the (re)production of discursive power in society, this is the first study of its kind. The analysis shows that Muslims are portrayed in the forum as a homogeneous outgroup that is embroiled in conflict, violence and extremism: characteristics that are described as emanating from Islam as a religion. These patterns are strikingly similar to – but often more extreme versions of – those previously found in analysis of traditional media. This indicates that, in this case, the internet forum seems to serve as an “online amplifier” that reflects and reinforces existing discourses in traditional media, which is likely to result in even stronger polarizing effects on public discourses."
317,"The rise of mobile food vending in US cities combines urban space and mobility with continuous online communication. Unlike traditional urban spaces that are predictable and known, contemporary vendors use information technology to generate impromptu social settings in unconventional and often underutilized spaces. This unique condition requires new methods that interpret online communication as a critical component in the production of new forms of public life. We suggest qualitative approaches combined with data-driven analyses are necessary when planning for emergent behavior. In Charlotte, NC, we investigate the daily operations, tweet content, and spatial and temporal sequencing of six vendors over an extended period of time. The study illustrates the interrelationship between data, urban space, and time and finds that a significant proportion of tweet content is used to announce vending locations in a time-based pattern and that the spatial construction of events is often independent of traditional urban form.",2016-09-01,2-s2.0-84983316624,New Media and Society,Revaluating urban space through tweets: An analysis of Twitter-based mobile food vendors and online communication,"The rise of mobile food vending in US cities combines urban space and mobility with continuous online communication. Unlike traditional urban spaces that are predictable and known, contemporary vendors use information technology to generate impromptu social settings in unconventional and often underutilized spaces. This unique condition requires new methods that interpret online communication as a critical component in the production of new forms of public life. We suggest qualitative approaches combined with data-driven analyses are necessary when planning for emergent behavior. In Charlotte, NC, we investigate the daily operations, tweet content, and spatial and temporal sequencing of six vendors over an extended period of time. The study illustrates the interrelationship between data, urban space, and time and finds that a significant proportion of tweet content is used to announce vending locations in a time-based pattern and that the spatial construction of events is often independent of traditional urban form."
318,"The performance of cross-lingual sentiment classification is sharply limited by the language gap, which means that each language has its own ways to express sentiments. Many methods have been designed to transmit sentiment information across languages by making use of machine translation, parallel corpora, auxiliary unlabeled samples and other resources. In this paper, a new approach is proposed based on the selection of training data, where labeled samples highly similar to the target language are put into the training set. The refined training samples are used to build up an effective cross-lingual sentiment classifier focusing on the target language. The proposed approach contains two major strategies: the aligned-translation topic model and the semi-supervised training data adjustment. The aligned-translation topic model provides a cross-language representation space in which the semi-supervised training data adjustment procedure attempts to select effective training samples to eliminate the negative influence of the semantic distribution differences between the original and target languages. The experiments show that the proposed approach is feasible for cross-language sentiment classification tasks and provides insight into the semantic relationship between two different languages.",2016-09-01,2-s2.0-85000885429,Knowledge-Based Systems,Cross-lingual sentiment classification: Similarity discovery plus training data adjustment,"The performance of cross-lingual sentiment classification is sharply limited by the language gap, which means that each language has its own ways to express sentiments. Many methods have been designed to transmit sentiment information across languages by making use of machine translation, parallel corpora, auxiliary unlabeled samples and other resources. In this paper, a new approach is proposed based on the selection of training data, where labeled samples highly similar to the target language are put into the training set. The refined training samples are used to build up an effective cross-lingual sentiment classifier focusing on the target language. The proposed approach contains two major strategies: the aligned-translation topic model and the semi-supervised training data adjustment. The aligned-translation topic model provides a cross-language representation space in which the semi-supervised training data adjustment procedure attempts to select effective training samples to eliminate the negative influence of the semantic distribution differences between the original and target languages. The experiments show that the proposed approach is feasible for cross-language sentiment classification tasks and provides insight into the semantic relationship between two different languages."
319,"Traditional pseudo relevance feedback (PRF) models choose top k feedback documents for query expansion and treat those documents equally. When k is determined, feedback terms are selected without considering the reliability of these documents for relevance. Because the performance of PRF is sensitive to the selection of feedback terms, noisy terms imported from these irrelevant documents or partially relevant documents will harm the final results extensively. Intuitively, terms in these documents should be considered less important for feedback term selection. Nonetheless, how to measure the reliability of feedback documents is a difficult problem. Recently, topic modeling has become more and more popular in the information retrieval (IR) area. In order to identify how reliable a feedback document is to be relevant, we attempt to adapt the topical information into PRF. However, topics are hard to be quantified and therefore the identification of topic is usually fuzzy. It is very challenging for integrating the obtained topical information effectively into IR and other text-processing-related areas. Current research work mainly focuses on mining relevant information from particular topics. This is extremely difficult when the boundaries of different topics are hard to define. In this article, we investigate a key factor of this problem, the topic number for topic modeling and how it makes topics ""fuzzy."" To effectively and efficiently apply topical information, we propose a new probabilistic framework, ""TopPRF,"" and threemodels, TS-COS, TS-EU, and TS-Entropy, via integrating ""Topic Space"" (TS) information into pseudo relevance feedback. Thesemethods discover how reliable a document is to be relevant through both term and topical information.When selecting feedback terms, candidate terms in more reliable feedback documents should obtain extra weights. Experimental results on various public collections justify that our proposed methods can significantly reduce the influence of ""fuzzy topics"" and obtain stable, good results over the strong baseline models. Our proposed probabilistic framework, TopPRF, and three topicspace- based models are capable of searching documents beyond traditional term matching only and provide a promising avenue for constructing better topic-space-based IR systems. Moreover, in-depth discussions and conclusions are made to help other researchers apply topical information effectively.",2016-08-01,2-s2.0-84986596754,ACM Transactions on Information Systems,TopPRF: A probabilistic framework for integrating topic space into pseudo relevance feedback,"Traditional pseudo relevance feedback (PRF) models choose top k feedback documents for query expansion and treat those documents equally. When k is determined, feedback terms are selected without considering the reliability of these documents for relevance. Because the performance of PRF is sensitive to the selection of feedback terms, noisy terms imported from these irrelevant documents or partially relevant documents will harm the final results extensively. Intuitively, terms in these documents should be considered less important for feedback term selection. Nonetheless, how to measure the reliability of feedback documents is a difficult problem. Recently, topic modeling has become more and more popular in the information retrieval (IR) area. In order to identify how reliable a feedback document is to be relevant, we attempt to adapt the topical information into PRF. However, topics are hard to be quantified and therefore the identification of topic is usually fuzzy. It is very challenging for integrating the obtained topical information effectively into IR and other text-processing-related areas. Current research work mainly focuses on mining relevant information from particular topics. This is extremely difficult when the boundaries of different topics are hard to define. In this article, we investigate a key factor of this problem, the topic number for topic modeling and how it makes topics ""fuzzy."" To effectively and efficiently apply topical information, we propose a new probabilistic framework, ""TopPRF,"" and threemodels, TS-COS, TS-EU, and TS-Entropy, via integrating ""Topic Space"" (TS) information into pseudo relevance feedback. Thesemethods discover how reliable a document is to be relevant through both term and topical information.When selecting feedback terms, candidate terms in more reliable feedback documents should obtain extra weights. Experimental results on various public collections justify that our proposed methods can significantly reduce the influence of ""fuzzy topics"" and obtain stable, good results over the strong baseline models. Our proposed probabilistic framework, TopPRF, and three topicspace- based models are capable of searching documents beyond traditional term matching only and provide a promising avenue for constructing better topic-space-based IR systems. Moreover, in-depth discussions and conclusions are made to help other researchers apply topical information effectively."
320,"Issue definitions, the way policy issues are understood, are an important component for understanding the policymaking process. Research on issue definitions has been divided between a macro level that examines collective issue definitions and a micro level focusing on the ways in which policy actors frame policy issues. This article develops a model of issue definitions that assumes issues are multidimensional, competition exists among policy actors in defining issues, and that collective issue definitions can be understood as the aggregation of individual issue definitions. This model is then estimated using quantitative text analysis. While various approaches to text analysis and categorization have been used by scholars, latent Dirichlet allocation (LDA), a specific type of topic modeling, is used to estimate issue definitions. Using LDA, witness testimony taken from Congressional hearings that occurred from 1975 to 2012 about the issue of used nuclear fuel (UNF) is examined and seven distinct dimensions of the UNF debate are estimated. The construct validity of these dimensions is checked by testing them against two major policy changes that occurred in the UNF domain. I conclude with a discussion of the strengths and weakness of topic modeling, and how this approach could be used to test hypotheses drawn from several of the major policymaking theories.",2016-08-01,2-s2.0-84979517851,Policy Studies Journal,Modeling Issue Definitions Using Quantitative Text Analysis,"Issue definitions, the way policy issues are understood, are an important component for understanding the policymaking process. Research on issue definitions has been divided between a macro level that examines collective issue definitions and a micro level focusing on the ways in which policy actors frame policy issues. This article develops a model of issue definitions that assumes issues are multidimensional, competition exists among policy actors in defining issues, and that collective issue definitions can be understood as the aggregation of individual issue definitions. This model is then estimated using quantitative text analysis. While various approaches to text analysis and categorization have been used by scholars, latent Dirichlet allocation (LDA), a specific type of topic modeling, is used to estimate issue definitions. Using LDA, witness testimony taken from Congressional hearings that occurred from 1975 to 2012 about the issue of used nuclear fuel (UNF) is examined and seven distinct dimensions of the UNF debate are estimated. The construct validity of these dimensions is checked by testing them against two major policy changes that occurred in the UNF domain. I conclude with a discussion of the strengths and weakness of topic modeling, and how this approach could be used to test hypotheses drawn from several of the major policymaking theories."
321,"In this paper we propose a fully unsupervised approach for product aspect discovery in on-line consumer reviews. We apply a two-step hierarchical clustering process in which we first cluster words representing aspects based on the semantic similarity of their contexts and then on the similarity of the hypernyms of the cluster members. Our approach also includes a method for assigning class labels to each of the clusters. We evaluated our methods on large datasets of restaurant and camera reviews and found that the two-step clustering process performed better than a single-step clustering process at identifying aspects and words refering to aspects. Finally, we compare our method to a state-of-the-art topic modelling approach by Titov and McDonald, and demonstrate better results on both datasets.",2016-08-01,2-s2.0-84979058881,Journal of Information Science,Discovering aspects of online consumer reviews,"In this paper we propose a fully unsupervised approach for product aspect discovery in on-line consumer reviews. We apply a two-step hierarchical clustering process in which we first cluster words representing aspects based on the semantic similarity of their contexts and then on the similarity of the hypernyms of the cluster members. Our approach also includes a method for assigning class labels to each of the clusters. We evaluated our methods on large datasets of restaurant and camera reviews and found that the two-step clustering process performed better than a single-step clustering process at identifying aspects and words refering to aspects. Finally, we compare our method to a state-of-the-art topic modelling approach by Titov and McDonald, and demonstrate better results on both datasets."
322,"Search engines assist users in expressing their information needs more accurately by reformulating the issued queries automatically and suggesting the generated formulations to the users. Many approaches to query suggestion draw on the information stored in query logs, recommending recorded queries that are textually similar to the current user's query or that frequently co-occurred with it in the past. In this paper, we propose an approach that concentrates on deducing the actual information need from the user's query. The challenge therein lies not only in processing keyword queries, which are often short and possibly ambiguous, but especially in handling the complexity of natural language that allows users to express the same or similar information needs in various differing ways. We expect a higher-level semantic representation of a user's query to more accurately reflect the information need than the explicit query terms alone can. To this aim, we employ latent Dirichlet allocation as a probabilistic topic model to reveal latent semantics in the query log. Our evaluations show that, whereas purely topic-based query suggestion performs the worst, the interpolation of our proposed topic-based model with the baseline word-based model that generates suggestions based on matching query terms achieves significant improvements in suggestion quality over the already well performing purely word-based approach.",2016-08-01,2-s2.0-84979009825,Journal of Information Science,Generating query suggestions by exploiting latent semantics in query logs,"Search engines assist users in expressing their information needs more accurately by reformulating the issued queries automatically and suggesting the generated formulations to the users. Many approaches to query suggestion draw on the information stored in query logs, recommending recorded queries that are textually similar to the current user's query or that frequently co-occurred with it in the past. In this paper, we propose an approach that concentrates on deducing the actual information need from the user's query. The challenge therein lies not only in processing keyword queries, which are often short and possibly ambiguous, but especially in handling the complexity of natural language that allows users to express the same or similar information needs in various differing ways. We expect a higher-level semantic representation of a user's query to more accurately reflect the information need than the explicit query terms alone can. To this aim, we employ latent Dirichlet allocation as a probabilistic topic model to reveal latent semantics in the query log. Our evaluations show that, whereas purely topic-based query suggestion performs the worst, the interpolation of our proposed topic-based model with the baseline word-based model that generates suggestions based on matching query terms achieves significant improvements in suggestion quality over the already well performing purely word-based approach."
323,"Interests of researchers who engage with research synthesis methods (RSM) intersect with library and information science (LIS) research and practice. This intersection is described by a summary of conceptualizations of research synthesis in a diverse set of research fields and in the context of Swanson's (1986) discussion of undiscovered public knowledge. Through a selective literature review, research topics that intersect with LIS and RSM are outlined. Topics identified include open access, information retrieval, bias and research information ethics, referencing practices, citation patterns, and data science. Subsequently, bibliometrics and topic modeling are used to present a systematic overview of the visibility of RSM in LIS. This analysis indicates that RSM became visible in LIS in the 1980s. Overall, LIS research has drawn substantially from general and internal medicine, the field's own literature, and business; and is drawn on by health and medical sciences, computing, and business. Through this analytical overview, it is confirmed that research synthesis is more visible in the health and medical literature in LIS; but suggests that, LIS, as a meta-science, has the potential to make substantive contributions to a broader variety of fields in the context of topics related to research synthesis methods.",2016-08-01,2-s2.0-84979034888,Journal of the Association for Information Science and Technology,"Research synthesis methods and library and information science: Shared problems, limited diffusion","Interests of researchers who engage with research synthesis methods (RSM) intersect with library and information science (LIS) research and practice. This intersection is described by a summary of conceptualizations of research synthesis in a diverse set of research fields and in the context of Swanson's (1986) discussion of undiscovered public knowledge. Through a selective literature review, research topics that intersect with LIS and RSM are outlined. Topics identified include open access, information retrieval, bias and research information ethics, referencing practices, citation patterns, and data science. Subsequently, bibliometrics and topic modeling are used to present a systematic overview of the visibility of RSM in LIS. This analysis indicates that RSM became visible in LIS in the 1980s. Overall, LIS research has drawn substantially from general and internal medicine, the field's own literature, and business; and is drawn on by health and medical sciences, computing, and business. Through this analytical overview, it is confirmed that research synthesis is more visible in the health and medical literature in LIS; but suggests that, LIS, as a meta-science, has the potential to make substantive contributions to a broader variety of fields in the context of topics related to research synthesis methods."
324,In this paper we present a personalized video composition method based on user intention learning. The proposed system generates video clips by choosing user's assets such as still images and video clips based on pre-designed templates. The search-and-suggestion technique plays an important role for providing appropriate templates. We mainly focus on the learning mechanism which will be gradually improving through repeated template recommendations. Our prototype demonstrates the feasibility of the proposed approach.,2016-07-25,2-s2.0-84982189365,"IEEE International Symposium on Broadband Multimedia Systems and Broadcasting, BMSB",Personalized video composition method based on user intention learning,In this paper we present a personalized video composition method based on user intention learning. The proposed system generates video clips by choosing user's assets such as still images and video clips based on pre-designed templates. The search-and-suggestion technique plays an important role for providing appropriate templates. We mainly focus on the learning mechanism which will be gradually improving through repeated template recommendations. Our prototype demonstrates the feasibility of the proposed approach.
325,"Information centers are increasingly being confronted with the challenges of shifting information environments. The development of a digital information society has dictated that libraries devise strategies to capture, describe, and provide access to these digital documents in addition to physical formats. This is nowhere more apparent than in the field of government information. With a public access mandate and a distribution model that has forever been destabilized by the development of low barrier Web publishing technologies, libraries providing access to government information face more challenges than ever. This article looks at the possibility of using topic modeling to increase access to the growing number of poorly described digital texts distributed to libraries and archives. The article provides a basic overview of what topic modeling is and its potential applications in libraries, describes some popular tools and potential workflows, and illustrates how the author tested a potential workflow.",2016-07-02,2-s2.0-84980022123,Journal of Web Librarianship,Using Topic Modeling to Enhance Access to Library Digital Collections,"Information centers are increasingly being confronted with the challenges of shifting information environments. The development of a digital information society has dictated that libraries devise strategies to capture, describe, and provide access to these digital documents in addition to physical formats. This is nowhere more apparent than in the field of government information. With a public access mandate and a distribution model that has forever been destabilized by the development of low barrier Web publishing technologies, libraries providing access to government information face more challenges than ever. This article looks at the possibility of using topic modeling to increase access to the growing number of poorly described digital texts distributed to libraries and archives. The article provides a basic overview of what topic modeling is and its potential applications in libraries, describes some popular tools and potential workflows, and illustrates how the author tested a potential workflow."
326,"The surge of interest in big social data has led to growing demand for social media analytics (SMA). Having robust SMA can help firms create value and achieve competitive advantages. However, most firms do not always know how to embrace big social data to establish a path to value. This study addresses this key question to deepen our understanding of how different types of SMA can be applied to create value. Specifically, the findings show the significant uses of opinion mining or sentiment analysis, topic modeling, engagement analysis, predictive analysis, social network analysis, and trend analysis. Finally, the study provides directions for the challenges and opportunities of SMA to maximize value.",2016-07-01,2-s2.0-84973529689,Journal of Organizational and End User Computing,How does social media analytics create value?,"The surge of interest in big social data has led to growing demand for social media analytics (SMA). Having robust SMA can help firms create value and achieve competitive advantages. However, most firms do not always know how to embrace big social data to establish a path to value. This study addresses this key question to deepen our understanding of how different types of SMA can be applied to create value. Specifically, the findings show the significant uses of opinion mining or sentiment analysis, topic modeling, engagement analysis, predictive analysis, social network analysis, and trend analysis. Finally, the study provides directions for the challenges and opportunities of SMA to maximize value."
327,"In this article we present an analysis of the discursive connections between Islamophobia and anti-feminism on a large Internet forum. We argue that the incipient shift from traditional media toward user-driven social media brings with it new media dynamics, relocating the (re)production of societal discourses and power structures and thus bringing about new ways in which discursive power is exercised. This clearly motivates the need to critically engage this field. Our research is based on the analysis of a corpus consisting of over 50 million posts, collected from the forum using custom web crawlers. In order to approach this vast material of unstructured text, we suggest a novel methodological synergy combining critical discourse analysis (CDA) and topic modeling – a type of statistical model for the automated categorization of large quantities of texts developed in computer science. By rendering an overview or ‘content map’ of the corpus, topic modeling provides an enriching complement to CDA, aiding discovery and adding analytical rigor.",2016-07-01,2-s2.0-84975078899,Discourse and Society,Combining CDA and topic modeling: Analyzing discursive connections between Islamophobia and anti-feminism on an online forum,"In this article we present an analysis of the discursive connections between Islamophobia and anti-feminism on a large Internet forum. We argue that the incipient shift from traditional media toward user-driven social media brings with it new media dynamics, relocating the (re)production of societal discourses and power structures and thus bringing about new ways in which discursive power is exercised. This clearly motivates the need to critically engage this field. Our research is based on the analysis of a corpus consisting of over 50 million posts, collected from the forum using custom web crawlers. In order to approach this vast material of unstructured text, we suggest a novel methodological synergy combining critical discourse analysis (CDA) and topic modeling – a type of statistical model for the automated categorization of large quantities of texts developed in computer science. By rendering an overview or ‘content map’ of the corpus, topic modeling provides an enriching complement to CDA, aiding discovery and adding analytical rigor."
328,"General graph random walk has been successfully applied in multi-document summarization, but it has some limitations to process documents by this way. In this paper, we propose a novel hypergraph based vertex-reinforced random walk framework for multi-document summarization. The framework first exploits the Hierarchical Dirichlet Process (HDP) topic model to learn a word-topic probability distribution in sentences. Then the hypergraph is used to capture both cluster relationship based on the word-topic probability distribution and pairwise similarity among sentences. Finally, a time-variant random walk algorithm for hypergraphs is developed to rank sentences which ensures sentence diversity by vertex-reinforcement in summaries. Experimental results on the public available dataset demonstrate the effectiveness of our framework.",2016-07-01,2-s2.0-84955246884,Information Processing and Management,Query-focused multi-document summarization using hypergraph-based ranking,"General graph random walk has been successfully applied in multi-document summarization, but it has some limitations to process documents by this way. In this paper, we propose a novel hypergraph based vertex-reinforced random walk framework for multi-document summarization. The framework first exploits the Hierarchical Dirichlet Process (HDP) topic model to learn a word-topic probability distribution in sentences. Then the hypergraph is used to capture both cluster relationship based on the word-topic probability distribution and pairwise similarity among sentences. Finally, a time-variant random walk algorithm for hypergraphs is developed to rank sentences which ensures sentence diversity by vertex-reinforcement in summaries. Experimental results on the public available dataset demonstrate the effectiveness of our framework."
329,"Social media data are increasingly perceived as alternative sources to public attitude surveys because of the volume of available data that are time-stamped and (sometimes) precisely located. Such data can be mined to provide planners, marketers and researchers with useful information about activities and opinions across time and space. However, in their raw form, textual data are still difficult to analyse coherently and Twitter streams pose particular interpretive challenges because they are restricted to just 140 characters. This paper explores the use of an unsupervised learning algorithm to classify geo-tagged Tweets from Inner London recorded during typical weekdays throughout 2013 into a small number of groups, following extensive text cleaning techniques. Our classification identifies 20 distinctive and interpretive topic groupings, which represent key types of Tweets, from describing activities or informal conversations between users, to the use of check-in applets. Our motivation is to use the classification to demonstrate how the nature of the content posted on Twitter varies according to the characteristics of places and users. Topics and attitudes expressed through Tweets are found to vary substantially across Inner London, and by time of day. Some observed variations in behaviour on Twitter can be attributed to the inferred demographic and socio-economic characteristics of users, but place and local activities can also exert a considerable influence. Overall, the classification was found to provide a valuable framework for investigating the content and coverage of Twitter usage across Inner London.",2016-07-01,2-s2.0-84962905600,"Computers, Environment and Urban Systems",The geography of Twitter topics in London,"Social media data are increasingly perceived as alternative sources to public attitude surveys because of the volume of available data that are time-stamped and (sometimes) precisely located. Such data can be mined to provide planners, marketers and researchers with useful information about activities and opinions across time and space. However, in their raw form, textual data are still difficult to analyse coherently and Twitter streams pose particular interpretive challenges because they are restricted to just 140 characters. This paper explores the use of an unsupervised learning algorithm to classify geo-tagged Tweets from Inner London recorded during typical weekdays throughout 2013 into a small number of groups, following extensive text cleaning techniques. Our classification identifies 20 distinctive and interpretive topic groupings, which represent key types of Tweets, from describing activities or informal conversations between users, to the use of check-in applets. Our motivation is to use the classification to demonstrate how the nature of the content posted on Twitter varies according to the characteristics of places and users. Topics and attitudes expressed through Tweets are found to vary substantially across Inner London, and by time of day. Some observed variations in behaviour on Twitter can be attributed to the inferred demographic and socio-economic characteristics of users, but place and local activities can also exert a considerable influence. Overall, the classification was found to provide a valuable framework for investigating the content and coverage of Twitter usage across Inner London."
330,"This paper presents a general framework for short text classification by learning vector representations of both words and hidden topics together. We refer to a large-scale external data collection named ""corpus"" which is topic consistent with short texts to be classified and then use the corpus to build topic model with Latent Dirichlet Allocation (LDA). For all the texts of the corpus and short texts, topics of words are viewed as new words and integrated into texts for data enriching. On the enriched corpus, we can learn vector representations of both words and topics. In this way, feature representations of short texts can be performed based on vectors of both words and topics for training and classification. On an open short text classification data set, learning vectors of both words and topics can significantly help reduce the classification error comparing with learning only word vectors. We also compared the proposed classification method with various baselines and experimental results justified the effectiveness of our word/topic vector representations.",2016-06-15,2-s2.0-84964318898,Knowledge-Based Systems,Improving short text classification by learning vector representations of both words and hidden topics,"This paper presents a general framework for short text classification by learning vector representations of both words and hidden topics together. We refer to a large-scale external data collection named ""corpus"" which is topic consistent with short texts to be classified and then use the corpus to build topic model with Latent Dirichlet Allocation (LDA). For all the texts of the corpus and short texts, topics of words are viewed as new words and integrated into texts for data enriching. On the enriched corpus, we can learn vector representations of both words and topics. In this way, feature representations of short texts can be performed based on vectors of both words and topics for training and classification. On an open short text classification data set, learning vectors of both words and topics can significantly help reduce the classification error comparing with learning only word vectors. We also compared the proposed classification method with various baselines and experimental results justified the effectiveness of our word/topic vector representations."
331,"Purpose – The paper addresses the problem of what drives the formation of latent discussion communities, if any, in the blogosphere: topical composition of posts or their authorship? The purpose of this paper is to contribute to the knowledge about structure of co-commenting. Design/methodology/approach – The research is based on a dataset of 17,386 full text posts written by top 2,000 LiveJournal bloggers and over 520,000 comments that result in about 4.5 million edges in the network of co-commenting, where posts are vertices. The Louvain algorithm is used to detect communities of co-commenting. Cosine similarity and topic modeling based on latent Dirichlet allocation are applied to study topical coherence within these communities. Findings – Bloggers unite into moderately manifest communities by commenting roughly the same sets of posts. The graph of co-commenting is sparse and connected by a minority of active non-top commenters. Communities are centered mainly around blog authors as opinion leaders and, to a lesser extent, around a shared topic or topics. Research limitations/implications – The research has to be replicated on other datasets with more thorough hand coding to ensure the reliability of results and to reveal average proportions of topic-centered communities. Practical implications – Knowledge about factors around which co-commenting communities emerge, in particular clustered opinion leaders that often attract such communities, can be used by policy makers in marketing and/or political campaigning when individual leadership is not enough or not applicable. Originality/value – The research contributes to the social studies of online communities. It is the first study of communities based on co-commenting that combines examination of the content of commented posts and their topics.",2016-06-06,2-s2.0-84969256807,Internet Research,Communities of co-commenting in the Russian LiveJournal and their topical coherence,"Purpose – The paper addresses the problem of what drives the formation of latent discussion communities, if any, in the blogosphere: topical composition of posts or their authorship? The purpose of this paper is to contribute to the knowledge about structure of co-commenting. Design/methodology/approach – The research is based on a dataset of 17,386 full text posts written by top 2,000 LiveJournal bloggers and over 520,000 comments that result in about 4.5 million edges in the network of co-commenting, where posts are vertices. The Louvain algorithm is used to detect communities of co-commenting. Cosine similarity and topic modeling based on latent Dirichlet allocation are applied to study topical coherence within these communities. Findings – Bloggers unite into moderately manifest communities by commenting roughly the same sets of posts. The graph of co-commenting is sparse and connected by a minority of active non-top commenters. Communities are centered mainly around blog authors as opinion leaders and, to a lesser extent, around a shared topic or topics. Research limitations/implications – The research has to be replicated on other datasets with more thorough hand coding to ensure the reliability of results and to reveal average proportions of topic-centered communities. Practical implications – Knowledge about factors around which co-commenting communities emerge, in particular clustered opinion leaders that often attract such communities, can be used by policy makers in marketing and/or political campaigning when individual leadership is not enough or not applicable. Originality/value – The research contributes to the social studies of online communities. It is the first study of communities based on co-commenting that combines examination of the content of commented posts and their topics."
332,"This article describes Russian Arctic policy agendas as they have been reflected in mainstream Russian media outlets. The research was based on modeling topic structures of three federal and three regional newspapers. Topic modeling was performed with the unsupervised LDA algorithm and complemented with hand labelling of topics. Data was collected by retrieval of relevant newspaper articles from the media database Integrum for the period 2011-2015 (N = 611); the corpus was further divided in two periods (2011-2013 and 2014-2015) to account for the potential effect of the Ukrainian crisis on agenda-setting. Both federal and regional newspapers were found to have been mostly concerned with the development of hydrocarbon resources, as coverage of this topic was the largest during both periods. However, during the second period (2014-2015), the repercussions of the Ukrainian crisis, namely the economic sanctions and securitization, gained significant attention while marginalizing other topics. The results of this analysis may serve as a foundation for application of sentiment analysis to allow an in-depth understanding of topic salience and impact on public opinion.",2016-06-01,2-s2.0-84961847756,Energy Research and Social Science,Vodka on ice? Unveiling Russian media perceptions of the Arctic,"This article describes Russian Arctic policy agendas as they have been reflected in mainstream Russian media outlets. The research was based on modeling topic structures of three federal and three regional newspapers. Topic modeling was performed with the unsupervised LDA algorithm and complemented with hand labelling of topics. Data was collected by retrieval of relevant newspaper articles from the media database Integrum for the period 2011-2015 (N = 611); the corpus was further divided in two periods (2011-2013 and 2014-2015) to account for the potential effect of the Ukrainian crisis on agenda-setting. Both federal and regional newspapers were found to have been mostly concerned with the development of hydrocarbon resources, as coverage of this topic was the largest during both periods. However, during the second period (2014-2015), the repercussions of the Ukrainian crisis, namely the economic sanctions and securitization, gained significant attention while marginalizing other topics. The results of this analysis may serve as a foundation for application of sentiment analysis to allow an in-depth understanding of topic salience and impact on public opinion."
333,"Taxi trajectories reflect human mobility over a road network. Pick-up and drop-off locations in different time periods represent origins and destinations of trips, respectively, demonstrating the spatiotemporal characteristics of human behavior. Each trip can be viewed as a displacement in the random walk model, and the distribution of extracted trips shows a distance decay effect. To identify the spatial similarity of trips at a finer scale, this paper investigates the distribution of trips through topic modeling techniques. Firstly, trip origins and trip destinations were identified from raw GPS data. Then, different trips were given semantic information, i.e., link identification numbers with a semantic enrichment process. Each taxi trajectory was composed of a series of trip destinations corresponding to the same taxi. Subsequently, each taxi trajectory was analogous to a document consisting of different words, and all taxi's trajectories could be regarded as document corpora, enabling a semantic analysis of massive trip destinations. Finally, we obtained different trip destination topics reflecting the spatial similarity and regional property of human mobility through LDA topic model training. The effectiveness of this approach was illustrated by a case study using a large dataset of taxi trajectories collected from 2 to 8 June 2014 in Wuhan, China.",2016-06-01,2-s2.0-85008957536,ISPRS International Journal of Geo-Information,Analyzing urban human mobility patterns through a thematic model at a finer scale,"Taxi trajectories reflect human mobility over a road network. Pick-up and drop-off locations in different time periods represent origins and destinations of trips, respectively, demonstrating the spatiotemporal characteristics of human behavior. Each trip can be viewed as a displacement in the random walk model, and the distribution of extracted trips shows a distance decay effect. To identify the spatial similarity of trips at a finer scale, this paper investigates the distribution of trips through topic modeling techniques. Firstly, trip origins and trip destinations were identified from raw GPS data. Then, different trips were given semantic information, i.e., link identification numbers with a semantic enrichment process. Each taxi trajectory was composed of a series of trip destinations corresponding to the same taxi. Subsequently, each taxi trajectory was analogous to a document consisting of different words, and all taxi's trajectories could be regarded as document corpora, enabling a semantic analysis of massive trip destinations. Finally, we obtained different trip destination topics reflecting the spatial similarity and regional property of human mobility through LDA topic model training. The effectiveness of this approach was illustrated by a case study using a large dataset of taxi trajectories collected from 2 to 8 June 2014 in Wuhan, China."
334,"Increasingly, social media data are linked to locations through embedded GPS coordinates. Many local governments are showing interest in the potential to repurpose these firsthand geo-data to gauge spatial and temporal dynamics of public opinions in ways that complement information collected through traditional public engagement methods. Using these geosocial data is not without challenges since they are usually unstructured, vary in quality, and often require considerable effort to extract information that is relevant to local governments' needs from large data volumes. Understanding local relevance requires development of both data processing methods and their use in empirical studies. This paper addresses this latter need through a case study that demonstrates how spatially-referenced Twitter data can shed light on citizens' transportation and planning concerns. A web-based toolkit that integrates text processing methods is used to model Twitter data collected for the Region of Waterloo (Ontario, Canada) between March 2014 and July 2015 and assess citizens' concerns related to the planning and construction of a new light rail transit line. The study suggests that geosocial media can help identify geographies of public perceptions concerning public facilities and services and have potential to complement other methods of gauging public sentiment.",2016-06-01,2-s2.0-84971349380,ISPRS International Journal of Geo-Information,Understanding public opinions from geosocial media,"Increasingly, social media data are linked to locations through embedded GPS coordinates. Many local governments are showing interest in the potential to repurpose these firsthand geo-data to gauge spatial and temporal dynamics of public opinions in ways that complement information collected through traditional public engagement methods. Using these geosocial data is not without challenges since they are usually unstructured, vary in quality, and often require considerable effort to extract information that is relevant to local governments' needs from large data volumes. Understanding local relevance requires development of both data processing methods and their use in empirical studies. This paper addresses this latter need through a case study that demonstrates how spatially-referenced Twitter data can shed light on citizens' transportation and planning concerns. A web-based toolkit that integrates text processing methods is used to model Twitter data collected for the Region of Waterloo (Ontario, Canada) between March 2014 and July 2015 and assess citizens' concerns related to the planning and construction of a new light rail transit line. The study suggests that geosocial media can help identify geographies of public perceptions concerning public facilities and services and have potential to complement other methods of gauging public sentiment."
335,"Explicit Semantic Analysis (ESA) is a knowledge-based method which builds the semantic representation of the words depending on the textual description of the concepts in the certain knowledge source. Due to its simplicity and success, ESA has received wide attention from researchers in the computational linguistics and information retrieval. However, the representation vectors formed by ESA method are generally very excessive, high dimensional, and may contain many redundant concepts. In this paper, we introduce a reduced semantic representation method that constructs the semantic interpretation of the words as the vectors over the latent topics from the original ESA representation vectors. For modeling the latent topics, the Latent Dirichlet Allocation (LDA) is adapted to the ESA vectors for extracting the topics as the probability distributions over the concepts rather than the words in the traditional model. The proposed method is applied to the wide knowledge sources used in the computational semantic analysis: WordNet and Wikipedia. For evaluation, we use the proposed method in two natural language processing tasks: measuring the semantic relatedness between words/texts and text clustering. The experimental results indicate that the proposed method overcomes the limitations of the representation of the ESA method.",2016-05-15,2-s2.0-84977992467,Knowledge-Based Systems,Reducing explicit semantic representation vectors using Latent Dirichlet Allocation,"Explicit Semantic Analysis (ESA) is a knowledge-based method which builds the semantic representation of the words depending on the textual description of the concepts in the certain knowledge source. Due to its simplicity and success, ESA has received wide attention from researchers in the computational linguistics and information retrieval. However, the representation vectors formed by ESA method are generally very excessive, high dimensional, and may contain many redundant concepts. In this paper, we introduce a reduced semantic representation method that constructs the semantic interpretation of the words as the vectors over the latent topics from the original ESA representation vectors. For modeling the latent topics, the Latent Dirichlet Allocation (LDA) is adapted to the ESA vectors for extracting the topics as the probability distributions over the concepts rather than the words in the traditional model. The proposed method is applied to the wide knowledge sources used in the computational semantic analysis: WordNet and Wikipedia. For evaluation, we use the proposed method in two natural language processing tasks: measuring the semantic relatedness between words/texts and text clustering. The experimental results indicate that the proposed method overcomes the limitations of the representation of the ESA method."
336,"The explosion of online user-generated content (UGC) and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e-government context. To better understand semantic searching of public comments on an online platform for citizens' opinions about urban affairs issues, this paper proposed an approach based on the latent Dirichlet allocation (LDA), a probabilistic topic modeling method, and designed a practical system to provide users-municipal administrators of B-city-with satisfying searching results and the longitudinal changing curves of related topics. The system is developed to respond to actual demand from B-city's local government, and the user evaluation experiment results show that a system based on the LDA method could provide information that is more helpful to relevant staff members. Municipal administrators could better understand citizens' online comments based on the proposed semantic search approach and could improve their decision-making process by considering public opinions.",2016-05-01,2-s2.0-84960448295,Information Processing and Management,Semantic search for public opinions on urban affairs: A probabilistic topic modeling-based approach,"The explosion of online user-generated content (UGC) and the development of big data analysis provide a new opportunity and challenge to understand and respond to public opinions in the G2C e-government context. To better understand semantic searching of public comments on an online platform for citizens' opinions about urban affairs issues, this paper proposed an approach based on the latent Dirichlet allocation (LDA), a probabilistic topic modeling method, and designed a practical system to provide users-municipal administrators of B-city-with satisfying searching results and the longitudinal changing curves of related topics. The system is developed to respond to actual demand from B-city's local government, and the user evaluation experiment results show that a system based on the LDA method could provide information that is more helpful to relevant staff members. Municipal administrators could better understand citizens' online comments based on the proposed semantic search approach and could improve their decision-making process by considering public opinions."
337,"We propose a method to analyze public opinion about political issues online by automatically detecting polarity in Twitter data. Previous studies have focused on the polarity classification of individual tweets. However, to understand the direction of public opinion on a political issue, it is important to analyze the degree of polarity on the major topics at the center of the discussion in addition to the individual tweets. The first stage of the proposed method detects polarity in tweets using the Lasso and Ridge models of shrinkage regression. The models are beneficial in that the regression results provide sentiment scores for the terms that appear in tweets. The second stage identifies the major topics via a latent Dirichlet analysis (LDA) topic model and estimates the degree of polarity on the LDA topics using term sentiment scores. To the best of our knowledge, our study is the first to predict the polarities of public opinion on topics in this manner. We conducted an experiment on a mayoral election in Seoul, South Korea and compared the total detection accuracy of the regression models with five support vector machine (SVM) models with different numbers of input terms selected by a feature selection algorithm. The results indicated that the performance of the Ridge model was approximately 7% higher on average than that of the SVM models. Additionally, the degree of polarity on the LDA topics estimated using the proposed method was compared with actual public opinion responses. The results showed that the polarity detection accuracy of the Lasso model was 83%, indicating that the proposed method was valid in most cases.",2016-05-01,2-s2.0-84965180016,Journal of Informetrics,Opinion polarity detection in Twitter data combining shrinkage regression and topic modeling,"We propose a method to analyze public opinion about political issues online by automatically detecting polarity in Twitter data. Previous studies have focused on the polarity classification of individual tweets. However, to understand the direction of public opinion on a political issue, it is important to analyze the degree of polarity on the major topics at the center of the discussion in addition to the individual tweets. The first stage of the proposed method detects polarity in tweets using the Lasso and Ridge models of shrinkage regression. The models are beneficial in that the regression results provide sentiment scores for the terms that appear in tweets. The second stage identifies the major topics via a latent Dirichlet analysis (LDA) topic model and estimates the degree of polarity on the LDA topics using term sentiment scores. To the best of our knowledge, our study is the first to predict the polarities of public opinion on topics in this manner. We conducted an experiment on a mayoral election in Seoul, South Korea and compared the total detection accuracy of the regression models with five support vector machine (SVM) models with different numbers of input terms selected by a feature selection algorithm. The results indicated that the performance of the Ridge model was approximately 7% higher on average than that of the SVM models. Additionally, the degree of polarity on the LDA topics estimated using the proposed method was compared with actual public opinion responses. The results showed that the polarity detection accuracy of the Lasso model was 83%, indicating that the proposed method was valid in most cases."
338,"Electronic Medical Record (EMR) has established itself as a valuable resource for large scale analysis of health data. A hospital EMR dataset typically consists of medical records of hospitalized patients. A medical record contains diagnostic information (diagnosis codes), procedures performed (procedure codes) and admission details. Traditional topic models, such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP), can be employed to discover disease topics from EMR data by treating patients as documents and diagnosis codes as words. This topic modeling helps to understand the constitution of patient diseases and offers a tool for better planning of treatment. In this paper, we propose a novel and flexible hierarchical Bayesian nonparametric model, the word distance dependent Chinese restaurant franchise (wddCRF), which incorporates word-to-word distances to discover semantically-coherent disease topics. We are motivated by the fact that diagnosis codes are connected in the form of ICD-10 tree structure which presents semantic relationships between codes. We exploit a decay function to incorporate distances between words at the bottom level of wddCRF. Efficient inference is derived for the wddCRF by using MCMC technique. Furthermore, since procedure codes are often correlated with diagnosis codes, we develop the correspondence wddCRF (Corr-wddCRF) to explore conditional relationships of procedure codes for a given disease pattern. Efficient collapsed Gibbs sampling is derived for the Corr-wddCRF. We evaluate the proposed models on two real-world medical datasets - PolyVascular disease and Acute Myocardial Infarction disease. We demonstrate that the Corr-wddCRF model discovers more coherent topics than the Corr-HDP. We also use disease topic proportions as new features and show that using features from the Corr-wddCRF outperforms the baselines on 14-days readmission prediction. Beside these, the prediction for procedure codes based on the Corr-wddCRF also shows considerable accuracy.",2016-05-01,2-s2.0-84961208266,Knowledge-Based Systems,Hierarchical Bayesian nonparametric models for knowledge discovery from electronic medical records,"Electronic Medical Record (EMR) has established itself as a valuable resource for large scale analysis of health data. A hospital EMR dataset typically consists of medical records of hospitalized patients. A medical record contains diagnostic information (diagnosis codes), procedures performed (procedure codes) and admission details. Traditional topic models, such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP), can be employed to discover disease topics from EMR data by treating patients as documents and diagnosis codes as words. This topic modeling helps to understand the constitution of patient diseases and offers a tool for better planning of treatment. In this paper, we propose a novel and flexible hierarchical Bayesian nonparametric model, the word distance dependent Chinese restaurant franchise (wddCRF), which incorporates word-to-word distances to discover semantically-coherent disease topics. We are motivated by the fact that diagnosis codes are connected in the form of ICD-10 tree structure which presents semantic relationships between codes. We exploit a decay function to incorporate distances between words at the bottom level of wddCRF. Efficient inference is derived for the wddCRF by using MCMC technique. Furthermore, since procedure codes are often correlated with diagnosis codes, we develop the correspondence wddCRF (Corr-wddCRF) to explore conditional relationships of procedure codes for a given disease pattern. Efficient collapsed Gibbs sampling is derived for the Corr-wddCRF. We evaluate the proposed models on two real-world medical datasets - PolyVascular disease and Acute Myocardial Infarction disease. We demonstrate that the Corr-wddCRF model discovers more coherent topics than the Corr-HDP. We also use disease topic proportions as new features and show that using features from the Corr-wddCRF outperforms the baselines on 14-days readmission prediction. Beside these, the prediction for procedure codes based on the Corr-wddCRF also shows considerable accuracy."
339,"This study proposes a standardised open framework to automatically generate and label discussion topics from Massive Open Online Courses (MOOCs). The proposed framework expects to overcome the issues experienced by MOOC participants and teaching staff in locating and navigating their information needs effectively. We analysed two MOOCs - Machine Learning and Statistics: Making Sense of Data offered during 2013 and obtained statistically significant results for automated topic labeling. However, more experiments with additional MOOCs from different MOOC platforms are necessary to generalise our findings.",2016-04-25,2-s2.0-84969916317,L@S 2016 - Proceedings of the 3rd 2016 ACM Conference on Learning at Scale,A framework for topic generation and labeling from MOOC discussions,"This study proposes a standardised open framework to automatically generate and label discussion topics from Massive Open Online Courses (MOOCs). The proposed framework expects to overcome the issues experienced by MOOC participants and teaching staff in locating and navigating their information needs effectively. We analysed two MOOCs - Machine Learning and Statistics: Making Sense of Data offered during 2013 and obtained statistically significant results for automated topic labeling. However, more experiments with additional MOOCs from different MOOC platforms are necessary to generalise our findings."
340,"In this study, we develop methods for computationally measuring the degree to which students engage in MOOC forums with other students holding different political beliefs. We examine a case study of a single MOOC about education policy, Saving Schools, where we obtain measures of student education policy preferences that correlate with political ideology. Contrary to assertions that online spaces often become echo chambers or ideological silos, we find that students in this case hold diverse political beliefs, participate equitably in forum discussions, directly engage (through replies and upvotes) with students holding opposing beliefs, and converge on a shared language rather than talking past one another. Research that focuses on the civic mission of MOOCs helps ensure that open online learning engages the same breadth of purposes that higher education aspires to serve.",2016-04-25,2-s2.0-84969932175,L@S 2016 - Proceedings of the 3rd 2016 ACM Conference on Learning at Scale,The civic mission of MOOCs: Measuring engagement across political differences in forums,"In this study, we develop methods for computationally measuring the degree to which students engage in MOOC forums with other students holding different political beliefs. We examine a case study of a single MOOC about education policy, Saving Schools, where we obtain measures of student education policy preferences that correlate with political ideology. Contrary to assertions that online spaces often become echo chambers or ideological silos, we find that students in this case hold diverse political beliefs, participate equitably in forum discussions, directly engage (through replies and upvotes) with students holding opposing beliefs, and converge on a shared language rather than talking past one another. Research that focuses on the civic mission of MOOCs helps ensure that open online learning engages the same breadth of purposes that higher education aspires to serve."
341,"Social media systems provide ever-growing huge volumes of information for dissemination and communication among communities of users, while recommender systems aim to mitigate information overload by filtering and providing users the most attractive and relevant items from information-sea. This paper aims at providing compound recommendation engine for social media systems, and focuses on exploiting multi-sourced information (e.g. social networks, item contents and user feedbacks) to predict the ratings of users to items and make recommendations. For this, we suppose the users' decisions on adopting item are affected both by their tastes and the favors of trusted friends, and extend Collaborative Topic Regression to jointly incorporates social trust ensemble, topic modeling and probabilistic matrix factorization. We propose corresponding approaches to learning the latent factors both of users and items, as well as additional parameters to be estimated. Empirical experiments on Lastfm and Delicious datasets show that our model is better and more robust than the state-of-the-art methods on making recommendations in term of accuracy. Experiments results also reveal some useful findings to enlighten the development of recommender systems in social media.",2016-04-01,2-s2.0-84956627416,Knowledge-Based Systems,Collaborative Topic Regression with social trust ensemble for recommendation in social media systems,"Social media systems provide ever-growing huge volumes of information for dissemination and communication among communities of users, while recommender systems aim to mitigate information overload by filtering and providing users the most attractive and relevant items from information-sea. This paper aims at providing compound recommendation engine for social media systems, and focuses on exploiting multi-sourced information (e.g. social networks, item contents and user feedbacks) to predict the ratings of users to items and make recommendations. For this, we suppose the users' decisions on adopting item are affected both by their tastes and the favors of trusted friends, and extend Collaborative Topic Regression to jointly incorporates social trust ensemble, topic modeling and probabilistic matrix factorization. We propose corresponding approaches to learning the latent factors both of users and items, as well as additional parameters to be estimated. Empirical experiments on Lastfm and Delicious datasets show that our model is better and more robust than the state-of-the-art methods on making recommendations in term of accuracy. Experiments results also reveal some useful findings to enlighten the development of recommender systems in social media."
342,"Understanding current technological changes is the basis for better forecasting of technological changes. Because technology is path dependent, monitoring past and current trends of technological development helps managers and decision makers to identify probable future technologies in order to prevent organizational failure. This study suggests a method based on patent-development paths, k-core analysis and topic modeling of past and current trends of technological development to identify technologies that have the potential to become disruptive technologies. We find that within the photovoltaic industry, thin-film technology is likely to replace the dominant technology, namely crystalline silicon. In addition, we identity the hidden technologies, namely multi-junction, dye-sensitized and concentration technologies, that have the potential to become disruptive technologies within the three main technologies of the photovoltaic industry.",2016-03-01,2-s2.0-84951842753,Technological Forecasting and Social Change,Identification and monitoring of possible disruptive technologies by patent-development paths and topic modeling,"Understanding current technological changes is the basis for better forecasting of technological changes. Because technology is path dependent, monitoring past and current trends of technological development helps managers and decision makers to identify probable future technologies in order to prevent organizational failure. This study suggests a method based on patent-development paths, k-core analysis and topic modeling of past and current trends of technological development to identify technologies that have the potential to become disruptive technologies. We find that within the photovoltaic industry, thin-film technology is likely to replace the dominant technology, namely crystalline silicon. In addition, we identity the hidden technologies, namely multi-junction, dye-sensitized and concentration technologies, that have the potential to become disruptive technologies within the three main technologies of the photovoltaic industry."
343,"Social movement frames are dynamic, shifting and embedded within an already existent cultural milieu-a milieu that affects mobilization opportunities. In this article, we invoke the concept of the ""cultural clearinghouse"" to tackle how broader cultural structures translate to frames or influence frame resonance. Our illustrative case, the Nobel Peace Prize, along with our use of topic modeling, a computational technique that identifies commonalities between texts, offer an important methodological advance for social movement scholars interested in culture, frame formation and resonance, and dynamic approaches to social movement discourse. Our findings show how peace discourse-as represented by Peace Prize acceptance speeches-increasingly has become embedded within broader cultural emphases on globalization and neoliberalism, versus earlier Christian and global institutional schemas. We conclude by discussing the usefulness of our conceptual and methodological advance for movement scholars with special attention to the coupling of new computational techniques and more traditional methods.",2016-03-01,2-s2.0-84979879763,Mobilization,"Oracles of peace: Topic modeling, cultural opportunity, and the Nobel Peace Prize, 1902-2012","Social movement frames are dynamic, shifting and embedded within an already existent cultural milieu-a milieu that affects mobilization opportunities. In this article, we invoke the concept of the ""cultural clearinghouse"" to tackle how broader cultural structures translate to frames or influence frame resonance. Our illustrative case, the Nobel Peace Prize, along with our use of topic modeling, a computational technique that identifies commonalities between texts, offer an important methodological advance for social movement scholars interested in culture, frame formation and resonance, and dynamic approaches to social movement discourse. Our findings show how peace discourse-as represented by Peace Prize acceptance speeches-increasingly has become embedded within broader cultural emphases on globalization and neoliberalism, versus earlier Christian and global institutional schemas. We conclude by discussing the usefulness of our conceptual and methodological advance for movement scholars with special attention to the coupling of new computational techniques and more traditional methods."
344,"Due to the increasing popularity of social media platforms, the amount of messages (posts) related to public events, especially posts sharing multimedia content, is steadily increasing. Sharing images can contribute to a rich and live coverage of the event. Yet, despite the value and interestingness of some posts, there is a lot of spam and redundancy, which makes it challenging to select the most important and characteristic posts for the event. In this work, we describe MGraph, a summarization framework that, given a set of social media posts about an event, selects a subset of shared images, simultaneously maximizing their relevance and minimizing their visual redundancy. MGraph employs a topic modelling technique based on different modalities to capture the relevance of posts to event topics, and a graph-based ranking algorithm to produce a diverse ranking of the selected high-relevance images. A user-centred evaluation on a dataset comprising a variety of real-world events demonstrates that MGraph considerably outperforms a number of state-of-the-art summarization algorithms in terms of relevance and diversity (25 and 7 % improvement respectively).",2016-03-01,2-s2.0-85013974761,International Journal of Multimedia Information Retrieval,MGraph: multimodal event summarization in social media using topic models and graph-based ranking,"Due to the increasing popularity of social media platforms, the amount of messages (posts) related to public events, especially posts sharing multimedia content, is steadily increasing. Sharing images can contribute to a rich and live coverage of the event. Yet, despite the value and interestingness of some posts, there is a lot of spam and redundancy, which makes it challenging to select the most important and characteristic posts for the event. In this work, we describe MGraph, a summarization framework that, given a set of social media posts about an event, selects a subset of shared images, simultaneously maximizing their relevance and minimizing their visual redundancy. MGraph employs a topic modelling technique based on different modalities to capture the relevance of posts to event topics, and a graph-based ranking algorithm to produce a diverse ranking of the selected high-relevance images. A user-centred evaluation on a dataset comprising a variety of real-world events demonstrates that MGraph considerably outperforms a number of state-of-the-art summarization algorithms in terms of relevance and diversity (25 and 7 % improvement respectively)."
345,"Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-theart aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language.",2016-03-01,2-s2.0-84962815428,IEEE/ACM Transactions on Audio Speech and Language Processing,An unsupervised cross-lingual topic model framework for sentiment classification,"Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-theart aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language."
346,"Understanding user intent is critical as we move towards provisioning next generation services. Towards this goal, we have developed a mobile in-device framework that mines user consumed content for inferring custom user interest. We build custom filters from user content that has wide ranging applications including filtering content automatically from applications employing web based data backend. Our framework is based on latent extraction of content based on a hybrid supervised and unsupervised topic modeling process. The extracted filters are coupled to popular web based services for search and retrieval of content backed by a unique UX applicable for mobile systems. Our trial results show a subjective satisfaction score of over 80% for filter results along with 94% precision score for supervised latent topic-based content inference.",2016-02-16,2-s2.0-84966577789,"Proceedings - 2015 International Conference on Science in Information Technology: Big Data Spectrum for Future Information Economy, ICSITech 2015",Enabling custom application content through semantic web filters,"Understanding user intent is critical as we move towards provisioning next generation services. Towards this goal, we have developed a mobile in-device framework that mines user consumed content for inferring custom user interest. We build custom filters from user content that has wide ranging applications including filtering content automatically from applications employing web based data backend. Our framework is based on latent extraction of content based on a hybrid supervised and unsupervised topic modeling process. The extracted filters are coupled to popular web based services for search and retrieval of content backed by a unique UX applicable for mobile systems. Our trial results show a subjective satisfaction score of over 80% for filter results along with 94% precision score for supervised latent topic-based content inference."
347,"Topic detection as a tool to detect topics from online media attracts much attention. Generally, a topic is characterized by a set of informative keywords/terms. Traditional approaches are usually based on various topic models, such as Latent Dirichlet Allocation (LDA). They cluster terms into a topic by mining semantic relations between terms. However, co-occurrence relations across the document are commonly neglected, which leads to the detection of incomplete information. Furthermore, the inability to discover latent co-occurrence relations via the context or other bridge terms prevents the important but rare topics from being detected. To tackle this issue, we propose a hybrid relations analysis approach to integrate semantic relations and co-occurrence relations for topic detection. Specifically, the approach fuses multiple relations into a term graph and detects topics from the graph using a graph analytical method. It can not only detect topics more effectively by combing mutually complementary relations, but also mine important rare topics by leveraging latent co-occurrence relations. Extensive experiments demonstrate the advantage of our approach over several benchmarks.",2016-02-01,2-s2.0-84955515486,Knowledge-Based Systems,A hybrid term-term relations analysis approach for topic detection,"Topic detection as a tool to detect topics from online media attracts much attention. Generally, a topic is characterized by a set of informative keywords/terms. Traditional approaches are usually based on various topic models, such as Latent Dirichlet Allocation (LDA). They cluster terms into a topic by mining semantic relations between terms. However, co-occurrence relations across the document are commonly neglected, which leads to the detection of incomplete information. Furthermore, the inability to discover latent co-occurrence relations via the context or other bridge terms prevents the important but rare topics from being detected. To tackle this issue, we propose a hybrid relations analysis approach to integrate semantic relations and co-occurrence relations for topic detection. Specifically, the approach fuses multiple relations into a term graph and detects topics from the graph using a graph analytical method. It can not only detect topics more effectively by combing mutually complementary relations, but also mine important rare topics by leveraging latent co-occurrence relations. Extensive experiments demonstrate the advantage of our approach over several benchmarks."
348,"The field of development studies analyses causes and potential solutions for global poverty and inequality. Since the Second World War there have been major changes in theories about the root causes of global poverty and the strategies necessary to tackle it. This article views the history of development studies from the perspective of the role of economics during 1975- 2014. A topic modelling analysis of published journal articles from Scopus suggests that there has been a relative decline in research that focuses on economics, and particularly for research concerned with firms and growth. In parallel, the analysis suggests that there has been a relatively slow increase in interest in the environment, a lack of scholarly interest in BRICS and dependency theory, and a relatively homogeneous treatment of gender issues.",2016-02-01,2-s2.0-84958637549,Profesional de la Informacion,Development studies research 1975-2014 in academic journal articles : the end of economics?,"The field of development studies analyses causes and potential solutions for global poverty and inequality. Since the Second World War there have been major changes in theories about the root causes of global poverty and the strategies necessary to tackle it. This article views the history of development studies from the perspective of the role of economics during 1975- 2014. A topic modelling analysis of published journal articles from Scopus suggests that there has been a relative decline in research that focuses on economics, and particularly for research concerned with firms and growth. In parallel, the analysis suggests that there has been a relatively slow increase in interest in the environment, a lack of scholarly interest in BRICS and dependency theory, and a relatively homogeneous treatment of gender issues."
349,"System for the global patent space automatic positioning of application materials to obtain a patent for an invention based on statistical and semantic approaches ""E-Patent Examiner"" - is a system for expert decision making when checking the inventive level, novelty and industrial applicability of the invention. System implementation will shorten the substantive examination. All of the current software performs the search of relevant application documents according to the expert query. ""E-Patent Examiner"" automatically analyzes the original text of the application. The approach proposed allows avoiding very large volumes of patent databases reformatting and gaining access to these patents for examining and the search for new technical solutions in areas where there are no patents, or they are on the pioneer level.",2016-01-20,2-s2.0-84963829809,"IISA 2015 - 6th International Conference on Information, Intelligence, Systems and Applications",E-patent examiner: Two-steps approach for patents prior-art retrieval,"System for the global patent space automatic positioning of application materials to obtain a patent for an invention based on statistical and semantic approaches ""E-Patent Examiner"" - is a system for expert decision making when checking the inventive level, novelty and industrial applicability of the invention. System implementation will shorten the substantive examination. All of the current software performs the search of relevant application documents according to the expert query. ""E-Patent Examiner"" automatically analyzes the original text of the application. The approach proposed allows avoiding very large volumes of patent databases reformatting and gaining access to these patents for examining and the search for new technical solutions in areas where there are no patents, or they are on the pioneer level."
350,"This paper addresses the problem of tracking the time evolution of communities within co-authorship networks. We consider an evolutionary clustering approach which adapts the statistical framework of shrinkage estimation to obtain a smoothed version of the overall affinity matrix as the optimal weighted average between the matrices of past and current affinities. Moreover, the current affinity matrix at each time step is formed as the optimal mixing between the raw structural relationships between authors coupled with the semantic similarity of their published works, captured by performing LDA-based topic modeling on the corresponding corpora of abstracts. The proposed parametric weighting scheme is simultaneously optimized on a real co-authorship dataset emerging from the network of participants in the ICMB conferences, held from 2002 to 2013. Finally, community detection at each time step is conducted by employing spectral clustering on the estimated overall weight matrix. The obtained results justify that our approach provides a clearer revelation of the inherent authors' communities dynamics when compared against incremental cluster formations that rely exclusively on the structural information of the co-authorship network.",2016-01-20,2-s2.0-84963804013,"IISA 2015 - 6th International Conference on Information, Intelligence, Systems and Applications",Tracking the evolution of communities in co-authorship networks: A semantically aware approach,"This paper addresses the problem of tracking the time evolution of communities within co-authorship networks. We consider an evolutionary clustering approach which adapts the statistical framework of shrinkage estimation to obtain a smoothed version of the overall affinity matrix as the optimal weighted average between the matrices of past and current affinities. Moreover, the current affinity matrix at each time step is formed as the optimal mixing between the raw structural relationships between authors coupled with the semantic similarity of their published works, captured by performing LDA-based topic modeling on the corresponding corpora of abstracts. The proposed parametric weighting scheme is simultaneously optimized on a real co-authorship dataset emerging from the network of participants in the ICMB conferences, held from 2002 to 2013. Finally, community detection at each time step is conducted by employing spectral clustering on the estimated overall weight matrix. The obtained results justify that our approach provides a clearer revelation of the inherent authors' communities dynamics when compared against incremental cluster formations that rely exclusively on the structural information of the co-authorship network."
352,"When the above article was first published online, the text in Figure 1 was corrupt resulting in words within the figure overlapping. The figure has now been corrected.",2016-01-02,2-s2.0-84977564478,Digital Journalism,"Corrigendum to: “Quantitative Analysis of Large Amounts of Journalistic Texts Using Topic Modelling.” (Digital Journalism, (2015), 10.1080/21670811.2015.1093271)","When the above article was first published online, the text in Figure 1 was corrupt resulting in words within the figure overlapping. The figure has now been corrected."
353,"Abstract: This study explores the nature and promise of citizen engagement in Twitter during calamities, specifically in the context of typhoon Yolanda. Through topic modeling and content analysis, the article explores the “acts of civic engagement” expressed in disaster tweets and how the character of the tweets changed over a five-month period. The article adopts the actualizing citizenship model which offers an alternative understanding of citizenship as performative and which is relevant for the analysis of social media engagement by citizens. The article concludes with a reflection of the nature and typology of Twitter-mediated communication during calamities, highlighting the active engagement of celebrities in disaster tweets, and analysis of whether and how these can be construed as acts of civic engagement.",2016-01-02,2-s2.0-84959191606,Philippine Political Science Journal,Social media and civic engagement during calamities: the case of Twitter use during typhoon Yolanda,"Abstract: This study explores the nature and promise of citizen engagement in Twitter during calamities, specifically in the context of typhoon Yolanda. Through topic modeling and content analysis, the article explores the “acts of civic engagement” expressed in disaster tweets and how the character of the tweets changed over a five-month period. The article adopts the actualizing citizenship model which offers an alternative understanding of citizenship as performative and which is relevant for the analysis of social media engagement by citizens. The article concludes with a reflection of the nature and typology of Twitter-mediated communication during calamities, highlighting the active engagement of celebrities in disaster tweets, and analysis of whether and how these can be construed as acts of civic engagement."
354,"User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses' strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers' minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.",2016-01-01,2-s2.0-84952982692,Business Horizons,Uncovering the message from the mess of big data,"User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses' strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers' minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers."
355,"This article presents an empirical study that investigated and compared two ""big data"" text analysis methods: dictionary-based analysis, perhaps the most popular automated analysis approach in social science research, and unsupervised topic modeling (i.e., Latent Dirichlet Allocation [LDA] analysis), one of the most widely used algorithms in the field of computer science and engineering. By applying two ""big data"" methods to make sense of the same dataset - 77 million tweets about the 2012 U.S. presidential election - the study provides a starting point for scholars to evaluate the efficacy and validity of different computer-assisted methods for conducting journalism and mass communication research, especially in the area of political communication.",2016-01-01,2-s2.0-84978910441,Journalism and Mass Communication Quarterly,Big social data analytics in journalism and mass communication: Comparing dictionary-based text analysis and unsupervised topic modeling,"This article presents an empirical study that investigated and compared two ""big data"" text analysis methods: dictionary-based analysis, perhaps the most popular automated analysis approach in social science research, and unsupervised topic modeling (i.e., Latent Dirichlet Allocation [LDA] analysis), one of the most widely used algorithms in the field of computer science and engineering. By applying two ""big data"" methods to make sense of the same dataset - 77 million tweets about the 2012 U.S. presidential election - the study provides a starting point for scholars to evaluate the efficacy and validity of different computer-assisted methods for conducting journalism and mass communication research, especially in the area of political communication."
356,"When the above article was first published online, the text in Figure 1 was corrupt resulting in words within the figure overlapping. The figure has now been corrected.",2016-01-01,2-s2.0-84977578187,Digital Journalism,"Erratum: Quantitative analysis of large amounts of journalistic texts using topic modelling. (Digital Journalism, (2015) 10.1080/21670811.2015.1093271)","When the above article was first published online, the text in Figure 1 was corrupt resulting in words within the figure overlapping. The figure has now been corrected."
357,"This article extends the methodological and empirical scope of public administration research and applies two Bayesian-inspired computational research methods - unsupervised latent trait scaling and topic modeling. The article uses these methods to examine government budgeting in thirteen Western countries, utilizing budgetary legislation as the research material. The empirical research question is: How are legal system traditions present in the words of texts of legislation on government budgeting? According to the results, at one end of the latent trait scale we find overseas inheritors of Britain's common law legal system, Canada, Australia, and New Zealand, and at the other end two representatives of civil law of the Napoleonic subtype, Italy and Spain. The other countries situate themselves in intermediate positions between the extremes. The topic modeling indicates three reasonably homogeneous groups of countries: The three overseas inheritors of common law and more weakly the United Kingdom, three countries representing the Napoleonic heritage, and, more weakly German-speaking and Nordic countries. In general, the article and its results emphasize the opportunities to extend Bayesian-inspired research in this research field.",2016-01-01,2-s2.0-84992176632,Halduskultuur,Digital Public-Administration research drawing from Bayesian inspiration: Latent trait scaling and topic modeling examination of budgetary legislation in thirteen countries,"This article extends the methodological and empirical scope of public administration research and applies two Bayesian-inspired computational research methods - unsupervised latent trait scaling and topic modeling. The article uses these methods to examine government budgeting in thirteen Western countries, utilizing budgetary legislation as the research material. The empirical research question is: How are legal system traditions present in the words of texts of legislation on government budgeting? According to the results, at one end of the latent trait scale we find overseas inheritors of Britain's common law legal system, Canada, Australia, and New Zealand, and at the other end two representatives of civil law of the Napoleonic subtype, Italy and Spain. The other countries situate themselves in intermediate positions between the extremes. The topic modeling indicates three reasonably homogeneous groups of countries: The three overseas inheritors of common law and more weakly the United Kingdom, three countries representing the Napoleonic heritage, and, more weakly German-speaking and Nordic countries. In general, the article and its results emphasize the opportunities to extend Bayesian-inspired research in this research field."
358,"Web corpora are often constructed automatically, and their contents are therefore often not well understood. One technique for assessing the composition of such a web corpus is to empirically measure its similarity to a reference corpus whose composition is known. In this paper we evaluate a number of measures of corpus similarity, including a method based on topic modelling which has not been previously evaluated for this task. To evaluate these methods we use known-similarity corpora that have been previously used for this purpose, as well as a number of newly-constructed known-similarity corpora targeting differences in genre, topic, time, and region. Our findings indicate that, overall, the topic modelling approach did not improve on a chi-square method that had previously been found to work well for measuring corpus similarity.",2016-01-01,2-s2.0-85037091213,"Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016",Evaluating a topic modelling approach to measuring corpus similarity,"Web corpora are often constructed automatically, and their contents are therefore often not well understood. One technique for assessing the composition of such a web corpus is to empirically measure its similarity to a reference corpus whose composition is known. In this paper we evaluate a number of measures of corpus similarity, including a method based on topic modelling which has not been previously evaluated for this task. To evaluate these methods we use known-similarity corpora that have been previously used for this purpose, as well as a number of newly-constructed known-similarity corpora targeting differences in genre, topic, time, and region. Our findings indicate that, overall, the topic modelling approach did not improve on a chi-square method that had previously been found to work well for measuring corpus similarity."
359,"The huge collections of news content which have become available through digital technologies both enable and warrant scientific inquiry, challenging journalism scholars to analyse unprecedented amounts of texts. We propose Latent Dirichlet Allocation (LDA) topic modelling as a tool to face this challenge. LDA is a cutting edge technique for content analysis, designed to automatically organize large archives of documents based on latent topics, measured as patterns of word (co-)occurrence. We explain how this technique works, how different choices by the researcher affect the results and how the results can be meaningfully interpreted. To demonstrate its usefulness for journalism research, we conducted a case study of the New York Times coverage of nuclear technology from 1945 to the present, partially replicating a study by Gamson and Modigliani. This shows that LDA is a useful tool for analysing trends and patterns in news content in large digital news archives relatively quickly.",2016-01-01,2-s2.0-84977493522,Digital Journalism,Quantitative analysis of large amounts of journalistic texts using topic modelling,"The huge collections of news content which have become available through digital technologies both enable and warrant scientific inquiry, challenging journalism scholars to analyse unprecedented amounts of texts. We propose Latent Dirichlet Allocation (LDA) topic modelling as a tool to face this challenge. LDA is a cutting edge technique for content analysis, designed to automatically organize large archives of documents based on latent topics, measured as patterns of word (co-)occurrence. We explain how this technique works, how different choices by the researcher affect the results and how the results can be meaningfully interpreted. To demonstrate its usefulness for journalism research, we conducted a case study of the New York Times coverage of nuclear technology from 1945 to the present, partially replicating a study by Gamson and Modigliani. This shows that LDA is a useful tool for analysing trends and patterns in news content in large digital news archives relatively quickly."
360,"The National Transportation Safety Board in the United States and the Transportation Safety Board of Canada publish reports about major railroad accidents. The text from these accident reports were analyzed using the text mining techniques of probabilistic topic modeling and k-means clustering to identify the recurring themes in major railroad accidents. The output from these analyses indicates that the railroad accidents can be successfully grouped into different topics. The output also suggests that recurring accident types are track defects, wheel defects, grade crossing accidents, and switching accidents. A major difference between the Canadian and U.S. reports is the finding that accidents related to bridges are found to be more prominent in the Canadian reports.",2016-01-01,2-s2.0-84978924535,"2016 Joint Rail Conference, JRC 2016",Text mining analysis of railroad accident investigation reports,"The National Transportation Safety Board in the United States and the Transportation Safety Board of Canada publish reports about major railroad accidents. The text from these accident reports were analyzed using the text mining techniques of probabilistic topic modeling and k-means clustering to identify the recurring themes in major railroad accidents. The output from these analyses indicates that the railroad accidents can be successfully grouped into different topics. The output also suggests that recurring accident types are track defects, wheel defects, grade crossing accidents, and switching accidents. A major difference between the Canadian and U.S. reports is the finding that accidents related to bridges are found to be more prominent in the Canadian reports."
361,"In this work we investigate the effectiveness of different text mining methods for the task of automated identification of interdisciplinary doctoral dissertations, considering solely the content of their abstracts. In contrast to previous attempts, we frame the interdisciplinarity detection as a two step classification process: we first predict the main discipline of the dissertation using a supervised multi-class classifier and then exploit the distribution of prediction confidences of the first classifier as input for the binary classification of interdisciplinarity. For both supervised classification models we experiment with several different sets of features ranging from standard lexical features such as TF-IDF weighted vectors over topic modelling distributions to latent semantic textual representations known as word embeddings. In contrast to previous findings, our experimental results suggest that interdisciplinarity is better detected when directly using textual features than when inferring from the results of main discipline classification.",2016-01-01,2-s2.0-84992443546,D-Lib Magazine,Capturing interdisciplinarity in academic abstracts,"In this work we investigate the effectiveness of different text mining methods for the task of automated identification of interdisciplinary doctoral dissertations, considering solely the content of their abstracts. In contrast to previous attempts, we frame the interdisciplinarity detection as a two step classification process: we first predict the main discipline of the dissertation using a supervised multi-class classifier and then exploit the distribution of prediction confidences of the first classifier as input for the binary classification of interdisciplinarity. For both supervised classification models we experiment with several different sets of features ranging from standard lexical features such as TF-IDF weighted vectors over topic modelling distributions to latent semantic textual representations known as word embeddings. In contrast to previous findings, our experimental results suggest that interdisciplinarity is better detected when directly using textual features than when inferring from the results of main discipline classification."
362,"Multiscale segmentation is a key prerequisite step for object-based classification methods. However, it is often not possible to determine a sole optimal scale for the image to be classified because in many cases different geo-objects and even an identical geo-object may appear at different scales in one image. In this paper, an object-based classification method based on mutliscale segmentation results in the framework of topic modelling is proposed to classify VHR satellite images in an entirely unsupervised fashion. In the stage of topic modelling, grayscale histogram distributions for each geo-object class and each segment are learned in an unsupervised manner from multiscale segments. In the stage of classification, each segment is allocated a geo-object class label by the similarity comparison between the grayscale histogram distributions of each segment and each geo-object class. Experimental results show that the proposed method can perform better than the traditional methods based on topic modelling.",2016-01-01,2-s2.0-84979582658,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",Topic modelling for object-based classification of vhr satellite images based on multiscale segmentations,"Multiscale segmentation is a key prerequisite step for object-based classification methods. However, it is often not possible to determine a sole optimal scale for the image to be classified because in many cases different geo-objects and even an identical geo-object may appear at different scales in one image. In this paper, an object-based classification method based on mutliscale segmentation results in the framework of topic modelling is proposed to classify VHR satellite images in an entirely unsupervised fashion. In the stage of topic modelling, grayscale histogram distributions for each geo-object class and each segment are learned in an unsupervised manner from multiscale segments. In the stage of classification, each segment is allocated a geo-object class label by the similarity comparison between the grayscale histogram distributions of each segment and each geo-object class. Experimental results show that the proposed method can perform better than the traditional methods based on topic modelling."
363,"In this paper, we analyze the sentiments derived from the conversations that occur in social networks. Our goal is to identify the sentiments of the users in the social network through their conversations. We conduct a study to determine whether users of social networks (twitter in particular) tend to gather together according to the likeness of their sentiments. In our proposed framework, (1) we use ANEW, a lexical dictionary to identify affective emotional feelings associated to a message according to the Russell's model of affection; (2) we design a topic modeling mechanism called Sent LDA, based on the Latent Dirichlet Allocation (LDA) generative model, which allows us to find the topic distribution in a general conversation and we associate topics with emotions; (3) we detect communities in the network according to the density and frequency of the messages among the users; and (4) we compare the sentiments of the communities by using the Russell's model of affect versus polarity and we measure the extent to which topic distribution strengthen likeness in the sentiments of the users of a community. This works contributes with a topic modeling methodology to analyze the sentiments in conversations that take place in social networks.",2016-01-01,2-s2.0-85017154367,"Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016",Sentiment analysis in social networks through topic modeling,"In this paper, we analyze the sentiments derived from the conversations that occur in social networks. Our goal is to identify the sentiments of the users in the social network through their conversations. We conduct a study to determine whether users of social networks (twitter in particular) tend to gather together according to the likeness of their sentiments. In our proposed framework, (1) we use ANEW, a lexical dictionary to identify affective emotional feelings associated to a message according to the Russell's model of affection; (2) we design a topic modeling mechanism called Sent LDA, based on the Latent Dirichlet Allocation (LDA) generative model, which allows us to find the topic distribution in a general conversation and we associate topics with emotions; (3) we detect communities in the network according to the density and frequency of the messages among the users; and (4) we compare the sentiments of the communities by using the Russell's model of affect versus polarity and we measure the extent to which topic distribution strengthen likeness in the sentiments of the users of a community. This works contributes with a topic modeling methodology to analyze the sentiments in conversations that take place in social networks."
364,"We examine two related propositions central to the subfield of comparative politics: that candidates for office adopt different electoral strategies under different electoral systems and rely more on particularism when faced with intraparty competition. We apply an innovative methodological approach that combines probabilistic topic modeling with in-depth qualitative interpretations of each topic to an original collection of 7,497 Japanese-language candidate election manifestos used in elections on either side of Japan's 1994 electoral reform. We find that the reform, which eliminated intraparty competition, was associated with a decline in particularism and an increase in promises of programmatic goods such as national security among candidates affiliated with Japan's Liberal Democratic Party. This is not explained by the entry of new candidates or other variables that could plausibly increase discussion of national security. Consistent with the theory, we find that opposition candidates relied on programmatic goods under both electoral systems.",2016-01-01,2-s2.0-84983364811,Journal of Politics,From pork to policy: The rise of programmatic campaigning in Japanese elections,"We examine two related propositions central to the subfield of comparative politics: that candidates for office adopt different electoral strategies under different electoral systems and rely more on particularism when faced with intraparty competition. We apply an innovative methodological approach that combines probabilistic topic modeling with in-depth qualitative interpretations of each topic to an original collection of 7,497 Japanese-language candidate election manifestos used in elections on either side of Japan's 1994 electoral reform. We find that the reform, which eliminated intraparty competition, was associated with a decline in particularism and an increase in promises of programmatic goods such as national security among candidates affiliated with Japan's Liberal Democratic Party. This is not explained by the entry of new candidates or other variables that could plausibly increase discussion of national security. Consistent with the theory, we find that opposition candidates relied on programmatic goods under both electoral systems."
365,"We conduct a large-scale empirical study on the sharing behavior in social media to measure the effect of message features and initial messengers on information diffusion. Our analysis focuses on messages created by companies and utilizes both textual and visual semantic content by employing state-of-the-art machine learning methods: topic modeling and deep learning. We find that messages with multiple conspicuous images and messengers with similar content are crucial in the diffusion process. Our approach for semantic content analysis, particularly for visual content, bridges advanced machine learning techniques for effective marketing and social media strategies.",2016-01-01,2-s2.0-84988014531,Lecture Notes in Business Information Processing,Sharing behavior in online social media: An empirical analysis with deep learning,"We conduct a large-scale empirical study on the sharing behavior in social media to measure the effect of message features and initial messengers on information diffusion. Our analysis focuses on messages created by companies and utilizes both textual and visual semantic content by employing state-of-the-art machine learning methods: topic modeling and deep learning. We find that messages with multiple conspicuous images and messengers with similar content are crucial in the diffusion process. Our approach for semantic content analysis, particularly for visual content, bridges advanced machine learning techniques for effective marketing and social media strategies."
366,"This paper proposes a new topic model that exploits word sense information in order to discover less redundant and more informative topics. Word sense information is obtained from WordNet and the discovered topics are groups of synsets, instead of mere surface words. A key feature is that all the known senses of a word are considered, with their probabilities. Alternative configurations of the model are described and compared to each other and to LDA, the most popular topic model. However, the obtained results suggest that there are no benefits of enriching LDA with word sense information.",2016-01-01,2-s2.0-85037132627,"Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016",Can topic modelling benefit from word sense information?,"This paper proposes a new topic model that exploits word sense information in order to discover less redundant and more informative topics. Word sense information is obtained from WordNet and the discovered topics are groups of synsets, instead of mere surface words. A key feature is that all the known senses of a word are considered, with their probabilities. Alternative configurations of the model are described and compared to each other and to LDA, the most popular topic model. However, the obtained results suggest that there are no benefits of enriching LDA with word sense information."
367,"This paper conceptualizes the term big data and describes its relevance in social research and journalistic practices. We explain large-scale text analysis techniques such as automated content analysis, data mining, machine learning, topic modeling, and sentiment analysis, which may help scientific discovery in social sciences and news production in journalism. We explain the required e-infrastructure for big data analysis with the use of cloud computing and we asses the use of the main packages and libraries for information retrieval and analysis in commercial software and programming languages such as Python or R.",2016-01-01,2-s2.0-84987704172,Profesional de la Informacion,Big data techniques: Large-scale text analysis for scientific and journalistic research Técnicas big data: Análisis de textos a gran escala para la investigación científica y periodística,"This paper conceptualizes the term big data and describes its relevance in social research and journalistic practices. We explain large-scale text analysis techniques such as automated content analysis, data mining, machine learning, topic modeling, and sentiment analysis, which may help scientific discovery in social sciences and news production in journalism. We explain the required e-infrastructure for big data analysis with the use of cloud computing and we asses the use of the main packages and libraries for information retrieval and analysis in commercial software and programming languages such as Python or R."
368,"Biology and biomedicine are flourishing disciplines, with massive biological data produced in experiments and huge amount of research papers published in journals. In such a big data context, unsupervised data mining methods such as topic models are used to extract topics from large-scale document collections. In this paper, we present a biological literature mining system based on topic modelling (BioTopic). Experiments show that the perplexity reduction percentage of our pre-processing method is 5% larger that of a traditional pre-processing method. The precision of our search performance reaches 86%, which is better that that of a unigram language model. Our method employs linguistic information from shallow parsing to better pre-process biological literature for topic models. BioTopic with finegrained pre-processing and topic modelling works better than traditional literature mining systems.",2016-01-01,2-s2.0-84964758635,International Journal of Data Mining and Bioinformatics,BioTopic: A topic-driven biological literature mining system,"Biology and biomedicine are flourishing disciplines, with massive biological data produced in experiments and huge amount of research papers published in journals. In such a big data context, unsupervised data mining methods such as topic models are used to extract topics from large-scale document collections. In this paper, we present a biological literature mining system based on topic modelling (BioTopic). Experiments show that the perplexity reduction percentage of our pre-processing method is 5% larger that of a traditional pre-processing method. The precision of our search performance reaches 86%, which is better that that of a unigram language model. Our method employs linguistic information from shallow parsing to better pre-process biological literature for topic models. BioTopic with finegrained pre-processing and topic modelling works better than traditional literature mining systems."
369,"Big data presents new challenges for understanding large text corpora. Topic modeling algorithms help understand the underlying patterns, or ""topics"", in data. Researchersauthor often read these topics in order to gain an understanding of the underlying corpus. It is important to evaluate the interpretability of these automatically generated topics. Methods have previously been designed to use crowdsourcing platforms to measure interpretability. In this paper, we demonstrate the necessity of a key concept, coherence, when assessing the topics and propose an effective method for its measurement. We show that the proposed measure of coherence captures a different aspect of the topics than existing measures. We further study the automation of these topic measures for scalability and reproducibility, showing that these measures can be automated.",2016-01-01,2-s2.0-84991772352,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Short Papers",A novel measure for coherence in statistical topic models,"Big data presents new challenges for understanding large text corpora. Topic modeling algorithms help understand the underlying patterns, or ""topics"", in data. Researchersauthor often read these topics in order to gain an understanding of the underlying corpus. It is important to evaluate the interpretability of these automatically generated topics. Methods have previously been designed to use crowdsourcing platforms to measure interpretability. In this paper, we demonstrate the necessity of a key concept, coherence, when assessing the topics and propose an effective method for its measurement. We show that the proposed measure of coherence captures a different aspect of the topics than existing measures. We further study the automation of these topic measures for scalability and reproducibility, showing that these measures can be automated."
370,"Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document.",2016-01-01,2-s2.0-85011874660,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",Generative topic embedding: A continuous representation of documents,"Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document."
371,"The idea behind this proposal is to investigate the possibility of utilizing NLP tools, statistical topic modeling techniques and freely available online resources to propose a system able to provide dialogue contribution suggestions which are relevant to the context, yet out of the main activity of the dialogue (i.e. off-Activity talk). The aim is to evaluate the effects of a tool that automatically suggests offactivity talks in form of some sentences relevant to the dialogue context. The evaluation is to be done over two test-sets of open domain and closed-domain in a conversational quiz-like setting. The outcome of this work will be a satisfactory point of entry to investigate the hypothesis that adding automatically generated offactivity talks feature to a conversational agent can lead to building up engagement of the dialogue partner(s).",2016-01-01,2-s2.0-85016596965,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Student Research Workshop",Thesis proposal: An investigation on the effectiveness of employing topic modeling techniques to provide topic awareness for conversational agents,"The idea behind this proposal is to investigate the possibility of utilizing NLP tools, statistical topic modeling techniques and freely available online resources to propose a system able to provide dialogue contribution suggestions which are relevant to the context, yet out of the main activity of the dialogue (i.e. off-Activity talk). The aim is to evaluate the effects of a tool that automatically suggests offactivity talks in form of some sentences relevant to the dialogue context. The evaluation is to be done over two test-sets of open domain and closed-domain in a conversational quiz-like setting. The outcome of this work will be a satisfactory point of entry to investigate the hypothesis that adding automatically generated offactivity talks feature to a conversational agent can lead to building up engagement of the dialogue partner(s)."
372,"This study proposes the bilingual segmented topic model (BiSTM), which hierarchically models documents by treating each document as a set of segments, e.g., sections. While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al, 2009; Ni et al, 2009), consider only cross-lingual alignments between entire documents, the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments. This study also presents a method for simultaneously inferring latent topics and segmentation boundaries, incorporating unsupervised topic segmentation (Du et al, 2013) into BiSTM. Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy).",2016-01-01,2-s2.0-85011810965,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",Bilingual segmented topic model,"This study proposes the bilingual segmented topic model (BiSTM), which hierarchically models documents by treating each document as a set of segments, e.g., sections. While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al, 2009; Ni et al, 2009), consider only cross-lingual alignments between entire documents, the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments. This study also presents a method for simultaneously inferring latent topics and segmentation boundaries, incorporating unsupervised topic segmentation (Du et al, 2013) into BiSTM. Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy)."
373,"Document collections often have links between documents-citations, hyperlinks, or revisions- and which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a network structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features. Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions.",2016-01-01,2-s2.0-85011840308,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",A discriminative topic model using document network structure,"Document collections often have links between documents-citations, hyperlinks, or revisions- and which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a network structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features. Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions."
374,"This paper proposes how to utilize a search engine in order to predict market shares. We propose to compare rates of concerns of those who search for Web pages among several companies which supply products, given a specific products domain. We measure concerns of those who search for Web pages through search engine suggests. Then, we analyze whether rates of concerns of those who search for Web pages have certain correlation with actual market share. We show that those statistics have certain correlations. We finally propose how to predict the market share of a specific product genre based on the rates of concerns of those who search for Web pages.",2016-01-01,2-s2.0-85037167793,"Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016",Analyzing time series changes of correlation between market share and concerns on companies measured through search engine suggests,"This paper proposes how to utilize a search engine in order to predict market shares. We propose to compare rates of concerns of those who search for Web pages among several companies which supply products, given a specific products domain. We measure concerns of those who search for Web pages through search engine suggests. Then, we analyze whether rates of concerns of those who search for Web pages have certain correlation with actual market share. We show that those statistics have certain correlations. We finally propose how to predict the market share of a specific product genre based on the rates of concerns of those who search for Web pages."
375,"In this chapter the authors use text-mining and topic modeling tools to analyze and visualize the digitized archive of the early twentieth-century Canadian middlebrow magazine The Western Home Monthly. The authors argue that modernist literary forms developed on the pages of middlebrow magazines, though more often for pragmatic reasons such as printing capabilities and the necessity of advertising revenue than for strictly artistic or aesthetic reasons. By juxtaposing the remediation of emerging media forms that occurs within the magazine (such as phonograph and radio) with the remediation of The Western Home Monthly into the digital “WHM,” the authors also reflect on the effects that digital remediation has on current scholarship.",2016-01-01,2-s2.0-85028806662,Reading Modernism with Machines: Digital Humanities and Modernist Literature,Remediation and the development of modernist forms in The Western Home Monthly,"In this chapter the authors use text-mining and topic modeling tools to analyze and visualize the digitized archive of the early twentieth-century Canadian middlebrow magazine The Western Home Monthly. The authors argue that modernist literary forms developed on the pages of middlebrow magazines, though more often for pragmatic reasons such as printing capabilities and the necessity of advertising revenue than for strictly artistic or aesthetic reasons. By juxtaposing the remediation of emerging media forms that occurs within the magazine (such as phonograph and radio) with the remediation of The Western Home Monthly into the digital “WHM,” the authors also reflect on the effects that digital remediation has on current scholarship."
376,"In this work, we introduced a corpus for categorizing edit types in Wikipedia. This fine-grained taxonomy of edit types enables us to differentiate editing actions and find editor roles in Wikipedia based on their low-level edit types. To do this, we first created an annotated corpus based on 1,996 edits obtained from 953 article revisions and built machine-learning models to automatically identify the edit categories associated with edits. Building on this automated measurement of edit types, we then applied a graphical model analogous to Latent Dirichlet Allocation to uncover the latent roles in editors' edit histories. Applying this technique revealed eight different roles editors play, such as Social Networker, Substantive Expert, etc.",2016-01-01,2-s2.0-85037132319,"Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016",Edit categories and editor role identification in Wikipedia,"In this work, we introduced a corpus for categorizing edit types in Wikipedia. This fine-grained taxonomy of edit types enables us to differentiate editing actions and find editor roles in Wikipedia based on their low-level edit types. To do this, we first created an annotated corpus based on 1,996 edits obtained from 953 article revisions and built machine-learning models to automatically identify the edit categories associated with edits. Building on this automated measurement of edit types, we then applied a graphical model analogous to Latent Dirichlet Allocation to uncover the latent roles in editors' edit histories. Applying this technique revealed eight different roles editors play, such as Social Networker, Substantive Expert, etc."
377,"Topic modeling has been an increasingly mature method to bridge the semantic gap between the low-level features and high-level semantic information. However, with more and more high spatial resolution (HSR) images to deal with, conventional probabilistic topic model (PTM) usually presents the images with a dense semantic representation. This consumes more time and requires more storage space. In addition, due to the complex spectral and spatial information, a combination of multiple complementary features is proved to be an effective strategy to improve the performance for HSR image scene classification. But it should be noticed that how the distinct features are fused to fully describe the challenging HSR images, which is a critical factor for scene classification. In this paper, a semantic-feature fusion fully sparse topic model (SFF-FSTM) is proposed for HSR imagery scene classification. In SFF-FSTM, three heterogeneous features - the mean and standard deviation based spectral feature, wavelet based texture feature, and dense scale-invariant feature transform (SIFT) based structural feature are effectively fused at the latent semantic level. The combination of multiple semantic-feature fusion strategy and sparse based FSTM is able to provide adequate feature representations, and can achieve comparable performance with limited training samples. Experimental results on the UC Merced dataset and Google dataset of SIRI-WHU demonstrate that the proposed method can improve the performance of scene classification compared with other scene classification methods for HSR imagery.",2016-01-01,2-s2.0-84979582947,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",Scene classfication based on the semantic-feature fusion fully sparse topic model for high spatial resolution remote sensing imagery,"Topic modeling has been an increasingly mature method to bridge the semantic gap between the low-level features and high-level semantic information. However, with more and more high spatial resolution (HSR) images to deal with, conventional probabilistic topic model (PTM) usually presents the images with a dense semantic representation. This consumes more time and requires more storage space. In addition, due to the complex spectral and spatial information, a combination of multiple complementary features is proved to be an effective strategy to improve the performance for HSR image scene classification. But it should be noticed that how the distinct features are fused to fully describe the challenging HSR images, which is a critical factor for scene classification. In this paper, a semantic-feature fusion fully sparse topic model (SFF-FSTM) is proposed for HSR imagery scene classification. In SFF-FSTM, three heterogeneous features - the mean and standard deviation based spectral feature, wavelet based texture feature, and dense scale-invariant feature transform (SIFT) based structural feature are effectively fused at the latent semantic level. The combination of multiple semantic-feature fusion strategy and sparse based FSTM is able to provide adequate feature representations, and can achieve comparable performance with limited training samples. Experimental results on the UC Merced dataset and Google dataset of SIRI-WHU demonstrate that the proposed method can improve the performance of scene classification compared with other scene classification methods for HSR imagery."
378,"Automatic assessment of sentiment in large text corpora is an important goal in social sciences. This paper describes a methodology and the results of the development of a system for Russian language sentiment analysis that includes: a publicly available sentiment lexicon, a publicly available test collection with sentiment markup and a crowdsourcing website for such markup. The lexicon is aimed at detecting sentiment in user-generated content (blogs, social media) related to social and political issues. Its prototype was formed based on other dictionaries and on the topic modeling performed on a large collection of blog posts. Topic modeling revealed relevant (social and political) topics and as a result-relevant words for the lexicon prototype and relevant texts for the training collection. Each word was assessed by at least three volunteers in the context of three different texts where the word occurred while the texts received their sentiment scores from the same volunteers as well. Both texts and words were scored from -2 (negative) to +2 (positive). Of 7,546 candidate words, 2,753 got non-neutral sentiment scores. The quality of the lexicon was assessed with SentiStrength software by comparing human text scores with the scores obtained automatically based on the created lexicon. 93% of texts were classified correctly at the error level of ±1 class, which closely matches the result of SentiStrength initial application to the English language tweets. Negative classes were much larger and better predicted. The lexicon and the text collection are publicly available at http://linis-crowd.org.",2016-01-01,2-s2.0-85020375816,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,An opinion word lexicon and a training dataset for Russian sentiment analysis of social media,"Automatic assessment of sentiment in large text corpora is an important goal in social sciences. This paper describes a methodology and the results of the development of a system for Russian language sentiment analysis that includes: a publicly available sentiment lexicon, a publicly available test collection with sentiment markup and a crowdsourcing website for such markup. The lexicon is aimed at detecting sentiment in user-generated content (blogs, social media) related to social and political issues. Its prototype was formed based on other dictionaries and on the topic modeling performed on a large collection of blog posts. Topic modeling revealed relevant (social and political) topics and as a result-relevant words for the lexicon prototype and relevant texts for the training collection. Each word was assessed by at least three volunteers in the context of three different texts where the word occurred while the texts received their sentiment scores from the same volunteers as well. Both texts and words were scored from -2 (negative) to +2 (positive). Of 7,546 candidate words, 2,753 got non-neutral sentiment scores. The quality of the lexicon was assessed with SentiStrength software by comparing human text scores with the scores obtained automatically based on the created lexicon. 93% of texts were classified correctly at the error level of ±1 class, which closely matches the result of SentiStrength initial application to the English language tweets. Negative classes were much larger and better predicted. The lexicon and the text collection are publicly available at http://linis-crowd.org."
379,"Parallel corpora are often injected with bilingual lexical resources for improved Indian language machine translation (MT). In absence of such lexical resources, multilingual topic models have been used to create coarse lexical resources in the past, using a Cartesian product approach. Our results show that for morphologically rich languages like Hindi, the Cartesian product approach is detrimental for MT. We then present a novel 'sentential' approach to use this coarse lexical resource from a multilingual topic model. Our coarse lexical resource when injected with a parallel corpus outperforms a system trained using parallel corpus and a good quality lexical resource. As demonstrated by the quality of our coarse lexical resource and its beneft to MT, we believe that our sentential approach to create such a resource will help MT for resource-constrained languages.",2016-01-01,2-s2.0-85037087089,"Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016","That'll do Fine!: A coarse lexical resource for English-Hindi MT, using polylingual topic models","Parallel corpora are often injected with bilingual lexical resources for improved Indian language machine translation (MT). In absence of such lexical resources, multilingual topic models have been used to create coarse lexical resources in the past, using a Cartesian product approach. Our results show that for morphologically rich languages like Hindi, the Cartesian product approach is detrimental for MT. We then present a novel 'sentential' approach to use this coarse lexical resource from a multilingual topic model. Our coarse lexical resource when injected with a parallel corpus outperforms a system trained using parallel corpus and a good quality lexical resource. As demonstrated by the quality of our coarse lexical resource and its beneft to MT, we believe that our sentential approach to create such a resource will help MT for resource-constrained languages."
380,"This paper presents a new Vietnamese text corpus which contains around 4.05 billion words. It is a collection of Wikipedia texts, newspaper articles and random web texts. The paper describes the process of collecting, cleaning and creating the corpus. Processing Vietnamese texts faced several challenges, for example, different from many Latin languages, Vietnamese language does not use blanks for separating words, hence using common tokenizers such as replacing blanks with word boundary does not work. A short review about different approaches of Vietnamese tokenization is presented together with how the corpus has been processed and created. After that, some statistical analysis on this data is reported including the number of syllable, average word length, sentence length and topic analysis. The corpus is integrated into a framework which allows searching and browsing. Using this web interface, users can find out how many times a particular word appears in the corpus, sample sentences where this word occurs, its left and right neighbors.",2016-01-01,2-s2.0-85037081466,"Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC 2016",Construction and analysis of a large Vietnamese text corpus,"This paper presents a new Vietnamese text corpus which contains around 4.05 billion words. It is a collection of Wikipedia texts, newspaper articles and random web texts. The paper describes the process of collecting, cleaning and creating the corpus. Processing Vietnamese texts faced several challenges, for example, different from many Latin languages, Vietnamese language does not use blanks for separating words, hence using common tokenizers such as replacing blanks with word boundary does not work. A short review about different approaches of Vietnamese tokenization is presented together with how the corpus has been processed and created. After that, some statistical analysis on this data is reported including the number of syllable, average word length, sentence length and topic analysis. The corpus is integrated into a framework which allows searching and browsing. Using this web interface, users can find out how many times a particular word appears in the corpus, sample sentences where this word occurs, its left and right neighbors."
381,"Facing the challenges of the digital age concerning lifelong learning, this contribution presents an approach to dynamically establish Connectivist communication networks. According the statement ""the pipe is more important than the content within the pipe"" by Georg Siemens, learning in digital age includes the connection of people to share required information. For this purpose, the Wiki-Learnia learning platform, which collects context information about users, is combined with the InterLect software, which identifies topics and semantic relations of contents. Based on mapping of both data sources, a wide range of matched users can be found for a specific content-related communication channel. By analyzing the course of conversation at runtime, participants can adaptively be added and removed from communication. Consequently, the presented solution serves as a just-in-time learning approach for finding direct help by experts.",2016-01-01,2-s2.0-85009070653,"Proceedings of the 13th International Conference on Cognition and Exploratory Learning in the Digital Age, CELDA 2016",Connectivist communication networks,"Facing the challenges of the digital age concerning lifelong learning, this contribution presents an approach to dynamically establish Connectivist communication networks. According the statement ""the pipe is more important than the content within the pipe"" by Georg Siemens, learning in digital age includes the connection of people to share required information. For this purpose, the Wiki-Learnia learning platform, which collects context information about users, is combined with the InterLect software, which identifies topics and semantic relations of contents. Based on mapping of both data sources, a wide range of matched users can be found for a specific content-related communication channel. By analyzing the course of conversation at runtime, participants can adaptively be added and removed from communication. Consequently, the presented solution serves as a just-in-time learning approach for finding direct help by experts."
382,"Learning analytics (LA) and Educational data mining (EDM) have emerged as promising technology-enhanced learning (TEL) research areas in recent years. Both areas deal with the development of methods that harness educational data sets to support the learning process. A key area of application for LA and EDM is learner modelling. Learner modelling enables to achieve adaptive and personalized learning environments, which are able to take into account the heterogeneous needs of learners and provide them with tailored learning experience suited for their unique needs. As learning is increasingly happening in open and distributed environments beyond the classroom and access to information in these environments is mostly interest-driven, learner interests need to constitute an important learner feature to be modeled. In this paper, we focus on the interest dimension of a learner model and present Wiki-LDA as a novel method to effectively mine user's interests in Twitter. We apply a mixed-method approach that combines Latent Dirichlet Allocation (LDA), text mining APIs, and wikipedia categories. Wiki-LDA has proven effective at the task of interest mining and classification on Twitter data, outperforming standard LDA.",2016-01-01,2-s2.0-84979530005,CSEDU 2016 - Proceedings of the 8th International Conference on Computer Supported Education,Wiki-LDA: A mixed-method approach for effective interest mining on twitter data,"Learning analytics (LA) and Educational data mining (EDM) have emerged as promising technology-enhanced learning (TEL) research areas in recent years. Both areas deal with the development of methods that harness educational data sets to support the learning process. A key area of application for LA and EDM is learner modelling. Learner modelling enables to achieve adaptive and personalized learning environments, which are able to take into account the heterogeneous needs of learners and provide them with tailored learning experience suited for their unique needs. As learning is increasingly happening in open and distributed environments beyond the classroom and access to information in these environments is mostly interest-driven, learner interests need to constitute an important learner feature to be modeled. In this paper, we focus on the interest dimension of a learner model and present Wiki-LDA as a novel method to effectively mine user's interests in Twitter. We apply a mixed-method approach that combines Latent Dirichlet Allocation (LDA), text mining APIs, and wikipedia categories. Wiki-LDA has proven effective at the task of interest mining and classification on Twitter data, outperforming standard LDA."
383,"Blogs play a special role in expressing opinions on the Internet. Due to their popularity and their public character, blogs attract the attention of many researchers. Presenting interaction between users in the form of a social network allows discovery of important users as well as groups. Finding groups and observing their evolution can help in understanding the whole structure of social networks and help in finding hidden dependencies. Such knowledge can be used, e.g., in marketing, advertising, and recommendation. In this paper, we present our approach to analyze group evolution in the context of changing the topics of discussions in groups. We assess our results using computer simulations.",2016-01-01,2-s2.0-85015174094,"Proceedings - 2016 3rd European Network Intelligence Conference, ENIC 2016",Analysis of dependences between group dynamics and topic changes,"Blogs play a special role in expressing opinions on the Internet. Due to their popularity and their public character, blogs attract the attention of many researchers. Presenting interaction between users in the form of a social network allows discovery of important users as well as groups. Finding groups and observing their evolution can help in understanding the whole structure of social networks and help in finding hidden dependencies. Such knowledge can be used, e.g., in marketing, advertising, and recommendation. In this paper, we present our approach to analyze group evolution in the context of changing the topics of discussions in groups. We assess our results using computer simulations."
384,"Understanding how a fictional relationship between two characters changes over time (e.g., from best friends to sworn enemies) is a key challenge in digital humanities scholarship. We present a novel unsupervised neural network for this task that incorporates dictionary learning to generate interpretable, accurate relationship trajectories. While previous work on characterizing literary relationships relies on plot summaries annotated with predefined labels, our model jointly learns a set of global relationship descriptors as well as a trajectory over these descriptors for each relationship in a dataset of raw text from novels. We find that our model learns descriptors of events (e.g., marriage or murder) as well as interpersonal states (love, sadness). Our model outperforms topic model baselines on two crowdsourced tasks, and we also find interesting correlations to annotations in an existing dataset.",2016-01-01,2-s2.0-84994081828,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",Feuding families and former friends: Unsupervised learning for dynamic fictional relationships,"Understanding how a fictional relationship between two characters changes over time (e.g., from best friends to sworn enemies) is a key challenge in digital humanities scholarship. We present a novel unsupervised neural network for this task that incorporates dictionary learning to generate interpretable, accurate relationship trajectories. While previous work on characterizing literary relationships relies on plot summaries annotated with predefined labels, our model jointly learns a set of global relationship descriptors as well as a trajectory over these descriptors for each relationship in a dataset of raw text from novels. We find that our model learns descriptors of events (e.g., marriage or murder) as well as interpersonal states (love, sadness). Our model outperforms topic model baselines on two crowdsourced tasks, and we also find interesting correlations to annotations in an existing dataset."
385,"Online newspaper articles can accumulate comments at volumes that prevent close reading. Summarisation of the comments allows interaction at a higher level and can lead to an understanding of the overall discussion. Comment summarisation requires topic clustering, comment ranking and extraction. Clustering must be robust as the subsequent extraction relies on a good set of clusters. Comment data, as with many social media datasets, contains very short documents and the number of words in the documents is a limiting factors on the performance of LDA clustering. We evaluate whether we can combine comments to form larger documents to improve the quality of clusters. We find that combining comments with comments that reply to them produce the highest quality clusters.",2016-01-01,2-s2.0-85016639139,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Student Research Workshop",Won't somebody please think of the children? Improving Topic Model Clustering of Newspaper Comments for Summarisation,"Online newspaper articles can accumulate comments at volumes that prevent close reading. Summarisation of the comments allows interaction at a higher level and can lead to an understanding of the overall discussion. Comment summarisation requires topic clustering, comment ranking and extraction. Clustering must be robust as the subsequent extraction relies on a good set of clusters. Comment data, as with many social media datasets, contains very short documents and the number of words in the documents is a limiting factors on the performance of LDA clustering. We evaluate whether we can combine comments to form larger documents to improve the quality of clusters. We find that combining comments with comments that reply to them produce the highest quality clusters."
386,The proceedings contain 22 papers. The topics discussed include: controlled and balanced dataset for Japanese lexical simplification; dependency forest based word alignment; identifying potential adverse drug events in tweets using bootstrapped lexicons; generating natural language descriptions for semantic representations of human brain activity; improving twitter community detection through contextual sentiment analysis; significance of an accurate Sandhi-splitter in shallow parsing of Dravidian languages; improving topic model clustering of newspaper comments for summarization; robust co-occurrence quantification for lexical distributional semantics; singleton detection using word embeddings and neural networks; a dataset for joint noun-noun compound bracketing and interpretation; and an investigation on the effectiveness of employing topic modeling techniques to provide topic awareness for conversational agents.,2016-01-01,2-s2.0-85016633893,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Student Research Workshop","54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Student Research Workshop",The proceedings contain 22 papers. The topics discussed include: controlled and balanced dataset for Japanese lexical simplification; dependency forest based word alignment; identifying potential adverse drug events in tweets using bootstrapped lexicons; generating natural language descriptions for semantic representations of human brain activity; improving twitter community detection through contextual sentiment analysis; significance of an accurate Sandhi-splitter in shallow parsing of Dravidian languages; improving topic model clustering of newspaper comments for summarization; robust co-occurrence quantification for lexical distributional semantics; singleton detection using word embeddings and neural networks; a dataset for joint noun-noun compound bracketing and interpretation; and an investigation on the effectiveness of employing topic modeling techniques to provide topic awareness for conversational agents.
387,"Uncovering thematic structures of SNS and blog posts is a crucial yet challenging task, because of the severe data sparsity induced by the short length of texts and diverse use of vocabulary. This hinders effective topic inference of traditional LDA because it infers topics based on document-level co-occurrence of words. To robustly infer topics in such contexts, we propose a latent concept topic model (LCTM). Unlike LDA, LCTM reveals topics via co-occurrence of latent concepts, which we introduce as latent variables to capture conceptual similarity of words. More specifically, LCTM models each topic as a distribution over the latent concepts, where each latent concept is a localized Gaussian distribution over the word embedding space. Since the number of unique concepts in a corpus is often much smaller than the number of unique words, LCTM is less susceptible to the data sparsity. Experiments on the 20Newsgroups show the effectiveness of LCTM in dealing with short texts as well as the capability of the model in handling held-out documents with a high degree of OOV words.",2016-01-01,2-s2.0-85016566609,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Short Papers",A latent concept topic model for robust topic inference using word embeddings,"Uncovering thematic structures of SNS and blog posts is a crucial yet challenging task, because of the severe data sparsity induced by the short length of texts and diverse use of vocabulary. This hinders effective topic inference of traditional LDA because it infers topics based on document-level co-occurrence of words. To robustly infer topics in such contexts, we propose a latent concept topic model (LCTM). Unlike LDA, LCTM reveals topics via co-occurrence of latent concepts, which we introduce as latent variables to capture conceptual similarity of words. More specifically, LCTM models each topic as a distribution over the latent concepts, where each latent concept is a localized Gaussian distribution over the word embedding space. Since the number of unique concepts in a corpus is often much smaller than the number of unique words, LCTM is less susceptible to the data sparsity. Experiments on the 20Newsgroups show the effectiveness of LCTM in dealing with short texts as well as the capability of the model in handling held-out documents with a high degree of OOV words."
388,"A humanoid, which refers to a robot that resembles a human body, imitates a human's intelligence, behavior, sense, and interaction in order to provide various types of services to human beings. Humanoids have been studied and developed constantly in order to improve their performance. Humanoids were previously developed for simple repetitive or hard work that required significant human power. However, intelligent service robots have been developed actively these days to provide necessary information and enjoyment; these include robots manufactured for home, entertainment, and personal use. It has become generally known that artificial intelligence humanoid technology will significantly benefit civilization. On the other hand, Successful Research and Development (R & D) on humanoids is possible only if they are developed in a proper direction in accordance with changes in markets and society. Therefore, it is necessary to analyze changes in technology markets and society for developing sustainable Management of Technology (MOT) strategies. In this study, patent data related to humanoids are analyzed by various data mining techniques, including topic modeling, cross-impact analysis, association rule mining, and social network analysis, to suggest sustainable strategies and methodologies for MOT.",2016-01-01,2-s2.0-84970950210,Sustainability (Switzerland),A Hybrid method of analyzing patents for sustainable technology management in humanoid robot industry,"A humanoid, which refers to a robot that resembles a human body, imitates a human's intelligence, behavior, sense, and interaction in order to provide various types of services to human beings. Humanoids have been studied and developed constantly in order to improve their performance. Humanoids were previously developed for simple repetitive or hard work that required significant human power. However, intelligent service robots have been developed actively these days to provide necessary information and enjoyment; these include robots manufactured for home, entertainment, and personal use. It has become generally known that artificial intelligence humanoid technology will significantly benefit civilization. On the other hand, Successful Research and Development (R & D) on humanoids is possible only if they are developed in a proper direction in accordance with changes in markets and society. Therefore, it is necessary to analyze changes in technology markets and society for developing sustainable Management of Technology (MOT) strategies. In this study, patent data related to humanoids are analyzed by various data mining techniques, including topic modeling, cross-impact analysis, association rule mining, and social network analysis, to suggest sustainable strategies and methodologies for MOT."
389,"When evaluating the quality of topics generated by a topic model, the convention is to score topic coherence - either manually or automatically - using the top-N topic words. This hyper-parameter N, or the cardinality of the topic, is often overlooked and selected arbitrarily. In this paper, we investigate the impact of this cardinality hyper-parameter on topic coherence evaluation. For two automatic topic coherence methodologies, we observe that the correlation with human ratings decreases systematically as the cardinality increases. More interestingly, we find that performance can be improved if the system scores and human ratings are aggregated over several topic cardinalities before computing the correlation. In contrast to the standard practice of using a fixed value of N (e.g. N = 5 or N = 10), our results suggest that calculating topic coherence over several different cardinalities and averaging results in a substantially more stable and robust evaluation. We release the code and the datasets used in this research, for reproducibility.",2016-01-01,2-s2.0-84994138713,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",The sensitivity of topic coherence evaluation to topic cardinality,"When evaluating the quality of topics generated by a topic model, the convention is to score topic coherence - either manually or automatically - using the top-N topic words. This hyper-parameter N, or the cardinality of the topic, is often overlooked and selected arbitrarily. In this paper, we investigate the impact of this cardinality hyper-parameter on topic coherence evaluation. For two automatic topic coherence methodologies, we observe that the correlation with human ratings decreases systematically as the cardinality increases. More interestingly, we find that performance can be improved if the system scores and human ratings are aggregated over several topic cardinalities before computing the correlation. In contrast to the standard practice of using a fixed value of N (e.g. N = 5 or N = 10), our results suggest that calculating topic coherence over several different cardinalities and averaging results in a substantially more stable and robust evaluation. We release the code and the datasets used in this research, for reproducibility."
390,"Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.1.",2016-01-01,2-s2.0-85016629898,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Short Papers",Nonparametric spherical topic modeling with word embeddings,"Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.1."
391,"Information diffusion model plays an important role in many real-world applications such as online marketing and e-government campaigns. Existing approaches often predict information diffusion by examining whether events are triggered by external trends or the social network itself. However, existing methods cannot take into account the semantically rich ""topics"" to estimate the correlations between users and messages describing some events. The main contribution of our work is the development of the Topic based Information Diffusion (TBID) model which can incorporate external trends model and topic based social descriptions to enhance the effectiveness of predicting information diffusion in online social networks. Experiments conducted based on real-world data sets confirm the distinct advantage of the proposed computational method. Our research opens the door to the development of a more effective personalized information recommendation model in online social media.",2015-12-08,2-s2.0-84964918090,"Proceedings - 12th IEEE International Conference on E-Business Engineering, ICEBE 2015",Topic Based Information Diffusion Prediction Model with External Trends,"Information diffusion model plays an important role in many real-world applications such as online marketing and e-government campaigns. Existing approaches often predict information diffusion by examining whether events are triggered by external trends or the social network itself. However, existing methods cannot take into account the semantically rich ""topics"" to estimate the correlations between users and messages describing some events. The main contribution of our work is the development of the Topic based Information Diffusion (TBID) model which can incorporate external trends model and topic based social descriptions to enhance the effectiveness of predicting information diffusion in online social networks. Experiments conducted based on real-world data sets confirm the distinct advantage of the proposed computational method. Our research opens the door to the development of a more effective personalized information recommendation model in online social media."
392,"How do humanities scholars make sense of new or otherwise unfamiliar archives? Is there a role for computational text analysis in the process of sensemaking? We propose that topic modeling, when conceived as a process of thematic exploration, can provide a new entry point into this process. To this end, we present research on a new software tool called TOME: Interactive TOpic Model and MEtadata Visualization, designed to support the exploratory thematic analysis of digitized archival collections. TOME is centered around a set of visualizations intended to facilitate the interpretation of the topic model and its incorporation into extant humanities research practices. In contrast to other topic model browsers, which present the model on its own terms, ours is informed by the process of conducting early-stage humanities research. Our article thus also demonstrates the conceptual conversions-in terms of both design and process-that interdisciplinary collaboration necessarily entails. In making these conversions explicit, and exploring the implications of their successes and failures, we take up the call, as voiced by Johanna Drucker (Humanities approaches to graphical display. Digital Humanities Quarterly, 5(1), 2011), to resist the 'intellectual Trojan horse' of visualization. We seek to model a new mode of interdisciplinary inquiry, one that brings the methodological emphasis of the digital humanities to bear on the practices of humanities research and computer science alike.",2015-12-01,2-s2.0-84974726320,Digital Scholarship in the Humanities,Exploratory thematic analysis for digitized archival collections,"How do humanities scholars make sense of new or otherwise unfamiliar archives? Is there a role for computational text analysis in the process of sensemaking? We propose that topic modeling, when conceived as a process of thematic exploration, can provide a new entry point into this process. To this end, we present research on a new software tool called TOME: Interactive TOpic Model and MEtadata Visualization, designed to support the exploratory thematic analysis of digitized archival collections. TOME is centered around a set of visualizations intended to facilitate the interpretation of the topic model and its incorporation into extant humanities research practices. In contrast to other topic model browsers, which present the model on its own terms, ours is informed by the process of conducting early-stage humanities research. Our article thus also demonstrates the conceptual conversions-in terms of both design and process-that interdisciplinary collaboration necessarily entails. In making these conversions explicit, and exploring the implications of their successes and failures, we take up the call, as voiced by Johanna Drucker (Humanities approaches to graphical display. Digital Humanities Quarterly, 5(1), 2011), to resist the 'intellectual Trojan horse' of visualization. We seek to model a new mode of interdisciplinary inquiry, one that brings the methodological emphasis of the digital humanities to bear on the practices of humanities research and computer science alike."
393,"Understanding technology convergence became crucial for pursuing innovation and economic growth. This paper attempts to predict the pattern of technology convergence by jointly applying the Association Rule and Link Prediction to entire IPCs related to triadic patents filed during the period from 1955 to 2011. We further use a topic model to discover emerging areas of the predicted technology convergence. The results show that the medical area is in the center of convergence, and we predict that technologies for treating respiratory system/blood/sense disorders are associated with the technologies of genetic engineering/peptide/heterocyclic compounds. After eliminating the majority of convergence, we found the convergence pattern among activating catalysts, printing, advanced networking, controlling devices, secured communication with in-memory system, television system with pattern recognition, and image processing and analyzing technologies. The results of our study are expected to contribute to firms that seek new innovative technological domain.",2015-11-01,2-s2.0-84949531077,Technological Forecasting and Social Change,Predicting the pattern of technology convergence using big-data technology on large-scale triadic patents,"Understanding technology convergence became crucial for pursuing innovation and economic growth. This paper attempts to predict the pattern of technology convergence by jointly applying the Association Rule and Link Prediction to entire IPCs related to triadic patents filed during the period from 1955 to 2011. We further use a topic model to discover emerging areas of the predicted technology convergence. The results show that the medical area is in the center of convergence, and we predict that technologies for treating respiratory system/blood/sense disorders are associated with the technologies of genetic engineering/peptide/heterocyclic compounds. After eliminating the majority of convergence, we found the convergence pattern among activating catalysts, printing, advanced networking, controlling devices, secured communication with in-memory system, television system with pattern recognition, and image processing and analyzing technologies. The results of our study are expected to contribute to firms that seek new innovative technological domain."
394,"Detailed knowledge regarding the whereabouts of people and their social activities in urban areas with high spatial and temporal resolution is still widely unexplored. Thus, the spatiotemporal analysis of Location Based Social Networks (LBSN) has great potential regarding the ability to sense spatial processes and to gain knowledge about urban dynamics, especially with respect to collective human mobility behavior. The objective of this paper is to explore the semantic association between georeferenced tweets and their respective spatiotemporal whereabouts. We apply a semantic topic model classification and spatial autocorrelation analysis to detect tweets indicating specific human social activities. We correlated observed tweet patterns with official census data for the case study of London in order to underline the significance and reliability of Twitter data. Our empirical results of semantic and spatiotemporal clustered tweets show an overall strong positive correlation in comparison with workplace population census data, being a good indicator and representative proxy for analyzing workplace-based activities.",2015-11-01,2-s2.0-84940177188,"Computers, Environment and Urban Systems",Twitter as an indicator for whereabouts of people? Correlating Twitter with UK census data,"Detailed knowledge regarding the whereabouts of people and their social activities in urban areas with high spatial and temporal resolution is still widely unexplored. Thus, the spatiotemporal analysis of Location Based Social Networks (LBSN) has great potential regarding the ability to sense spatial processes and to gain knowledge about urban dynamics, especially with respect to collective human mobility behavior. The objective of this paper is to explore the semantic association between georeferenced tweets and their respective spatiotemporal whereabouts. We apply a semantic topic model classification and spatial autocorrelation analysis to detect tweets indicating specific human social activities. We correlated observed tweet patterns with official census data for the case study of London in order to underline the significance and reliability of Twitter data. Our empirical results of semantic and spatiotemporal clustered tweets show an overall strong positive correlation in comparison with workplace population census data, being a good indicator and representative proxy for analyzing workplace-based activities."
395,"We investigate video hyperlinking based on speech transcripts, leveraging a hierarchical topical structure to address two essential aspects of hyperlinking, namely, serendipity control and link justification. We propose and compare different approaches exploiting a hierarchy of topic models as an intermediate representation to compare the transcripts of video segments. These hierarchical representations offer a basis to characterize the hyperlinks, thanks to the knowledge of the topics who contributed to the creation of the links, and to control serendipity by choosing to give more weights to either general or specific topics. Experiments are performed on BBC videos from the Search and Hyperlinking task at MediaEval. Link precisions similar to those of direct text comparison are achieved however exhibiting different targets along with a potential control of serendipity.",2015-10-30,2-s2.0-84964329611,"SLAM 2015 - Proceedings of the 2015 Workshop on Speech, Language and Audio in Multimedia, co-located with ACM MM 2015",Hierarchical topic models for language-based video hyperlinking,"We investigate video hyperlinking based on speech transcripts, leveraging a hierarchical topical structure to address two essential aspects of hyperlinking, namely, serendipity control and link justification. We propose and compare different approaches exploiting a hierarchy of topic models as an intermediate representation to compare the transcripts of video segments. These hierarchical representations offer a basis to characterize the hyperlinks, thanks to the knowledge of the topics who contributed to the creation of the links, and to control serendipity by choosing to give more weights to either general or specific topics. Experiments are performed on BBC videos from the Search and Hyperlinking task at MediaEval. Link precisions similar to those of direct text comparison are achieved however exhibiting different targets along with a potential control of serendipity."
396,"Topic models such as Latent Dirichlet Allocation (LDA) [3] have been extensively used for characterizing text collections according to the topics discussed in documents. Organizing documents according to topic can be applied to different information access tasks such as document clustering, content-based recommendation or summarization. Spoken documents such as podcasts typically involve more than one speaker (e.g., meetings, interviews, chat shows or news with reporters). This paper presents a work-inprogress based on a variation of LDA that includes in the model the different speakers participating in conversational audio transcripts. Intuitively, each speaker has her own background knowledge which generates different topic and word distributions. We believe that informing a topic model with speaker segmentation (e.g., using existing speaker diarization techniques) may enhance discovery of topics in multi-speaker audio content.",2015-10-30,2-s2.0-84964407181,"SLAM 2015 - Proceedings of the 2015 Workshop on Speech, Language and Audio in Multimedia, co-located with ACM MM 2015",SpeakerLDA: Discovering topics in transcribed multi-speaker audio contents,"Topic models such as Latent Dirichlet Allocation (LDA) [3] have been extensively used for characterizing text collections according to the topics discussed in documents. Organizing documents according to topic can be applied to different information access tasks such as document clustering, content-based recommendation or summarization. Spoken documents such as podcasts typically involve more than one speaker (e.g., meetings, interviews, chat shows or news with reporters). This paper presents a work-inprogress based on a variation of LDA that includes in the model the different speakers participating in conversational audio transcripts. Intuitively, each speaker has her own background knowledge which generates different topic and word distributions. We believe that informing a topic model with speaker segmentation (e.g., using existing speaker diarization techniques) may enhance discovery of topics in multi-speaker audio content."
397,"Social networking services, such as Twitter and Sina Weibo, have tremendous popularity in recent years. Mass of short texts and social links are aggregated into these service platforms. To realize personalized services on social network, topic inference from both short texts and social links plays more and more important role. Most conventional topic modeling methods focus on analyzing formal texts, e.g., papers, news and blogs, and usually assume that the links are only generated by topical factors. As a result, on social network, the learned topics of these methods are usually affected by topic-irrelevant links. Recently, a few approaches use artificial priors to recognize the links generated by the popularity factor in topic modeling. However, employing global priors, these methods can not well capture the distinct properties of each link and still suffer from the effect of topic-irrelevant links. To address the above limitations, we propose a novel Social-Relational Topic Model (SRTM), which can alleviate the effect of topic-irrelevant links by analyzing relational users' topics of each link. SRTM jointly models texts and social links for learning the topic distribution and topical influence of each user. The experimental results show that, our model outperforms the state-of-the-arts in topic modeling and social link prediction.",2015-10-17,2-s2.0-84959275720,"International Conference on Information and Knowledge Management, Proceedings",Social-relational topic model for social networks,"Social networking services, such as Twitter and Sina Weibo, have tremendous popularity in recent years. Mass of short texts and social links are aggregated into these service platforms. To realize personalized services on social network, topic inference from both short texts and social links plays more and more important role. Most conventional topic modeling methods focus on analyzing formal texts, e.g., papers, news and blogs, and usually assume that the links are only generated by topical factors. As a result, on social network, the learned topics of these methods are usually affected by topic-irrelevant links. Recently, a few approaches use artificial priors to recognize the links generated by the popularity factor in topic modeling. However, employing global priors, these methods can not well capture the distinct properties of each link and still suffer from the effect of topic-irrelevant links. To address the above limitations, we propose a novel Social-Relational Topic Model (SRTM), which can alleviate the effect of topic-irrelevant links by analyzing relational users' topics of each link. SRTM jointly models texts and social links for learning the topic distribution and topical influence of each user. The experimental results show that, our model outperforms the state-of-the-arts in topic modeling and social link prediction."
398,"The main objective of the workshop is to bring together researchers who are interested in applications of topic models and improving their output. Our goal is to create a broad platform for researchers to share ideas that could improve the usability and interpretation of topic models. We expect this will promote topic model applications in other research areas, making their use more effective.",2015-10-17,2-s2.0-84958248001,"International Conference on Information and Knowledge Management, Proceedings",TM 2015 - Topic models: Post-processing and applications workshop,"The main objective of the workshop is to bring together researchers who are interested in applications of topic models and improving their output. Our goal is to create a broad platform for researchers to share ideas that could improve the usability and interpretation of topic models. We expect this will promote topic model applications in other research areas, making their use more effective."
399,"Document network is a kind of intriguing dataset which can provide both topical (textual content) and topological (relational link) information. A key point in viably modeling such datasets is to discover proper denominators beneath the two different types of data, text and link. Most previous work introduces the assumption that documents closely linked with each other share common latent topics. However, the heterophily (i.e., tendency to link to different others) of nodes is neglected, which is pervasive in social networks. In this paper, we simultaneously incorporate community detection and topic modeling in a unified framework, and appeal to Canonical Correlation Analysis (CCA) to capture the latent semantic correlations between the two heterogeneous latent factors, community and topic. Despite of the homophily (i.e., tendency to link to similar others) or heterophily, CCA can properly capture the inherent correlations which fit the dataset itself without any prior hypothesis. Logistic normal prior is also employed in modeling network to better capture the community correlations. We derive efficient inference and learning algorithms based on variational EM methods. The effectiveness of our proposed model is comprehensively verified on three different types of datasets which are namely hyperlinked networks of web pages, social networks of friends and coauthor networks of publications. Experimental results show that our approach achieves significant improvements on both topic modeling and community detection compared with the current state of the art. Meanwhile, our model is impressive in discovering correlations between extracted topics and communities.",2015-10-17,2-s2.0-84958254390,"International Conference on Information and Knowledge Management, Proceedings",Discovering canonical correlations between topical and topological information in document networks,"Document network is a kind of intriguing dataset which can provide both topical (textual content) and topological (relational link) information. A key point in viably modeling such datasets is to discover proper denominators beneath the two different types of data, text and link. Most previous work introduces the assumption that documents closely linked with each other share common latent topics. However, the heterophily (i.e., tendency to link to different others) of nodes is neglected, which is pervasive in social networks. In this paper, we simultaneously incorporate community detection and topic modeling in a unified framework, and appeal to Canonical Correlation Analysis (CCA) to capture the latent semantic correlations between the two heterogeneous latent factors, community and topic. Despite of the homophily (i.e., tendency to link to similar others) or heterophily, CCA can properly capture the inherent correlations which fit the dataset itself without any prior hypothesis. Logistic normal prior is also employed in modeling network to better capture the community correlations. We derive efficient inference and learning algorithms based on variational EM methods. The effectiveness of our proposed model is comprehensively verified on three different types of datasets which are namely hyperlinked networks of web pages, social networks of friends and coauthor networks of publications. Experimental results show that our approach achieves significant improvements on both topic modeling and community detection compared with the current state of the art. Meanwhile, our model is impressive in discovering correlations between extracted topics and communities."
400,"Inferring interests of users in social network is important for many applications such as personalized search, recommender systems and online advertising. Most previous studies inferred users' interests based on text posted in social network, which is usually not related to their interests. In this paper, we propose a modified topic model, Bi-Labeled LDA with a term weighting scheme, to extract interest tags for users in social network. The proposed model utilize only users' relationship information without requirement for text information, and incorporates supervision into traditional LDA. Specifically, we introduce method to extract tags for non-famous user through their relationship with famous users in Twitter, and study why a non-famous user follows famous users simultaneously. Comparison with state-of-the-art methods on real dataset shows that our method is far more superior in terms of precision and recall of the extracted tag set, and also more applicable for many personalized applications. Besides, we find that a reasonable term weighting scheme can actually improve the performance further.",2015-10-17,2-s2.0-84958251275,"International Conference on Information and Knowledge Management, Proceedings",Extracting interest tags for non-famous users in social network,"Inferring interests of users in social network is important for many applications such as personalized search, recommender systems and online advertising. Most previous studies inferred users' interests based on text posted in social network, which is usually not related to their interests. In this paper, we propose a modified topic model, Bi-Labeled LDA with a term weighting scheme, to extract interest tags for users in social network. The proposed model utilize only users' relationship information without requirement for text information, and incorporates supervision into traditional LDA. Specifically, we introduce method to extract tags for non-famous user through their relationship with famous users in Twitter, and study why a non-famous user follows famous users simultaneously. Comparison with state-of-the-art methods on real dataset shows that our method is far more superior in terms of precision and recall of the extracted tag set, and also more applicable for many personalized applications. Besides, we find that a reasonable term weighting scheme can actually improve the performance further."
401,"Accurate mortality prediction is an important task in intensive care units in order to channel prompt care to patients in the most critical condition and to reduce nurses' alarm fatigue. Nursing notes carry valuable information in this regard, but nothing has been reported about the effectiveness of temporal analysis of nursing notes in mortality prediction tasks. We propose a time series model that uncovers the temporal dynamics of patients' underlying states from nursing notes. The effectiveness of this information in mortality prediction is examined for mortality prediction for five different time spans ranging from one day to one year. Our experiments show that the model captures both patient states and their temporal dynamics that have a strong correlation with patient mortality. The results also show that incorporating temporal information improves performance in long-term mortality prediction, but has no significant effect in short-term prediction.",2015-10-17,2-s2.0-84958246551,"International Conference on Information and Knowledge Management, Proceedings",Time series analysis of nursing notes for mortality prediction via a state transition topic model,"Accurate mortality prediction is an important task in intensive care units in order to channel prompt care to patients in the most critical condition and to reduce nurses' alarm fatigue. Nursing notes carry valuable information in this regard, but nothing has been reported about the effectiveness of temporal analysis of nursing notes in mortality prediction tasks. We propose a time series model that uncovers the temporal dynamics of patients' underlying states from nursing notes. The effectiveness of this information in mortality prediction is examined for mortality prediction for five different time spans ranging from one day to one year. Our experiments show that the model captures both patient states and their temporal dynamics that have a strong correlation with patient mortality. The results also show that incorporating temporal information improves performance in long-term mortality prediction, but has no significant effect in short-term prediction."
402,"Formulating and reformulating reliable textual queries have been recognized as a challenging task in Information Retrieval (IR), even for experienced users. Most existing query expansion methods, especially those based on implicit relevance feedback, utilize the user's historical interaction data, such as clicks, scrolling and viewing time on documents, to derive a refined query model. It is further expected that the user's search experience would be largely improved if we could dig out user's latent query intention, in real-time, by capturing the user's current interaction at the term level directly. In this paper, we propose a real-time eye tracking based query expansion method, which is able to: (1) automatically capture the terms that the user is viewing by utilizing eye tracking techniques; (2) derive the user's latent intent based on the eye tracking terms and by using the Latent Dirichlet Allocation (LDA) approach. A systematic user study has been carried out and the experimental results demonstrate the effectiveness of our proposed methods.",2015-10-17,2-s2.0-84959312260,"International Conference on Information and Knowledge Management, Proceedings",A real-time eye tracking based query expansion approach via latent topic modeling,"Formulating and reformulating reliable textual queries have been recognized as a challenging task in Information Retrieval (IR), even for experienced users. Most existing query expansion methods, especially those based on implicit relevance feedback, utilize the user's historical interaction data, such as clicks, scrolling and viewing time on documents, to derive a refined query model. It is further expected that the user's search experience would be largely improved if we could dig out user's latent query intention, in real-time, by capturing the user's current interaction at the term level directly. In this paper, we propose a real-time eye tracking based query expansion method, which is able to: (1) automatically capture the terms that the user is viewing by utilizing eye tracking techniques; (2) derive the user's latent intent based on the eye tracking terms and by using the Latent Dirichlet Allocation (LDA) approach. A systematic user study has been carried out and the experimental results demonstrate the effectiveness of our proposed methods."
403,"To date, data generates and arrives in the form of stream to propagate discussions of public events in microblog services. Discovering event-oriented topics from the stream will lead to a better understanding of the change of public concern. However, as the massive scale of the data stream, traditional static topic models, such as LDA, are no longer fit for topic detection and tracking tasks. In this paper, we propose a central topic model (CenTM), where a Multi-view Clustering algorithm with Two-phase Random Walk (MC-TRW) is devised to aggregate the LDA's latent topics into central topics. Furthermore, we leverage the aggregation of central topics alternately with MC-TRW and sequential topic inference to improve the scalability in the stream fashion, so as to derive the dynamic central topic model (DCenTM). Specifically, our model is able to uncover the intrinsic characteristics of the central topics and predict the trend of their intensity along a life cycle. Experimental results demonstrate that the proposed central topic model is event-oriented and of high generalization, it therefore can dispose the topic trend prediction effectively and precisely in massive data stream.",2015-10-17,2-s2.0-84959303766,"International Conference on Information and Knowledge Management, Proceedings",Central topic model for event-oriented topics mining in microblog stream,"To date, data generates and arrives in the form of stream to propagate discussions of public events in microblog services. Discovering event-oriented topics from the stream will lead to a better understanding of the change of public concern. However, as the massive scale of the data stream, traditional static topic models, such as LDA, are no longer fit for topic detection and tracking tasks. In this paper, we propose a central topic model (CenTM), where a Multi-view Clustering algorithm with Two-phase Random Walk (MC-TRW) is devised to aggregate the LDA's latent topics into central topics. Furthermore, we leverage the aggregation of central topics alternately with MC-TRW and sequential topic inference to improve the scalability in the stream fashion, so as to derive the dynamic central topic model (DCenTM). Specifically, our model is able to uncover the intrinsic characteristics of the central topics and predict the trend of their intensity along a life cycle. Experimental results demonstrate that the proposed central topic model is event-oriented and of high generalization, it therefore can dispose the topic trend prediction effectively and precisely in massive data stream."
404,"Microblogs contain the most up-to-date and abundant opinion information on current events. Aspect-based opinion mining is a good way to get a comprehensive summarization of events. The most popular aspect based opinion mining models are used in the field of product and service. However, existing models are not suitable for event mining. In this paper we propose a novel probabilistic generative model (ASEM) to simultaneously discover aspects and the specified opinions. ASEM incorporate a sequence labeling model(CRF) into a generative topic model. Additionally, we adopt a set of features for separating aspects and sentiments. Moreover, we novelly present a continuously learning model. It can utilize the knowledge of one event to learn another, and get a better performance. We use five real world events to do experiment. The experimental results show that ASEM extracts aspects and sentiments well, and ASEM outperforms other state-of-art models and the intuitive two-step method.",2015-10-17,2-s2.0-84958256507,"International Conference on Information and Knowledge Management, Proceedings",ASEM: Mining aspects and sentiment of events from microblog,"Microblogs contain the most up-to-date and abundant opinion information on current events. Aspect-based opinion mining is a good way to get a comprehensive summarization of events. The most popular aspect based opinion mining models are used in the field of product and service. However, existing models are not suitable for event mining. In this paper we propose a novel probabilistic generative model (ASEM) to simultaneously discover aspects and the specified opinions. ASEM incorporate a sequence labeling model(CRF) into a generative topic model. Additionally, we adopt a set of features for separating aspects and sentiments. Moreover, we novelly present a continuously learning model. It can utilize the knowledge of one event to learn another, and get a better performance. We use five real world events to do experiment. The experimental results show that ASEM extracts aspects and sentiments well, and ASEM outperforms other state-of-art models and the intuitive two-step method."
405,"In social media, users have contributed enormous behavior data online which can be leveraged for user modeling and conduct personalized services. Temporal user modeling, which incorporates the timestamp of these behavior data and understands users' interest evolution, have attracted attention recently. With the recognition that user interests are vulnerable to transient events, many current temporal user modeling solutions propose to first identify the transient events and then consider the identified events into user behavior modeling. In this work, in the context of microblogs, we propose a unified probabilistic framework to simultaneously model the process of transient event detection and temporal user tweeting. The outputs of the framework include: (1) one long-term topic space spanning over general categories, (2) one short-term topic space for each time interval corresponding to the transient events, and (3) users' interest distributions over the long- and short-term topic spaces. Qualitative and quantitative experimental evaluation are conducted on a large-scale Twitter dataset, with more than 2 million users and 0.3 billion tweets. The promising results demonstrate the advantage of the proposed topic models.",2015-10-17,2-s2.0-84958252637,"International Conference on Information and Knowledge Management, Proceedings",A probabilistic framework for temporal user modeling on microblogs,"In social media, users have contributed enormous behavior data online which can be leveraged for user modeling and conduct personalized services. Temporal user modeling, which incorporates the timestamp of these behavior data and understands users' interest evolution, have attracted attention recently. With the recognition that user interests are vulnerable to transient events, many current temporal user modeling solutions propose to first identify the transient events and then consider the identified events into user behavior modeling. In this work, in the context of microblogs, we propose a unified probabilistic framework to simultaneously model the process of transient event detection and temporal user tweeting. The outputs of the framework include: (1) one long-term topic space spanning over general categories, (2) one short-term topic space for each time interval corresponding to the transient events, and (3) users' interest distributions over the long- and short-term topic spaces. Qualitative and quantitative experimental evaluation are conducted on a large-scale Twitter dataset, with more than 2 million users and 0.3 billion tweets. The promising results demonstrate the advantage of the proposed topic models."
406,"A common and convenient approach for user to describe his information need is to provide a set of keywords. Therefore, the technique to understand the need becomes crucial. In this paper, for the information need about a topic or category, we propose a novel method called TDCS(Topic Distilling with Compressive Sensing) for explicit and accurate modeling the topic implied by several keywords. The task is transformed as a topic reconstruction problem in the semantic space with a reasonable intuition that the topic is sparse in the semantic space. The latent semantic space could be mined from documents via unsupervised methods, e.g. LSI. Compressive sensing is leveraged to obtain a sparse representation from only a few keywords. In order to make the distilled topic more robust, an iterative learning approach is adopted. The experiment results show the effectiveness of our method. Moreover, with only a few semantic concepts remained for the topic, our method is efficient for subsequent text mining tasks.",2015-10-17,2-s2.0-84958232629,"International Conference on Information and Knowledge Management, Proceedings",Topic modeling in semantic space with keywords,"A common and convenient approach for user to describe his information need is to provide a set of keywords. Therefore, the technique to understand the need becomes crucial. In this paper, for the information need about a topic or category, we propose a novel method called TDCS(Topic Distilling with Compressive Sensing) for explicit and accurate modeling the topic implied by several keywords. The task is transformed as a topic reconstruction problem in the semantic space with a reasonable intuition that the topic is sparse in the semantic space. The latent semantic space could be mined from documents via unsupervised methods, e.g. LSI. Compressive sensing is leveraged to obtain a sparse representation from only a few keywords. In order to make the distilled topic more robust, an iterative learning approach is adopted. The experiment results show the effectiveness of our method. Moreover, with only a few semantic concepts remained for the topic, our method is efficient for subsequent text mining tasks."
407,"As the volume of publications has increased dramatically, an urgent need has developed to assist researchers in locating high-quality, candidate-cited papers from a research repository. Traditional scholarly recommendation approaches ignore the chronological nature of citation recommendations. In this study, we propose a novel method called ""Chronological Citation Recommendation"" which assumes initial user information needs could shift while users are searching for papers in different time slices. We model the information-need shifts with two-level modeling: dynamic time-related ranking feature construction and dynamic evolving feature weight training. In more detail, we employed a supervised document influence model to characterize the content ""time-varying"" dynamics and constructed a novel heterogeneous graph that encapsulates dynamic topic-based information, time-decay paper/topic citation information, and word-based information. We applied multiple meta-paths for different ranking hypotheses which carried different types of information for citation recommendation in various time slices, along with information-need shifting. We also used multiple learning-to-rank models to optimize the feature weights for different time slices to generate the final ""Chronological Citation Recommendation"" rankings. The use of Chronological Citation Recommendation suggests time-series ranking lists based on initial user textual information need and characterizes the information-need shifting. Experiments on the ACM corpus show that Chronological Citation Recommendation can significantly enhance citation recommendation performance.",2015-10-17,2-s2.0-84958257628,"International Conference on Information and Knowledge Management, Proceedings",Chronological citation recommendation with information-need shifting,"As the volume of publications has increased dramatically, an urgent need has developed to assist researchers in locating high-quality, candidate-cited papers from a research repository. Traditional scholarly recommendation approaches ignore the chronological nature of citation recommendations. In this study, we propose a novel method called ""Chronological Citation Recommendation"" which assumes initial user information needs could shift while users are searching for papers in different time slices. We model the information-need shifts with two-level modeling: dynamic time-related ranking feature construction and dynamic evolving feature weight training. In more detail, we employed a supervised document influence model to characterize the content ""time-varying"" dynamics and constructed a novel heterogeneous graph that encapsulates dynamic topic-based information, time-decay paper/topic citation information, and word-based information. We applied multiple meta-paths for different ranking hypotheses which carried different types of information for citation recommendation in various time slices, along with information-need shifting. We also used multiple learning-to-rank models to optimize the feature weights for different time slices to generate the final ""Chronological Citation Recommendation"" rankings. The use of Chronological Citation Recommendation suggests time-series ranking lists based on initial user textual information need and characterizes the information-need shifting. Experiments on the ACM corpus show that Chronological Citation Recommendation can significantly enhance citation recommendation performance."
408,"The classification of web pages content is essential to many information retrieval tasks. In this paper, we propose a new methodology for a multilayer soft classification. Our approach is based on the connection between the semi-supervised Latent Dirichlet Allocation (LDA) and the Random Forest classifier. We compute with LDA the distribution of topics in each document and use the results to train the Random Forest classifier. The trained classifier is then able to categorize each web document in different layers of the categories hierarchy. We have applied our methodology on a collected data set from dmoz and have obtained satisfactory results.",2015-10-07,2-s2.0-84954551242,"2015 15th International Conference on Innovations for Community Services, I4CS 2015",Multilayer classification of web pages using random forest and semi-supervised latent dirichlet allocation,"The classification of web pages content is essential to many information retrieval tasks. In this paper, we propose a new methodology for a multilayer soft classification. Our approach is based on the connection between the semi-supervised Latent Dirichlet Allocation (LDA) and the Random Forest classifier. We compute with LDA the distribution of topics in each document and use the results to train the Random Forest classifier. The trained classifier is then able to categorize each web document in different layers of the categories hierarchy. We have applied our methodology on a collected data set from dmoz and have obtained satisfactory results."
409,"Cross-language text similarity calculation is a critical and fundamental problem in natural language processing. It is widely used in cross-language research, such as cross-language information retrieval. In this paper, we used the LDA (Latent Dirichlet Allocation) model to calculate similarities of Tibetan and Chinese texts at the topic level. Through topic modelling and forecasting, the texts are mapped to the feature space of topics. This method reduced the dimensions of text space vector and improved the speed and efficiency of computation.",2015-09-29,2-s2.0-84958622078,ICEIEC 2015 - Proceedings of 2015 IEEE 5th International Conference on Electronics Information and Emergency Communication,Research on cross-language text similarity calculation,"Cross-language text similarity calculation is a critical and fundamental problem in natural language processing. It is widely used in cross-language research, such as cross-language information retrieval. In this paper, we used the LDA (Latent Dirichlet Allocation) model to calculate similarities of Tibetan and Chinese texts at the topic level. Through topic modelling and forecasting, the texts are mapped to the feature space of topics. This method reduced the dimensions of text space vector and improved the speed and efficiency of computation."
410,"Buyers express their opinions openly in free text feedback comments. The users are attracted to online-shopping not only due to the convenience in accessing the information of items on-sold, but also the availability of the other buyer's feedback on their purchasing experience, item-related and/or seller-related. The Mining E-Commerce feedback comments for trust evaluation System proposes a novel technique that uses a multi-dimensional trust evaluation model, for computing comprehensive trust scores for sellers in e-commerce applications. The system computes dimension trust scores and the dimension weights. It automatically extracting dimension ratings from feedback comments and by combining natural language processing with opinion mining and summarization techniques in trust evaluation to improve the accuracy for mining process.",2015-09-23,2-s2.0-84965071508,ICETECH 2015 - 2015 IEEE International Conference on Engineering and Technology,Mining E-commerce feedback comments for trust evaluation,"Buyers express their opinions openly in free text feedback comments. The users are attracted to online-shopping not only due to the convenience in accessing the information of items on-sold, but also the availability of the other buyer's feedback on their purchasing experience, item-related and/or seller-related. The Mining E-Commerce feedback comments for trust evaluation System proposes a novel technique that uses a multi-dimensional trust evaluation model, for computing comprehensive trust scores for sellers in e-commerce applications. The system computes dimension trust scores and the dimension weights. It automatically extracting dimension ratings from feedback comments and by combining natural language processing with opinion mining and summarization techniques in trust evaluation to improve the accuracy for mining process."
411,"In their quest for data-driven insight, firms align their resources to produce information that is actionable. Moreover, the bundling and utilization of these valuable resources is what defines an organizational capability. Thus, in this paper we conceptualize a new type of capability - data analytics capabilities, DAC, as the ability to assemble, coordinate, mobilize, and deploy analytics-based resources with strategic purpose. Using text as data, we explore the use of probabilistic topic modeling on historical press releases, in an attempt to identify types of DAC from successful data analytics investments. Press and news releases frequently articulate a firm's resource allocation strategy, proving an opportunity to automatically classify these into topics that can suggest categorization of DAC. We explore 8-year historical press releases and apply Latent Dirichlet Allocation topic modeling to 273 press releases.",2015-09-21,2-s2.0-84955621444,Portland International Conference on Management of Engineering and Technology,Decoding data analytics capabilities from topic modeling on press releases,"In their quest for data-driven insight, firms align their resources to produce information that is actionable. Moreover, the bundling and utilization of these valuable resources is what defines an organizational capability. Thus, in this paper we conceptualize a new type of capability - data analytics capabilities, DAC, as the ability to assemble, coordinate, mobilize, and deploy analytics-based resources with strategic purpose. Using text as data, we explore the use of probabilistic topic modeling on historical press releases, in an attempt to identify types of DAC from successful data analytics investments. Press and news releases frequently articulate a firm's resource allocation strategy, proving an opportunity to automatically classify these into topics that can suggest categorization of DAC. We explore 8-year historical press releases and apply Latent Dirichlet Allocation topic modeling to 273 press releases."
412,"In this chapter, the authors survey the general problem of analyzing a social network in order to make predictions about its behavior, content, or the systems and phenomena that generated it. They begin by defining five basic tasks that can be performed using social networks: (1) link prediction; (2) pathway and community formation; (3) recommendation and decision support; (4) risk analysis; and (5) planning, especially causal interventional planning. Next, they discuss frameworks for using predictive analytics, availability of annotation, text associated with (or produced within) a social network, information propagation history (e.g., upvotes and shares), trust, and reputation data. They also review challenges such as imbalanced and partial data, concept drift especially as it manifests within social media, and the need for active learning, online learning, and transfer learning. They then discuss general methodologies for predictive analytics involving network topology and dynamics, heterogeneous information network analysis, stochastic simulation, and topic modeling using the abovementioned text corpora. They continue by describing applications such as predicting ""who will follow whom?"" in a social network, making entity-to-entity recommendations (person-to-person, business-to-business [B2B], consumer-tobusiness [C2B], or business-to-consumer [B2C]), and analyzing big data (especially transactional data) for Customer Relationship Management (CRM) applications. Finally, the authors examine a few specific recommender systems and systems for interaction discovery, as part of brief case studies.",2015-09-21,2-s2.0-84956714680,Emerging Methods in Predictive Analytics: Risk Management and Decision-Making,Predictive analytics of social networks: A survey of tasks and techniques,"In this chapter, the authors survey the general problem of analyzing a social network in order to make predictions about its behavior, content, or the systems and phenomena that generated it. They begin by defining five basic tasks that can be performed using social networks: (1) link prediction; (2) pathway and community formation; (3) recommendation and decision support; (4) risk analysis; and (5) planning, especially causal interventional planning. Next, they discuss frameworks for using predictive analytics, availability of annotation, text associated with (or produced within) a social network, information propagation history (e.g., upvotes and shares), trust, and reputation data. They also review challenges such as imbalanced and partial data, concept drift especially as it manifests within social media, and the need for active learning, online learning, and transfer learning. They then discuss general methodologies for predictive analytics involving network topology and dynamics, heterogeneous information network analysis, stochastic simulation, and topic modeling using the abovementioned text corpora. They continue by describing applications such as predicting ""who will follow whom?"" in a social network, making entity-to-entity recommendations (person-to-person, business-to-business [B2B], consumer-tobusiness [C2B], or business-to-consumer [B2C]), and analyzing big data (especially transactional data) for Customer Relationship Management (CRM) applications. Finally, the authors examine a few specific recommender systems and systems for interaction discovery, as part of brief case studies."
413,"This article examines social media reversion, when a user intentionally ceases using a social media site but then later resumes use of the site. We analyze a convenience sample of survey data from people who volunteered to stay off Facebook for 99 days but, in some cases, returned before that time. We conduct three separate analyses to triangulate on the phenomenon of reversion: simple quantitative predictors of reversion, factor analysis of adjectives used by respondents to describe their experiences of not using Facebook, and statistical topic analysis of free-text responses. Significant factors predicting either increased or decreased likelihood of reversion include, among others, prior use of Facebook, experiences associated with perceived addiction, issues of social boundary negotiation such as privacy and surveillance, use of other social media, and friends’ reactions to non-use. These findings contribute to the growing literature on technology non-use by demonstrating how social media users negotiate, both with each other and with themselves, among types and degrees of use and non-use.",2015-09-11,2-s2.0-85019820435,Social Media and Society,"Missing Photos, Suffering Withdrawal, or Finding Freedom? How Experiences of Social Media Non-Use Influence the Likelihood of Reversion","This article examines social media reversion, when a user intentionally ceases using a social media site but then later resumes use of the site. We analyze a convenience sample of survey data from people who volunteered to stay off Facebook for 99 days but, in some cases, returned before that time. We conduct three separate analyses to triangulate on the phenomenon of reversion: simple quantitative predictors of reversion, factor analysis of adjectives used by respondents to describe their experiences of not using Facebook, and statistical topic analysis of free-text responses. Significant factors predicting either increased or decreased likelihood of reversion include, among others, prior use of Facebook, experiences associated with perceived addiction, issues of social boundary negotiation such as privacy and surveillance, use of other social media, and friends’ reactions to non-use. These findings contribute to the growing literature on technology non-use by demonstrating how social media users negotiate, both with each other and with themselves, among types and degrees of use and non-use."
414,"One limitation of most existing probabilistic latent topic models for document classification is that the topic model itself does not consider useful side-information, namely, class labels of documents. Topic models, which in turn consider the side-information, popularly known as supervised topic models, do not consider the word order structure in documents. One of the motivations behind considering the word order structure is to capture the semantic fabric of the document. We investigate a low-dimensional latent topic model for document classification. Class label information and word order structure are integrated into a supervised topic model enabling a more effective interaction among such information for solving document classification. We derive a collapsed Gibbs sampler for our model. Likewise, supervised topic models with word order structure have not been explored in document retrieval learning. We propose a novel supervised topic model for document retrieval learning which can be regarded as a pointwise model for tackling the learning-to-rank task. Available relevance assessments and word order structure are integrated into the topic model itself. We conduct extensive experiments on several publicly available benchmark datasets, and show that our model improves upon the state-of-the-art models.",2015-08-25,2-s2.0-84933177559,Information Retrieval,Supervised topic models with word order structure for document classification and retrieval learning,"One limitation of most existing probabilistic latent topic models for document classification is that the topic model itself does not consider useful side-information, namely, class labels of documents. Topic models, which in turn consider the side-information, popularly known as supervised topic models, do not consider the word order structure in documents. One of the motivations behind considering the word order structure is to capture the semantic fabric of the document. We investigate a low-dimensional latent topic model for document classification. Class label information and word order structure are integrated into a supervised topic model enabling a more effective interaction among such information for solving document classification. We derive a collapsed Gibbs sampler for our model. Likewise, supervised topic models with word order structure have not been explored in document retrieval learning. We propose a novel supervised topic model for document retrieval learning which can be regarded as a pointwise model for tackling the learning-to-rank task. Available relevance assessments and word order structure are integrated into the topic model itself. We conduct extensive experiments on several publicly available benchmark datasets, and show that our model improves upon the state-of-the-art models."
415,"Object recognition systems usually require fully complete manually labeled training data to train classifier. In this paper, we study the problem of object recognition, where the training samples are missing during the classifier learning stage, a task also known as zero-shot learning. We propose a novel zero-shot learning strategy that utilizes the topic model and hierarchical class concept. Our proposed method advanced where cumbersome human annotation stage (i.e., attribute-based classification) is eliminated. We achieve comparable performance with state-of-the-art algorithms in four public datasets: PubFig (67.09\%), Cifar-100 (54.85\%), Caltech-256 ( 52.14\%), and Animals with Attributes (49.65\%), when unseen classes exist in the classification task.",2015-08-01,2-s2.0-85027957052,IEEE Transactions on Human-Machine Systems,Zero-Shot Object Recognition System Based on Topic Model,"Object recognition systems usually require fully complete manually labeled training data to train classifier. In this paper, we study the problem of object recognition, where the training samples are missing during the classifier learning stage, a task also known as zero-shot learning. We propose a novel zero-shot learning strategy that utilizes the topic model and hierarchical class concept. Our proposed method advanced where cumbersome human annotation stage (i.e., attribute-based classification) is eliminated. We achieve comparable performance with state-of-the-art algorithms in four public datasets: PubFig (67.09\%), Cifar-100 (54.85\%), Caltech-256 ( 52.14\%), and Animals with Attributes (49.65\%), when unseen classes exist in the classification task."
416,"Many large companies today face the risk of data breaches via malicious software, compromising their business. These types of attacks are usually executed using hacker assets. Researching hacker assets within underground communities can help identify the tools which may be used in a cyberattack, provide knowledge on how to implement and use such assets and assist in organizing tools in a manner conducive to ethical reuse and education. This study aims to understand the functions and characteristics of assets in hacker forums by applying classification and topic modeling techniques. This research contributes to hacker literature by gaining a deeper understanding of hacker assets in well-known forums and organizing them in a fashion conducive to educational reuse. Additionally, companies can apply our framework to forums of their choosing to extract their assets and appropriate functions.",2015-07-23,2-s2.0-84963758376,"2015 IEEE International Conference on Intelligence and Security Informatics: Securing the World through an Alignment of Technology, Intelligence, Humans and Organizations, ISI 2015",Exploring hacker assets in underground forums,"Many large companies today face the risk of data breaches via malicious software, compromising their business. These types of attacks are usually executed using hacker assets. Researching hacker assets within underground communities can help identify the tools which may be used in a cyberattack, provide knowledge on how to implement and use such assets and assist in organizing tools in a manner conducive to ethical reuse and education. This study aims to understand the functions and characteristics of assets in hacker forums by applying classification and topic modeling techniques. This research contributes to hacker literature by gaining a deeper understanding of hacker assets in well-known forums and organizing them in a fashion conducive to educational reuse. Additionally, companies can apply our framework to forums of their choosing to extract their assets and appropriate functions."
417,"Topic-based ranking of authors, papers and journals can serve as a vital tool for identifying authorities of a given topic within a particular domain. Existing methods that measure topic-based scholarly output are limited to homogeneous networks. This study proposes a new informative metric called Topic-based Heterogeneous Rank (TH Rank) which measures the impact of a scholarly entity with respect to a given topic in a heterogeneous scholarly network containing authors, papers and journals. TH Rank calculates topic-dependent ranks for authors by considering the combined impact of the multiple factors which contribute to an author’s level of prestige. Information retrieval serves as the test field and articles about information retrieval published between 1956 and 2014 were extracted from web of science. Initial results show that TH Rank can effectively identify the most prestigious authors, papers and journals related to a specific topic.",2015-07-11,2-s2.0-84930821438,Scientometrics,Topic-based heterogeneous rank,"Topic-based ranking of authors, papers and journals can serve as a vital tool for identifying authorities of a given topic within a particular domain. Existing methods that measure topic-based scholarly output are limited to homogeneous networks. This study proposes a new informative metric called Topic-based Heterogeneous Rank (TH Rank) which measures the impact of a scholarly entity with respect to a given topic in a heterogeneous scholarly network containing authors, papers and journals. TH Rank calculates topic-dependent ranks for authors by considering the combined impact of the multiple factors which contribute to an author’s level of prestige. Information retrieval serves as the test field and articles about information retrieval published between 1956 and 2014 were extracted from web of science. Initial results show that TH Rank can effectively identify the most prestigious authors, papers and journals related to a specific topic."
418,"Identifying the concepts covered in a university course based on a high level description is a necessary step in the evaluation of a university's program of study. To this end, data describing university courses is readily available on the Internet in vast quantities. However, understanding natural language course descriptions requires manual inspection and, often, implicit knowledge of the subject area. Additionally, a holistic approach to curricular evaluation involves analysis of the prerequisite structure within a department, specifically the conceptual overlap between courses in a prerequisite chain. In this work we apply existing topic modeling techniques to sets of course descriptions extracted from publicly available university course catalogs. The inferred topic models correspond to concepts taught in the described courses. The inference process is unsupervised and generates topics without the need for manual inspection. We present an application framework for data ingestion and processing, along with a user-facing web-based application for inferred topic presentation. The software provides tools to view the inferred topics for a university's courses, quickly compare departments by their topic composition, and visually analyze conceptual overlap in departmental prerequisite structures. The tool is available online at http://edmine.cs.gmu.edu/.",2015-07-09,2-s2.0-84959293493,ICER 2015 - Proceedings of the 2015 ACM Conference on International Computing Education Research,What are we teaching? Automated evaluation of CS curricula content using topic modeling,"Identifying the concepts covered in a university course based on a high level description is a necessary step in the evaluation of a university's program of study. To this end, data describing university courses is readily available on the Internet in vast quantities. However, understanding natural language course descriptions requires manual inspection and, often, implicit knowledge of the subject area. Additionally, a holistic approach to curricular evaluation involves analysis of the prerequisite structure within a department, specifically the conceptual overlap between courses in a prerequisite chain. In this work we apply existing topic modeling techniques to sets of course descriptions extracted from publicly available university course catalogs. The inferred topic models correspond to concepts taught in the described courses. The inference process is unsupervised and generates topics without the need for manual inspection. We present an application framework for data ingestion and processing, along with a user-facing web-based application for inferred topic presentation. The software provides tools to view the inferred topics for a university's courses, quickly compare departments by their topic composition, and visually analyze conceptual overlap in departmental prerequisite structures. The tool is available online at http://edmine.cs.gmu.edu/."
419,"Abstract A series of events generates multiple types of time series data, such as numeric and text data over time, and the variations of the data types capture the events from different angles. This paper aims to integrate the analyses on such numerical and text time-series data influenced by common events with a single model to better understand the events. Specifically, we present a topic model, called an associative topic model (ATM), which finds the soft cluster of time-series text data guided by time-series numerical value. The identified clusters are represented as word distributions per clusters, and these word distributions indicate what the corresponding events were. We applied ATM to financial indexes and president approval rates. First, ATM identifies topics associated with the characteristics of time-series data from the multiple types of data. Second, ATM predicts numerical time-series data with a higher level of accuracy than does the iterative model, which is supported by lower mean squared errors.",2015-07-06,2-s2.0-84934763419,Information Processing and Management,Associative topic models with numerical time series,"Abstract A series of events generates multiple types of time series data, such as numeric and text data over time, and the variations of the data types capture the events from different angles. This paper aims to integrate the analyses on such numerical and text time-series data influenced by common events with a single model to better understand the events. Specifically, we present a topic model, called an associative topic model (ATM), which finds the soft cluster of time-series text data guided by time-series numerical value. The identified clusters are represented as word distributions per clusters, and these word distributions indicate what the corresponding events were. We applied ATM to financial indexes and president approval rates. First, ATM identifies topics associated with the characteristics of time-series data from the multiple types of data. Second, ATM predicts numerical time-series data with a higher level of accuracy than does the iterative model, which is supported by lower mean squared errors."
420,"Ecological and environmental sciences have become more advanced and complex, requiring observational and experimental data from multiple places, times, and thematic scales to verify their hypotheses. Over time, such data have not only increased in amount, but also in diversity and heterogeneity of the data sources that spread throughout the world. This heterogeneity poses a huge challenge for scientists who have to manually search for desired data. ONEMercury has recently been implemented as part of the DataONE project to alleviate such problems and to serve as a portal for accessing environmental and observational data across the globe. ONEMercury harvests metadata records from multiple archives and repositories, and makes them searchable. However, harvested metadata records sometimes are poorly annotated or lacking meaningful keywords, which could impede effective retrieval. We propose a methodology that learns the annotation from well-annotated collections of metadata records to automatically annotate poorly annotated ones. The problem is first transformed into the tag recommendation problem with a controlled tag library. Then, two variants of an algorithm for automatic tag recommendation are presented. The experiments on four datasets of environmental science metadata records show that our methods perform well and also shed light on the natures of different datasets. We also discuss relevant topics such as using topical coherence to fine-tune parameters and experiments on cross-archive annotation.",2015-06-27,2-s2.0-84929943959,International Journal on Digital Libraries,A generalized topic modeling approach for automatic document annotation,"Ecological and environmental sciences have become more advanced and complex, requiring observational and experimental data from multiple places, times, and thematic scales to verify their hypotheses. Over time, such data have not only increased in amount, but also in diversity and heterogeneity of the data sources that spread throughout the world. This heterogeneity poses a huge challenge for scientists who have to manually search for desired data. ONEMercury has recently been implemented as part of the DataONE project to alleviate such problems and to serve as a portal for accessing environmental and observational data across the globe. ONEMercury harvests metadata records from multiple archives and repositories, and makes them searchable. However, harvested metadata records sometimes are poorly annotated or lacking meaningful keywords, which could impede effective retrieval. We propose a methodology that learns the annotation from well-annotated collections of metadata records to automatically annotate poorly annotated ones. The problem is first transformed into the tag recommendation problem with a controlled tag library. Then, two variants of an algorithm for automatic tag recommendation are presented. The experiments on four datasets of environmental science metadata records show that our methods perform well and also shed light on the natures of different datasets. We also discuss relevant topics such as using topical coherence to fine-tune parameters and experiments on cross-archive annotation."
421,"The rapid growth of information in the digital world especially on the web, calls for automated methods of organizing the digital information for convenient access and efficient information retrieval. Topic modeling is a branch of machine learning and probabilistic graphical modeling that helps in arranging the web pages according to their topical structure. The topic distribution over a set of documents (web pages) and the affinity of a document toward a specific topic can be revealed using topic modeling. Topic modeling algorithms are typically computationally expensive due to their iterative nature. Recent research efforts have attempted to parallelize specific topic models and are successful in their attempts. These parallel algorithms however have tightly-coupled parallel processes which require frequent synchronization and are also tightly coupled with the underlying topic model which is used for inferring the topic hierarchy. In this paper, we propose a parallel algorithm to infer topic hierarchies from a large scale document corpus. A key feature of the proposed algorithm is that it exploits coarse grained parallelism and the components running in parallel need not synchronize after every iteration, thus the algorithm lends itself to be implemented on a geographically dispersed set of processing elements interconnected through a network. The parallel algorithm realizes a speed up of 53.5 on a 32-node cluster of dual-core workstations and at the same time achieving approximately the same likelihood or predictive accuracy as that of the sequential algorithm, with respect to the performance of Information Retrieval tasks.",2015-06-26,2-s2.0-84935009369,Information Processing and Management,Design and evaluation of a parallel algorithm for inferring topic hierarchies,"The rapid growth of information in the digital world especially on the web, calls for automated methods of organizing the digital information for convenient access and efficient information retrieval. Topic modeling is a branch of machine learning and probabilistic graphical modeling that helps in arranging the web pages according to their topical structure. The topic distribution over a set of documents (web pages) and the affinity of a document toward a specific topic can be revealed using topic modeling. Topic modeling algorithms are typically computationally expensive due to their iterative nature. Recent research efforts have attempted to parallelize specific topic models and are successful in their attempts. These parallel algorithms however have tightly-coupled parallel processes which require frequent synchronization and are also tightly coupled with the underlying topic model which is used for inferring the topic hierarchy. In this paper, we propose a parallel algorithm to infer topic hierarchies from a large scale document corpus. A key feature of the proposed algorithm is that it exploits coarse grained parallelism and the components running in parallel need not synchronize after every iteration, thus the algorithm lends itself to be implemented on a geographically dispersed set of processing elements interconnected through a network. The parallel algorithm realizes a speed up of 53.5 on a 32-node cluster of dual-core workstations and at the same time achieving approximately the same likelihood or predictive accuracy as that of the sequential algorithm, with respect to the performance of Information Retrieval tasks."
422,"Topic models are a well known clustering approach for textual data, which provides promising applications in the bibliometric context for the purpose of discovering scientific topics and trends in a corpus of scientific publications. However, topic models per se provide poorly descriptive metadata featuring the discovered clusters of publications and they are not related to the other important metadata usually available with publications, such as authors affiliation, publication venue, and publication year. In this paper, we propose a methodological approach to topic modeling and post-processing of topic models results to the end of describing in depth a field of research over time. In particular, we work on a selection of publications from the international statistical literature, we propose an approach that allows us to identify sophisticated topic descriptors, and we analyze the links between topics and their temporal evolution.",2015-05-01,2-s2.0-84936928849,Scientometrics,A decade of research in statistics: a topic model approach,"Topic models are a well known clustering approach for textual data, which provides promising applications in the bibliometric context for the purpose of discovering scientific topics and trends in a corpus of scientific publications. However, topic models per se provide poorly descriptive metadata featuring the discovered clusters of publications and they are not related to the other important metadata usually available with publications, such as authors affiliation, publication venue, and publication year. In this paper, we propose a methodological approach to topic modeling and post-processing of topic models results to the end of describing in depth a field of research over time. In particular, we work on a selection of publications from the international statistical literature, we propose an approach that allows us to identify sophisticated topic descriptors, and we analyze the links between topics and their temporal evolution."
423,"Predicting the duration of traffic incidents sequentially during the incident clearance period is helpful in deploying efficient measures and minimizing traffic congestion related to such incidents. This study proposes a competing risk mixture hazard-based model to analyze the effect of various factors on traffic incident duration and predict the duration sequentially. First, topic modeling, a text analysis technique, is used to process the textual features of the traffic incident to extract time-dependent topics. Given four specific clearance methods and the uncertainty of these methods when used during traffic incidents, the proposed mixture model uses the multinomial logistic model and parametric hazard-based model to assess the influence of covariates on the probability of clearance methods and on the duration of the incident. Subsequently, the performance of estimated mixture model in sequentially predicting the incident duration is compared with that of the non-mixture model. The prediction results show that the presented mixture model outperforms the non-mixture model.",2015-05-01,2-s2.0-84925456603,Transportation Research Part C: Emerging Technologies,Competing risk mixture model and text analysis for sequential incident duration prediction,"Predicting the duration of traffic incidents sequentially during the incident clearance period is helpful in deploying efficient measures and minimizing traffic congestion related to such incidents. This study proposes a competing risk mixture hazard-based model to analyze the effect of various factors on traffic incident duration and predict the duration sequentially. First, topic modeling, a text analysis technique, is used to process the textual features of the traffic incident to extract time-dependent topics. Given four specific clearance methods and the uncertainty of these methods when used during traffic incidents, the proposed mixture model uses the multinomial logistic model and parametric hazard-based model to assess the influence of covariates on the probability of clearance methods and on the duration of the incident. Subsequently, the performance of estimated mixture model in sequentially predicting the incident duration is compared with that of the non-mixture model. The prediction results show that the presented mixture model outperforms the non-mixture model."
424,"In this work, two simple methods of tagging scientific publications with labels reflecting their content are presented and compared. As a first source of labels, Wikipedia is employed. A second label set is constructed from the noun phrases occurring in the analyzed corpus. The corpus itself consists of abstracts from 0.7 million scientific documents deposited in the ArXiv preprint collection. We present a comparison of both approaches, which shows that discussed methods are to a large extent complementary. Moreover, the results give interesting insights into the completeness of Wikipedia knowledge in various scientific domains. As a next step, we examine the statistical properties of the obtained tags. It turns out that both methods show qualitatively similar rank–frequency dependence, which is best approximated by the stretched exponential curve. The distribution of the number of distinct tags per document follows also the same distribution for both methods and is well described by the negative binomial distribution. The developed tags are meant for use as features in various text mining tasks. Therefore, as a final step we show the preliminary results on their application to topic modeling.",2015-04-21,2-s2.0-84908284247,International Journal on Digital Libraries,Towards robust tags for scientific publications from natural language processing tools and Wikipedia,"In this work, two simple methods of tagging scientific publications with labels reflecting their content are presented and compared. As a first source of labels, Wikipedia is employed. A second label set is constructed from the noun phrases occurring in the analyzed corpus. The corpus itself consists of abstracts from 0.7 million scientific documents deposited in the ArXiv preprint collection. We present a comparison of both approaches, which shows that discussed methods are to a large extent complementary. Moreover, the results give interesting insights into the completeness of Wikipedia knowledge in various scientific domains. As a next step, we examine the statistical properties of the obtained tags. It turns out that both methods show qualitatively similar rank–frequency dependence, which is best approximated by the stretched exponential curve. The distribution of the number of distinct tags per document follows also the same distribution for both methods and is well described by the negative binomial distribution. The developed tags are meant for use as features in various text mining tasks. Therefore, as a final step we show the preliminary results on their application to topic modeling."
425,"We explore the double-edged sword of recombination in generating breakthrough innovation: recombination of distant or diverse knowledge is needed because knowledge in a narrow domain might trigger myopia, but recombination can be counterproductive when local search is needed to identify anomalies. We take into account how creativity shapes both the cognitive novelty of the idea and the subsequent realization of economic value. We develop a text-based measure of novel ideas in patents using topic modeling to identify those patents that originate new topics in a body of knowledge. We find that, counter to theories of recombination, patents that originate new topics are more likely to be associated with local search, while economic value is the product of broader recombinations as well as novelty.",2015-01-01,2-s2.0-84941187958,Strategic Management Journal,The double-edged sword of recombination in breakthrough innovation,"We explore the double-edged sword of recombination in generating breakthrough innovation: recombination of distant or diverse knowledge is needed because knowledge in a narrow domain might trigger myopia, but recombination can be counterproductive when local search is needed to identify anomalies. We take into account how creativity shapes both the cognitive novelty of the idea and the subsequent realization of economic value. We develop a text-based measure of novel ideas in patents using topic modeling to identify those patents that originate new topics in a body of knowledge. We find that, counter to theories of recombination, patents that originate new topics are more likely to be associated with local search, while economic value is the product of broader recombinations as well as novelty."
426,"This article reviews 40 years of the Journal of Consumer Research (JCR). Using text mining, we uncover the key phrases associated with consumer research. We use a topic modeling procedure to uncover 16 topics that have been featured in the journal since its inception and to show the trends in topics over time. For example, we highlight the decline in family decision-making research and the flourishing of social identity and influence research since the journal’s inception. A citation analysis shows which JCR articles have had the most impact and compares the topics in top-cited articles with all JCR journal articles. We show that methodological and consumer culture articles tend to be heavily cited. We conclude by investigating the scholars who have been the top contributors to the journal across the four decades of its existence. And to better understand which schools have contributed most to the knowledge of consumer research over this history, we provide an analysis of where these top-performing scholars were trained. Our approach shows that the JCR archives can be an excellent source of data for scholars trying to understand the complicated, challenging, and dynamic field of consumer research.",2015-01-01,2-s2.0-84936752613,Journal of Consumer Research,The journal of consumer research at 40: A historical analysis,"This article reviews 40 years of the Journal of Consumer Research (JCR). Using text mining, we uncover the key phrases associated with consumer research. We use a topic modeling procedure to uncover 16 topics that have been featured in the journal since its inception and to show the trends in topics over time. For example, we highlight the decline in family decision-making research and the flourishing of social identity and influence research since the journal’s inception. A citation analysis shows which JCR articles have had the most impact and compares the topics in top-cited articles with all JCR journal articles. We show that methodological and consumer culture articles tend to be heavily cited. We conclude by investigating the scholars who have been the top contributors to the journal across the four decades of its existence. And to better understand which schools have contributed most to the knowledge of consumer research over this history, we provide an analysis of where these top-performing scholars were trained. Our approach shows that the JCR archives can be an excellent source of data for scholars trying to understand the complicated, challenging, and dynamic field of consumer research."
427,"Studies have shown that perceptual maps derived from online consumer-generated data are effective for depicting market structure such as demonstrating positioning of competitive brands. However, most text mining algorithms would require manual reading to merge extracted product features with synonyms. In response, Topic modeling is introduced to group synonyms together under a topic automatically, leading to convenient and accurate evaluation of brands based on consumers' online reviews. To ensure the feasibility of employing Topic modeling in assessing competitive brands, we developed a unique and novel framework named WVAP (Weights from Valid Posterior Probability) based on Scree plot technique. WVAP can filter the noises in posterior distribution obtained from Topic modeling, and improve accuracy in brand evaluation. A case study exploring online reviews of mobile phones is conducted. We extract topics to reflect the features of the cell phones with a qualified validity. In addition to perceptual maps derived by multi-dimensional scaling (MDS) for product positioning, we also rank these products by TOPSIS (Technique for Order Performance by Similarity to Ideal Solution) so as to visualize the market structure from different perspectives. Our case study of cell phones shows that the proposed framework is effective in mining online reviews and providing insights into the competitive landscape.",2015-01-01,2-s2.0-84925010368,Electronic Commerce Research and Applications,"Visualizing market structure through online product reviews: Integrate topic modeling, TOPSIS, and multi-dimensional scaling approaches","Studies have shown that perceptual maps derived from online consumer-generated data are effective for depicting market structure such as demonstrating positioning of competitive brands. However, most text mining algorithms would require manual reading to merge extracted product features with synonyms. In response, Topic modeling is introduced to group synonyms together under a topic automatically, leading to convenient and accurate evaluation of brands based on consumers' online reviews. To ensure the feasibility of employing Topic modeling in assessing competitive brands, we developed a unique and novel framework named WVAP (Weights from Valid Posterior Probability) based on Scree plot technique. WVAP can filter the noises in posterior distribution obtained from Topic modeling, and improve accuracy in brand evaluation. A case study exploring online reviews of mobile phones is conducted. We extract topics to reflect the features of the cell phones with a qualified validity. In addition to perceptual maps derived by multi-dimensional scaling (MDS) for product positioning, we also rank these products by TOPSIS (Technique for Order Performance by Similarity to Ideal Solution) so as to visualize the market structure from different perspectives. Our case study of cell phones shows that the proposed framework is effective in mining online reviews and providing insights into the competitive landscape."
428,The FRA railroad grade crossing accident database contains text comment fields that may provide additional information about grade crossing accidents. New text mining algorithms provide the potential to automatically extract information from text that can enhance traditional numeric analyses. Topic modeling algorithms are statistical methods that analyze the words of original texts to automatically discover the themes that run through them. A frequently used topic-modeling algorithm is Latent Dirichlet Analysis (LDA). In this paper we will show several examples of how labeled LDA can be applied to the FRA grade crossing data to better understand categories of words and phrases that are associated with various types of grade crossing accidents.,2015-01-01,2-s2.0-84936818134,"2015 Joint Rail Conference, JRC 2015",Applying topic modeling to railroad grade crossing accident report text,The FRA railroad grade crossing accident database contains text comment fields that may provide additional information about grade crossing accidents. New text mining algorithms provide the potential to automatically extract information from text that can enhance traditional numeric analyses. Topic modeling algorithms are statistical methods that analyze the words of original texts to automatically discover the themes that run through them. A frequently used topic-modeling algorithm is Latent Dirichlet Analysis (LDA). In this paper we will show several examples of how labeled LDA can be applied to the FRA grade crossing data to better understand categories of words and phrases that are associated with various types of grade crossing accidents.
429,"People often share their opinions or impressions about TV shows (e.g., dramas) with other viewers through social media such as personal blogs and Twitter. As such, broadcast media, especially TV, lead to audience engagement on social media. Moreover, the audience engagement, in turn, impacts broadcast media ratings. Social TV analyzes audience's TV-related social media behaviors and tries to use the behaviors in marketing activities such as advertisement; however, this is purely based on the quantity o f engagement in social media. In this study, we analyze the subjects of the audience engagement on social media about specific TV dramas through topic modeling, and examines the relationship between changes in the topics and viewer ratings of the TV dramas.",2015-01-01,2-s2.0-84959256115,Proceedings of the International Conference on Electronic Business (ICEB),Relationship between audience engagement on social media and broadcast media ratings,"People often share their opinions or impressions about TV shows (e.g., dramas) with other viewers through social media such as personal blogs and Twitter. As such, broadcast media, especially TV, lead to audience engagement on social media. Moreover, the audience engagement, in turn, impacts broadcast media ratings. Social TV analyzes audience's TV-related social media behaviors and tries to use the behaviors in marketing activities such as advertisement; however, this is purely based on the quantity o f engagement in social media. In this study, we analyze the subjects of the audience engagement on social media about specific TV dramas through topic modeling, and examines the relationship between changes in the topics and viewer ratings of the TV dramas."
430,"In this paper, we investigate the challenging task of understanding short text (STU task) by jointly considering topic modeling and knowledge incorporation. Knowledge incorporation can solve the content sparsity problem effectively for topic modeling. Specifically, the phrase topic model is proposed to leverage the auto-mined knowledge, i.e., the phrases, to guide the generative process of short text. Experimental results illustrate the effectiveness of the mechanism that utilizes knowledge to improve topic modeling's performance.",2015-01-01,2-s2.0-84960089788,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",Short text understanding by leveraging knowledge into topic model,"In this paper, we investigate the challenging task of understanding short text (STU task) by jointly considering topic modeling and knowledge incorporation. Knowledge incorporation can solve the content sparsity problem effectively for topic modeling. Specifically, the phrase topic model is proposed to leverage the auto-mined knowledge, i.e., the phrases, to guide the generative process of short text. Experimental results illustrate the effectiveness of the mechanism that utilizes knowledge to improve topic modeling's performance."
431,"This study explores relationship between the Internet and the Russian national election of 2011–2012. In contrast to other studies, we focus on the blogosphere as a political factor. Our conclusions are based on a study of the LiveJournal blogging platform represented by a sample of political posts from the top 2000 bloggers for 13-week-long periods. Sampling from the population of about 180,000 posts was performed automatically with a topic modelling algorithm, while the analysis of the resulting 3690 texts was carried out manually by five coders. We found that the most influential Russian blogs perform the role of a media ‘stronghold’ of the political opposition. Moreover, we established a relationship between the weekly pre-election ratings of the opposition parties and presidential candidates and the indicators of political activity in the blogosphere. Our results cautiously suggest that political activity on the Internet is not simply an online projection of offline political activity: it can itself provoke activity in offline political life.",2015-01-01,2-s2.0-84942944543,New Media and Society,‘LiveJournal Libra!’: The political blogosphere and voting preferences in Russia in 2011–2012,"This study explores relationship between the Internet and the Russian national election of 2011–2012. In contrast to other studies, we focus on the blogosphere as a political factor. Our conclusions are based on a study of the LiveJournal blogging platform represented by a sample of political posts from the top 2000 bloggers for 13-week-long periods. Sampling from the population of about 180,000 posts was performed automatically with a topic modelling algorithm, while the analysis of the resulting 3690 texts was carried out manually by five coders. We found that the most influential Russian blogs perform the role of a media ‘stronghold’ of the political opposition. Moreover, we established a relationship between the weekly pre-election ratings of the opposition parties and presidential candidates and the indicators of political activity in the blogosphere. Our results cautiously suggest that political activity on the Internet is not simply an online projection of offline political activity: it can itself provoke activity in offline political life."
432,"This study examines the effects of organizational attention on technological search in the multibusiness firm. We argue that attentional specialization and coupling, or (respectively) attention given to problems within and across units, affect a unit's ability to engage in distant and local search by shaping how problems are perceived and addressed. We test this theory by applying a probabilistic topic model to all Motorola patents issued from 1974 to 1997, thus identifying and measuring attention to technical problems. Our results suggest that (a) subunits with specialized attention are not myopic but instead explore broadly and (b) tight attentional coupling across units increases the breadth of search. This study contributes to attention-based views of the firm and to studies on organizational design and search.",2015-01-01,2-s2.0-84939780271,Advances in Strategic Management,Organizational attention and technological search in the multibusiness firm: Motorola from 1974 to 1997,"This study examines the effects of organizational attention on technological search in the multibusiness firm. We argue that attentional specialization and coupling, or (respectively) attention given to problems within and across units, affect a unit's ability to engage in distant and local search by shaping how problems are perceived and addressed. We test this theory by applying a probabilistic topic model to all Motorola patents issued from 1974 to 1997, thus identifying and measuring attention to technical problems. Our results suggest that (a) subunits with specialized attention are not myopic but instead explore broadly and (b) tight attentional coupling across units increases the breadth of search. This study contributes to attention-based views of the firm and to studies on organizational design and search."
433,"Patent classification systems and citation networks are used extensively in innovation studies. However, non-unique mapping of classification codes onto specific products/markets and the difficulties in accurately capturing knowledge flows based just on citation linkages present limitations to these conventional patent analysis approaches. We present a natural language processing based hierarchical technique that enables the automatic identification and classification of patent datasets into technology areas and sub-areas. The key novelty of our technique is to use topic modeling to map patents to probability distributions over real world categories/topics. Accuracy and usefulness of our technique are tested on a dataset of 10,201 patents in solar photovoltaics filed in the United States Patent and Trademark Office (USPTO) between 2002 and 2013. We show that linguistic features from topic models can be used to effectively identify the main technology area that a patent's invention applies to. Our computational experiments support the view that the topic distribution of a patent offers a reduced-form representation of the knowledge content in a patent. Accordingly, we suggest that this hidden thematic structure in patents can be useful in studies of the policy-innovation-geography nexus. To that end, we also demonstrate an application of our technique for identifying patterns in technological convergence.",2015-01-01,2-s2.0-84954123056,Technological Forecasting and Social Change,Topic based classification and pattern identification in patents,"Patent classification systems and citation networks are used extensively in innovation studies. However, non-unique mapping of classification codes onto specific products/markets and the difficulties in accurately capturing knowledge flows based just on citation linkages present limitations to these conventional patent analysis approaches. We present a natural language processing based hierarchical technique that enables the automatic identification and classification of patent datasets into technology areas and sub-areas. The key novelty of our technique is to use topic modeling to map patents to probability distributions over real world categories/topics. Accuracy and usefulness of our technique are tested on a dataset of 10,201 patents in solar photovoltaics filed in the United States Patent and Trademark Office (USPTO) between 2002 and 2013. We show that linguistic features from topic models can be used to effectively identify the main technology area that a patent's invention applies to. Our computational experiments support the view that the topic distribution of a patent offers a reduced-form representation of the knowledge content in a patent. Accordingly, we suggest that this hidden thematic structure in patents can be useful in studies of the policy-innovation-geography nexus. To that end, we also demonstrate an application of our technique for identifying patterns in technological convergence."
434,"Micro-blogging services and location-based social networks, such as Twitter, Weibo, and Foursquare, enable users to post short messages with timestamps and geographical annotations. The rich spatial-temporalsemantic information of individuals embedded in these geo-annotated short messages provides exciting opportunity to develop many context-aware applications in ubiquitous computing environments. Example applications include contextual recommendation and contextual search. To obtain accurate recommendations and most relevant search results, it is important to capture users' contextual information (e.g., time and location) and to understand users' topical interests and intentions. While time and location can be readily captured by smartphones, understanding user's interests and intentions calls for effective methods in modeling user mobility behavior. Here, user mobility refers to who visits which place at what time for what activity. That is, user mobility behavior modeling must consider user (Who), spatial (Where), temporal (When), and activity (What) aspects. Unfortunately, no previous studies on user mobility behavior modeling have considered all of the four aspects jointly, which have complex interdependencies. In our preliminary study, we propose the first solution named W",2015-01-01,2-s2.0-84923814459,ACM Transactions on Information Systems,"Who, where, when, and what: A nonparametric Bayesian approach to context-aware recommendation and search for Twitter users","Micro-blogging services and location-based social networks, such as Twitter, Weibo, and Foursquare, enable users to post short messages with timestamps and geographical annotations. The rich spatial-temporalsemantic information of individuals embedded in these geo-annotated short messages provides exciting opportunity to develop many context-aware applications in ubiquitous computing environments. Example applications include contextual recommendation and contextual search. To obtain accurate recommendations and most relevant search results, it is important to capture users' contextual information (e.g., time and location) and to understand users' topical interests and intentions. While time and location can be readily captured by smartphones, understanding user's interests and intentions calls for effective methods in modeling user mobility behavior. Here, user mobility refers to who visits which place at what time for what activity. That is, user mobility behavior modeling must consider user (Who), spatial (Where), temporal (When), and activity (What) aspects. Unfortunately, no previous studies on user mobility behavior modeling have considered all of the four aspects jointly, which have complex interdependencies. In our preliminary study, we propose the first solution named W"
435,"In recent years, topic modeling is gaining significant momentum in information retrieval (IR). Researchers have found that utilizing the topic information generated through topic modeling together with traditional TF-IDF information generates superior results in document retrieval. However, in order to apply this idea to real-life IR systems, some critical problems need to be solved: how to store the topic information and how to utilize it with the TF-IDF information for efficient document retrieval. In this paper, we propose the Topic Enhanced Inverted Index (TEII) to incorporate the topic information into the inverted index for efficient top-k document retrieval. Specifically, we explore two different types of TEIIs. We first propose the incremental TEII, which includes the topic information into the traditional inverted index by adding topic-based inverted lists. The incremental TEII is beneficial for legacy IR systems, since it does not change the existing TF-IDF-based inverted lists. As a more flexible alternative, we propose the hybrid TEII to incorporate the topic information into each posting of the inverted index. In the hybrid TEII, two relaxation methods are proposed to support dynamic estimation of the upper bound impact of each posting. The hybrid TEII is highly extensible for incorporating different ranking factors and we show an extension of the hybrid TEII by considering the static quality of the documents in the corpus. Based on the incremental and hybrid TEIIs, we develop several query processing algorithms to support efficient top-k document retrieval on TEIIs. Empirical evaluation on the TREC dataset verifies the effectiveness and efficiency of the proposed index structures and query processing algorithms.",2015-01-01,2-s2.0-84944352133,Knowledge-Based Systems,TEII: Topic enhanced inverted index for top-k document retrieval,"In recent years, topic modeling is gaining significant momentum in information retrieval (IR). Researchers have found that utilizing the topic information generated through topic modeling together with traditional TF-IDF information generates superior results in document retrieval. However, in order to apply this idea to real-life IR systems, some critical problems need to be solved: how to store the topic information and how to utilize it with the TF-IDF information for efficient document retrieval. In this paper, we propose the Topic Enhanced Inverted Index (TEII) to incorporate the topic information into the inverted index for efficient top-k document retrieval. Specifically, we explore two different types of TEIIs. We first propose the incremental TEII, which includes the topic information into the traditional inverted index by adding topic-based inverted lists. The incremental TEII is beneficial for legacy IR systems, since it does not change the existing TF-IDF-based inverted lists. As a more flexible alternative, we propose the hybrid TEII to incorporate the topic information into each posting of the inverted index. In the hybrid TEII, two relaxation methods are proposed to support dynamic estimation of the upper bound impact of each posting. The hybrid TEII is highly extensible for incorporating different ranking factors and we show an extension of the hybrid TEII by considering the static quality of the documents in the corpus. Based on the incremental and hybrid TEIIs, we develop several query processing algorithms to support efficient top-k document retrieval on TEIIs. Empirical evaluation on the TREC dataset verifies the effectiveness and efficiency of the proposed index structures and query processing algorithms."
436,"The essential work of feature-specific opinion mining is centered on the product features. Previous related research work has often taken into account explicit features but ignored implicit features, However, implicit feature identification, which can help us better understand the reviews, is an essential aspect of feature-specific opinion mining. This paper is mainly centered on implicit feature identification in Chinese product reviews. We think that based on the explicit synonymous feature group and the sentences which contain explicit features, several Support Vector Machine (SVM) classifiers can be established to classify the non-explicit sentences. Nevertheless, instead of simply using traditional feature selection methods, we believe an explicit topic model in which each topic is pre-defined could perform better. In this paper, we first extend a popular topic modeling method, called Latent Dirichlet Allocation (LDA), to construct an explicit topic model. Then some types of prior knowledge, such as: must-links, cannot-links and relevance-based prior knowledge, are extracted and incorporated into the explicit topic model automatically. Experiments show that the explicit topic model, which incorporates pre-existing knowledge, outperforms traditional feature selection methods and other existing methods by a large margin and the identification task can be completed better.",2015-01-01,2-s2.0-84923080938,Knowledge-Based Systems,Implicit feature identification in Chinese reviews using explicit topic mining model,"The essential work of feature-specific opinion mining is centered on the product features. Previous related research work has often taken into account explicit features but ignored implicit features, However, implicit feature identification, which can help us better understand the reviews, is an essential aspect of feature-specific opinion mining. This paper is mainly centered on implicit feature identification in Chinese product reviews. We think that based on the explicit synonymous feature group and the sentences which contain explicit features, several Support Vector Machine (SVM) classifiers can be established to classify the non-explicit sentences. Nevertheless, instead of simply using traditional feature selection methods, we believe an explicit topic model in which each topic is pre-defined could perform better. In this paper, we first extend a popular topic modeling method, called Latent Dirichlet Allocation (LDA), to construct an explicit topic model. Then some types of prior knowledge, such as: must-links, cannot-links and relevance-based prior knowledge, are extracted and incorporated into the explicit topic model automatically. Experiments show that the explicit topic model, which incorporates pre-existing knowledge, outperforms traditional feature selection methods and other existing methods by a large margin and the identification task can be completed better."
437,"Social network analysis (SNA) has been effectively used in counter-terrorism analysis by generating homogeneous network. In this paper, we consider a large dataset reporting various terrorist attacks over the globe and represent the dataset as a heterogeneous network. The objective of this paper is to the explore the effect of various link prediction frameworks such as topic modeling, network topology and graph kernels. We propose bipartite based link prediction over topic feature relationship, heterogeneous version of node proximity based link prediction and graph kernel methods. From various experimental observation, it is evident that bipartite method based on topic modeling also return comparable results (sometimes better) as that of node proximity and graph kernel.",2015-01-01,2-s2.0-84973882827,"Proceedings - 2015 IEEE International Conference on Smart City, SmartCity 2015, Held Jointly with 8th IEEE International Conference on Social Computing and Networking, SocialCom 2015, 5th IEEE International Conference on Sustainable Computing and Communications, SustainCom 2015, 2015 International Conference on Big Data Intelligence and Computing, DataCom 2015, 5th International Symposium on Cloud and Service Computing, SC2 2015",Link prediction using social network analysis over heterogeneous terrorist network,"Social network analysis (SNA) has been effectively used in counter-terrorism analysis by generating homogeneous network. In this paper, we consider a large dataset reporting various terrorist attacks over the globe and represent the dataset as a heterogeneous network. The objective of this paper is to the explore the effect of various link prediction frameworks such as topic modeling, network topology and graph kernels. We propose bipartite based link prediction over topic feature relationship, heterogeneous version of node proximity based link prediction and graph kernel methods. From various experimental observation, it is evident that bipartite method based on topic modeling also return comparable results (sometimes better) as that of node proximity and graph kernel."
438,"In this paper we present an approach to Word Sense Disambiguation based on Topic Modeling (LDA). Our approach consists of two different steps, where first a binary classifier is applied to decide whether the most frequent sense applies or not, and then another classifier deals with the non most frequent sense cases. An exhaustive evaluation is performed on the Spanish corpus Ancora, to analyze the performance of our two-step system and the impact of the context and the different parameters in the system. Our best experiment reaches an accuracy of 74.53, which is 6 points over the highest baseline. All the software developed for these experiments has been made freely available, to enable reproducibility and allow the re-usage of the software.",2015-01-01,2-s2.0-84941214834,Procesamiento de Lenguaje Natural,Topic modeling and word sense disambiguation on the Ancora corpus,"In this paper we present an approach to Word Sense Disambiguation based on Topic Modeling (LDA). Our approach consists of two different steps, where first a binary classifier is applied to decide whether the most frequent sense applies or not, and then another classifier deals with the non most frequent sense cases. An exhaustive evaluation is performed on the Spanish corpus Ancora, to analyze the performance of our two-step system and the impact of the context and the different parameters in the system. Our best experiment reaches an accuracy of 74.53, which is 6 points over the highest baseline. All the software developed for these experiments has been made freely available, to enable reproducibility and allow the re-usage of the software."
439,"Front-page news selection is the task of finding important news articles in news aggregators. In this study, we examine news selection for public front pages using raw text, without any meta-attributes such as click counts. A novel algorithm is introduced by jointly considering the importance and diversity of selected news articles and the length of front pages. We estimate the importance of news, based on topic modelling, to provide the required diversity. Then we select important documents from important topics using a priority-based method that helps in fitting news content into the length of the front page. A user study is subsequently conducted to measure effectiveness and diversity, using our newly-generated annotation program. Annotation results show that up to seven of 10 news articles are important and up to nine of them are from different topics. Challenges in selecting public front-page news are addressed with an emphasis on future research.",2015-01-01,2-s2.0-84942106504,Journal of Information Science,A front-page news-selection algorithm based on topic modelling using raw text,"Front-page news selection is the task of finding important news articles in news aggregators. In this study, we examine news selection for public front pages using raw text, without any meta-attributes such as click counts. A novel algorithm is introduced by jointly considering the importance and diversity of selected news articles and the length of front pages. We estimate the importance of news, based on topic modelling, to provide the required diversity. Then we select important documents from important topics using a priority-based method that helps in fitting news content into the length of the front page. A user study is subsequently conducted to measure effectiveness and diversity, using our newly-generated annotation program. Annotation results show that up to seven of 10 news articles are important and up to nine of them are from different topics. Challenges in selecting public front-page news are addressed with an emphasis on future research."
440,"A number of algorithms exist in measuring clothing similarity for clothing recommendations in E-commerce. The clothing similarity mostly depends on its shape, texture and style. In this paper we introduce three models of defining feature space for clothing recommendations. The sketch-based image search mainly focuses on defining similarity of clothing in contour dimension. The spatial bagof-feature approach is employed to measure the clothing similarity of local image patterns. Finally, we introduce a query adaptive shape model which combines shape characteristics and labels of clothing, in order to take the semantic information of clothing. A large number of simulations are given to show the feasibility and performance of the clothing recommendations by using content-based image search.",2015-01-01,2-s2.0-84943811019,Multimedia Data Mining and Analytics: Disruptive Innovation,Content based image search for clothing recommendations in E-Commerce,"A number of algorithms exist in measuring clothing similarity for clothing recommendations in E-commerce. The clothing similarity mostly depends on its shape, texture and style. In this paper we introduce three models of defining feature space for clothing recommendations. The sketch-based image search mainly focuses on defining similarity of clothing in contour dimension. The spatial bagof-feature approach is employed to measure the clothing similarity of local image patterns. Finally, we introduce a query adaptive shape model which combines shape characteristics and labels of clothing, in order to take the semantic information of clothing. A large number of simulations are given to show the feasibility and performance of the clothing recommendations by using content-based image search."
441,"Boosting algorithms have received significant attention over the past several years and are considered to be the state-of-the-art classifiers for multi-label classification tasks. The disadvantage of using boosting algorithms for text categorization (TC) is the vast number of features that are generated using the traditional Bag-of-Words (BOW) text representation, which dramatically increases the computational complexity. In this paper, an alternative text representation method using topic modeling for enhancing and accelerating multi-label boosting algorithms is concerned. An extensive empirical experimental comparison of eight multi-label boosting algorithms using topic-based and BOW representation methods was undertaken. For the evaluation, three well-known multi-label TC datasets were used. Furthermore, to justify boosting algorithms performance, three well-known instance-based multi-label algorithms were involved in the evaluation. For completely credible evaluations, all algorithms were evaluated using their native software tools, except for data formats and user settings. The experimental results demonstrated that the topic-based representation significantly accelerated all algorithms and slightly enhanced the classification performance, especially for near-balanced and balanced datasets. For the imbalanced dataset, BOW representation led to the best performance. The MP-Boost algorithm is the most efficient and effective algorithm for imbalanced datasets using BOW representation. For topic-based representation, AdaBoost.MH with meta base learners, Hamming Tree (AdaMH-Tree) and Product (AdaMH-Product) achieved the best performance; however, with respect to the computational time, these algorithms are the slowest overall. Moreover, the results indicated that topic-based representation is more significant for instance-based algorithms; nevertheless, boosting algorithms, such as MP-Boost, AdaMH-Tree and AdaMH-Product, notably exceed their performance.",2015-01-01,2-s2.0-84942085806,Journal of Information Science,Boosting algorithms with topic modeling for multi-label text categorization: A comparative empirical study,"Boosting algorithms have received significant attention over the past several years and are considered to be the state-of-the-art classifiers for multi-label classification tasks. The disadvantage of using boosting algorithms for text categorization (TC) is the vast number of features that are generated using the traditional Bag-of-Words (BOW) text representation, which dramatically increases the computational complexity. In this paper, an alternative text representation method using topic modeling for enhancing and accelerating multi-label boosting algorithms is concerned. An extensive empirical experimental comparison of eight multi-label boosting algorithms using topic-based and BOW representation methods was undertaken. For the evaluation, three well-known multi-label TC datasets were used. Furthermore, to justify boosting algorithms performance, three well-known instance-based multi-label algorithms were involved in the evaluation. For completely credible evaluations, all algorithms were evaluated using their native software tools, except for data formats and user settings. The experimental results demonstrated that the topic-based representation significantly accelerated all algorithms and slightly enhanced the classification performance, especially for near-balanced and balanced datasets. For the imbalanced dataset, BOW representation led to the best performance. The MP-Boost algorithm is the most efficient and effective algorithm for imbalanced datasets using BOW representation. For topic-based representation, AdaBoost.MH with meta base learners, Hamming Tree (AdaMH-Tree) and Product (AdaMH-Product) achieved the best performance; however, with respect to the computational time, these algorithms are the slowest overall. Moreover, the results indicated that topic-based representation is more significant for instance-based algorithms; nevertheless, boosting algorithms, such as MP-Boost, AdaMH-Tree and AdaMH-Product, notably exceed their performance."
442,"This article aims to explain the widespread attention to contemporary protesting artists among Western audiences by focusing on the case of Pussy Riot. Social movement scholarship provides a first step into understanding how Pussy Riot legitimately protests Russian politics through its punk performances. It then turns to the concept of cosmopolitanism as a performance in everyday life to explain Pussy Riot's appeal among Western audiences. By collecting and analyzing 9001 tweets through a thematic hashtag analysis and topic modeling, this article analyzes how audiences talk about Pussy Riot and shows how Twitter affords users to perform cosmopolitan selves by sharing their ideas and experiences on Pussy Riot with others. Although we distinguish between four types of cosmopolitan selves, the results clearly show Pussy Riot is mainly reflected upon in a media context: Twitter users predominantly talk about Pussy Riot's media appearances rather than readily engage with its explicit political advocacy.",2015-01-01,2-s2.0-84939772902,International Journal of Consumer Studies,Western solidarity with Pussy Riot and the Twittering of cosmopolitan selves,"This article aims to explain the widespread attention to contemporary protesting artists among Western audiences by focusing on the case of Pussy Riot. Social movement scholarship provides a first step into understanding how Pussy Riot legitimately protests Russian politics through its punk performances. It then turns to the concept of cosmopolitanism as a performance in everyday life to explain Pussy Riot's appeal among Western audiences. By collecting and analyzing 9001 tweets through a thematic hashtag analysis and topic modeling, this article analyzes how audiences talk about Pussy Riot and shows how Twitter affords users to perform cosmopolitan selves by sharing their ideas and experiences on Pussy Riot with others. Although we distinguish between four types of cosmopolitan selves, the results clearly show Pussy Riot is mainly reflected upon in a media context: Twitter users predominantly talk about Pussy Riot's media appearances rather than readily engage with its explicit political advocacy."
443,"As news events on the same subject occur, our knowledge about the subject will accumulate and become more comprehensive. In this paper, we formally define the problem of incremental knowledge learning from similar news events on the same subject, where each event consists of a set of news articles reporting about it. The knowledge is represented by a topic hierarchy presenting topics at different levels of granularity. Though topic (hierarchy) mining from text has been researched a lot, incremental learning from similar events remains under developed. In this paper, we propose a scalable two-phase framework to incrementally learn a topic hierarchy for a subject from events on the subject as the events occur. First, we recursively construct a topic hierarchy for each event based on a novel topic model considering the named entities and entity types in news articles. Second, we incrementally merge the topic hierarchies through top-down hierarchical topic alignment. Extensive experimental results on real datasets demonstrate the effectiveness and efficiency of the proposed framework in terms of both qualitative and quantitative measures.",2015-01-01,2-s2.0-84944322844,Knowledge-Based Systems,Incremental learning from news events,"As news events on the same subject occur, our knowledge about the subject will accumulate and become more comprehensive. In this paper, we formally define the problem of incremental knowledge learning from similar news events on the same subject, where each event consists of a set of news articles reporting about it. The knowledge is represented by a topic hierarchy presenting topics at different levels of granularity. Though topic (hierarchy) mining from text has been researched a lot, incremental learning from similar events remains under developed. In this paper, we propose a scalable two-phase framework to incrementally learn a topic hierarchy for a subject from events on the subject as the events occur. First, we recursively construct a topic hierarchy for each event based on a novel topic model considering the named entities and entity types in news articles. Second, we incrementally merge the topic hierarchies through top-down hierarchical topic alignment. Extensive experimental results on real datasets demonstrate the effectiveness and efficiency of the proposed framework in terms of both qualitative and quantitative measures."
444,"Abstract The reviews in social media are produced continuously by a large and uncontrolled number of users. To capture the mixture of sentiment and topics simultaneously in reviews is still a challenging task. In this paper, we present a novel probabilistic model framework based on the non-parametric hierarchical Dirichlet process (HDP) topic model, called non-parametric joint sentiment topic mixture model (NJST), which adds a sentiment level to the HDP topic model and detects sentiment and topics simultaneously from reviews. Then considered the dynamic nature of social media data, we propose dynamic NJST (dNJST) which adds time decay dependencies of historical epochs to the current epochs. Compared with the existing sentiment topic mixture models which are based on latent Dirichlet allocation (LDA), the biggest difference of NJST and dNJST is that they can determine topic number automatically. We implement NJST and dNJST with online variational inference algorithms, and incorporate the sentiment priors of words into NJST and dNJST with HowNet lexicon. The experiment results in some Chinese social media dataset show that dNJST can effectively detect and track dynamic sentiment and topics.",2015-01-01,2-s2.0-84928068815,Knowledge-Based Systems,Dynamic non-parametric joint sentiment topic mixture model,"Abstract The reviews in social media are produced continuously by a large and uncontrolled number of users. To capture the mixture of sentiment and topics simultaneously in reviews is still a challenging task. In this paper, we present a novel probabilistic model framework based on the non-parametric hierarchical Dirichlet process (HDP) topic model, called non-parametric joint sentiment topic mixture model (NJST), which adds a sentiment level to the HDP topic model and detects sentiment and topics simultaneously from reviews. Then considered the dynamic nature of social media data, we propose dynamic NJST (dNJST) which adds time decay dependencies of historical epochs to the current epochs. Compared with the existing sentiment topic mixture models which are based on latent Dirichlet allocation (LDA), the biggest difference of NJST and dNJST is that they can determine topic number automatically. We implement NJST and dNJST with online variational inference algorithms, and incorporate the sentiment priors of words into NJST and dNJST with HowNet lexicon. The experiment results in some Chinese social media dataset show that dNJST can effectively detect and track dynamic sentiment and topics."
445,"This paper presents a work carried out by ISPRAS on aspect extraction task at SentiRuEval 2015. Our team submitted one run for Task A and Task B and got best precision for both tasks for all domains among all participants. Our method also showed the best F1-measure for exact aspect term matching for task A for automobile domain and both for Task A and Task B for restaurant domain. The method is based on sequential classification of tokens with SVM. It uses local, global, syntactic-based, GloVe, topic modeling and automatic term recognition features. In this paper we also present evaluation of significance of different feature groups for the task.",2015-01-01,2-s2.0-84952781113,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,A high precision method for aspect extraction in Russian,"This paper presents a work carried out by ISPRAS on aspect extraction task at SentiRuEval 2015. Our team submitted one run for Task A and Task B and got best precision for both tasks for all domains among all participants. Our method also showed the best F1-measure for exact aspect term matching for task A for automobile domain and both for Task A and Task B for restaurant domain. The method is based on sequential classification of tokens with SVM. It uses local, global, syntactic-based, GloVe, topic modeling and automatic term recognition features. In this paper we also present evaluation of significance of different feature groups for the task."
446,"AdaBoost.MH is a boosting algorithm that is considered to be one of the most accurate algorithms for multilabel classification. It works by iteratively building a committee of weak hypotheses of decision stumps. To build the weak hypotheses, in each iteration, AdaBoost.MH obtains the whole extracted features and examines them one by one to check their ability to characterize the appropriate category. Using Bag-Of-Words for text representation dramatically increases the computational time of AdaBoost.MH learning, especially for large-scale datasets. In this paper we demonstrate how to improve the efficiency and effectiveness of AdaBoost.MH using latent topics, rather than words. A well-known probabilistic topic modelling method, Latent Dirichlet Allocation, is used to estimate the latent topics in the corpus as features for AdaBoost.MH. To evaluate LDA-AdaBoost.MH, the following four datasets have been used: Reuters-21578-ModApte, WebKB, 20-Newsgroups and a collection of Arabic news. The experimental results confirmed that representing the texts as a small number of latent topics, rather than a large number of words, significantly decreased the computational time of AdaBoost.MH learning and improved its performance for text categorization.",2015-01-01,2-s2.0-84920861875,Journal of Information Science,LDA-AdaBoost.MH: Accelerated AdaBoost.MH based on latent Dirichlet allocation for text categorization,"AdaBoost.MH is a boosting algorithm that is considered to be one of the most accurate algorithms for multilabel classification. It works by iteratively building a committee of weak hypotheses of decision stumps. To build the weak hypotheses, in each iteration, AdaBoost.MH obtains the whole extracted features and examines them one by one to check their ability to characterize the appropriate category. Using Bag-Of-Words for text representation dramatically increases the computational time of AdaBoost.MH learning, especially for large-scale datasets. In this paper we demonstrate how to improve the efficiency and effectiveness of AdaBoost.MH using latent topics, rather than words. A well-known probabilistic topic modelling method, Latent Dirichlet Allocation, is used to estimate the latent topics in the corpus as features for AdaBoost.MH. To evaluate LDA-AdaBoost.MH, the following four datasets have been used: Reuters-21578-ModApte, WebKB, 20-Newsgroups and a collection of Arabic news. The experimental results confirmed that representing the texts as a small number of latent topics, rather than a large number of words, significantly decreased the computational time of AdaBoost.MH learning and improved its performance for text categorization."
447,"In e-learning environment, more and more larger-scale text resources are generated by teaching–learning interactions. Finding latent topics in these resources can help us understand the teaching contents and the learners’ interests and focuses. Latent Dirichlet allocation (LDA) has been widely used in many areas to extract the latent topics in a text corpus. However, the extracted topics cannot be understood by the end user. Adding more auxiliary information to LDA to guide the process of topic extraction is a good way to improve the interpretability of topic modeling. Co-occurrence information in corpus is such information, but it is not sufficient yet to measure the similarity between word pairs, especially in sparse document space. To deal with this problem, we propose a new semantic similarity-enhanced topic model in this paper. In this model, we use not only co-occurrence information but also the semantic similarity based on WordNet as auxiliary information. Those two kinds of information are combined into a topic-word component though generative Pólya urn model. The distribution of documents over the extracted topics obtained by the new model can be inputted to the classifier. The accuracy of extracting topics can improve the performance of the classifier. Our experiments on newsgroup corpus show that the semantic similarity-enhanced topic model performs better than the topic models with only single information separately.",2015-01-01,2-s2.0-85031741648,Lecture Notes in Educational Technology,Semantic similarity-enhanced topic models for document analysis,"In e-learning environment, more and more larger-scale text resources are generated by teaching–learning interactions. Finding latent topics in these resources can help us understand the teaching contents and the learners’ interests and focuses. Latent Dirichlet allocation (LDA) has been widely used in many areas to extract the latent topics in a text corpus. However, the extracted topics cannot be understood by the end user. Adding more auxiliary information to LDA to guide the process of topic extraction is a good way to improve the interpretability of topic modeling. Co-occurrence information in corpus is such information, but it is not sufficient yet to measure the similarity between word pairs, especially in sparse document space. To deal with this problem, we propose a new semantic similarity-enhanced topic model in this paper. In this model, we use not only co-occurrence information but also the semantic similarity based on WordNet as auxiliary information. Those two kinds of information are combined into a topic-word component though generative Pólya urn model. The distribution of documents over the extracted topics obtained by the new model can be inputted to the classifier. The accuracy of extracting topics can improve the performance of the classifier. Our experiments on newsgroup corpus show that the semantic similarity-enhanced topic model performs better than the topic models with only single information separately."
448,"The present study investigates topic coverage and sentiment dynamics of two different media sources, Twitter and news publications, on the hot health issue of Ebola. We conduct content and sentiment analysis by: (1) applying vocabulary control to collected datasets; (2) employing the n-gram LDA topic modeling technique; (3) adopting entity extraction and entity network; and (4) introducing the concept of topic-based sentiment scores. With the query term 'Ebola' or 'Ebola virus', we collected 16,189 news articles from 1006 different publications and 7,106,297 tweets with the Twitter stream API. The experiments indicate that topic coverage of Twitter is narrower and more blurry than that of the news media. In terms of sentiment dynamics, the life span and variance of sentiment on Twitter is shorter and smaller than in the news. In addition, we observe that news articles focus more on event-related entities such as person, organization and location, whereas Twitter covers more time-oriented entities. Based on the results, we report on the characteristics of Twitter and news media as two distinct news outlets in terms of content coverage and sentiment dynamics.",2015-01-01,2-s2.0-85002368675,Journal of Information Science,Topic-based content and sentiment analysis of Ebola virus on Twitter and in the news,"The present study investigates topic coverage and sentiment dynamics of two different media sources, Twitter and news publications, on the hot health issue of Ebola. We conduct content and sentiment analysis by: (1) applying vocabulary control to collected datasets; (2) employing the n-gram LDA topic modeling technique; (3) adopting entity extraction and entity network; and (4) introducing the concept of topic-based sentiment scores. With the query term 'Ebola' or 'Ebola virus', we collected 16,189 news articles from 1006 different publications and 7,106,297 tweets with the Twitter stream API. The experiments indicate that topic coverage of Twitter is narrower and more blurry than that of the news media. In terms of sentiment dynamics, the life span and variance of sentiment on Twitter is shorter and smaller than in the news. In addition, we observe that news articles focus more on event-related entities such as person, organization and location, whereas Twitter covers more time-oriented entities. Based on the results, we report on the characteristics of Twitter and news media as two distinct news outlets in terms of content coverage and sentiment dynamics."
449,"This paper studies how to incorporate the external word correlation knowledge to improve the coherence of topic modeling. Existing topic models assume words are generated independently and lack the mechanism to utilize the rich similarity relationships among words to learn coherent topics. To solve this problem, we build a Markov Random Field (MRF) regularized Latent Dirichlet Allocation (LDA) model, which defines a MRF on the latent topic layer of LDA to encourage words labeled as similar to share the same topic label. Under our model, the topic assignment of each word is not independent, but rather affected by the topic labels of its correlated words. Similar words have better chance to be put into the same topic due to the regularization of MRF, hence the coherence of topics can be boosted. In addition, our model can accommodate the subtlety that whether two words are similar depends on which topic they appear in, which allows word with multiple senses to be put into different topics properly. We derive a variational inference method to infer the posterior probabilities and learn model parameters and present techniques to deal with the hardto-compute partition function in MRF. Experiments on two datasets demonstrate the effectiveness of our model.",2015-01-01,2-s2.0-84960086722,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",Incorporating word correlation knowledge into topic modeling,"This paper studies how to incorporate the external word correlation knowledge to improve the coherence of topic modeling. Existing topic models assume words are generated independently and lack the mechanism to utilize the rich similarity relationships among words to learn coherent topics. To solve this problem, we build a Markov Random Field (MRF) regularized Latent Dirichlet Allocation (LDA) model, which defines a MRF on the latent topic layer of LDA to encourage words labeled as similar to share the same topic label. Under our model, the topic assignment of each word is not independent, but rather affected by the topic labels of its correlated words. Similar words have better chance to be put into the same topic due to the regularization of MRF, hence the coherence of topics can be boosted. In addition, our model can accommodate the subtlety that whether two words are similar depends on which topic they appear in, which allows word with multiple senses to be put into different topics properly. We derive a variational inference method to infer the posterior probabilities and learn model parameters and present techniques to deal with the hardto-compute partition function in MRF. Experiments on two datasets demonstrate the effectiveness of our model."
450,"Search engine query logs are recognized as an important information source that contains millions of users' web search needs. Discovering Geographic Web Search Topics (G-WSTs) from a query log can support a variety of downstream web applications such as finding commonality between locations and profiling search engine users. However, the task of discovering G-WSTs is nontrivial, not only because of the diversity of the information in web search but also due to the sheer size of query log. In this paper, we propose a new framework, Scalable Geographic Web Search Topic Discovery (SG-WSTD), which contains highly scalable functionalities such as search session derivation, geographic information extraction and geographic web search topic discovery to discover G-WSTs from query log. Within SG-WSTD, two probabilistic topic models are proposed to discover G-WSTs from two complementary perspectives. The first one is the Discrete Search Topic Model (DSTM), which discovers G-WSTs that capture the commonalities between discrete locations. The second is the Regional Search Topic Model (RSTM), which focuses on a specific geographic region on the map and discovers G-WSTs that demonstrate geographic locality. Since query log is typically voluminous, we implement the functionalities in SG-WSTD based on the MapReduce paradigm to solve the efficiency bottleneck. We evaluate SG-WSTD against several strong baselines on a real-life query log from AOL. The proposed framework demonstrates significantly improved data interpretability, better prediction performance, higher topic distinctiveness and superior scalability in the experimentation.",2015-01-01,2-s2.0-84929502247,Knowledge-Based Systems,SG-WSTD: A framework for scalable geographic web search topic discovery,"Search engine query logs are recognized as an important information source that contains millions of users' web search needs. Discovering Geographic Web Search Topics (G-WSTs) from a query log can support a variety of downstream web applications such as finding commonality between locations and profiling search engine users. However, the task of discovering G-WSTs is nontrivial, not only because of the diversity of the information in web search but also due to the sheer size of query log. In this paper, we propose a new framework, Scalable Geographic Web Search Topic Discovery (SG-WSTD), which contains highly scalable functionalities such as search session derivation, geographic information extraction and geographic web search topic discovery to discover G-WSTs from query log. Within SG-WSTD, two probabilistic topic models are proposed to discover G-WSTs from two complementary perspectives. The first one is the Discrete Search Topic Model (DSTM), which discovers G-WSTs that capture the commonalities between discrete locations. The second is the Regional Search Topic Model (RSTM), which focuses on a specific geographic region on the map and discovers G-WSTs that demonstrate geographic locality. Since query log is typically voluminous, we implement the functionalities in SG-WSTD based on the MapReduce paradigm to solve the efficiency bottleneck. We evaluate SG-WSTD against several strong baselines on a real-life query log from AOL. The proposed framework demonstrates significantly improved data interpretability, better prediction performance, higher topic distinctiveness and superior scalability in the experimentation."
451,"This paper proposes a range of solutions to the challenges of extracting large and highquality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivotbased approaches inspired by the state-of the- art technique of bilingual topic modelling, extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an Icelandic- Korean lexicon from Wikipedia.",2015-01-01,2-s2.0-84967016963,"29th Pacific Asia Conference on Language, Information and Computation, PACLIC 2015",Pivot-based topic models for low-resource lexicon extraction,"This paper proposes a range of solutions to the challenges of extracting large and highquality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivotbased approaches inspired by the state-of the- art technique of bilingual topic modelling, extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an Icelandic- Korean lexicon from Wikipedia."
452,"Advances in open-online education have led to a dramatic increase in the size, diversity, and traceability of learner populations, offering tremendous opportunities to study detailed learning behavior of users around the world. This paper adapts the topic modeling approach of Latent Dirichlet Allocation (LDA) to uncover behavioral structure from student logs in a MITx Massive Open Online Course, 8.02x: Electricity and Magnetism. LDA is typically found in the field of natural language processing, where it identifies the latent topic structure within a collection of documents. However, this framework can be adapted for analysis of user-behavioral patterns by considering user interactions with courseware as a ""bag of interactions"" equivalent to the ""bag of words"" model found in topic modeling. By employing this representation, LDA forms probabilistic use cases that clusters students based on their behavior. Through the probability distributions associated with each use case, this approach provides an interpretable representation of user access patterns, while reducing the dimensionality of the data and improving accuracy. Using only the first week of logs, we can predict whether or not a student will earn a certificate with 0.81-0.01 crossvalidation accuracy. Thus, the method presented in this paper is a powerful tool in understanding user behavior and predicting outcomes.",2015-01-01,2-s2.0-84928031842,L@S 2015 - 2nd ACM Conference on Learning at Scale,Probabilistic use cases: Discovering behavioral patterns for predicting certification,"Advances in open-online education have led to a dramatic increase in the size, diversity, and traceability of learner populations, offering tremendous opportunities to study detailed learning behavior of users around the world. This paper adapts the topic modeling approach of Latent Dirichlet Allocation (LDA) to uncover behavioral structure from student logs in a MITx Massive Open Online Course, 8.02x: Electricity and Magnetism. LDA is typically found in the field of natural language processing, where it identifies the latent topic structure within a collection of documents. However, this framework can be adapted for analysis of user-behavioral patterns by considering user interactions with courseware as a ""bag of interactions"" equivalent to the ""bag of words"" model found in topic modeling. By employing this representation, LDA forms probabilistic use cases that clusters students based on their behavior. Through the probability distributions associated with each use case, this approach provides an interpretable representation of user access patterns, while reducing the dimensionality of the data and improving accuracy. Using only the first week of logs, we can predict whether or not a student will earn a certificate with 0.81-0.01 crossvalidation accuracy. Thus, the method presented in this paper is a powerful tool in understanding user behavior and predicting outcomes."
453,"Alzheimer’s disease (AD) is one of degenerative brain diseases, whose cause is hard to be diagnosed accurately. As the number of AD patients has increased, researchers have strived to understand the disease and develop its treatment, such as medical experiments and literature analysis. In the area of literature analysis, several traditional studies analyzed the literature at the macro level like author, journal, and institution. However, analysis of the literature both at the macro level and micro level will allow for better recognizing the AD research field. Therefore, in this study we adopt a more comprehensive approach to analyze the AD literature, which consists of productivity analysis (year, journal/proceeding, author, and Medical Subject Heading terms), network analysis (co-occurrence frequency, centrality, and community) and content analysis. To this end, we collect metadata of 96,081 articles retrieved from PubMed. We specifically perform the concept graph-based network analysis applying the five centrality measures after mapping the semantic relationship between the UMLS concepts from the AD literature. We also analyze the time-series topical trend using the Dirichlet multinomial regression topic modeling technique. The results indicate that the year 2013 is the most productive year and Journal of Alzheimer’s Disease the most productive journal. In discovery of the core biological entities and their relationships resided in the AD related PubMed literature, the relationship with glycogen storage disease is founded most frequently mentioned. In addition, we analyze 16 main topics of the AD literature and find a noticeable increasing trend in the topic of transgenic mouse.",2015-01-01,2-s2.0-84928137003,Scientometrics,Identifying the landscape of alzheimer’s disease research with network and content analysis,"Alzheimer’s disease (AD) is one of degenerative brain diseases, whose cause is hard to be diagnosed accurately. As the number of AD patients has increased, researchers have strived to understand the disease and develop its treatment, such as medical experiments and literature analysis. In the area of literature analysis, several traditional studies analyzed the literature at the macro level like author, journal, and institution. However, analysis of the literature both at the macro level and micro level will allow for better recognizing the AD research field. Therefore, in this study we adopt a more comprehensive approach to analyze the AD literature, which consists of productivity analysis (year, journal/proceeding, author, and Medical Subject Heading terms), network analysis (co-occurrence frequency, centrality, and community) and content analysis. To this end, we collect metadata of 96,081 articles retrieved from PubMed. We specifically perform the concept graph-based network analysis applying the five centrality measures after mapping the semantic relationship between the UMLS concepts from the AD literature. We also analyze the time-series topical trend using the Dirichlet multinomial regression topic modeling technique. The results indicate that the year 2013 is the most productive year and Journal of Alzheimer’s Disease the most productive journal. In discovery of the core biological entities and their relationships resided in the AD related PubMed literature, the relationship with glycogen storage disease is founded most frequently mentioned. In addition, we analyze 16 main topics of the AD literature and find a noticeable increasing trend in the topic of transgenic mouse."
454,"Social Networks became a major actor in information propagation. Using the Twitter popular platform, mobile users post or relay messages from different locations. The tweet content, meaning and location, show how an event-such as the bursty one ""JeSuisCharlie"", happened in France in January 2015, is comprehended in different countries. This research aims at clustering the tweets according to the co-occurrence of their terms, including the country, and forecasting the probable country of a non-located tweet, knowing its content. First, we present the process of collecting a large quantity of data from the Twitter website. We finally have a set of 2,189 located tweets about ""Charlie"", from the 7th to the 14th of January. We describe an original method adapted from the Author-Topic (AT) model based on the Latent Dirichlet Allocation (LDA) method. We define an homogeneous space containing both lexical content (words) and spatial information (country). During a training process on a part of the sample, we provide a set of clusters (topics) based on statistical relations between lexical and spatial terms. During a clustering task, we evaluate the method effectiveness on the rest of the sample that reaches up to 95% of good assignment. It shows that our model is pertinent to foresee tweet location after a learning process.",2015-01-01,2-s2.0-84959352893,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","A topic modeling based representation to detect tweet locations. example of the event ""je suis charlie","Social Networks became a major actor in information propagation. Using the Twitter popular platform, mobile users post or relay messages from different locations. The tweet content, meaning and location, show how an event-such as the bursty one ""JeSuisCharlie"", happened in France in January 2015, is comprehended in different countries. This research aims at clustering the tweets according to the co-occurrence of their terms, including the country, and forecasting the probable country of a non-located tweet, knowing its content. First, we present the process of collecting a large quantity of data from the Twitter website. We finally have a set of 2,189 located tweets about ""Charlie"", from the 7th to the 14th of January. We describe an original method adapted from the Author-Topic (AT) model based on the Latent Dirichlet Allocation (LDA) method. We define an homogeneous space containing both lexical content (words) and spatial information (country). During a training process on a part of the sample, we provide a set of clusters (topics) based on statistical relations between lexical and spatial terms. During a clustering task, we evaluate the method effectiveness on the rest of the sample that reaches up to 95% of good assignment. It shows that our model is pertinent to foresee tweet location after a learning process."
455,"In the context-based image retrieval, the textual information surrounding the image plays a central role for ranking returned results. Although this technique outperforms content-based approaches, it may fail when the query keywords does not match the textual content of many documents containing relevant images. In addition, users are usually not experts and provide ambiguous queries that lead to heterogeneous results. To solve these problems, researchers are trying to re-rank primary results using other techniques such as query expansion, concept-based retrieval, etc. In this paper, we propose to use LDA topic model to re-rank results and therefore improve retrieval precision. We apply this model in two levels: global level represented by the whole document containing the image and local level represented by the paragraph containing an image (considered as a specific textual information for the image). Results show a significant improvement over the standard text retrieval approach by re-ranking with the LDA model applied to the local level.",2015-01-01,2-s2.0-84955284482,Lecture Notes in Business Information Processing,An LDA topic model adaptation for context-based image retrieval,"In the context-based image retrieval, the textual information surrounding the image plays a central role for ranking returned results. Although this technique outperforms content-based approaches, it may fail when the query keywords does not match the textual content of many documents containing relevant images. In addition, users are usually not experts and provide ambiguous queries that lead to heterogeneous results. To solve these problems, researchers are trying to re-rank primary results using other techniques such as query expansion, concept-based retrieval, etc. In this paper, we propose to use LDA topic model to re-rank results and therefore improve retrieval precision. We apply this model in two levels: global level represented by the whole document containing the image and local level represented by the paragraph containing an image (considered as a specific textual information for the image). Results show a significant improvement over the standard text retrieval approach by re-ranking with the LDA model applied to the local level."
456,"Probabilistic topic models are unsupervised generative models which model document content as a two-step generation process, that is, documents are observed as mixtures of latent concepts or topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable texts. We define multilingual probabilistic topic modeling (MuPTM) and present the first full overview of the current research, methodology, advantages and limitations in MuPTM. As a representative example, we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA (BiLDA). We provide a thorough overview of this representative multilingual model from its high-level modeling assumptions down to its mathematical foundations. We demonstrate how to use the data representation by means of output sets of (i) per-topic word distributions and (ii) per-document topic distributions coming from a multilingual probabilistic topic model in various real-life cross-lingual tasks involving different languages, without any external language pair dependent translation resource: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. We also briefly review several other applications present in the relevant literature, and introduce and illustrate two related modeling concepts: topic smoothing and topic pruning. In summary, this article encompasses the current research in multilingual probabilistic topic modeling. By presenting a series of potential applications, we reveal the importance of the language-independent and language pair independent data representations by means of MuPTM. We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross-lingual topics, that is, how to effectively employ learned per-topic word distributions and per-document topic distributions of any multilingual probabilistic topic model in various cross-lingual applications.",2015-01-01,2-s2.0-84908086437,Information Processing and Management,Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications,"Probabilistic topic models are unsupervised generative models which model document content as a two-step generation process, that is, documents are observed as mixtures of latent concepts or topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable texts. We define multilingual probabilistic topic modeling (MuPTM) and present the first full overview of the current research, methodology, advantages and limitations in MuPTM. As a representative example, we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA (BiLDA). We provide a thorough overview of this representative multilingual model from its high-level modeling assumptions down to its mathematical foundations. We demonstrate how to use the data representation by means of output sets of (i) per-topic word distributions and (ii) per-document topic distributions coming from a multilingual probabilistic topic model in various real-life cross-lingual tasks involving different languages, without any external language pair dependent translation resource: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. We also briefly review several other applications present in the relevant literature, and introduce and illustrate two related modeling concepts: topic smoothing and topic pruning. In summary, this article encompasses the current research in multilingual probabilistic topic modeling. By presenting a series of potential applications, we reveal the importance of the language-independent and language pair independent data representations by means of MuPTM. We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross-lingual topics, that is, how to effectively employ learned per-topic word distributions and per-document topic distributions of any multilingual probabilistic topic model in various cross-lingual applications."
457,"In many social networks, people interact based on their interests. Community detection algorithms are then useful to reveal the sub-structures of a network and in particular interest groups. Identifying these users’ communities and the interests that bind them can help us assist their life-cycle. Certain kinds of online communities such as question-and-answer (Q&A) sites, have no explicit social network structure. Therefore, many traditional community detection techniques do not apply directly. In this paper, we propose an efficient approach for extracting topic from Q&A to detect communities of interest. Then we compare three detection methods we applied on a dataset extracted from the popular Q&A site StackOverflow. Our method based on topic modeling and user membership assignment is shown to be much simpler and faster while preserving the quality of the detection.",2015-01-01,2-s2.0-84947275419,Social Network Analysis and Mining,Detecting topics and overlapping communities in question and answer sites,"In many social networks, people interact based on their interests. Community detection algorithms are then useful to reveal the sub-structures of a network and in particular interest groups. Identifying these users’ communities and the interests that bind them can help us assist their life-cycle. Certain kinds of online communities such as question-and-answer (Q&A) sites, have no explicit social network structure. Therefore, many traditional community detection techniques do not apply directly. In this paper, we propose an efficient approach for extracting topic from Q&A to detect communities of interest. Then we compare three detection methods we applied on a dataset extracted from the popular Q&A site StackOverflow. Our method based on topic modeling and user membership assignment is shown to be much simpler and faster while preserving the quality of the detection."
458,"The rise of the Massive Open Online Course (MOOC) has led to its application in Cartographic education. Students in these classes generate enormous amounts of text data in the form of discussion forum posts. Here we explore the topics and geographic references found in over 95,000 forum posts collected during the 2013 launch of Maps and the Geospatial Revolution, a MOOC taught on Coursera. Using Phrase Nets, topic modeling methods, and a named-entity extraction geocoding tool, we show how students describe their use of maps, what key topics drove conversations during the class, and the geography associated with placenames mentioned in posts. These results help shed light on how novices use and understand Cartography and show how places found in discussion forum text reflect the global reach of MOOCs.",2015-01-01,2-s2.0-85007191457,Lecture Notes in Geoinformation and Cartography,Exploring class discussions from a massive open online course (MOOC) on cartography,"The rise of the Massive Open Online Course (MOOC) has led to its application in Cartographic education. Students in these classes generate enormous amounts of text data in the form of discussion forum posts. Here we explore the topics and geographic references found in over 95,000 forum posts collected during the 2013 launch of Maps and the Geospatial Revolution, a MOOC taught on Coursera. Using Phrase Nets, topic modeling methods, and a named-entity extraction geocoding tool, we show how students describe their use of maps, what key topics drove conversations during the class, and the geography associated with placenames mentioned in posts. These results help shed light on how novices use and understand Cartography and show how places found in discussion forum text reflect the global reach of MOOCs."
459,"This article presents a method for recommending scientific articles taking into consideration their degree of generality or specificity. This approach is based on the idea that less expert people in a specific topic prefer to read more general articles to be introduced into it, while people with more expertise prefer to read more specific articles. Compared to other recommendation techniques that focus on the analysis of user profiles, our proposal is purely based on content analysis. We present two methods for recommending articles, based on Topic Modelling. The first one is based on the divergence of topics given in the documents, while the second uses the similarities that exist between these topics. By using the proposed methods it was possible to determine the degree of specificity of an article, and the results obtained with them overcame those produced by an information retrieval traditional system.",2015-01-01,2-s2.0-84941200268,Procesamiento de Lenguaje Natural,An appro ach to the recommendation of scientific articles acc ording to their degree of specificity Una aproximación a la recomendación de artículos cientifícos según su grado de especificidad,"This article presents a method for recommending scientific articles taking into consideration their degree of generality or specificity. This approach is based on the idea that less expert people in a specific topic prefer to read more general articles to be introduced into it, while people with more expertise prefer to read more specific articles. Compared to other recommendation techniques that focus on the analysis of user profiles, our proposal is purely based on content analysis. We present two methods for recommending articles, based on Topic Modelling. The first one is based on the divergence of topics given in the documents, while the second uses the similarities that exist between these topics. By using the proposed methods it was possible to determine the degree of specificity of an article, and the results obtained with them overcame those produced by an information retrieval traditional system."
460,"In this study we analyzed and discussed that the MIS-related journals under the ISI subject category of IS&LS are simultaneously given with subject category Management, using methods of topic modeling, journal clustering and subject category prediction. In the experiment of journal clustering, all journals under subject category Management and other journals also having similar topical features can be gathered into a cluster, and ""management"" is their common and the most distinct topic. Because the journals belonged to this cluster are almost same to those in the MIS clusters generated by the previous studies, we considered it as the MIS cluster in this study. In the second experiment, we used the classification and regression tree (CART) technique to predict assignment of subject category with that the journals in the original subject category Management and in the MIS cluster produced in this study as positive examples, respectively. The trees generated by the two tests both used the occurring probabilities of the topic ""management"" as the main classification rule. However, in the latter test, we did not only obtain a simpler classification tree but also had a result with less predicting errors. This means that if all journals in the MIS cluster could be given with subject category Management, the retrieval results can be more effective and complete.",2015-01-01,2-s2.0-84939626163,Journal of Educational Media and Library Sciences,A study of the subject categorization of the MIS-related journals in the ISI databases using topical features in the text content and machine learning methods,"In this study we analyzed and discussed that the MIS-related journals under the ISI subject category of IS&LS are simultaneously given with subject category Management, using methods of topic modeling, journal clustering and subject category prediction. In the experiment of journal clustering, all journals under subject category Management and other journals also having similar topical features can be gathered into a cluster, and ""management"" is their common and the most distinct topic. Because the journals belonged to this cluster are almost same to those in the MIS clusters generated by the previous studies, we considered it as the MIS cluster in this study. In the second experiment, we used the classification and regression tree (CART) technique to predict assignment of subject category with that the journals in the original subject category Management and in the MIS cluster produced in this study as positive examples, respectively. The trees generated by the two tests both used the occurring probabilities of the topic ""management"" as the main classification rule. However, in the latter test, we did not only obtain a simpler classification tree but also had a result with less predicting errors. This means that if all journals in the MIS cluster could be given with subject category Management, the retrieval results can be more effective and complete."
461,"In addition to hosting user-generated video content, YouTube provides recommendation services, where sets of related and recommended videos are presented to users, based on factors such as co-visitation count and prior viewing history. This article is specifically concerned with extreme right (ER) video content, portions of which contravene hate laws and are thus illegal in certain countries, which are recommended by YouTube to some users. We develop a categorization of this content based on various schema found in a selection of academic literature on the ER, which is then used to demonstrate the political articulations of YouTube’s recommender system, particularly the narrowing of the range of content to which users are exposed and the potential impacts of this. For this purpose, we use two data sets of English and German language ER YouTube channels, along with channels suggested by YouTube’s related video service. A process is observable whereby users accessing an ER YouTube video are likely to be recommended further ER content, leading to immersion in an ideological bubble in just a few short clicks. The evidence presented in this article supports a shift of the almost exclusive focus on users as content creators and protagonists in extremist cyberspaces to also consider online platform providers as important actors in these same spaces.",2015-01-01,2-s2.0-84936939596,Social Science Computer Review,Down the (White) Rabbit Hole: The Extreme Right and Online Recommender Systems,"In addition to hosting user-generated video content, YouTube provides recommendation services, where sets of related and recommended videos are presented to users, based on factors such as co-visitation count and prior viewing history. This article is specifically concerned with extreme right (ER) video content, portions of which contravene hate laws and are thus illegal in certain countries, which are recommended by YouTube to some users. We develop a categorization of this content based on various schema found in a selection of academic literature on the ER, which is then used to demonstrate the political articulations of YouTube’s recommender system, particularly the narrowing of the range of content to which users are exposed and the potential impacts of this. For this purpose, we use two data sets of English and German language ER YouTube channels, along with channels suggested by YouTube’s related video service. A process is observable whereby users accessing an ER YouTube video are likely to be recommended further ER content, leading to immersion in an ideological bubble in just a few short clicks. The evidence presented in this article supports a shift of the almost exclusive focus on users as content creators and protagonists in extremist cyberspaces to also consider online platform providers as important actors in these same spaces."
462,"Natural language interaction (NLI) is vital and ubiquitous by nature in education environments. It will keep playing key roles in ubiquitous learning and even show stronger presence there. NLI may happen ubiquitously, with many varied forms of texts, bigger textual data, and different learning situations on all kinds of devices, to meet new user needs, thus pose challenges on its design and development. This chapter introduces how natural language processing (NLP) technologies can be employed to help build and improve NLI that can support ubiquitous learning. We emphasize semantic analysis such as semantic role labeling and semantic similarity, and develop and use them to enhance question and answer processing, automated question answering, and automatic text summarization that are involved in our educational systems. Our proposed approaches can improve the technology of natural language processing and help develop different NLI systems in the ubiquitous learning environments and eventually benefit learners.",2015-01-01,2-s2.0-84966321698,Lecture Notes in Educational Technology,Semantic analysis-enhanced natural language interaction in ubiquitous learning,"Natural language interaction (NLI) is vital and ubiquitous by nature in education environments. It will keep playing key roles in ubiquitous learning and even show stronger presence there. NLI may happen ubiquitously, with many varied forms of texts, bigger textual data, and different learning situations on all kinds of devices, to meet new user needs, thus pose challenges on its design and development. This chapter introduces how natural language processing (NLP) technologies can be employed to help build and improve NLI that can support ubiquitous learning. We emphasize semantic analysis such as semantic role labeling and semantic similarity, and develop and use them to enhance question and answer processing, automated question answering, and automatic text summarization that are involved in our educational systems. Our proposed approaches can improve the technology of natural language processing and help develop different NLI systems in the ubiquitous learning environments and eventually benefit learners."
463,"Citizens' opinions are crucial for action on climate change, but are, owing to the complexity of the issue, diverse and potentially unformed. We contribute to the understanding of public views on climate change and to knowledge needed by decision-makers by using a new approach to analyse answers to the open survey question ' what comes to mind when you hear the words ' climate change' ?'. We apply automated text analysis, specifically structural topic modelling, which induces distinct topics based on the relative frequencies of the words used in 2,115 responses. From these data, originating from the new, nationally representative Norwegian Citizen Panel, four distinct topics emerge: Weather/Ice, Future/Impact, Money/Consumption and Attribution. We find that Norwegians emphasize societal aspects of climate change more than do respondents in previous US and UK studies. Furthermore, variables that explain variation in closed questions, such as gender and education, yield different and surprising results when employed to explain variation in what respondents emphasize. Finally, the sharp distinction between scepticism and acceptance of conventional climate science, often seen in previous studies, blurs in many textual responses as scepticism frequently turns into ambivalence.",2015-01-01,2-s2.0-84937877530,Nature Climate Change,Explaining topic prevalence in answers to open-ended survey questions about climate change,"Citizens' opinions are crucial for action on climate change, but are, owing to the complexity of the issue, diverse and potentially unformed. We contribute to the understanding of public views on climate change and to knowledge needed by decision-makers by using a new approach to analyse answers to the open survey question ' what comes to mind when you hear the words ' climate change' ?'. We apply automated text analysis, specifically structural topic modelling, which induces distinct topics based on the relative frequencies of the words used in 2,115 responses. From these data, originating from the new, nationally representative Norwegian Citizen Panel, four distinct topics emerge: Weather/Ice, Future/Impact, Money/Consumption and Attribution. We find that Norwegians emphasize societal aspects of climate change more than do respondents in previous US and UK studies. Furthermore, variables that explain variation in closed questions, such as gender and education, yield different and surprising results when employed to explain variation in what respondents emphasize. Finally, the sharp distinction between scepticism and acceptance of conventional climate science, often seen in previous studies, blurs in many textual responses as scepticism frequently turns into ambivalence."
464,"This panel showcases analyses of emerging trends of e-participation—use of information communication technologies (ICTs) and social media for political participation. E-participation has gained popularity in many countries, and some cases have shown that e-participation has actually brought significant changes in societies. In this panel, we address two issues: (1) the impacts of e-participation on society, and (2) the role of information science in understanding this phenomenon. We will focus on introducing emerging trends in e-participation and their impact on individuals and societies in South Korea, the United States, and Honduras. Further, we introduce methods for efficiently analyzing e-participation data, and discuss challenges involved in implementing these methodologies. Because this is intended to be an interactive panel, the audience will be encouraged to engage in discussions and contributing their own experiences and ideas related to current trends in e-participation in various countries. SPONSORSHIP: SIG III, SIG IEP.",2015-01-01,2-s2.0-84987704798,Proceedings of the Association for Information Science and Technology,Emerging trends in the use and adoption of E-participation around the world,"This panel showcases analyses of emerging trends of e-participation—use of information communication technologies (ICTs) and social media for political participation. E-participation has gained popularity in many countries, and some cases have shown that e-participation has actually brought significant changes in societies. In this panel, we address two issues: (1) the impacts of e-participation on society, and (2) the role of information science in understanding this phenomenon. We will focus on introducing emerging trends in e-participation and their impact on individuals and societies in South Korea, the United States, and Honduras. Further, we introduce methods for efficiently analyzing e-participation data, and discuss challenges involved in implementing these methodologies. Because this is intended to be an interactive panel, the audience will be encouraged to engage in discussions and contributing their own experiences and ideas related to current trends in e-participation in various countries. SPONSORSHIP: SIG III, SIG IEP."
465,"As an important issue in sentiment analysis, sentence-level polarity classification plays a critical role in many opinion-mining applications such as opinion question answering, opinion retrieval and opinion summarization. Employing a supervised learning paradigm to train a classifier from sentences often faces the data sparseness problem owing to the short-length limit introduced to texts. In this article, regarding this problem, we exploit two different feature sets learned from external data sets as additional features to enrich data representation: one is a latent topic feature set obtained using a topic model, and the other is a related word feature set derived using word embeddings. Furthermore, we propose an ensemble approach by using these additional features to guide the design of different members of the ensemble. Experimental results on the public movie review dataset demonstrate that the enriched representations are effective for improving the performance of polarity classification, and the proposed ensemble approach can further improve the overall performance.",2015-01-01,2-s2.0-84936995312,Journal of Information Science,Using data-driven feature enrichment of text representation and ensemble technique for sentence-level polarity classification,"As an important issue in sentiment analysis, sentence-level polarity classification plays a critical role in many opinion-mining applications such as opinion question answering, opinion retrieval and opinion summarization. Employing a supervised learning paradigm to train a classifier from sentences often faces the data sparseness problem owing to the short-length limit introduced to texts. In this article, regarding this problem, we exploit two different feature sets learned from external data sets as additional features to enrich data representation: one is a latent topic feature set obtained using a topic model, and the other is a related word feature set derived using word embeddings. Furthermore, we propose an ensemble approach by using these additional features to guide the design of different members of the ensemble. Experimental results on the public movie review dataset demonstrate that the enriched representations are effective for improving the performance of polarity classification, and the proposed ensemble approach can further improve the overall performance."
466,"This paper proposes a surrounding word sense model (SWSM) that uses the distri- bution of word senses that appear near am- biguous words for unsupervised all-words word sense disambiguation in Japanese. Although it was inspired by the topic model, ambiguous Japanese words tend to have similar topics since coarse semantic polysemy is less likely to occur than that in Western languages as Japanese uses Chi- nese characters, which are ideograms. We thus propose a model that uses the dis- tribution of word senses that appear near ambiguous words: SWSM. We embed- ded the concept dictionary of an Elec- tronic Dictionary Research (EDR) elec- tronic dictionary in the system and used the Japanese Corpus of EDR for the exper- iments, which demonstrated that SWSM outperformed a system with a random baseline and a system that used a topic model called Dirichlet Allocation with WORDNET (LDAWN), especially when there were high levels of entropy for the word sense distribution of ambiguous words.",2015-01-01,2-s2.0-84967185376,"29th Pacific Asia Conference on Language, Information and Computation, PACLIC 2015",Surrounding word sense model for Japanese all-words word sense disambiguation,"This paper proposes a surrounding word sense model (SWSM) that uses the distri- bution of word senses that appear near am- biguous words for unsupervised all-words word sense disambiguation in Japanese. Although it was inspired by the topic model, ambiguous Japanese words tend to have similar topics since coarse semantic polysemy is less likely to occur than that in Western languages as Japanese uses Chi- nese characters, which are ideograms. We thus propose a model that uses the dis- tribution of word senses that appear near ambiguous words: SWSM. We embed- ded the concept dictionary of an Elec- tronic Dictionary Research (EDR) elec- tronic dictionary in the system and used the Japanese Corpus of EDR for the exper- iments, which demonstrated that SWSM outperformed a system with a random baseline and a system that used a topic model called Dirichlet Allocation with WORDNET (LDAWN), especially when there were high levels of entropy for the word sense distribution of ambiguous words."
467,"In recent years many automated topic coherence formulas (using the top-m words of a topic inferred by latent Dirichlet allocation) based on word similarities have been proposed and evaluated against human ratings. We treat a wordy topic as an object and quantitatively describe it via normalized mean values of pair-wise word similarities. Two types of word similarities, thesaurus and local corpus-based, are used as the descriptive features of a topic. We perform topic classification using represented topics as input and bi-level human ratings about topic coherence as class labels. Classification results (precision, recall and accuracy) based on two datasets and three supervised classification algorithms suggest that the novel topic representation is consistent with human ratings. Corpus-based word similarities are positively correlated with human ratings whereas thesaurus-based similarities have negative relations. The proposed representation of topics opens a window for us to investigate the utilization of topics with different perspectives.",2015-01-01,2-s2.0-84942088631,Journal of Information Science,LDA topics: Representation and evaluation,"In recent years many automated topic coherence formulas (using the top-m words of a topic inferred by latent Dirichlet allocation) based on word similarities have been proposed and evaluated against human ratings. We treat a wordy topic as an object and quantitatively describe it via normalized mean values of pair-wise word similarities. Two types of word similarities, thesaurus and local corpus-based, are used as the descriptive features of a topic. We perform topic classification using represented topics as input and bi-level human ratings about topic coherence as class labels. Classification results (precision, recall and accuracy) based on two datasets and three supervised classification algorithms suggest that the novel topic representation is consistent with human ratings. Corpus-based word similarities are positively correlated with human ratings whereas thesaurus-based similarities have negative relations. The proposed representation of topics opens a window for us to investigate the utilization of topics with different perspectives."
468,"Suicide is one of major public health problems worldwide. Traditionally, suicidal ideation is assessed by surveys or interviews, which lacks of a real-Time assessment of personal mental state. Online social networks, with large amount of user-generated data, offer opportunities to gain insights of suicide assessment and prevention. In this paper, we explore potentiality to identify and monitor suicide expressed in microblog on social networks. First, we identify users who have committed suicide and collect millions of microblogs from social networks. Second, we build suicide psychological lexicon by psychological standards and word embedding technique. Third, by leveraging both language styles and online behaviors, we employ Topic Model and other machine learning algorithms to identify suicidal ideation. Our approach achieves the best results on topic-500, yielding F1 =measure of 80:0%, Precision of 87:1%, Recall of 73:9%, and Accuracy of 93:2%. Furthermore, a prototype system for monitoring suicidal ideation on several social networks is deployed.",2015-01-01,2-s2.0-84967163556,"29th Pacific Asia Conference on Language, Information and Computation, PACLIC 2015",Topic model for identifying suicidal ideation in Chinese microblog,"Suicide is one of major public health problems worldwide. Traditionally, suicidal ideation is assessed by surveys or interviews, which lacks of a real-Time assessment of personal mental state. Online social networks, with large amount of user-generated data, offer opportunities to gain insights of suicide assessment and prevention. In this paper, we explore potentiality to identify and monitor suicide expressed in microblog on social networks. First, we identify users who have committed suicide and collect millions of microblogs from social networks. Second, we build suicide psychological lexicon by psychological standards and word embedding technique. Third, by leveraging both language styles and online behaviors, we employ Topic Model and other machine learning algorithms to identify suicidal ideation. Our approach achieves the best results on topic-500, yielding F1 =measure of 80:0%, Precision of 87:1%, Recall of 73:9%, and Accuracy of 93:2%. Furthermore, a prototype system for monitoring suicidal ideation on several social networks is deployed."
469,"Replicated Softmax model, a well-known undirected topic model, is powerful in ex-tracting semantic representations of docu-ments. Traditional learning strategies such as Contrastive Divergence are very inef-ficient. This paper provides a novel esti-mator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification.",2015-01-01,2-s2.0-84944064715,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",Efficient Learning for undirected topic models,"Replicated Softmax model, a well-known undirected topic model, is powerful in ex-tracting semantic representations of docu-ments. Traditional learning strategies such as Contrastive Divergence are very inef-ficient. This paper provides a novel esti-mator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efficiency and high accuracy on document retrieval and classification."
470,"Topic Model such as Latent Dirichlet Allocation(LDA) makes assumption that topic assignment of different words are conditionally independent. In this paper, we propose a new model Extended Global Topic Random Field (EGTRF) to model non-linear dependencies between words. Specifically, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words. Word similarity information learned from large corpus is incorporated to enhance word topic assignment. Parameters are estimated efficiently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability.",2015-01-01,2-s2.0-84944049639,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",Extended topic model forword dependency,"Topic Model such as Latent Dirichlet Allocation(LDA) makes assumption that topic assignment of different words are conditionally independent. In this paper, we propose a new model Extended Global Topic Random Field (EGTRF) to model non-linear dependencies between words. Specifically, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words. Word similarity information learned from large corpus is incorporated to enhance word topic assignment. Parameters are estimated efficiently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability."
471,"There has been significant progress transforming semi-structured data about places into knowledge graphs that can be used in a wide variety of geographic information systems such as digital gazetteers or geographic information retrieval systems. For instance, in addition to information about events, actors, and objects, DBpedia contains data about hundreds of thousands of places from Wikipedia and publishes it as Linked Data. Repositories that store data about places are among the most interlinked hubs on the Linked Data cloud. However, most content about places resides in unstructured natural language text, and therefore it is not captured in these knowledge graphs. Instead, place representations are limited to facts such as their population counts, geographic locations, and relations to other entities, for example, headquarters of companies or historical figures. In this paper, we present a novel method to enrich the information stored about places in knowledge graphs using thematic signatures that are derived from unstructured text through the process of topic modeling. As proof of concept, we demonstrate that this enables the automatic categorization of articles into place types defined in the DBpedia ontology (e.g., mountain) and also provides a mechanism to infer relationships between place types that are not captured in existing ontologies. This method can also be used to uncover miscategorized places, which is a common problem arising from the automatic lifting of unstructured and semi-structured data.",2015-01-01,2-s2.0-84930044547,International Journal of Geographical Information Science,Thematic signatures for cleansing and enriching place-related linked data,"There has been significant progress transforming semi-structured data about places into knowledge graphs that can be used in a wide variety of geographic information systems such as digital gazetteers or geographic information retrieval systems. For instance, in addition to information about events, actors, and objects, DBpedia contains data about hundreds of thousands of places from Wikipedia and publishes it as Linked Data. Repositories that store data about places are among the most interlinked hubs on the Linked Data cloud. However, most content about places resides in unstructured natural language text, and therefore it is not captured in these knowledge graphs. Instead, place representations are limited to facts such as their population counts, geographic locations, and relations to other entities, for example, headquarters of companies or historical figures. In this paper, we present a novel method to enrich the information stored about places in knowledge graphs using thematic signatures that are derived from unstructured text through the process of topic modeling. As proof of concept, we demonstrate that this enables the automatic categorization of articles into place types defined in the DBpedia ontology (e.g., mountain) and also provides a mechanism to infer relationships between place types that are not captured in existing ontologies. This method can also be used to uncover miscategorized places, which is a common problem arising from the automatic lifting of unstructured and semi-structured data."
472,"Biterm Topic Model (BTM) is designed to model the generative process of the word co-occurrence patterns in short texts such as tweets. However, two aspects of BTM may restrict its performance: 1) user individualities are ignored to obtain the corpus level words co-occurrence patterns; and 2) the strong assumptions that two co-occurring words will be assigned the same topic label could not distinguish background words from topical words. In this paper, we propose Twitter-BTM model to address those issues by considering user level personalization in BTM. Firstly, we use user based biterms aggregation to learn user specific topic distribution. Secondly, each user's preference between background words and topical words is estimated by incorporating a background topic. Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several stateof-the-art baselines.",2015-01-01,2-s2.0-84944078879,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",User based aggregation for biterm topic model,"Biterm Topic Model (BTM) is designed to model the generative process of the word co-occurrence patterns in short texts such as tweets. However, two aspects of BTM may restrict its performance: 1) user individualities are ignored to obtain the corpus level words co-occurrence patterns; and 2) the strong assumptions that two co-occurring words will be assigned the same topic label could not distinguish background words from topical words. In this paper, we propose Twitter-BTM model to address those issues by considering user level personalization in BTM. Firstly, we use user based biterms aggregation to learn user specific topic distribution. Secondly, each user's preference between background words and topical words is estimated by incorporating a background topic. Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several stateof-the-art baselines."
473,"What are the fundamental research questions in Information Systems? How do various research topics relate with one another to form the IS research landscape and how do they evolve over time? This study is an initial attempt to answer these questions using topic models to investigate the topics examined by the premier IS journals in 1977-2014. We present an IS Topic Graph that contains 33 research areas, 31 of which are closely connected with one another. Further analyses of this graph reveal how different IS research areas are intertwined to the extent that they are almost inseparable. Looking into IS research at a finer level, we identify 300 research topics, and a chronological analysis reveals a trend of topic diversification and externalization. To guide future research, an intelligent literature search tool called ISTopic is built and is available at http://www.istopic.org for public access.",2015-01-01,2-s2.0-84964680201,"2015 International Conference on Information Systems: Exploring the Information Frontier, ICIS 2015",ISTopic: Understanding information systems research through topic models,"What are the fundamental research questions in Information Systems? How do various research topics relate with one another to form the IS research landscape and how do they evolve over time? This study is an initial attempt to answer these questions using topic models to investigate the topics examined by the premier IS journals in 1977-2014. We present an IS Topic Graph that contains 33 research areas, 31 of which are closely connected with one another. Further analyses of this graph reveal how different IS research areas are intertwined to the extent that they are almost inseparable. Looking into IS research at a finer level, we identify 300 research topics, and a chronological analysis reveals a trend of topic diversification and externalization. To guide future research, an intelligent literature search tool called ISTopic is built and is available at http://www.istopic.org for public access."
474,"Content analysis, a widely-applied social science research method, is increasingly being supplemented by topic modeling. However, while the discourse on content analysis centers heavily on reproducibility, computer scientists often focus more on scalability and less on coding reliability, leading to growing skepticism on the usefulness of topic models for automated content analysis. In response, we introduce TopicCheck, an interactive tool for assessing topic model stability. Our contributions are threefold. First, from established guidelines on reproducible content analysis, we distill a set of design requirements on how to computationally assess the stability of an automated coding process. Second, we devise an interactive alignment algorithm for matching latent topics from multiple models, and enable sensitivity evaluation across a large number of models. Finally, we demonstrate that our tool enables social scientists to gain novel insights into three active research questions.",2015-01-01,2-s2.0-84956624433,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",TopicCheck: Interactive alignment for assessing topic model stability,"Content analysis, a widely-applied social science research method, is increasingly being supplemented by topic modeling. However, while the discourse on content analysis centers heavily on reproducibility, computer scientists often focus more on scalability and less on coding reliability, leading to growing skepticism on the usefulness of topic models for automated content analysis. In response, we introduce TopicCheck, an interactive tool for assessing topic model stability. Our contributions are threefold. First, from established guidelines on reproducible content analysis, we distill a set of design requirements on how to computationally assess the stability of an automated coding process. Second, we devise an interactive alignment algorithm for matching latent topics from multiple models, and enable sensitivity evaluation across a large number of models. Finally, we demonstrate that our tool enables social scientists to gain novel insights into three active research questions."
475,"In a recent years Tourism and Travel stores offering a huge quantity of services and traveling information by online. Additionally, this huge volume of information smoothly accessed by electronic devices, like phone, computer with the availability of internet connection. When tourists are visiting any cities, most of them aimed to explore the interesting fact or things about the places and events etc. in spite of the fruitful progress there are still several opportunities remaining to discover. Firstly, we investigate the properties of the travel package and expand TAST (Tourist-Area-Season topic) model. Thereafter, we expand the Tourist-Area-Season topic (TAST) model for confining the relationship over the every group of the tourist. This research work expressing an outline for the recommender system and describing the latest production of recommendation techniques that are generally divided in the three phase: Collaborative, Hybrid recommendation and Content-based approaches. Finally, we appraise the Tourist-Area-Season topic (TAST) model with the cocktail recommendation approach on the real-world travel package data.",2015-01-01,2-s2.0-84936745464,Contemporary Engineering Sciences,Recommendation and ranking of travel package to tourist,"In a recent years Tourism and Travel stores offering a huge quantity of services and traveling information by online. Additionally, this huge volume of information smoothly accessed by electronic devices, like phone, computer with the availability of internet connection. When tourists are visiting any cities, most of them aimed to explore the interesting fact or things about the places and events etc. in spite of the fruitful progress there are still several opportunities remaining to discover. Firstly, we investigate the properties of the travel package and expand TAST (Tourist-Area-Season topic) model. Thereafter, we expand the Tourist-Area-Season topic (TAST) model for confining the relationship over the every group of the tourist. This research work expressing an outline for the recommender system and describing the latest production of recommendation techniques that are generally divided in the three phase: Collaborative, Hybrid recommendation and Content-based approaches. Finally, we appraise the Tourist-Area-Season topic (TAST) model with the cocktail recommendation approach on the real-world travel package data."
476,"Contemporary transformations of rural areas involve changes in land uses, economic perspectives, connectivity, livelihoods, but also in lifestyles, whereupon a traditional view of 'the rural' and, consequently, of 'rural development' no longer holds. Accordingly, EU's 2007-2013 Rural Development policy (RDP) is one framework to incorporate aspects labelled as quality of life (QOL) alongside traditional rural tenets. With a new rendition of the RDP underway, this paper scopes the content and extent of the expired RDP regarding its incorporation of QOL, in order to better identify considerations for future policy making. Using novel methodology called topic modelling, a series of latent semantic structures within the RDP could be unravelled and re-interpreted via a dual categorization system based on RDP's own view on QOL, and on definitions provided by independent research. Corroborated by other audits, the findings indicate a thematic overemphasis on agriculture, with the focus on QOL being largely insignificant. Such results point to a rationale different than the assumed one, at the same time reinforcing an outdated view of rurality in the face of the ostensibly fundamental turn towards viewing rural areas in a wider, more humanistic, perspective. This unexpected issue of underrepresentation is next addressed through three possible drivers: conceptual(lingering productionist view of the rural), ideological (capitalist prerogative preventing non-pecuniary values from entering policy) and material (institutional lock-ins incapable ofaccommodating significant deviations from an agricultural focus). The paper ends with a critical discussion and some reflections on the broader concept of rurality.",2014-09-01,2-s2.0-84913590978,Bulletin of Geography,Quality of life in rural areas: A topic for the rural development policy?,"Contemporary transformations of rural areas involve changes in land uses, economic perspectives, connectivity, livelihoods, but also in lifestyles, whereupon a traditional view of 'the rural' and, consequently, of 'rural development' no longer holds. Accordingly, EU's 2007-2013 Rural Development policy (RDP) is one framework to incorporate aspects labelled as quality of life (QOL) alongside traditional rural tenets. With a new rendition of the RDP underway, this paper scopes the content and extent of the expired RDP regarding its incorporation of QOL, in order to better identify considerations for future policy making. Using novel methodology called topic modelling, a series of latent semantic structures within the RDP could be unravelled and re-interpreted via a dual categorization system based on RDP's own view on QOL, and on definitions provided by independent research. Corroborated by other audits, the findings indicate a thematic overemphasis on agriculture, with the focus on QOL being largely insignificant. Such results point to a rationale different than the assumed one, at the same time reinforcing an outdated view of rurality in the face of the ostensibly fundamental turn towards viewing rural areas in a wider, more humanistic, perspective. This unexpected issue of underrepresentation is next addressed through three possible drivers: conceptual(lingering productionist view of the rural), ideological (capitalist prerogative preventing non-pecuniary values from entering policy) and material (institutional lock-ins incapable ofaccommodating significant deviations from an agricultural focus). The paper ends with a critical discussion and some reflections on the broader concept of rurality."
477,"We use different text-processing algorithms to gain insight into the political rhetoric used in conservative and liberal weblogs. We specifically focus on the online debate regarding the issue of the ""Ground Zero Mosque,"" which has been one of the most controversial political issues in U.S. politics in the last several years. Overall, our results show that there are significant differences in the use of various linguistic features related to sentiments of collective identity, moral concerns, and emotional dynamics between liberals and conservatives, thus highlighting the differences between the ideological and moral frameworks of these two groups. © 2014 Copyright Taylor and Francis Group, LLC.",2014-02-28,2-s2.0-84894435300,Journal of Information Technology and Politics,"Analyzing Political Rhetoric in Conservative and Liberal Weblogs Related to the Construction of the ""Ground Zero Mosque""","We use different text-processing algorithms to gain insight into the political rhetoric used in conservative and liberal weblogs. We specifically focus on the online debate regarding the issue of the ""Ground Zero Mosque,"" which has been one of the most controversial political issues in U.S. politics in the last several years. Overall, our results show that there are significant differences in the use of various linguistic features related to sentiments of collective identity, moral concerns, and emotional dynamics between liberals and conservatives, thus highlighting the differences between the ideological and moral frameworks of these two groups. © 2014 "
478,"The application of word sense disambiguation (WSD) techniques to information retrieval (IR) has yet to provide convincing retrieval results. Major obstacles to effective WSD in IR include coverage and granularity problems of word sense inventories, sparsity of document context, and limited information provided by short queries. In this paper, to alleviate these issues, we propose the construction of latent context models for terms using latent Dirichlet allocation. We propose building one latent context per word, using a well principled representation of local context based on word features. In particular, context words are weighted using a decaying function according to their distance to the target word, which is learnt from data in an unsupervised manner. The resulting latent features are used to discriminate word contexts, so as to constrict query's semantic scope. Consistent and substantial improvements, including on difficult queries, are observed on TREC test collections, and the techniques combines well with blind relevance feedback. Compared to traditional topic modeling, WSD and positional indexing techniques, the proposed retrieval model is more effective and scales well on large-scale collections. © 2013 Springer Science+Business Media New York.",2014-02-01,2-s2.0-84895903458,Information Retrieval,Latent word context model for information retrieval,"The application of word sense disambiguation (WSD) techniques to information retrieval (IR) has yet to provide convincing retrieval results. Major obstacles to effective WSD in IR include coverage and granularity problems of word sense inventories, sparsity of document context, and limited information provided by short queries. In this paper, to alleviate these issues, we propose the construction of latent context models for terms using latent Dirichlet allocation. We propose building one latent context per word, using a well principled representation of local context based on word features. In particular, context words are weighted using a decaying function according to their distance to the target word, which is learnt from data in an unsupervised manner. The resulting latent features are used to discriminate word contexts, so as to constrict query's semantic scope. Consistent and substantial improvements, including on difficult queries, are observed on TREC test collections, and the techniques combines well with blind relevance feedback. Compared to traditional topic modeling, WSD and positional indexing techniques, the proposed retrieval model is more effective and scales well on large-scale collections. "
479,"A variety of generative topic models have been successfully applied to model corpus of documents with continuous metadata. But there is no efficient model dealing with documents having a user-item-word structure. This structure forms a 3-way text tensor, and texts correlate with each other through users and items. In this paper we propose an elegant Tensor topic model (TTM) for text tensors inspired by Tucker decomposition, in which both user and item dimensions are co-reduced together with vocabulary space. So we get low-rank representations for not only words but also users and items from TTM. Also, general rules are developed to transform decomposition model into a probabilistic one. Experiments show that TTM outperforms existing topic models in modeling texts with a user-item-word structure.",2014-01-01,2-s2.0-84920712716,"Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014",Collaborative topic modeling for text tensors,"A variety of generative topic models have been successfully applied to model corpus of documents with continuous metadata. But there is no efficient model dealing with documents having a user-item-word structure. This structure forms a 3-way text tensor, and texts correlate with each other through users and items. In this paper we propose an elegant Tensor topic model (TTM) for text tensors inspired by Tucker decomposition, in which both user and item dimensions are co-reduced together with vocabulary space. So we get low-rank representations for not only words but also users and items from TTM. Also, general rules are developed to transform decomposition model into a probabilistic one. Experiments show that TTM outperforms existing topic models in modeling texts with a user-item-word structure."
480,"Many articles in the online encyclopedia Wikipedia have hyperlinks to ambiguous article titles; these ambiguous links should be replaced with links to unambiguous articles, a process known as disambiguation. We propose a novel statistical topic model based on link text, which we refer to as the Link Text Topic Model (LTTM), that we use to suggest new link targets for ambiguous links. To evaluate our model, we describe a method for extracting ground truth for this link disambiguation task from edits made to Wikipedia in a specific time period. We use this ground truth to demonstrate the superiority of LTTM over other existing link- and content-based approaches to disambiguating links in Wikipedia. Finally, we build a web service that uses LTTM to make suggestions to human editors wanting to fix ambiguous links in Wikipedia.",2014-01-01,2-s2.0-84904755752,ACM Transactions on Information Systems,Topic modeling for wikipedia link disambiguation,"Many articles in the online encyclopedia Wikipedia have hyperlinks to ambiguous article titles; these ambiguous links should be replaced with links to unambiguous articles, a process known as disambiguation. We propose a novel statistical topic model based on link text, which we refer to as the Link Text Topic Model (LTTM), that we use to suggest new link targets for ambiguous links. To evaluate our model, we describe a method for extracting ground truth for this link disambiguation task from edits made to Wikipedia in a specific time period. We use this ground truth to demonstrate the superiority of LTTM over other existing link- and content-based approaches to disambiguating links in Wikipedia. Finally, we build a web service that uses LTTM to make suggestions to human editors wanting to fix ambiguous links in Wikipedia."
481,"What the network learners have said in network learning forum posts directly reflects the needs of the learners during the network learning. By mining topics from forum posts, the network learning support service could be greatly improved. For this purpose, the Learner-Topic model was employed in this paper to mine the learner's posts for discovering its topics. And then use the topics for learning resources recommendation. © 2014 WIT Press.",2014-01-01,2-s2.0-84903172778,WIT Transactions on Information and Communication Technologies,Network learning forum posts topic discovery,"What the network learners have said in network learning forum posts directly reflects the needs of the learners during the network learning. By mining topics from forum posts, the network learning support service could be greatly improved. For this purpose, the Learner-Topic model was employed in this paper to mine the learner's posts for discovering its topics. And then use the topics for learning resources recommendation. "
482,"User based collaborative filtering (UCF) and item based collaborative filtering (ICF) are two quite successful approaches applied in recommender system, both of which try to predict the rating value of items for the target user based on the items previously rated by other users. However, existing UCF and ICF base on only a rating and neglect the fact that users' same rating to an item may be based on different item features. Another deficiency of UCF and ICF is that both approaches are limited due to the data sparsity problem. In this paper, a hybrid collaborative filtering recommender system that enhances traditional CF (UCF, ICF) by user's free-text review information is presented. Latent Dirichlet Allocation (LDA) is used in our study to infer the topic probability distribution of the users' reviews and then we propose two aggregation methods to uncover user-topicinterest profile and item-topic-features profile, which will be used to measure similarity between users and between items. Finally we try to combine this new proposed similarity measurement with that of traditional CF. Experiments we conducted show that our algorithms perform better than UCF, ICF and the popular recommendation algorithm Slope One on datasets with different sparsity.",2014-01-01,2-s2.0-84907399810,"2014 International Conference on Information and Communications Technologies, ICT 2014",Collaborative filtering enhanced by user free-text reviews topic modelling,"User based collaborative filtering (UCF) and item based collaborative filtering (ICF) are two quite successful approaches applied in recommender system, both of which try to predict the rating value of items for the target user based on the items previously rated by other users. However, existing UCF and ICF base on only a rating and neglect the fact that users' same rating to an item may be based on different item features. Another deficiency of UCF and ICF is that both approaches are limited due to the data sparsity problem. In this paper, a hybrid collaborative filtering recommender system that enhances traditional CF (UCF, ICF) by user's free-text review information is presented. Latent Dirichlet Allocation (LDA) is used in our study to infer the topic probability distribution of the users' reviews and then we propose two aggregation methods to uncover user-topicinterest profile and item-topic-features profile, which will be used to measure similarity between users and between items. Finally we try to combine this new proposed similarity measurement with that of traditional CF. Experiments we conducted show that our algorithms perform better than UCF, ICF and the popular recommendation algorithm Slope One on datasets with different sparsity."
483,"This paper introduces Probabilistic Topic Modeling (PTM) as a promising approach to naturalistic driving data analyses. Naturalistic driving data present an unprecedented opportunity to understand driver behavior. Novel strategies are needed to achieve a more complete picture of these datasets than is provided by the local event-based analytic strategy that currently dominates the field. PTM is a text analysis method for uncovering word-based themes across documents. In this application, documents were represented by drives and words were created from speed and acceleration data using Symbolic Aggregate approximation (SAX). A twenty-topic Latent Dirichlet Allocation (LDA) topic model was developed using words from 10,705 documents (real-world drives) by 26 drivers. The resulting LDA model clustered the drives into meaningful topics. Topic membership probabilities were successfully used as features in subsequent analyses to differentiate between healthy drivers and those suffering from Obstructive Sleep Apnea.",2014-01-01,2-s2.0-84957671415,Proceedings of the Human Factors and Ergonomics Society,Variations on a theme: Topic modeling of naturalistic driving data,"This paper introduces Probabilistic Topic Modeling (PTM) as a promising approach to naturalistic driving data analyses. Naturalistic driving data present an unprecedented opportunity to understand driver behavior. Novel strategies are needed to achieve a more complete picture of these datasets than is provided by the local event-based analytic strategy that currently dominates the field. PTM is a text analysis method for uncovering word-based themes across documents. In this application, documents were represented by drives and words were created from speed and acceleration data using Symbolic Aggregate approximation (SAX). A twenty-topic Latent Dirichlet Allocation (LDA) topic model was developed using words from 10,705 documents (real-world drives) by 26 drivers. The resulting LDA model clustered the drives into meaningful topics. Topic membership probabilities were successfully used as features in subsequent analyses to differentiate between healthy drivers and those suffering from Obstructive Sleep Apnea."
484,"A keyword query is the representation of the information need of a user, and is the result of a complex cognitive process which often results in under-specification. We propose an unsupervised method namely Latent Concept Modeling (LCM). for mining and modeling latent search concepts in order to recreate the conceptual view of the original information need. We use Latent Dirichlet Allocation (LDA). to exhibit highly-specific query-related topics from pseudo-relevant feedback documents. We define these topics as the latent concepts of the user query. We perform a thorough evaluation of our approach over two large ad-hoc TREC collections. Our findings reveal that the proposed method accurately models latent concepts, while being very effective in a query expansion retrieval setting.© 2014 Lavoisier.",2014-01-01,2-s2.0-84903555060,Document Numerique,Accurate and effective Latent Concept Modeling for ad hoc information retrieval,"A keyword query is the representation of the information need of a user, and is the result of a complex cognitive process which often results in under-specification. We propose an unsupervised method namely Latent Concept Modeling (LCM). for mining and modeling latent search concepts in order to recreate the conceptual view of the original information need. We use Latent Dirichlet Allocation (LDA). to exhibit highly-specific query-related topics from pseudo-relevant feedback documents. We define these topics as the latent concepts of the user query. We perform a thorough evaluation of our approach over two large ad-hoc TREC collections. Our findings reveal that the proposed method accurately models latent concepts, while being very effective in a query expansion retrieval setting."
485,The proceedings contain 56 papers. The topics discussed include: computational experiment of service policy in collaborative procurement; a visualized framework of automatic orchestration engine supporting hybrid cloud resources; a human-centric user model for intelligent healthcare; dominant bidding strategy in mobile app advertising auction; personal healthcare record integration method based on linked data model; optimistic fair-exchange with anonymity for bitcoin users; value evaluation of enterprise management informatization based on comprehensive method; research on undergraduates' perception of WeChat acceptance; in-house crowdsourcing-based entity resolution: dealing with common names; collaborative topic modeling for text tensors; and price pattern detection using finite state machine with fuzzy transitions.,2014-01-01,2-s2.0-84925679353,"Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014","Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014",The proceedings contain 56 papers. The topics discussed include: computational experiment of service policy in collaborative procurement; a visualized framework of automatic orchestration engine supporting hybrid cloud resources; a human-centric user model for intelligent healthcare; dominant bidding strategy in mobile app advertising auction; personal healthcare record integration method based on linked data model; optimistic fair-exchange with anonymity for bitcoin users; value evaluation of enterprise management informatization based on comprehensive method; research on undergraduates' perception of WeChat acceptance; in-house crowdsourcing-based entity resolution: dealing with common names; collaborative topic modeling for text tensors; and price pattern detection using finite state machine with fuzzy transitions.
486,"We are developing indicators for the emergence of science and technology (S&T) topics. To do so, we extract information from various S&T information resources. This paper compares alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and term commonality weighting. We compare topic modeling to Principal Components Analysis for a test set of 4104 abstract records on Dye-Sensitized Solar Cells. Results suggest potential to enhance understanding regarding technological topics to help track technological emergence. © 2013 Elsevier B.V.",2014-01-01,2-s2.0-84902280190,Journal of Engineering and Technology Management - JET-M,Comparing methods to extract technical content for technological intelligence,"We are developing indicators for the emergence of science and technology (S&T) topics. To do so, we extract information from various S&T information resources. This paper compares alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and term commonality weighting. We compare topic modeling to Principal Components Analysis for a test set of 4104 abstract records on Dye-Sensitized Solar Cells. Results suggest potential to enhance understanding regarding technological topics to help track technological emergence. "
487,"Topic modeling is a type of statistical model for discovering the latent ""topics"" that occur in a collection of documents through machine learning. Currently, latent Dirichlet allocation (LDA) is a popular and common modeling approach. In this paper, we investigate methods, including LDA and its extensions, for separating a set of scientific publications into several clusters. To evaluate the results, we generate a collection of documents that contain academic papers from several different fields and see whether papers in the same field will be clustered together. We explore potential scientometric applications of such text analysis capabilities. © 2014 Akadémiai Kiadó, Budapest, Hungary.",2014-01-01,2-s2.0-84906048744,Scientometrics,Clustering scientific documents with topic modeling,"Topic modeling is a type of statistical model for discovering the latent ""topics"" that occur in a collection of documents through machine learning. Currently, latent Dirichlet allocation (LDA) is a popular and common modeling approach. In this paper, we investigate methods, including LDA and its extensions, for separating a set of scientific publications into several clusters. To evaluate the results, we generate a collection of documents that contain academic papers from several different fields and see whether papers in the same field will be clustered together. We explore potential scientometric applications of such text analysis capabilities. "
488,"In this poster, topics in the field of Data Science were explored from Wikipedia documents based on clustering, principal component analysis (PCA), and topic modeling. As a pilot study, we analyzed part of the dataset of Wikipedia documents to initially identify topics discussed in Data Science. Hierarchical clustering resulted in six clusters of topics while PCA identified eleven dimensions in the Data Science field. In addition, topic modeling based on latent Dirichlet allocation (LDA) produced fifty topics related to Data Science. The researchers plan to further examine hierarchical, structural relationships between topics using structural equation modeling and social network analysis. The findings from this study will be useful to understand what topics are currently discussed in the area of Data Science.",2014-01-01,2-s2.0-84961904366,Proceedings of the ASIST Annual Meeting,Exploring topics in the field of data science by analyzing wikipedia documents: A preliminary result wikipedia documents: A preliminary result,"In this poster, topics in the field of Data Science were explored from Wikipedia documents based on clustering, principal component analysis (PCA), and topic modeling. As a pilot study, we analyzed part of the dataset of Wikipedia documents to initially identify topics discussed in Data Science. Hierarchical clustering resulted in six clusters of topics while PCA identified eleven dimensions in the Data Science field. In addition, topic modeling based on latent Dirichlet allocation (LDA) produced fifty topics related to Data Science. The researchers plan to further examine hierarchical, structural relationships between topics using structural equation modeling and social network analysis. The findings from this study will be useful to understand what topics are currently discussed in the area of Data Science."
489,"The rise of online social media has led to an explosion in user-generated content. However, user-generated content is difficult to analyze in isolation from its context. Accordingly, context detection and tracking its evolution is essential to understanding social media. This paper presents a statistical model that can detect interpretable topics along with their contexts. A topic is represented by a cluster of words that frequently occur together, and a context is represented by a cluster of hashtags that frequently occur with a topic. The model combines a context with a related topic by jointly modeling words with hashtags and time. Experiments on real datasets demonstrate that the proposed model successfully discovers both meaningful topics and contexts, and tracks their evolution.",2014-01-01,2-s2.0-84937715645,"International Conference on Information and Knowledge Management, Proceedings",Context over time: Modeling context evolution in social media,"The rise of online social media has led to an explosion in user-generated content. However, user-generated content is difficult to analyze in isolation from its context. Accordingly, context detection and tracking its evolution is essential to understanding social media. This paper presents a statistical model that can detect interpretable topics along with their contexts. A topic is represented by a cluster of words that frequently occur together, and a context is represented by a cluster of hashtags that frequently occur with a topic. The model combines a context with a related topic by jointly modeling words with hashtags and time. Experiments on real datasets demonstrate that the proposed model successfully discovers both meaningful topics and contexts, and tracks their evolution."
490,"It remains a challenge to deal with the diversity of the cross-domain feature space when using transfer learning in the recommendation system. To solve the difficulty, we propose a fusion topic model to extract the latent topic in the crossdomain. There are two layers in the proposed model. Firstly, the model simulates the user–item relationship in every subdomain with an author–topic model separately, and extracts the subdomain level topics. In addition, the model extracts the full-domain level topics in the whole domain using subdomain level topics as words in the author–topic model. By using Gibbs sampling, the method can extract two different levels of topics. The experiment on the public dataset shows our method has good performance. The results of the experiment indicate that extracting multilevel topics can help to discover the correlation in the MovieLens dataset and the Book-Crossing dataset, and to extract the crossdomain feature space.",2014-01-01,2-s2.0-84907864904,WIT Transactions on Information and Communication Technologies,Fusion topic model transfer learning for cross-domain recommendation,"It remains a challenge to deal with the diversity of the cross-domain feature space when using transfer learning in the recommendation system. To solve the difficulty, we propose a fusion topic model to extract the latent topic in the crossdomain. There are two layers in the proposed model. Firstly, the model simulates the user–item relationship in every subdomain with an author–topic model separately, and extracts the subdomain level topics. In addition, the model extracts the full-domain level topics in the whole domain using subdomain level topics as words in the author–topic model. By using Gibbs sampling, the method can extract two different levels of topics. The experiment on the public dataset shows our method has good performance. The results of the experiment indicate that extracting multilevel topics can help to discover the correlation in the MovieLens dataset and the Book-Crossing dataset, and to extract the crossdomain feature space."
491,"There are many news articles reported online everyday. Within an ongoing topic, people can find a huge amount of news articles. A topic often consists of several events, and people are interested in the whole evolution of a topic along a timeline. This requests for finding and identifying the dependent relationships between events. In order to understand the whole evolution of a topic effectively, we propose a framework of event relationship analysis. We define three kinds of event relationships which are coccurrence dependence relationship, event reference relationship, and temporal proximity relationship for modeling how an event is dependent on another event within a topic. Through combining three kinds of relationships, we can discover an Event Evolution Graph (EEG) for users to view the evolution of a topic. Experiments conducted on a real data set show that our method outperforms baseline methods.",2014-01-01,2-s2.0-84920730935,"Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014",Discovering event evolution graphs based on news articles relationships,"There are many news articles reported online everyday. Within an ongoing topic, people can find a huge amount of news articles. A topic often consists of several events, and people are interested in the whole evolution of a topic along a timeline. This requests for finding and identifying the dependent relationships between events. In order to understand the whole evolution of a topic effectively, we propose a framework of event relationship analysis. We define three kinds of event relationships which are coccurrence dependence relationship, event reference relationship, and temporal proximity relationship for modeling how an event is dependent on another event within a topic. Through combining three kinds of relationships, we can discover an Event Evolution Graph (EEG) for users to view the evolution of a topic. Experiments conducted on a real data set show that our method outperforms baseline methods."
492,"Managers and researchers alike have long recognized the importance of corporate textual risk disclosures. Yet it is a nontrivial task to discover and quantify variables of interest from unstructured text. In this paper, we develop a variation of the latent Dirichlet allocation topic model and its learning algorithm for simultaneously discovering and quantifying risk types from textual risk disclosures. We conduct comprehensive evaluations in terms of both conventional statistical fit and substantive fit with respect to the quality of discovered information. Experimental results show that our proposed method outperforms all competing methods, and could find more meaningful topics (risk types). By taking advantage of our proposed method for measuring risk types from textual data, we study how risk disclosures in 10-K forms affect the risk perceptions of investors. Different from prior studies, our results provide support for all three competing arguments regarding whether and how risk disclosures affect the risk perceptions of investors, depending on the specific risk types disclosed. We find that around two-thirds of risk types lack informativeness and have no significant influence. Moreover, we find that the informative risk types do not necessarily increase the risk perceptions of investors-the disclosure of three types of systematic and liquidity risks will increase the risk perceptions of investors, whereas the other five types of unsystematic risks will decrease them. © 2014 INFORMS.",2014-01-01,2-s2.0-84902288534,Management Science,Simultaneously discovering and quantifying risk types from textual risk disclosures,"Managers and researchers alike have long recognized the importance of corporate textual risk disclosures. Yet it is a nontrivial task to discover and quantify variables of interest from unstructured text. In this paper, we develop a variation of the latent Dirichlet allocation topic model and its learning algorithm for simultaneously discovering and quantifying risk types from textual risk disclosures. We conduct comprehensive evaluations in terms of both conventional statistical fit and substantive fit with respect to the quality of discovered information. Experimental results show that our proposed method outperforms all competing methods, and could find more meaningful topics (risk types). By taking advantage of our proposed method for measuring risk types from textual data, we study how risk disclosures in 10-K forms affect the risk perceptions of investors. Different from prior studies, our results provide support for all three competing arguments regarding whether and how risk disclosures affect the risk perceptions of investors, depending on the specific risk types disclosed. We find that around two-thirds of risk types lack informativeness and have no significant influence. Moreover, we find that the informative risk types do not necessarily increase the risk perceptions of investors-the disclosure of three types of systematic and liquidity risks will increase the risk perceptions of investors, whereas the other five types of unsystematic risks will decrease them. "
493,"Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the “forced topic” problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.",2014-01-01,2-s2.0-84921377150,Information Retrieval,Group topic model: organizing topics into groups,"Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the “forced topic” problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches."
494,"Graph-based learning algorithms have been shown to be an effective approach for query-focused multi-document summarization (MDS). In this paper, we extend the standard graph ranking algorithm by proposing a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach based on topic modeling techniques. Experimental results on TAC datasets show that by considering topic information, we can effectively improve the summary performance.",2014-01-01,2-s2.0-84951870623,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",Query-focused multi-document summarization: Combining a topic model with graph-based Semi-supervised Learning,"Graph-based learning algorithms have been shown to be an effective approach for query-focused multi-document summarization (MDS). In this paper, we extend the standard graph ranking algorithm by proposing a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach based on topic modeling techniques. Experimental results on TAC datasets show that by considering topic information, we can effectively improve the summary performance."
495,"Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.'s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906928276,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",Anchors regularized: Adding robustness and extensibility to scalable topic-modeling algorithms,"Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.'s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. "
496,"In this paper, we present multiple approaches to improve sentiment analysis on Twitter data. We first establish a state-of-the-art baseline with a rich feature set. Then we build a topic-based sentiment mixture model with topic-specific data in a semi-supervised training framework. The topic information is generated through topic modeling based on an efficient implementation of Latent Dirichlet Allocation (LDA). The proposed sentiment model outperforms the top system in the task of Sentiment Analysis in Twitter in SemEval-2013 in terms of averaged F scores. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906922088,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",Improving Twitter sentiment analysis with topic-based mixture modeling and semi-supervised training,"In this paper, we present multiple approaches to improve sentiment analysis on Twitter data. We first establish a state-of-the-art baseline with a rich feature set. Then we build a topic-based sentiment mixture model with topic-specific data in a semi-supervised training framework. The topic information is generated through topic modeling based on an efficient implementation of Latent Dirichlet Allocation (LDA). The proposed sentiment model outperforms the top system in the task of Sentiment Analysis in Twitter in SemEval-2013 in terms of averaged F scores. "
497,"Research and education are organically connected in that lectures convey the results of research, which is frequently initiated by inspiring lectures. As a result, the contents of lecture materials and research publications and the research capabilities of universities should be considered in the investigations of the relationships between research and teaching. We examine the relationship between research and teaching using automatic text analysis. In particular, we scrutinize the relatedness of the content of research papers with the content of lecture materials to investigate the association between teaching and research. We adopt topic modeling for the correlation analysis of research capabilities and the reflectiveness of research topics in lecture materials. We select the field of machine learning as a case study because the field is contemporary and because data related to teaching and research are easily accessible via the Internet. The results reveal interesting characteristics of lecture materials and research publications in the field of machine learning. The research capability of an institute is independent of the lecture materials. However, for introductory courses, teaching and research measures showed a weak negative relationship, and there is little relationship between the measures for advanced courses.",2014-01-01,2-s2.0-84921843423,Scientometrics,Coherence analysis of research and education using topic modeling,"Research and education are organically connected in that lectures convey the results of research, which is frequently initiated by inspiring lectures. As a result, the contents of lecture materials and research publications and the research capabilities of universities should be considered in the investigations of the relationships between research and teaching. We examine the relationship between research and teaching using automatic text analysis. In particular, we scrutinize the relatedness of the content of research papers with the content of lecture materials to investigate the association between teaching and research. We adopt topic modeling for the correlation analysis of research capabilities and the reflectiveness of research topics in lecture materials. We select the field of machine learning as a case study because the field is contemporary and because data related to teaching and research are easily accessible via the Internet. The results reveal interesting characteristics of lecture materials and research publications in the field of machine learning. The research capability of an institute is independent of the lecture materials. However, for introductory courses, teaching and research measures showed a weak negative relationship, and there is little relationship between the measures for advanced courses."
498,"In consideration of the features of micro-blogging content such as short text, sparse feature words and the huge scale, a method to detect micro-blogging hot topic was proposed in this paper based on MapReduce programming model. This method first employs the hidden topic analysis to solve the problem of short micro-blogging content and sparse feature words. Then the CURE algorithm is used to alleviate the problem that the Kmeans algorithm is sensitive to the initial points. Finally, the hot topic clustering results are obtained through the parallel Kmeans clustering algorithm based on the MapReduce programming model. The experimental results show that proposed method can effectively improve the micro-blogging hot topic detection efficiency.",2014-01-01,2-s2.0-84928012766,"International Conference on Logistics, Engineering, Management and Computer Science, LEMCS 2014",A distributed approach for Chinese microblog hot topic detection,"In consideration of the features of micro-blogging content such as short text, sparse feature words and the huge scale, a method to detect micro-blogging hot topic was proposed in this paper based on MapReduce programming model. This method first employs the hidden topic analysis to solve the problem of short micro-blogging content and sparse feature words. Then the CURE algorithm is used to alleviate the problem that the Kmeans algorithm is sensitive to the initial points. Finally, the hot topic clustering results are obtained through the parallel Kmeans clustering algorithm based on the MapReduce programming model. The experimental results show that proposed method can effectively improve the micro-blogging hot topic detection efficiency."
499,"With the considerable growth of user-generated content, online reviews are becoming extremely valuable sources for mining customers' opinions on products and services. However, most of the traditional opinion mining methods are coarse-grained and cannot understand natural languages. Thus, aspect-based opinion mining and summarization are of great interest in academic and industrial research. In this paper, we study an approach to extract product and service aspect words, as well as sentiment words, automatically from reviews. An unsupervised dependency analysis-based approach is presented to extract Appraisal Expression Patterns (AEPs) from reviews, which represent the manner in which people express opinions regarding products or services and can be regarded as a condensed representation of the syntactic relationship between aspect and sentiment words. AEPs are high-level, domain-independent types of information, and have excellent domain adaptability. An AEP-based Latent Dirichlet Allocation (AEP-LDA) model is also proposed. This is a sentence-level, probabilistic generative model which assumes that all words in a sentence are drawn from one topic - a generally true assumption, based on our observation. The model also assumes that every review corpus is composed of several mutually corresponding aspect and sentiment topics, as well as a background word topic. The AEP information is incorporated into the AEP-LDA model for mining aspect and sentiment words simultaneously. The experimental results on reviews of restaurants, hotels, MP3 players, and cameras show that the AEP-LDA model outperforms other approaches in identifying aspect and sentiment words. © 2014 Elsevier B.V. All rights reserved.",2014-01-01,2-s2.0-84897411279,Knowledge-Based Systems,Incorporating appraisal expression patterns into topic modeling for aspect and sentiment word identification,"With the considerable growth of user-generated content, online reviews are becoming extremely valuable sources for mining customers' opinions on products and services. However, most of the traditional opinion mining methods are coarse-grained and cannot understand natural languages. Thus, aspect-based opinion mining and summarization are of great interest in academic and industrial research. In this paper, we study an approach to extract product and service aspect words, as well as sentiment words, automatically from reviews. An unsupervised dependency analysis-based approach is presented to extract Appraisal Expression Patterns (AEPs) from reviews, which represent the manner in which people express opinions regarding products or services and can be regarded as a condensed representation of the syntactic relationship between aspect and sentiment words. AEPs are high-level, domain-independent types of information, and have excellent domain adaptability. An AEP-based Latent Dirichlet Allocation (AEP-LDA) model is also proposed. This is a sentence-level, probabilistic generative model which assumes that all words in a sentence are drawn from one topic - a generally true assumption, based on our observation. The model also assumes that every review corpus is composed of several mutually corresponding aspect and sentiment topics, as well as a background word topic. The AEP information is incorporated into the AEP-LDA model for mining aspect and sentiment words simultaneously. The experimental results on reviews of restaurants, hotels, MP3 players, and cameras show that the AEP-LDA model outperforms other approaches in identifying aspect and sentiment words. "
500,"One of the most challenging problems in aspect-based opinion mining is aspect extraction, which aims to identify expressions that describe aspects of products (called aspect expressions) and categorize domain-specific synonymous expressions. Although a number of methods of aspect extraction have been proposed before, very few of them are designed to improve the interpretability of generated aspects. Existing methods either generate multiple fine-grained aspects without proper categorization or categorize semantically unrelated product aspects (e.g., by unsupervised topic modeling). In this paper, we first examine previous studies on product aspect extraction. To overcome the limitations of existing methods, two novel semi-supervised models for product aspect extraction are then proposed. More specifically, the proposed methodology first extracts seeding aspects and related terms from detailed product descriptions readily available on E-commerce websites. Next, product reviews are regrouped according to these seeding aspects so that more effective textual contexts for topic modeling are built. Finally, two novel semi-supervised topic models are developed to extract human-comprehensible product aspects. For the first proposed topic model, the Fine-grained Labeled LDA (FL-LDA), seeding aspects are applied to guide the model to discover words that are related to these seeding aspects. For the second model, the Unified Fine-grained Labeled LDA (UFL-LDA), we incorporate unlabeled documents to extend the FL-LDA model so that words related to the seeding aspects or other high-frequency words in customer reviews are extracted. Our experimental results demonstrate that the proposed methods outperform state-of-The-art methods.",2014-01-01,2-s2.0-84908025051,Knowledge-Based Systems,Product aspect extraction supervised with online domain knowledge,"One of the most challenging problems in aspect-based opinion mining is aspect extraction, which aims to identify expressions that describe aspects of products (called aspect expressions) and categorize domain-specific synonymous expressions. Although a number of methods of aspect extraction have been proposed before, very few of them are designed to improve the interpretability of generated aspects. Existing methods either generate multiple fine-grained aspects without proper categorization or categorize semantically unrelated product aspects (e.g., by unsupervised topic modeling). In this paper, we first examine previous studies on product aspect extraction. To overcome the limitations of existing methods, two novel semi-supervised models for product aspect extraction are then proposed. More specifically, the proposed methodology first extracts seeding aspects and related terms from detailed product descriptions readily available on E-commerce websites. Next, product reviews are regrouped according to these seeding aspects so that more effective textual contexts for topic modeling are built. Finally, two novel semi-supervised topic models are developed to extract human-comprehensible product aspects. For the first proposed topic model, the Fine-grained Labeled LDA (FL-LDA), seeding aspects are applied to guide the model to discover words that are related to these seeding aspects. For the second model, the Unified Fine-grained Labeled LDA (UFL-LDA), we incorporate unlabeled documents to extend the FL-LDA model so that words related to the seeding aspects or other high-frequency words in customer reviews are extracted. Our experimental results demonstrate that the proposed methods outperform state-of-The-art methods."
501,"In this study, we used the approach of topic modeling to uncover the possible structure of research topics in the field of Informetrics, to explore the distribution of the topics over years, and to compare the core journals. In order to infer the structure of the topics in the field, the data of the papers published in the Journal of Informetrics and Scientometrics during 2007 to 2013 are retrieved from the database of the Web of Science as input of the approach of topic modeling. The results of this study show that when the number of topics was set to 10, the topic model has the smallest perplexity. Although data scopes and analysis methods are different to previous studies, the generating topics of this study are consistent with those results produced by analyses of experts. Empirical case studies and measurements of bibliometric indicators were concerned important in every year during the whole analytic period, and the field was increasing stability. Both the two core journals broadly paid more attention to all of the topics in the field of Informetrics. The Journal of Informetrics put particular emphasis on construction and applications of bibliometric indicators and Scientometrics focused on the evaluation and the factors of productivity of countries, institutions, domains, and journals.",2014-01-01,2-s2.0-84925454869,Journal of Educational Media and Library Sciences,Analyses of Research Topics in the Field of Informetrics Based on the Method of Topic Modeling,"In this study, we used the approach of topic modeling to uncover the possible structure of research topics in the field of Informetrics, to explore the distribution of the topics over years, and to compare the core journals. In order to infer the structure of the topics in the field, the data of the papers published in the Journal of Informetrics and Scientometrics during 2007 to 2013 are retrieved from the database of the Web of Science as input of the approach of topic modeling. The results of this study show that when the number of topics was set to 10, the topic model has the smallest perplexity. Although data scopes and analysis methods are different to previous studies, the generating topics of this study are consistent with those results produced by analyses of experts. Empirical case studies and measurements of bibliometric indicators were concerned important in every year during the whole analytic period, and the field was increasing stability. Both the two core journals broadly paid more attention to all of the topics in the field of Informetrics. The Journal of Informetrics put particular emphasis on construction and applications of bibliometric indicators and Scientometrics focused on the evaluation and the factors of productivity of countries, institutions, domains, and journals."
502,"Twitter is used extensively in the United States as well as globally, creating many opportunities to augment decision support systems with Twitter-driven predictive analytics. Twitter is an ideal data source for decision support: its users, who number in the millions, publicly discuss events, emotions, and innumerable other topics; its content is authored and distributed in real time at no charge; and individual messages (also known as tweets) are often tagged with precise spatial and temporal coordinates. This article presents research investigating the use of spatiotemporally tagged tweets for crime prediction. We use Twitter-specific linguistic analysis and statistical topic modeling to automatically identify discussion topics across a major city in the United States. We then incorporate these topics into a crime prediction model and show that, for 19 of the 25 crime types we studied, the addition of Twitter data improves crime prediction performance versus a standard approach based on kernel density estimation. We identify a number of performance bottlenecks that could impact the use of Twitter in an actual decision support system. We also point out important areas of future work for this research, including deeper semantic analysis of message content, temporal modeling, and incorporation of auxiliary data sources. This research has implications specifically for criminal justice decision makers in charge of resource allocation for crime prevention. More generally, this research has implications for decision makers concerned with geographic spaces occupied by Twitter-using individuals. ©2014 Elsevier B.V. All rights reserved.",2014-01-01,2-s2.0-84897492871,Decision Support Systems,Predicting crime using Twitter and kernel density estimation,"Twitter is used extensively in the United States as well as globally, creating many opportunities to augment decision support systems with Twitter-driven predictive analytics. Twitter is an ideal data source for decision support: its users, who number in the millions, publicly discuss events, emotions, and innumerable other topics; its content is authored and distributed in real time at no charge; and individual messages (also known as tweets) are often tagged with precise spatial and temporal coordinates. This article presents research investigating the use of spatiotemporally tagged tweets for crime prediction. We use Twitter-specific linguistic analysis and statistical topic modeling to automatically identify discussion topics across a major city in the United States. We then incorporate these topics into a crime prediction model and show that, for 19 of the 25 crime types we studied, the addition of Twitter data improves crime prediction performance versus a standard approach based on kernel density estimation. We identify a number of performance bottlenecks that could impact the use of Twitter in an actual decision support system. We also point out important areas of future work for this research, including deeper semantic analysis of message content, temporal modeling, and incorporation of auxiliary data sources. This research has implications specifically for criminal justice decision makers in charge of resource allocation for crime prevention. More generally, this research has implications for decision makers concerned with geographic spaces occupied by Twitter-using individuals. "
503,"Aspect extraction is an important task in sentiment analysis. Topic modeling is a popular method for the task. However, unsupervised topic models often generate incoherent aspects. To address the issue, several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling. In this paper, we take a major step forward and show that in the big data era, without any user input, it is possible to learn prior knowledge automatically from a large amount of review data available on the Web. Such knowledge can then be used by a topic model to discover more coherent aspects. There are two key challenges: (1) learning quality knowledge from reviews of diverse domains, and (2) making the model fault-tolerant to handle possibly wrong knowledge. A novel approach is proposed to solve these problems. Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906929305,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",Aspect extraction with automated prior knowledge learning,"Aspect extraction is an important task in sentiment analysis. Topic modeling is a popular method for the task. However, unsupervised topic models often generate incoherent aspects. To address the issue, several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling. In this paper, we take a major step forward and show that in the big data era, without any user input, it is possible to learn prior knowledge automatically from a large amount of review data available on the Web. Such knowledge can then be used by a topic model to discover more coherent aspects. There are two key challenges: (1) learning quality knowledge from reviews of diverse domains, and (2) making the model fault-tolerant to handle possibly wrong knowledge. A novel approach is proposed to solve these problems. Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines. "
504,"A knowledge organization system (KOS) can help easily indicate the deep knowledge structure of a patent document set. Compared to classification code systems, a personalized KOS made up of topics can represent the technology information in a more agile, detailed manner. This paper presents an approach to automatically construct a KOS of patent documents based on term clumping, Latent Dirichlet Allocation (LDA) model, K-Means clustering and Principal Components Analysis (PCA). Term clumping is adopted to generate a better bag-of-words for topic modeling and LDA model is applied to generate raw topics. Then by iteratively using K-Means clustering and PCA on the document set and topics matrix, we generated new upper topics and computed the relationships between topics to construct a KOS. Finally, documents are mapped to the KOS. The nodes of the KOS are topics which are represented by terms and their weights and the leaves are patent documents. We evaluated the approach with a set of Large Aperture Optical Elements (LAOE) patent documents as an empirical study and constructed the LAOE KOS. The method used discovered the deep semantic relationships between the topics and helped better describe the technology themes of LAOE. Based on the KOS, two types of applications were implemented: the automatic classification of patents documents and the categorical refinements above search results. © 2014 Akadémiai Kiadó, Budapest, Hungary.",2014-01-01,2-s2.0-84906081668,Scientometrics,Empirical study of constructing a knowledge organization system of patent documents using topic modeling,"A knowledge organization system (KOS) can help easily indicate the deep knowledge structure of a patent document set. Compared to classification code systems, a personalized KOS made up of topics can represent the technology information in a more agile, detailed manner. This paper presents an approach to automatically construct a KOS of patent documents based on term clumping, Latent Dirichlet Allocation (LDA) model, K-Means clustering and Principal Components Analysis (PCA). Term clumping is adopted to generate a better bag-of-words for topic modeling and LDA model is applied to generate raw topics. Then by iteratively using K-Means clustering and PCA on the document set and topics matrix, we generated new upper topics and computed the relationships between topics to construct a KOS. Finally, documents are mapped to the KOS. The nodes of the KOS are topics which are represented by terms and their weights and the leaves are patent documents. We evaluated the approach with a set of Large Aperture Optical Elements (LAOE) patent documents as an empirical study and constructed the LAOE KOS. The method used discovered the deep semantic relationships between the topics and helped better describe the technology themes of LAOE. Based on the KOS, two types of applications were implemented: the automatic classification of patents documents and the categorical refinements above search results. "
505,"Public comments submitted during agency rulemakings can provide rich insight into stakeholders' viewpoints around contentious political issues but have been largely untapped as a data source by social scientists. This is in part due to the lack of access to comments in machine-readable formats and in part due to the difficulty in analyzing large corpora of textual data. However, new online repositories and analytic methodologies are beginning to open up this trove of data for researchers. Using data from the online portal regulations.gov, we employ probabilistic topic modeling to identify latent themes in a series of regulatory debates about electronic monitoring in the U.S. trucking industry. Our model suggests that different types of commenters use alternative discursive frames in talking about monitoring. Comments submitted by individuals were more likely to place the electronic monitoring debate in the context of broader logistical problems plaguing the industry, such as long wait times at shippers' terminals, while organizational stakeholders were more likely than individuals to frame their comments in terms of technological standards and language suggesting cost / benefit quantification. © The Author(s) 2013.",2014-01-01,2-s2.0-84898883621,Social Science Computer Review,Driving Regulation: Using Topic Models to Examine Political Contention in the U.S. Trucking Industry,"Public comments submitted during agency rulemakings can provide rich insight into stakeholders' viewpoints around contentious political issues but have been largely untapped as a data source by social scientists. This is in part due to the lack of access to comments in machine-readable formats and in part due to the difficulty in analyzing large corpora of textual data. However, new online repositories and analytic methodologies are beginning to open up this trove of data for researchers. Using data from the online portal regulations.gov, we employ probabilistic topic modeling to identify latent themes in a series of regulatory debates about electronic monitoring in the U.S. trucking industry. Our model suggests that different types of commenters use alternative discursive frames in talking about monitoring. Comments submitted by individuals were more likely to place the electronic monitoring debate in the context of broader logistical problems plaguing the industry, such as long wait times at shippers' terminals, while organizational stakeholders were more likely than individuals to frame their comments in terms of technological standards and language suggesting cost / benefit quantification. "
506,"This study proposes a temporal analysis method to utilize heterogeneous resources such as papers, patents, and web news articles in an integrated manner. We analyzed the time gap phenomena between three resources and two academic areas by conducting text mining-based content analysis. To this end, a topic modeling technique, Latent Dirichlet Allocation (LDA) was used to estimate the optimal time gaps among three resources (papers, patents, and web news articles) in two research domains. The contributions of this study are summarized as follows: firstly, we propose a new temporal analysis method to understand the content characteristics and trends of heterogeneous multiple resources in an integrated manner. We applied it to measure the exact time intervals between academic areas by understanding the time gap phenomena. The results of temporal analysis showed that the resources of the medical field had more up-to-date property than those of the computer field, and thus prompter disclosure to the public. Secondly, we adopted a power-law exponent measurement and content analysis to evaluate the proposed method. With the proposed method, we demonstrate how to analyze heterogeneous resources more precisely and comprehensively. © 2014 Elsevier Ltd.",2014-01-01,2-s2.0-84924525150,Journal of Informetrics,Time gap analysis by the topic model-based temporal technique,"This study proposes a temporal analysis method to utilize heterogeneous resources such as papers, patents, and web news articles in an integrated manner. We analyzed the time gap phenomena between three resources and two academic areas by conducting text mining-based content analysis. To this end, a topic modeling technique, Latent Dirichlet Allocation (LDA) was used to estimate the optimal time gaps among three resources (papers, patents, and web news articles) in two research domains. The contributions of this study are summarized as follows: firstly, we propose a new temporal analysis method to understand the content characteristics and trends of heterogeneous multiple resources in an integrated manner. We applied it to measure the exact time intervals between academic areas by understanding the time gap phenomena. The results of temporal analysis showed that the resources of the medical field had more up-to-date property than those of the computer field, and thus prompter disclosure to the public. Secondly, we adopted a power-law exponent measurement and content analysis to evaluate the proposed method. With the proposed method, we demonstrate how to analyze heterogeneous resources more precisely and comprehensively. "
507,"Probabilistic topic models are statistical methods whose aim is to discover the latent structure in a large collection of documents. The intuition behind topic models is that, by generating documents by latent topics, the word distribution for each topic can be modelled and the prior distribution over the topic learned. In this paper we propose to apply this concept by modelling the topics of sentences for the aspect detection problem in review documents in order to improve sentiment analysis systems. Aspect detection in sentiment analysis helps customers effectively navigate into detailed information about their features of interest. The proposed approach assumes that the aspects of words in a sentence form a Markov chain. The novelty of the model is the extraction of multiword aspects from text data while relaxing the bag-of-words assumption. Experimental results show that the model is indeed able to perform the task significantly better when compared with standard topic models.",2014-01-01,2-s2.0-84907098242,Journal of Information Science,ADM-LDA: An aspect detection model based on topic modelling using the structure of review sentences,"Probabilistic topic models are statistical methods whose aim is to discover the latent structure in a large collection of documents. The intuition behind topic models is that, by generating documents by latent topics, the word distribution for each topic can be modelled and the prior distribution over the topic learned. In this paper we propose to apply this concept by modelling the topics of sentences for the aspect detection problem in review documents in order to improve sentiment analysis systems. Aspect detection in sentiment analysis helps customers effectively navigate into detailed information about their features of interest. The proposed approach assumes that the aspects of words in a sentence form a Markov chain. The novelty of the model is the extraction of multiword aspects from text data while relaxing the bag-of-words assumption. Experimental results show that the model is indeed able to perform the task significantly better when compared with standard topic models."
508,"Automatic lexical acquisition has been an active area of research in computational linguistics for over two decades, but the automatic identification of new word-senses has received attention only very recently. Previous work on this topic has been limited by the availability of appropriate evaluation resources. In this paper we present the largest corpus-based dataset of diachronic sense differences to date, which we believe will encourage further work in this area. We then describe several extensions to a state-of-the-art topic modelling approach for identifying new word-senses. This adapted method shows superior performance on our dataset of two different corpus pairs to that of the original method for both: (a) types having taken on a novel sense over time; and (b) the token instances of such novel senses.",2014-01-01,2-s2.0-84945972429,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",Novel word-sense identification,"Automatic lexical acquisition has been an active area of research in computational linguistics for over two decades, but the automatic identification of new word-senses has received attention only very recently. Previous work on this topic has been limited by the availability of appropriate evaluation resources. In this paper we present the largest corpus-based dataset of diachronic sense differences to date, which we believe will encourage further work in this area. We then describe several extensions to a state-of-the-art topic modelling approach for identifying new word-senses. This adapted method shows superior performance on our dataset of two different corpus pairs to that of the original method for both: (a) types having taken on a novel sense over time; and (b) the token instances of such novel senses."
509,"In this paper we present a statistical machine learning approach to formal neologism detection going some way beyond the use of exclusion lists. We explore the impact of three groups of features: form related, morpho-lexical and thematic features. The latter type of features has not yet been used in this kind of application and represents a way to access the semantic context of new words. The results suggest that form related features are helpful at the overall classification task, while morpho-lexical and thematic features better single out true neologisms.",2014-01-01,2-s2.0-85021226908,"Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014",From non word to new word: Automatically identifying neologisms in French newspapers,"In this paper we present a statistical machine learning approach to formal neologism detection going some way beyond the use of exclusion lists. We explore the impact of three groups of features: form related, morpho-lexical and thematic features. The latter type of features has not yet been used in this kind of application and represents a way to access the semantic context of new words. The results suggest that form related features are helpful at the overall classification task, while morpho-lexical and thematic features better single out true neologisms."
510,"Emotion lexicons play a crucial role in sentiment analysis and opinion mining. In this paper, we propose a novel Emotion-aware LDA (EaLDA) model to build a domainspecific lexicon for predefined emotions that include anger, disgust, fear, joy, sadness, surprise. The model uses a minimal set of domain-independent seed words as prior knowledge to discover a domainspecific lexicon, learning a fine-grained emotion lexicon much richer and adaptive to a specific domain. By comprehensive experiments, we show that our model can generate a high-quality fine-grained domain-specific emotion lexicon. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906923438,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",A topic model for building fine-grained domain-specific emotion lexicon,"Emotion lexicons play a crucial role in sentiment analysis and opinion mining. In this paper, we propose a novel Emotion-aware LDA (EaLDA) model to build a domainspecific lexicon for predefined emotions that include anger, disgust, fear, joy, sadness, surprise. The model uses a minimal set of domain-independent seed words as prior knowledge to discover a domainspecific lexicon, learning a fine-grained emotion lexicon much richer and adaptive to a specific domain. By comprehensive experiments, we show that our model can generate a high-quality fine-grained domain-specific emotion lexicon. "
511,"Topic modelling has been popularly used to discover latent topics from text documents. Most existing models work on individual words. That is, they treat each topic as a distribution over words. However, using only individual words has several shortcomings. First, it increases the co-occurrences of words which may be incorrect because a phrase with two words is not equivalent to two separate words. These extra and often incorrect co-occurrences result in poorer output topics. A multi-word phrase should be treated as one term by itself. Second, individual words are often difficult to use in practice because the meaning of a word in a phrase and the meaning of a word in isolation can be quite different. Third, topics as a list of individual words are also difficult to understand by users who are not domain experts and do not have any knowledge of topic models. In this paper, we aim to solve these problems by considering phrases in their natural form. One simple way to include phrases in topic modelling is to treat each phrase as a single term. However, this method is not ideal because the meaning of a phrase is often related to its composite words. That information is lost. This paper proposes to use the generalized Pólya Urn (GPU) model to solve the problem, which gives superior results. GPU enables the connection of a phrase with its content words naturally. Our experimental results using 32 review datasets show that the proposed approach is highly effective.",2014-01-01,2-s2.0-84925450583,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",Review topic discovery with phrases using the pólya urn model,"Topic modelling has been popularly used to discover latent topics from text documents. Most existing models work on individual words. That is, they treat each topic as a distribution over words. However, using only individual words has several shortcomings. First, it increases the co-occurrences of words which may be incorrect because a phrase with two words is not equivalent to two separate words. These extra and often incorrect co-occurrences result in poorer output topics. A multi-word phrase should be treated as one term by itself. Second, individual words are often difficult to use in practice because the meaning of a word in a phrase and the meaning of a word in isolation can be quite different. Third, topics as a list of individual words are also difficult to understand by users who are not domain experts and do not have any knowledge of topic models. In this paper, we aim to solve these problems by considering phrases in their natural form. One simple way to include phrases in topic modelling is to treat each phrase as a single term. However, this method is not ideal because the meaning of a phrase is often related to its composite words. That information is lost. This paper proposes to use the generalized Pólya Urn (GPU) model to solve the problem, which gives superior results. GPU enables the connection of a phrase with its content words naturally. Our experimental results using 32 review datasets show that the proposed approach is highly effective."
512,"Probabilistic topic modeling is a rapidly developing branch of statistical text analysis. The topic model uncovers a hidden thematic structure of the text collection. Learning a topic model from a document collection has an infinite set of solutions. The nonuniqueness results in a weak interpretability and instability of the solution. To tackle these problems we use a new multiobjective approach - Additive Regularization of Topic Models (ARTM). ARTM is a non-Bayesian framework free of redundant probabilistic assumptions, which dramatically simplifies the inference of topic models and makes topic models easy to design, infer, and explain. With ARTM we combine four regularizers to concentrate common vocabulary words in background topics, to make domain topics sparse and distinct, and to eliminate insignificant topics. In our experiments the combination of the regularizers improves sparsity, coherence, purity, and contrast criteria at once almost without any loss of the perplexity.",2014-01-01,2-s2.0-84904806775,Komp'juternaja Lingvistika i Intellektual'nye Tehnologii,Regularization of probabilistic topic models to improve interpretability and determine the number of topics,"Probabilistic topic modeling is a rapidly developing branch of statistical text analysis. The topic model uncovers a hidden thematic structure of the text collection. Learning a topic model from a document collection has an infinite set of solutions. The nonuniqueness results in a weak interpretability and instability of the solution. To tackle these problems we use a new multiobjective approach - Additive Regularization of Topic Models (ARTM). ARTM is a non-Bayesian framework free of redundant probabilistic assumptions, which dramatically simplifies the inference of topic models and makes topic models easy to design, infer, and explain. With ARTM we combine four regularizers to concentrate common vocabulary words in background topics, to make domain topics sparse and distinct, and to eliminate insignificant topics. In our experiments the combination of the regularizers improves sparsity, coherence, purity, and contrast criteria at once almost without any loss of the perplexity."
513,"A generalization of Bregman divergence is developed and utilized to unify vector Poisson and Gaussian channel models, from the perspective of the gradient of mutual information. The gradient is with respect to the measurement matrix in a compressive-sensing setting, and mutual information is considered for signal recovery and classification. Existing gradient-of-mutual-information results for scalar Poisson models are recovered as special cases, as are known results for the vector Gaussian model. The Bregman-divergence generalization yields a Bregman matrix, and this matrix induces numerous matrix-valued metrics. The metrics associated with the Bregman matrix are detailed, as are its other properties. The Bregman matrix is also utilized to connect the relative entropy and mismatched minimum mean squared error. Two applications are considered: 1) compressive sensing with a Poisson measurement model and 2) compressive topic modeling for analysis of a document corpora (word-count data). In both of these settings, we use the developed theory to optimize the compressive measurement matrix, for signal recovery and classification. © 1963-2012 IEEE.",2014-01-01,2-s2.0-84899632397,IEEE Transactions on Information Theory,A bregman matrix and the gradient of mutual information for vector poisson and gaussian channels,"A generalization of Bregman divergence is developed and utilized to unify vector Poisson and Gaussian channel models, from the perspective of the gradient of mutual information. The gradient is with respect to the measurement matrix in a compressive-sensing setting, and mutual information is considered for signal recovery and classification. Existing gradient-of-mutual-information results for scalar Poisson models are recovered as special cases, as are known results for the vector Gaussian model. The Bregman-divergence generalization yields a Bregman matrix, and this matrix induces numerous matrix-valued metrics. The metrics associated with the Bregman matrix are detailed, as are its other properties. The Bregman matrix is also utilized to connect the relative entropy and mismatched minimum mean squared error. Two applications are considered: 1) compressive sensing with a Poisson measurement model and 2) compressive topic modeling for analysis of a document corpora (word-count data). In both of these settings, we use the developed theory to optimize the compressive measurement matrix, for signal recovery and classification. "
514,"The sense in which a word is used determines the translation of the word. In this paper, we propose a sense-based translation model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature. We test the effectiveness of the proposed sensebased translation model on a large-scale Chinese-to-English translation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906926164,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",A sense-based translation model for statistical machine translation,"The sense in which a word is used determines the translation of the word. In this paper, we propose a sense-based translation model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. Our method is significantly different from previous word sense disambiguation reformulated for machine translation in that the latter neglects word senses in nature. We test the effectiveness of the proposed sensebased translation model on a large-scale Chinese-to-English translation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation. "
515,"Within the same research field, different subfields and topics may exhibit varied citation behaviors and scholarly communication patterns. For a more effect scientific evaluation at the topic level, this study proposes a topic-based PageRank approach. This approach aims to evaluate the scientific impact of research entities (e.g., papers, authors, journals, and institutions) at the topic-level. The proposed topic-based PageRank, when applied to a data set on library and information science publications, has effectively detected a variety of research topics and identified authors, papers, and journals of the highest impact from each topic. Evaluation results show that compared with the standard PageRank and a topic modeling technique, the proposed topic-based PageRank has the best performance on relevance and impact. Different perspectives of organizing scientific literature are also discussed and this study recommends the mode of organization that integrates stable research domains and dynamic topics. © 2014 Akadémiai Kiadó, Budapest, Hungary.",2014-01-01,2-s2.0-84904116482,Scientometrics,Topic-based Pagerank: Toward a topic-level scientific evaluation,"Within the same research field, different subfields and topics may exhibit varied citation behaviors and scholarly communication patterns. For a more effect scientific evaluation at the topic level, this study proposes a topic-based PageRank approach. This approach aims to evaluate the scientific impact of research entities (e.g., papers, authors, journals, and institutions) at the topic-level. The proposed topic-based PageRank, when applied to a data set on library and information science publications, has effectively detected a variety of research topics and identified authors, papers, and journals of the highest impact from each topic. Evaluation results show that compared with the standard PageRank and a topic modeling technique, the proposed topic-based PageRank has the best performance on relevance and impact. Different perspectives of organizing scientific literature are also discussed and this study recommends the mode of organization that integrates stable research domains and dynamic topics. "
516,"Unsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data. Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions. This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text. We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren't attested in the corpus, and identifying novel senses in the corpus which aren't captured in the sense inventory. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906924147,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference","Learning word sense distributions, detecting unattested senses and identifying novel senses using topic models","Unsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data. Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions. This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text. We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren't attested in the corpus, and identifying novel senses in the corpus which aren't captured in the sense inventory. "
517,"A great deal of work has been done to understand how science contributes to technological innovation and medicine. This is no surprise given the amount of money invested annually in R&D. However, what is not well known is that US science (R&D) investment is only one-sixth that of the annual revenue received by non-profit organizations (NPOs) in the US. The large majority of NPO revenues are devoted to the remaining landscape of altruistic causes - those not relying as heavily on scientific inquiry. Given this broader context, one might reasonably expect the non-profit world to have been as well characterized as that of scientific research. The unfortunate truth is that no map of altruistic missions and causes exists; the landscape of altruistic activity is virtually unknown. In this paper, we present the first maps of altruistic mission space. These maps were created using the text from websites of 125,000 non-profit organizations (NPOs) in the US. The maps consist of 357 topics covering areas such as religion, education, sports, culture, human services, public policy and medical care. The role of science in this altruistic landscape is examined. Possible applications are discussed. © 2014 Elsevier Ltd.",2014-01-01,2-s2.0-84899562519,Journal of Informetrics,Mapping altruism,"A great deal of work has been done to understand how science contributes to technological innovation and medicine. This is no surprise given the amount of money invested annually in R&D. However, what is not well known is that US science (R&D) investment is only one-sixth that of the annual revenue received by non-profit organizations (NPOs) in the US. The large majority of NPO revenues are devoted to the remaining landscape of altruistic causes - those not relying as heavily on scientific inquiry. Given this broader context, one might reasonably expect the non-profit world to have been as well characterized as that of scientific research. The unfortunate truth is that no map of altruistic missions and causes exists; the landscape of altruistic activity is virtually unknown. In this paper, we present the first maps of altruistic mission space. These maps were created using the text from websites of 125,000 non-profit organizations (NPOs) in the US. The maps consist of 357 topics covering areas such as religion, education, sports, culture, human services, public policy and medical care. The role of science in this altruistic landscape is examined. Possible applications are discussed. "
518,"Update summarization is a form of multidocument summarization where a document set must be summarized in the context of other documents assumed to be known. Efficient update summarization must focus on identifying new information and avoiding repetition of known information. In Query-focused summarization, the task is to produce a summary as an answer to a given query. We introduce a new task, Query-Chain Summarization, which combines aspects of the two previous tasks: starting from a given document set, increasingly specific queries are considered, and a new summary is produced at each step. This process models exploratory search: a user explores a new topic by submitting a sequence of queries, inspecting a summary of the result set and phrasing a new query at each step. We present a novel dataset comprising 22 querychains sessions of length up to 3 with 3 matching human summaries each in the consumerhealth domain. Our analysis demonstrates that summaries produced in the context of such exploratory process are different from informative summaries. We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant. Evaluation indicates the algorithm improves on strong baselines. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906926285,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",Query-chain focused summarization,"Update summarization is a form of multidocument summarization where a document set must be summarized in the context of other documents assumed to be known. Efficient update summarization must focus on identifying new information and avoiding repetition of known information. In Query-focused summarization, the task is to produce a summary as an answer to a given query. We introduce a new task, Query-Chain Summarization, which combines aspects of the two previous tasks: starting from a given document set, increasingly specific queries are considered, and a new summary is produced at each step. This process models exploratory search: a user explores a new topic by submitting a sequence of queries, inspecting a summary of the result set and phrasing a new query at each step. We present a novel dataset comprising 22 querychains sessions of length up to 3 with 3 matching human summaries each in the consumerhealth domain. Our analysis demonstrates that summaries produced in the context of such exploratory process are different from informative summaries. We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant. Evaluation indicates the algorithm improves on strong baselines. "
519,"The proliferation of digital information technologies and related infrastructure has given rise to novel ways of capturing, storing and analyzing data. In this paper, we describe the research and development of an information system called Interactive Knowledge Networks for Engineering Education Research (iKNEER). This system utilizes a framework that combines large-scale data mining techniques, social network mapping algorithms, and time-series analysis, to provide a mechanism for analyzing and understanding data about the engineering education community. We provide a detailed description of the algorithms, workflows, and the technical architecture we use to make sense of publications, conference proceedings, funding information, and a range of products derived from research in EER (also known as knowledge products). Finally, we demonstrate one possible application of iKNEER by applying topic modeling techniques to a subset of the data to identify the emergence and growth of research topics within the community thereby illustrating the unique epistemic value of this knowledge platform. The system can be found at http://www.ikneer.org.",2014-01-01,2-s2.0-84907246814,Advances in Engineering Education,Tools for large-scale data analytic examination of relational and epistemic networks in engineering education,"The proliferation of digital information technologies and related infrastructure has given rise to novel ways of capturing, storing and analyzing data. In this paper, we describe the research and development of an information system called Interactive Knowledge Networks for Engineering Education Research (iKNEER). This system utilizes a framework that combines large-scale data mining techniques, social network mapping algorithms, and time-series analysis, to provide a mechanism for analyzing and understanding data about the engineering education community. We provide a detailed description of the algorithms, workflows, and the technical architecture we use to make sense of publications, conference proceedings, funding information, and a range of products derived from research in EER (also known as knowledge products). Finally, we demonstrate one possible application of iKNEER by applying topic modeling techniques to a subset of the data to identify the emergence and growth of research topics within the community thereby illustrating the unique epistemic value of this knowledge platform. The system can be found at http://www.ikneer.org."
520,"We study the problem of constructing the topic-based model over different domains for text classification. In real-world applications, there are abundant unlabeled documents but sparse labeled documents. It is challenging to construct a reliable and adaptive model to classify a large amount of documents containing different domains. The classifiers trained from a source domain shall perform poorly for the test data in a target domain. Also, the trained model is vulnerable to the weakness of classification among ambiguous classes. In this study, we tackle the issues of domain mismatch and confusing classes and conduct the discriminative transfer learning for text classification. We propose a Bayesian bridging topic models (BTM) from a variety of labeled and unlabeled documents and perform the transfer learning for cross-domain text classification. A structural model is built and its parameters are estimated by maximizing the joint marginal likelihood of labeled and unlabeled data via a variational inference procedure. We also construct the discriminative learning on our proposed model for adjust parameters by using the minimum classification error criterion. We show that improvements over cross-domain text classification using the proposed model can be achieved better performance than other models.",2014-01-01,2-s2.0-84906968647,Journal of Information Science and Engineering,Bayesian bridging topic models for classification,"We study the problem of constructing the topic-based model over different domains for text classification. In real-world applications, there are abundant unlabeled documents but sparse labeled documents. It is challenging to construct a reliable and adaptive model to classify a large amount of documents containing different domains. The classifiers trained from a source domain shall perform poorly for the test data in a target domain. Also, the trained model is vulnerable to the weakness of classification among ambiguous classes. In this study, we tackle the issues of domain mismatch and confusing classes and conduct the discriminative transfer learning for text classification. We propose a Bayesian bridging topic models (BTM) from a variety of labeled and unlabeled documents and perform the transfer learning for cross-domain text classification. A structural model is built and its parameters are estimated by maximizing the joint marginal likelihood of labeled and unlabeled data via a variational inference procedure. We also construct the discriminative learning on our proposed model for adjust parameters by using the minimum classification error criterion. We show that improvements over cross-domain text classification using the proposed model can be achieved better performance than other models."
521,"The Doubly Correlated Topic Model is a generative probabilistic topic model for automatically identifying topics from the corpus of the text documents. It is a mixed membership model, based on the fact that a document exhibits a number of topics. We used word co-occurrence statistical information for identifying an initial set of topics as posterior information for the model. Posterior inference methods utilized by the existing models are intractable and therefore provide an approximate solution. Consideration of co-occurred words as initial topics provides a tighter bound on the topic coherence. The proposed model is motivated by the Latent Dirichlet Allocation Model. The Doubly Correlated Topic Model differs from the Latent Dirichlet Allocation Model in its posterior inference; it uses the highest ranked co-occurred words as initial topics rather than obtaining from Dirichlet priors. The results of the proposed model suggest some improved performance on entropy and topical coherence over different datasets. © The Author(s) 2014.",2014-01-01,2-s2.0-84902177525,Journal of Information Science,Performance of LDA and DCT models,"The Doubly Correlated Topic Model is a generative probabilistic topic model for automatically identifying topics from the corpus of the text documents. It is a mixed membership model, based on the fact that a document exhibits a number of topics. We used word co-occurrence statistical information for identifying an initial set of topics as posterior information for the model. Posterior inference methods utilized by the existing models are intractable and therefore provide an approximate solution. Consideration of co-occurred words as initial topics provides a tighter bound on the topic coherence. The proposed model is motivated by the Latent Dirichlet Allocation Model. The Doubly Correlated Topic Model differs from the Latent Dirichlet Allocation Model in its posterior inference; it uses the highest ranked co-occurred words as initial topics rather than obtaining from Dirichlet priors. The results of the proposed model suggest some improved performance on entropy and topical coherence over different datasets. "
522,"Creating cross-language article links among different online encyclopedias is now an important task in the unification of multilingual knowledge bases. In this paper, we propose a cross-language article linking method using a mixed-language topic model and hypernym translation features based on an SVM model to link English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China. To evaluate our approach, we compile a data set from the top 500 Baidu Baike articles and their corresponding English Wiki articles. The evaluation results show that our approach achieves 80.95% in MRR and 87.46% in recall. Our method does not heavily depend on linguistic characteristics and can be easily extended to generate crosslanguage article links among different online encyclopedias in other languages. © 2014 Association for Computational Linguistics.",2014-01-01,2-s2.0-84906930272,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",Cross-language and cross-encyclopedia article linking using mixed-language topic model and hypernym translation,"Creating cross-language article links among different online encyclopedias is now an important task in the unification of multilingual knowledge bases. In this paper, we propose a cross-language article linking method using a mixed-language topic model and hypernym translation features based on an SVM model to link English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China. To evaluate our approach, we compile a data set from the top 500 Baidu Baike articles and their corresponding English Wiki articles. The evaluation results show that our approach achieves 80.95% in MRR and 87.46% in recall. Our method does not heavily depend on linguistic characteristics and can be easily extended to generate crosslanguage article links among different online encyclopedias in other languages. "
523,"With the development of computer networks, network education has received more and more attention. In the network environment, due to the limitation of time, teachers cannot answer in a timely manner all the questions that students ask. Therefore, an intelligent Q&A (questions and answers) system based on the LDA (latent Dirichlet allocation) topic model was developed, and is discussed in this article. In order to solve the difficult problems under network circumstances, considered in this research were the knowledge points and characteristics of FAQs (frequently asked questions) for the course, Database Principles. The intelligent Q&A system was based on the LDA model and on topics-documentation-knowledge points. This intelligent Q&A system allows users to describe problems in natural language and, then, the problems are submitted to the system. The system returns accurate answers related to the topic. Students opined that the Q&A system performed very well and could answer all the questions they posed about Database Principles. © 2014 WIETE.",2014-01-01,2-s2.0-84899647813,World Transactions on Engineering and Technology Education,An intelligent Q&A system based on the LDA topic model for the teaching of database principles,"With the development of computer networks, network education has received more and more attention. In the network environment, due to the limitation of time, teachers cannot answer in a timely manner all the questions that students ask. Therefore, an intelligent Q&A (questions and answers) system based on the LDA (latent Dirichlet allocation) topic model was developed, and is discussed in this article. In order to solve the difficult problems under network circumstances, considered in this research were the knowledge points and characteristics of FAQs (frequently asked questions) for the course, Database Principles. The intelligent Q&A system was based on the LDA model and on topics-documentation-knowledge points. This intelligent Q&A system allows users to describe problems in natural language and, then, the problems are submitted to the system. The system returns accurate answers related to the topic. Students opined that the Q&A system performed very well and could answer all the questions they posed about Database Principles. "
524,"Identifying communities and analysing their dynamics in social networks is very important research problem. However, qualitative analysis (taking into account the scale of the problem) still poses serious problems. Several methods for analysis are proposed, but there is missing tool allowing visualisation of dynamics of communities and enabling performing analysis on different levels of details. This paper describes a tool enabling analysis of social group dynamics with taking into account many aspects of groups (contexts). In paper the analysis of group density, sentiment and topic modelling for groups is presented. Presented results are based on real-world data from blogosphere.",2014-01-01,2-s2.0-84911012021,Social Network Analysis and Mining,GEVi: context-based graphical analysis of social group dynamics,"Identifying communities and analysing their dynamics in social networks is very important research problem. However, qualitative analysis (taking into account the scale of the problem) still poses serious problems. Several methods for analysis are proposed, but there is missing tool allowing visualisation of dynamics of communities and enabling performing analysis on different levels of details. This paper describes a tool enabling analysis of social group dynamics with taking into account many aspects of groups (contexts). In paper the analysis of group density, sentiment and topic modelling for groups is presented. Presented results are based on real-world data from blogosphere."
525,"Location-based check-in services in various social media applications have enabled individuals to share their activity-related choices providing a new source of human activity data. Although geo-location data has the potential to infer multi-day patterns of individual activities, appropriate methodological approaches are needed. This paper presents a technique to analyze large-scale geo-location data from social media to infer individual activity patterns. A data-driven modeling approach, based on topic modeling, is proposed to classify patterns in individual activity choices. The model provides an activity generation mechanism which when combined with the data from traditional surveys is potentially a useful component of an activity-travel simulator. Using the model, aggregate patterns of users' weekly activities are extracted from the data. The model is extended to also find user-specific activity patterns. We extend the model to account for missing activities (a major limitation of social media data) and demonstrate how information from activity-based diaries can be complemented with longitudinal geo-location information. This work provides foundational tools that can be used when geo-location data is available to predict disaggregate activity patterns. © 2014 Elsevier Ltd.",2014-01-01,2-s2.0-84901405586,Transportation Research Part C: Emerging Technologies,Urban activity pattern classification using topic models from online geo-location data,"Location-based check-in services in various social media applications have enabled individuals to share their activity-related choices providing a new source of human activity data. Although geo-location data has the potential to infer multi-day patterns of individual activities, appropriate methodological approaches are needed. This paper presents a technique to analyze large-scale geo-location data from social media to infer individual activity patterns. A data-driven modeling approach, based on topic modeling, is proposed to classify patterns in individual activity choices. The model provides an activity generation mechanism which when combined with the data from traditional surveys is potentially a useful component of an activity-travel simulator. Using the model, aggregate patterns of users' weekly activities are extracted from the data. The model is extended to also find user-specific activity patterns. We extend the model to account for missing activities (a major limitation of social media data) and demonstrate how information from activity-based diaries can be complemented with longitudinal geo-location information. This work provides foundational tools that can be used when geo-location data is available to predict disaggregate activity patterns. "
526,"This article addresses the 'meaning problem' of unsupervised topic modeling algorithms using a tool called the Networked Corpus, which offers a way to visualize topic models alongside the texts themselves. We argue that the relationship between quantitative methods and qualitative interpretation can be reframed by investigating the long history of machine learning procedures and their historical antecedents. The new method of visualization presented by the Networked Corpus enables users to compare the results of topic models with earlier methods of topical representation such as the 18th-century subject index. Although the article provides a brief description of the tool, the primary focus is to describe an argument for this kind of comparative analysis between topic models and older genres that perform similar tasks. Such comparative analysis provides a new method for developing conceptual histories of the categories of meaning on which the topic model and the index depend. These devices are linked by a shared attempt to represent what a text is 'about', but the concept of 'aboutness' has evolved over time. The Networked Corpus enables researchers to discover congruities and contradictions in how topic models and indexes represent texts in order to examine what kinds of information each historically situated device prioritizes. © The Author 2014. Published by Oxford University Press on behalf of EADH. All rights reserved.",2014-01-01,2-s2.0-84906851953,Literary and Linguistic Computing,Visibility and meaning in topic models and 18th-century subject indexes,"This article addresses the 'meaning problem' of unsupervised topic modeling algorithms using a tool called the Networked Corpus, which offers a way to visualize topic models alongside the texts themselves. We argue that the relationship between quantitative methods and qualitative interpretation can be reframed by investigating the long history of machine learning procedures and their historical antecedents. The new method of visualization presented by the Networked Corpus enables users to compare the results of topic models with earlier methods of topical representation such as the 18th-century subject index. Although the article provides a brief description of the tool, the primary focus is to describe an argument for this kind of comparative analysis between topic models and older genres that perform similar tasks. Such comparative analysis provides a new method for developing conceptual histories of the categories of meaning on which the topic model and the index depend. These devices are linked by a shared attempt to represent what a text is 'about', but the concept of 'aboutness' has evolved over time. The Networked Corpus enables researchers to discover congruities and contradictions in how topic models and indexes represent texts in order to examine what kinds of information each historically situated device prioritizes. "
527,"We study the problem of recommending scientific articles to users in an online community and present a novel matrix factorization model, the topic regression Matrix Factorization (tr-MF), to solve the problem. The main idea of tr-MF lies in extending the matrix factorization with a probabilistic topic modeling. Instead of regularizing item factors through the probabilistic topic modeling as in the framework of the CTR model, tr-MF introduces a regression model to regularize user factors through the probabilistic topic modeling under the basic hypothesis that users share the similar preferences if they rate similar sets of items. Consequently, tr-MF provides interpretable latent factors for users and items, and makes accurate predictions for community users. Specifically, it is effective in making predictions for users with only few ratings or even no ratings, and supports tasks that are specific to a certain field, neither of which is addressed in the existing literature. Further, we demonstrate the efficacy of tr-MF on a large subset of the data from CiteULike, a bibliography sharing service dataset. The proposed model outperforms the state-of-the-art matrix factorization models with a significant margin. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889559998,"International Conference on Information and Knowledge Management, Proceedings",Scientific articles recommendation,"We study the problem of recommending scientific articles to users in an online community and present a novel matrix factorization model, the topic regression Matrix Factorization (tr-MF), to solve the problem. The main idea of tr-MF lies in extending the matrix factorization with a probabilistic topic modeling. Instead of regularizing item factors through the probabilistic topic modeling as in the framework of the CTR model, tr-MF introduces a regression model to regularize user factors through the probabilistic topic modeling under the basic hypothesis that users share the similar preferences if they rate similar sets of items. Consequently, tr-MF provides interpretable latent factors for users and items, and makes accurate predictions for community users. Specifically, it is effective in making predictions for users with only few ratings or even no ratings, and supports tasks that are specific to a certain field, neither of which is addressed in the existing literature. Further, we demonstrate the efficacy of tr-MF on a large subset of the data from CiteULike, a bibliography sharing service dataset. The proposed model outperforms the state-of-the-art matrix factorization models with a significant margin. "
528,"Many applications require analyzing textual topics in conjunction with external time series variables such as stock prices. We develop a novel general text mining framework for discovering such causal topics from text. Our framework naturally combines any given probabilistic topic model with time-series causal analysis to discover topics that are both coherent semantically and correlated with time series data. We iteratively refine topics, increasing the correlation of discovered topics with the time series. Time series data provides feedback at each iteration by imposing prior distributions on parameters. Experimental results show that the proposed framework is effective. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889604012,"International Conference on Information and Knowledge Management, Proceedings",Mining causal topics in text data: Iterative topic modeling with time series feedback,"Many applications require analyzing textual topics in conjunction with external time series variables such as stock prices. We develop a novel general text mining framework for discovering such causal topics from text. Our framework naturally combines any given probabilistic topic model with time-series causal analysis to discover topics that are both coherent semantically and correlated with time series data. We iteratively refine topics, increasing the correlation of discovered topics with the time series. Time series data provides feedback at each iteration by imposing prior distributions on parameters. Experimental results show that the proposed framework is effective. "
529,"Interactive web search involves selecting which documents to read further and locating the parts of the documents that are relevant to the user's current activity. In this paper, we introduce UIMaP: User Interest Modeling and Personalization, a search task based personal user interest model to support users' information gathering tasks. The novelty of our approach lies in the use of topic modeling to generate fine-grained models of user interest and visualizations that direct user's attention to documents or parts of documents that match user's inferred interests. User annotations are used to help generate personalized visualizations for user's search tasks. Based on 1267 user annotations from 17 users, we show the performance comparisons of four different topic models: LDA+H, LDA+KL, LDA+JSD, and LDA+TopN. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889589230,"International Conference on Information and Knowledge Management, Proceedings",Mining user interest from search tasks and annotations,"Interactive web search involves selecting which documents to read further and locating the parts of the documents that are relevant to the user's current activity. In this paper, we introduce UIMaP: User Interest Modeling and Personalization, a search task based personal user interest model to support users' information gathering tasks. The novelty of our approach lies in the use of topic modeling to generate fine-grained models of user interest and visualizations that direct user's attention to documents or parts of documents that match user's inferred interests. User annotations are used to help generate personalized visualizations for user's search tasks. Based on 1267 user annotations from 17 users, we show the performance comparisons of four different topic models: LDA+H, LDA+KL, LDA+JSD, and LDA+TopN. "
530,"In this paper, we propose a framework of recommending users and communities in social media. Given a user's profile, our framework is capable of recommending influential users and topic-cohesive interactive communities that are most relevant to the given user. In our framework, we present a generative topic model to discover user-oriented and community-oriented topics simultaneously, which enables us to capture the exact topic interests of users, as well as the focuses of communities. Extensive evaluation on a data set obtained from Twitter has demonstrated the effectiveness of our proposed framework compared with other probabilistic topic model based recommendation methods. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889590408,"International Conference on Information and Knowledge Management, Proceedings",FRec: A novel framework of recommending users and communities in social media,"In this paper, we propose a framework of recommending users and communities in social media. Given a user's profile, our framework is capable of recommending influential users and topic-cohesive interactive communities that are most relevant to the given user. In our framework, we present a generative topic model to discover user-oriented and community-oriented topics simultaneously, which enables us to capture the exact topic interests of users, as well as the focuses of communities. Extensive evaluation on a data set obtained from Twitter has demonstrated the effectiveness of our proposed framework compared with other probabilistic topic model based recommendation methods. "
531,"This paper studies text summarization by extracting hierarchical topics from a given collection of documents. We propose a new approach of text modeling via network analysis. We convert documents into a word influence network, and find the words summarizing the major topics with an efficient influence maximization algorithm. Besides, the influence capability of the topic words on other words in the network reveal the relations among the topic words. Then we cluster the words and build hierarchies for the topics. Experiments on large collections of Web documents show that a simple method based on the influence analysis is effective, compared with existing generative topic modeling and random walk based ranking. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889577016,"International Conference on Information and Knowledge Management, Proceedings",Content coverage maximization on word networks for hierarchical topic summarization,"This paper studies text summarization by extracting hierarchical topics from a given collection of documents. We propose a new approach of text modeling via network analysis. We convert documents into a word influence network, and find the words summarizing the major topics with an efficient influence maximization algorithm. Besides, the influence capability of the topic words on other words in the network reveal the relations among the topic words. Then we cluster the words and build hierarchies for the topics. Experiments on large collections of Web documents show that a simple method based on the influence analysis is effective, compared with existing generative topic modeling and random walk based ranking. "
532,"We build a system to extract user interests from Twitter messages. Specifically, we extract interest candidates using linguistic patterns and rank them using four different keyphrase ranking techniques: TFIDF, TextRank, LDA-TextRank, and Relevance-Interestingness-Rank (RI-Rank). We also explore the complementary relation between TFIDF and TextRank in ranking interest candidates. Top ranked interests are evaluated with user feedback gathered from an online survey. The results show that TFIDF and TextRank are both suitable for extracting user interests from tweets. Moreover, the combination of TFIDF and TextRank consistently yields the highest user positive feedback. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889571444,"International Conference on Information and Knowledge Management, Proceedings",Interest mining from user tweets,"We build a system to extract user interests from Twitter messages. Specifically, we extract interest candidates using linguistic patterns and rank them using four different keyphrase ranking techniques: TFIDF, TextRank, LDA-TextRank, and Relevance-Interestingness-Rank (RI-Rank). We also explore the complementary relation between TFIDF and TextRank in ranking interest candidates. Top ranked interests are evaluated with user feedback gathered from an online survey. The results show that TFIDF and TextRank are both suitable for extracting user interests from tweets. Moreover, the combination of TFIDF and TextRank consistently yields the highest user positive feedback. "
533,"Microblogging platforms, such as Twitter, already play an important role in cultural, social and political events around the world. Discovering high-level topics from social streams is therefore important for many downstream applications. However, traditional text mining methods that rely on the bag-of-words model are insufficient to uncover the rich semantics and temporal aspects of topics in Twitter. In particular, topics in Twitter are inherently dynamic and often focus on specific entities, such as people or organizations. In this paper, we therefore propose a method for mining multi-faceted topics from Twitter streams. The Multi-Faceted Topic Model (MfTM) is proposed to jointly model latent semantics among terms and entities and captures the temporal characteristics of each topic. We develop an efficient online inference method for MfTM, which enables our model to be applied to large-scale and streaming data. Our experimental evaluation shows the effectiveness and efficiency of our model compared with state-of-the-art baselines. We further demonstrate the effectiveness of our framework in the context of tweet clustering. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889594913,"International Conference on Information and Knowledge Management, Proceedings",Dynamic multi-faceted topic discovery in twitter,"Microblogging platforms, such as Twitter, already play an important role in cultural, social and political events around the world. Discovering high-level topics from social streams is therefore important for many downstream applications. However, traditional text mining methods that rely on the bag-of-words model are insufficient to uncover the rich semantics and temporal aspects of topics in Twitter. In particular, topics in Twitter are inherently dynamic and often focus on specific entities, such as people or organizations. In this paper, we therefore propose a method for mining multi-faceted topics from Twitter streams. The Multi-Faceted Topic Model (MfTM) is proposed to jointly model latent semantics among terms and entities and captures the temporal characteristics of each topic. We develop an efficient online inference method for MfTM, which enables our model to be applied to large-scale and streaming data. Our experimental evaluation shows the effectiveness and efficiency of our model compared with state-of-the-art baselines. We further demonstrate the effectiveness of our framework in the context of tweet clustering. "
534,"Community Question Answering (CQA) websites, where people share expertise on open platforms, have become large repositories of valuable knowledge. To bring the best value out of these knowledge repositories, it is critically important for CQA services to know how to find the right experts, retrieve archived similar questions and recommend best answers to new questions. To tackle this cluster of closely related problems in a principled approach, we proposed Topic Expertise Model (TEM), a novel probabilistic generative model with GMM hybrid, to jointly model topics and expertise by integrating textual content model and link structure analysis. Based on TEM results, we proposed CQARank to measure user interests and expertise score under different topics. Leveraging the question answering history based on long-term community reviews and voting, our method could find experts with both similar topical preference and high topical expertise. Experiments carried out on Stack Overflow data, the largest CQA focused on computer programming, show that our method achieves significant improvement over existing methods on multiple metrics. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889610414,"International Conference on Information and Knowledge Management, Proceedings",CQARank: Jointly model topics and expertise in Community Question Answering,"Community Question Answering (CQA) websites, where people share expertise on open platforms, have become large repositories of valuable knowledge. To bring the best value out of these knowledge repositories, it is critically important for CQA services to know how to find the right experts, retrieve archived similar questions and recommend best answers to new questions. To tackle this cluster of closely related problems in a principled approach, we proposed Topic Expertise Model (TEM), a novel probabilistic generative model with GMM hybrid, to jointly model topics and expertise by integrating textual content model and link structure analysis. Based on TEM results, we proposed CQARank to measure user interests and expertise score under different topics. Leveraging the question answering history based on long-term community reviews and voting, our method could find experts with both similar topical preference and high topical expertise. Experiments carried out on Stack Overflow data, the largest CQA focused on computer programming, show that our method achieves significant improvement over existing methods on multiple metrics. "
535,"A large number of studies have been devoted to modeling the contents and interactions between users on Twitter. In this paper, we propose a method inspired from Social Role Theory (SRT), which assumes that a user behaves differently in different roles in the generation process of Twitter content. We consider the two most distinctive social roles on Twitter: originator and propagator, who respectively posts original messages and retweets or forwards the messages from others. In addition, we also consider role-specific social interactions, especially implicit interactions between users who share some common interests. All the above elements are integrated into a novel regularized topic model. We evaluate the proposed method on real Twitter data. The results show that our method is more effective than the existing ones which do not distinguish social roles. Copyright 2013 ACM.",2013-12-11,2-s2.0-84889562628,"International Conference on Information and Knowledge Management, Proceedings",Originator or propagator? Incorporating social role theory into topic models for twitter content analysis,"A large number of studies have been devoted to modeling the contents and interactions between users on Twitter. In this paper, we propose a method inspired from Social Role Theory (SRT), which assumes that a user behaves differently in different roles in the generation process of Twitter content. We consider the two most distinctive social roles on Twitter: originator and propagator, who respectively posts original messages and retweets or forwards the messages from others. In addition, we also consider role-specific social interactions, especially implicit interactions between users who share some common interests. All the above elements are integrated into a novel regularized topic model. We evaluate the proposed method on real Twitter data. The results show that our method is more effective than the existing ones which do not distinguish social roles. "
536,"Cross-domain text classification aims to automatically train a precise text classifier for a target domain by using labelled text data from a related source domain. To this end, one of the most promising ideas is to induce a new feature representation so that the distributional difference between domains can be reduced and a more accurate classifier can be learned in this new feature space. However, most existing methods do not explore the duality of the marginal distribution of examples and the conditional distribution of class labels given labeled training examples in the source domain. Besides, few previous works attempt to explicitly distinguish the domain-independent and domain-specific latent features and align the domain-specific features to further improve the cross-domain learning. In this paper, we propose a model called Partially Supervised Cross-Collection LDA topic model (PSCCLDA) for cross-domain learning with the purpose of addressing these two issues in a unified way. Experimental results on nine datasets show that our model outperforms two standard classifiers and four state-of-the-art methods, which demonstrates the effectiveness of our proposed model. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889569618,"International Conference on Information and Knowledge Management, Proceedings",A partially supervised cross-collection topic model for cross-domain text classification,"Cross-domain text classification aims to automatically train a precise text classifier for a target domain by using labelled text data from a related source domain. To this end, one of the most promising ideas is to induce a new feature representation so that the distributional difference between domains can be reduced and a more accurate classifier can be learned in this new feature space. However, most existing methods do not explore the duality of the marginal distribution of examples and the conditional distribution of class labels given labeled training examples in the source domain. Besides, few previous works attempt to explicitly distinguish the domain-independent and domain-specific latent features and align the domain-specific features to further improve the cross-domain learning. In this paper, we propose a model called Partially Supervised Cross-Collection LDA topic model (PSCCLDA) for cross-domain learning with the purpose of addressing these two issues in a unified way. Experimental results on nine datasets show that our model outperforms two standard classifiers and four state-of-the-art methods, which demonstrates the effectiveness of our proposed model. "
537,"Dirichlet process mixture (DPM) model is one of the most important Bayesian nonparametric models owing to its efficiency of inference and flexibility for various applications. A fundamental assumption made by DPM model is that all data items are generated from a single, shared DP. This assumption, however, is restrictive in many practical settings where samples are generated from a collection of dependent DPs, each associated with a point in some covariate space. For example, documents in the proceedings of a conference are organized by year, or photos may be tagged and recorded with GPS locations. We present a general method for constructing dependent Dirichlet processes (DP) on arbitrary covariate space. The approach is based on restricting and projecting a DP defined on a space of continuous functions with different domains, which results in a collection of dependent random measures, each associated with a point in covariate space and is marginally DP distributed. The constructed collection of dependent DPs can be used as a nonparametric prior of infinite dynamic mixture models, which allow each mixture component to appear/disappear and vary in a subspace of covariate space. Furthermore, we discuss choices of base distributions of functions in a variety of settings as a flexible method to control dependencies. In addition, we develop an efficient Gibbs sampler for model inference where all underlying random measures are integrated out. Finally, experiment results on temporal modeling and spatial modeling datasets demonstrate the effectiveness of the method in modeling dynamic mixture models on different types of covariates. Copyright is held by the owner/author(s).",2013-12-11,2-s2.0-84889599699,"International Conference on Information and Knowledge Management, Proceedings",Functional dirichlet process,"Dirichlet process mixture (DPM) model is one of the most important Bayesian nonparametric models owing to its efficiency of inference and flexibility for various applications. A fundamental assumption made by DPM model is that all data items are generated from a single, shared DP. This assumption, however, is restrictive in many practical settings where samples are generated from a collection of dependent DPs, each associated with a point in some covariate space. For example, documents in the proceedings of a conference are organized by year, or photos may be tagged and recorded with GPS locations. We present a general method for constructing dependent Dirichlet processes (DP) on arbitrary covariate space. The approach is based on restricting and projecting a DP defined on a space of continuous functions with different domains, which results in a collection of dependent random measures, each associated with a point in covariate space and is marginally DP distributed. The constructed collection of dependent DPs can be used as a nonparametric prior of infinite dynamic mixture models, which allow each mixture component to appear/disappear and vary in a subspace of covariate space. Furthermore, we discuss choices of base distributions of functions in a variety of settings as a flexible method to control dependencies. In addition, we develop an efficient Gibbs sampler for model inference where all underlying random measures are integrated out. Finally, experiment results on temporal modeling and spatial modeling datasets demonstrate the effectiveness of the method in modeling dynamic mixture models on different types of covariates. "
538,"In real-time emergency response an accurate picture of the situation is needed quickly. Often during large-scale disasters, cell towers become overloaded, and the only way of communication is through text messages. It becomes important to gather information from text messages sent to emergency numbers in order to respond quickly and efficiently with life-saving efforts. In addition, responders are unable to manually handle the large volume of incoming texts. To add to this difficult problem, these data sources tend to be microtext. This research developed a methodology to summarize text messages sent during an emergency, including analysis of locations. The real-time disaster needs were then input into a mixed integer programming resource allocation model for distribution of resources for disaster aid. Prior research included resource allocation and text modeling, but the combination of the two is a novel application not only in this arena, but more broadly across domains. © 2013 IEEE.",2013-12-01,2-s2.0-84893337598,"2013 IEEE International Conference on Technologies for Homeland Security, HST 2013",Optimization of emergency response using higher order learning and clustering of 911 text messages,"In real-time emergency response an accurate picture of the situation is needed quickly. Often during large-scale disasters, cell towers become overloaded, and the only way of communication is through text messages. It becomes important to gather information from text messages sent to emergency numbers in order to respond quickly and efficiently with life-saving efforts. In addition, responders are unable to manually handle the large volume of incoming texts. To add to this difficult problem, these data sources tend to be microtext. This research developed a methodology to summarize text messages sent during an emergency, including analysis of locations. The real-time disaster needs were then input into a mixed integer programming resource allocation model for distribution of resources for disaster aid. Prior research included resource allocation and text modeling, but the combination of the two is a novel application not only in this arena, but more broadly across domains. "
539,"Topic modeling provides a valuable method for identifying the linguistic contexts that surround social institutions or policy domains. This article uses Latent Dirichlet Allocation (LDA) to analyze how one such policy domain, government assistance to artists and arts organizations, was framed in almost 8000 articles. These comprised all articles that referred to government support for the arts in the U.S. published in five U.S. newspapers between 1986 and 1997-a period during which such assistance, once noncontroversial, became a focus of contention. We illustrate the strengths of topic modeling as a means of analyzing large text corpora, discuss the proper choice of models and interpretation of model results, describe means of validating topic-model solutions, and demonstrate the use of topic models in combination with other statistical tools to estimate differences between newspapers in the prevalence of different frames. Throughout, we emphasize affinities between the topic-modeling approach and such central concepts in the study of culture as framing, polysemy, heteroglossia, and the relationality of meaning. © 2013 Elsevier B.V.",2013-12-01,2-s2.0-84888133421,Poetics,Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding,"Topic modeling provides a valuable method for identifying the linguistic contexts that surround social institutions or policy domains. This article uses Latent Dirichlet Allocation (LDA) to analyze how one such policy domain, government assistance to artists and arts organizations, was framed in almost 8000 articles. These comprised all articles that referred to government support for the arts in the U.S. published in five U.S. newspapers between 1986 and 1997-a period during which such assistance, once noncontroversial, became a focus of contention. We illustrate the strengths of topic modeling as a means of analyzing large text corpora, discuss the proper choice of models and interpretation of model results, describe means of validating topic-model solutions, and demonstrate the use of topic models in combination with other statistical tools to estimate differences between newspapers in the prevalence of different frames. Throughout, we emphasize affinities between the topic-modeling approach and such central concepts in the study of culture as framing, polysemy, heteroglossia, and the relationality of meaning. "
540,"Sociologists wishing to employ topic models in their research need a helpful guide that describes the variety of topic modeling procedures, their issues, and various means of resolving them so as to convincingly answer sociological questions. We present this overview by recounting a series of our prior collaborative projects that have employed and developed various forms of topic models to understand language differentiation in academe. With each project, we encountered a variety of model-specific issues concerning the validity of topics and their suitability to our data and research questions. We developed a variety of novel visualization techniques to make sense of topic-solutions and used a variety of techniques to validate our results. In addition, we created a variety of new topic modeling techniques and procedures suitable to different kinds of data and research questions. © 2013 Elsevier B.V.",2013-12-01,2-s2.0-84888137563,Poetics,Differentiating language usage through topic models,"Sociologists wishing to employ topic models in their research need a helpful guide that describes the variety of topic modeling procedures, their issues, and various means of resolving them so as to convincingly answer sociological questions. We present this overview by recounting a series of our prior collaborative projects that have employed and developed various forms of topic models to understand language differentiation in academe. With each project, we encountered a variety of model-specific issues concerning the validity of topics and their suitability to our data and research questions. We developed a variety of novel visualization techniques to make sense of topic-solutions and used a variety of techniques to validate our results. In addition, we created a variety of new topic modeling techniques and procedures suitable to different kinds of data and research questions. "
541,"Given a small, well-understood corpus that is of interest to a Humanities scholar, we propose sub-corpus topic modeling (STM) as a tool for discovering meaningful passages in a larger collection of less well-understood texts. STM allows Humanities scholars to discover unknown passages from the vast sea of works that Moretti calls the ""great unread"" and to significantly increase the researcher's ability to discuss aspects of influence and the development of intellectual movements across a broader swath of the literary landscape. In this article, we test three typical Humanities research problems: in the first, a researcher wants to find text passages that exhibit similarities to a collection of influential non literary texts from a single author (here, Darwin); in the second, a researcher wants to discover literary passages related to a well understood corpus of literary texts (here, emblematic texts from the Modern Breakthrough); and in the third, a researcher hopes to understand the influence that a particular domain (here, folklore) has had on the realm of literature over a series of decades. We explore these research challenges with three experiments. © 2013 Elsevier B.V.",2013-12-01,2-s2.0-84888135996,Poetics,Trawling in the Sea of the Great Unread: Sub-corpus topic modeling and Humanities research,"Given a small, well-understood corpus that is of interest to a Humanities scholar, we propose sub-corpus topic modeling (STM) as a tool for discovering meaningful passages in a larger collection of less well-understood texts. STM allows Humanities scholars to discover unknown passages from the vast sea of works that Moretti calls the ""great unread"" and to significantly increase the researcher's ability to discuss aspects of influence and the development of intellectual movements across a broader swath of the literary landscape. In this article, we test three typical Humanities research problems: in the first, a researcher wants to find text passages that exhibit similarities to a collection of influential non literary texts from a single author (here, Darwin); in the second, a researcher wants to discover literary passages related to a well understood corpus of literary texts (here, emblematic texts from the Modern Breakthrough); and in the third, a researcher hopes to understand the influence that a particular domain (here, folklore) has had on the realm of literature over a series of decades. We explore these research challenges with three experiments. "
542,"External factors such as author gender, author nationality, and date of publication can affect both the choice of literary themes in novels and the expression of those themes, but the extent of this association is difficult to quantify. In this work, we apply statistical methods to identify and extract hundreds of topics (themes) from a corpus of 19th-century British, Irish, and American fiction. We use these topics as a measurable, data-driven proxy for literary themes and assess how external factors may predict fluctuations in the use of themes and the individual word choices within themes. We use topics not only to measure these associations but also to evaluate whether this evidence is statistically significant. © 2013 Elsevier B.V.",2013-12-01,2-s2.0-84888138562,Poetics,Significant themes in 19th-century literature,"External factors such as author gender, author nationality, and date of publication can affect both the choice of literary themes in novels and the expression of those themes, but the extent of this association is difficult to quantify. In this work, we apply statistical methods to identify and extract hundreds of topics (themes) from a corpus of 19th-century British, Irish, and American fiction. We use these topics as a measurable, data-driven proxy for literary themes and assess how external factors may predict fluctuations in the use of themes and the individual word choices within themes. We use topics not only to measure these associations but also to evaluate whether this evidence is statistically significant. "
543,"The rapid development of technology promotes the vast expansion of new items in many domains of consumer products. Problem occurs when the new items are continuously added but cannot get reached by the consumers. Many existing recommender systems work well only for well-known items with sufficient ratings but fail to discover new items, and content-based approaches suffer from insufficient item features. In this paper, we show that critic reviews of the items can be used to boost new item recommendation. We propose a scalable framework that incorporates the topics inferred from the critic reviews into the recommendation process by employing topic modeling and non-negative matrix factorization. The results of our experiment show that our proposed method is able to generate high quality new item recommendations which are not supported by many state-of-the-art methods, and also outperforms the state-of-the-art methods in recommending existing items. © (2013) by the AIS/ICIS Administrative Office All rights reserved.",2013-12-01,2-s2.0-84897820158,International Conference on Information Systems (ICIS 2013): Reshaping Society Through Information Systems Design,Using critic reviews to boost new item recommendation,"The rapid development of technology promotes the vast expansion of new items in many domains of consumer products. Problem occurs when the new items are continuously added but cannot get reached by the consumers. Many existing recommender systems work well only for well-known items with sufficient ratings but fail to discover new items, and content-based approaches suffer from insufficient item features. In this paper, we show that critic reviews of the items can be used to boost new item recommendation. We propose a scalable framework that incorporates the topics inferred from the critic reviews into the recommendation process by employing topic modeling and non-negative matrix factorization. The results of our experiment show that our proposed method is able to generate high quality new item recommendations which are not supported by many state-of-the-art methods, and also outperforms the state-of-the-art methods in recommending existing items. "
544,"Banditry and unrest in eighteenth and nineteenth century China have attracted substantial attention from several generations of researchers. Often, they apply particular ontologies a priori to the source base. Given their reliance on state documents, these studies are subject to the perspectives of record-keepers and their theories of violence. It is particularly difficult to apply fixed definitions to concepts like ""banditry"" and ""unrest""-a problem that applies as much to modern researchers as to our historical informants. To better view the nature of violence in the Qing Dynasty-as routine crime, and as rebellion and unrest-it is important to develop a model of how administrators understood it. Therefore, rather than assuming a fixed set of categories, this study models Qing administrators' typologies of violence based on the frequencies of term co-occurrence. Based on the term groupings in the model, five topics relate to violent unrest. Each topic accounts for a particular statistical pattern of word use corresponding to patterns of occurrence, observation and recording of related phenomena. These groupings give some insight into the ""crime rates"" of the eighteenth and nineteenth centuries, and more importantly, these groupings cast light on their understandings of crime, rebellion and unrest. © 2013 Elsevier B.V.",2013-12-01,2-s2.0-84888134353,Poetics,"Rebellion, crime and violence in Qing China, 1722-1911: A topic modeling approach","Banditry and unrest in eighteenth and nineteenth century China have attracted substantial attention from several generations of researchers. Often, they apply particular ontologies a priori to the source base. Given their reliance on state documents, these studies are subject to the perspectives of record-keepers and their theories of violence. It is particularly difficult to apply fixed definitions to concepts like ""banditry"" and ""unrest""-a problem that applies as much to modern researchers as to our historical informants. To better view the nature of violence in the Qing Dynasty-as routine crime, and as rebellion and unrest-it is important to develop a model of how administrators understood it. Therefore, rather than assuming a fixed set of categories, this study models Qing administrators' typologies of violence based on the frequencies of term co-occurrence. Based on the term groupings in the model, five topics relate to violent unrest. Each topic accounts for a particular statistical pattern of word use corresponding to patterns of occurrence, observation and recording of related phenomena. These groupings give some insight into the ""crime rates"" of the eighteenth and nineteenth centuries, and more importantly, these groupings cast light on their understandings of crime, rebellion and unrest. "
545,"The content of academic journals provides insight into disciplinary boundaries and priorities. This paper uses correlated topic modeling (CTM), an innovative approach to textual analysis, for a cross-national comparison of the development of research agendas in the discipline of demography. Using articles from leading demographic journals from 1946 to 2005, CTM shows how the set of concepts relevant to the study of fertility was defined differently in France and Great Britain. Results indicate that demographic research agendas reflected both cultural and institutional differences that shaped different understandings of fertility decline. While British demography focused on high-fertility contexts, French demography focused on lower-fertility contexts. This difference reflects national intellectual traditions shaped by larger cultural discourses: the dominance of demographic transition theory and fears of overpopulation in Britain versus the co-existence in France of a second salient model, a theory of demographic ""revolution"" with sustained low fertility leading to depopulation. Relationships between expert concerns and broader public concerns are then examined in the British case by comparing journal publications to mass-media coverage of fertility and population issues. This comparison shows that British academic demography passed over some policy-relevant population issues, such as discussions of immigrant fertility, that were featured in the popular press. © 2013 Elsevier B.V.",2013-12-01,2-s2.0-84888129895,Poetics,Defining population problems: Using topic models for cross-national comparison of disciplinary development,"The content of academic journals provides insight into disciplinary boundaries and priorities. This paper uses correlated topic modeling (CTM), an innovative approach to textual analysis, for a cross-national comparison of the development of research agendas in the discipline of demography. Using articles from leading demographic journals from 1946 to 2005, CTM shows how the set of concepts relevant to the study of fertility was defined differently in France and Great Britain. Results indicate that demographic research agendas reflected both cultural and institutional differences that shaped different understandings of fertility decline. While British demography focused on high-fertility contexts, French demography focused on lower-fertility contexts. This difference reflects national intellectual traditions shaped by larger cultural discourses: the dominance of demographic transition theory and fears of overpopulation in Britain versus the co-existence in France of a second salient model, a theory of demographic ""revolution"" with sustained low fertility leading to depopulation. Relationships between expert concerns and broader public concerns are then examined in the British case by comparing journal publications to mass-media coverage of fertility and population issues. This comparison shows that British academic demography passed over some policy-relevant population issues, such as discussions of immigrant fertility, that were featured in the popular press. "
546,"Online discussion forums have emerged as a popular Web application to build and support online communities for numerous engineering interest areas and practice. However, a review of engineering education literature reveals scant research on the use of online discussion forums for engineering learning beyond the classroom. This study addresses this gap in knowledge through a study of the ""Homework Help"" section on AllAboutCircuits.com to examine what students sought help for and for what purpose. We downloaded over 6,000 discussion messages spanning over 8 years and extracted the textual data with a Python program. Instead of analyzing the data through manual means, we utilized the Natural Language Toolkit (NLTK) to capture textual patterns and leverage a topic modeling approach, Latent Dirichlet Allocation, to identify connected clusters of words. Linguistic Inquiry and Word Count (LIWC) analysis was also used to determine how often students use words associated with cognitive processes. We found that the homework help section of informal online discussion forums cater to students seeking help on fundamental ECE topics. Our findings also suggest that online discussion forums are supportive learning environments, as students freely engage in meaningful inquiries and social interactions with other learners. © 2013 IEEE.",2013-12-01,2-s2.0-84893244301,"Proceedings - Frontiers in Education Conference, FIE",Towards an understanding of ECE students' Use of online homework help forums,"Online discussion forums have emerged as a popular Web application to build and support online communities for numerous engineering interest areas and practice. However, a review of engineering education literature reveals scant research on the use of online discussion forums for engineering learning beyond the classroom. This study addresses this gap in knowledge through a study of the ""Homework Help"" section on AllAboutCircuits.com to examine what students sought help for and for what purpose. We downloaded over 6,000 discussion messages spanning over 8 years and extracted the textual data with a Python program. Instead of analyzing the data through manual means, we utilized the Natural Language Toolkit (NLTK) to capture textual patterns and leverage a topic modeling approach, Latent Dirichlet Allocation, to identify connected clusters of words. Linguistic Inquiry and Word Count (LIWC) analysis was also used to determine how often students use words associated with cognitive processes. We found that the homework help section of informal online discussion forums cater to students seeking help on fundamental ECE topics. Our findings also suggest that online discussion forums are supportive learning environments, as students freely engage in meaningful inquiries and social interactions with other learners. "
547,"The proliferation of the Web has led to the simultaneous explosive growth of both textual and link information. Many techniques have been developed to cope with this information explosion phenomenon. Early efforts include the development of non-Bayesian Web community discovery methods that exploit only link information to identify groups of topical coherent Web pages. Most non-Bayesian methods produce hard clustering results and cannot provide semantic interpretation. Recently, there has been growing interest in applying Bayesian-based approaches to discovering Web community. The Bayesian approaches for Web community discovery possess many good characteristics such as soft clustering results and ability to provide semantic interpretation of the extracted communities. This chapter presents a systematic survey and discussions of non-Bayesian and Bayesian-based approaches to the Web community discovery problem. © 2013, IGI Global.",2013-12-01,2-s2.0-84898222357,Social Media Mining and Social Network Analysis: Emerging Research,Topic modeling for web community discovery,"The proliferation of the Web has led to the simultaneous explosive growth of both textual and link information. Many techniques have been developed to cope with this information explosion phenomenon. Early efforts include the development of non-Bayesian Web community discovery methods that exploit only link information to identify groups of topical coherent Web pages. Most non-Bayesian methods produce hard clustering results and cannot provide semantic interpretation. Recently, there has been growing interest in applying Bayesian-based approaches to discovering Web community. The Bayesian approaches for Web community discovery possess many good characteristics such as soft clustering results and ability to provide semantic interpretation of the extracted communities. This chapter presents a systematic survey and discussions of non-Bayesian and Bayesian-based approaches to the Web community discovery problem. "
548,"We present a series of visualizations of online discussions that combine topic modeling with other dimensions of the discussion contributions, to help faculty assess and improve learning from discussions. After applying probabilistic latent semantic analysis (pLSA) to calculate the relative conceptual distance between discussion posts, we projected posts or collections of posts into a two-dimensional space. By color-coding points according to their temporal position in the course or according to the author's final grade, we captured patterns in students' contributions that connect the topic modeling factors to more intuitively familiar characteristics. We consider how some possible qualitative features of the discussion may be represented in the topic space and outline future work to develop these tools further. © ISLS.",2013-10-31,2-s2.0-84886544418,"Computer-Supported Collaborative Learning Conference, CSCL","Visualizing topics, time, and grades in online class discussions","We present a series of visualizations of online discussions that combine topic modeling with other dimensions of the discussion contributions, to help faculty assess and improve learning from discussions. After applying probabilistic latent semantic analysis (pLSA) to calculate the relative conceptual distance between discussion posts, we projected posts or collections of posts into a two-dimensional space. By color-coding points according to their temporal position in the course or according to the author's final grade, we captured patterns in students' contributions that connect the topic modeling factors to more intuitively familiar characteristics. We consider how some possible qualitative features of the discussion may be represented in the topic space and outline future work to develop these tools further. "
549,"Although Computer Supported Collaborative Learning (CSCL) technologies have gained an increasing role in educational environments, there are few automatic systems that address involvement, knowledge-building and collaboration in order to support tutors in the time consuming process of analyzing conversations. We propose a cohesion-based analysis model integrating multiple natural language techniques, an intervention scoring mechanism and a comprehensive collaboration assessment method, derived from social knowledge-building, reflected at utterance level through cohesion. Furthermore, by combining a holistic perspective of the entire conversation with a more fine grained view focused on each participant, we obtain a thorough evaluation of chat conversations with focus on topics modeling, participant interaction and collaboration. In order to sustain our model, we have performed a preliminary validation study that proves that our analysis is consistent with tutor evaluations. © ISLS.",2013-10-31,2-s2.0-84886572583,"Computer-Supported Collaborative Learning Conference, CSCL",Cohesion-based analysis of CSCL conversations: Holistic and individual perspectives,"Although Computer Supported Collaborative Learning (CSCL) technologies have gained an increasing role in educational environments, there are few automatic systems that address involvement, knowledge-building and collaboration in order to support tutors in the time consuming process of analyzing conversations. We propose a cohesion-based analysis model integrating multiple natural language techniques, an intervention scoring mechanism and a comprehensive collaboration assessment method, derived from social knowledge-building, reflected at utterance level through cohesion. Furthermore, by combining a holistic perspective of the entire conversation with a more fine grained view focused on each participant, we obtain a thorough evaluation of chat conversations with focus on topics modeling, participant interaction and collaboration. In order to sustain our model, we have performed a preliminary validation study that proves that our analysis is consistent with tutor evaluations. "
550,"Folklorists, like most practitioners in a field, understand the history of their discipline through a combination of their own reading and the consensus inherited from their graduate training and professional interactions. Disciplinary history, an effectively oral form of communication, codifies quickly. Highly contingent and random processes become widely understood as historically inevitable. In this preliminary report on a larger project examining the application of computational methodologies in the service of intellectual history, we explore the use of topic modeling as a way to understand the ebb and flow of topics and paradigms within a domain. Using JSTOR's Data for Research application programming interface to access the contents of 6,778 articles from three folklore studies journals (Journal of American Folklore, Western Folklore, Journal of Folklore Research), we used one form of topic modeling, Latent Dirichlet Allocation, to delineate 50 distinct topics drawn from 125 years of research publication. Of particular interest here was the legendary ""turn toward performance"" in our field. Copyright © 2013 by the Board of Trustees of the University of Illinois.",2013-09-01,2-s2.0-84890951775,Journal of American Folklore,Computing folklore studies: Mapping over a century of scholarly production through topics,"Folklorists, like most practitioners in a field, understand the history of their discipline through a combination of their own reading and the consensus inherited from their graduate training and professional interactions. Disciplinary history, an effectively oral form of communication, codifies quickly. Highly contingent and random processes become widely understood as historically inevitable. In this preliminary report on a larger project examining the application of computational methodologies in the service of intellectual history, we explore the use of topic modeling as a way to understand the ebb and flow of topics and paradigms within a domain. Using JSTOR's Data for Research application programming interface to access the contents of 6,778 articles from three folklore studies journals (Journal of American Folklore, Western Folklore, Journal of Folklore Research), we used one form of topic modeling, Latent Dirichlet Allocation, to delineate 50 distinct topics drawn from 125 years of research publication. Of particular interest here was the legendary ""turn toward performance"" in our field. "
551,"The advent of social media is changing the existing information behavior by letting users access to real-time online information channels without the constraints of time and space. It also generates a huge amount of data worth discovering novel knowledge. Social media, therefore, has created an enormous challenge for scientists trying to keep pace with developments in their field. Most of the previous studies have adopted broadbrush approaches which tend to result in providing limited analysis. To handle these problems properly, we introduce our real-time Twitter trend mining system, RT",2013-08-12,2-s2.0-84881177326,"Proceedings - 2013 International Conference on Social Intelligence and Technology, SOCIETY 2013",RT2M : Real-time twitter trend mining system,"The advent of social media is changing the existing information behavior by letting users access to real-time online information channels without the constraints of time and space. It also generates a huge amount of data worth discovering novel knowledge. Social media, therefore, has created an enormous challenge for scientists trying to keep pace with developments in their field. Most of the previous studies have adopted broadbrush approaches which tend to result in providing limited analysis. To handle these problems properly, we introduce our real-time Twitter trend mining system, RT"
552,"Spam is one of the major problems of today's Internet because it brings financial damage to companies and annoys individual users. Among those approaches developed to detect spam, the content-based machine learning algorithms are important and popular. However, these algorithms are trained using statistical representations of the terms that usually appear in the e-mails. Additionally, these methods are unable to account for the underlying semantics of terms within the messages. In this paper, we present a Bayesian topic model to address these limitations. We explore the use of semantics in spam filtering by representing e-mails as vectors of topics with a topic model: the Latent Dirichlet Allocation (LDA). Based upon this representation, the relationship between the topics and spam can be discovered by using a Bayesian method. We test this model on the Enron-Spam datasets and results show that the proposed model performs better than the baseline and can detect the internal semantics of spam messages. © 2013 by Binary Information Press.",2013-08-10,2-s2.0-84883267393,Journal of Information and Computational Science,A Bayesian topic model for spam filtering,"Spam is one of the major problems of today's Internet because it brings financial damage to companies and annoys individual users. Among those approaches developed to detect spam, the content-based machine learning algorithms are important and popular. However, these algorithms are trained using statistical representations of the terms that usually appear in the e-mails. Additionally, these methods are unable to account for the underlying semantics of terms within the messages. In this paper, we present a Bayesian topic model to address these limitations. We explore the use of semantics in spam filtering by representing e-mails as vectors of topics with a topic model: the Latent Dirichlet Allocation (LDA). Based upon this representation, the relationship between the topics and spam can be discovered by using a Bayesian method. We test this model on the Enron-Spam datasets and results show that the proposed model performs better than the baseline and can detect the internal semantics of spam messages. "
553,"Public health related tweets are difficult to identify in large conversational datasets like Twitter.com. Even more challenging is the visualization and analyses of the spatial patterns encoded in tweets. This study has the following objectives: how can topic modeling be used to identify relevant public health topics such as obesity on Twitter.com? What are the common obesity related themes? What is the spatial pattern of the themes? What are the research challenges of using large conversational datasets from social networking sites? Obesity is chosen as a test theme to demonstrate the effectiveness of topic modeling using Latent Dirichlet Allocation (LDA) and spatial analysis using Geographic Information System (GIS). The dataset is constructed from tweets (originating from the United States) extracted from Twitter.com on obesityrelated queries. Examples of such queries are 'food deserts', 'fast food', and 'childhood obesity'. The tweets are also georeferenced and time stamped. Three cohesive and meaningful themes such as 'childhood obesity and schools', 'obesity prevention', and 'obesity and food habits' are extracted from the LDA model. The GIS analysis of the extracted themes show distinct spatial pattern between rural and urban areas, northern and southern states, and between coasts and inland states. Further, relating the themes with ancillary datasets such as US census and locations of fast food restaurants based upon the location of the tweets in a GIS environment opened new avenues for spatial analyses and mapping. Therefore the techniques used in this study provide a possible toolset for computational social scientists in general, and health researchers in specific, to better understand health problems from large conversational datasets. © 2013 Cartography and Geographic Information Society.",2013-06-10,2-s2.0-84878526821,Cartography and Geographic Information Science,What are we 'tweeting' about obesity? Mapping tweets with topic modeling and Geographic Information System,"Public health related tweets are difficult to identify in large conversational datasets like Twitter.com. Even more challenging is the visualization and analyses of the spatial patterns encoded in tweets. This study has the following objectives: how can topic modeling be used to identify relevant public health topics such as obesity on Twitter.com? What are the common obesity related themes? What is the spatial pattern of the themes? What are the research challenges of using large conversational datasets from social networking sites? Obesity is chosen as a test theme to demonstrate the effectiveness of topic modeling using Latent Dirichlet Allocation (LDA) and spatial analysis using Geographic Information System (GIS). The dataset is constructed from tweets (originating from the United States) extracted from Twitter.com on obesityrelated queries. Examples of such queries are 'food deserts', 'fast food', and 'childhood obesity'. The tweets are also georeferenced and time stamped. Three cohesive and meaningful themes such as 'childhood obesity and schools', 'obesity prevention', and 'obesity and food habits' are extracted from the LDA model. The GIS analysis of the extracted themes show distinct spatial pattern between rural and urban areas, northern and southern states, and between coasts and inland states. Further, relating the themes with ancillary datasets such as US census and locations of fast food restaurants based upon the location of the tweets in a GIS environment opened new avenues for spatial analyses and mapping. Therefore the techniques used in this study provide a possible toolset for computational social scientists in general, and health researchers in specific, to better understand health problems from large conversational datasets. "
554,"Bruce Ackerman argues that major shifts in constitutional law can occur outside the Article V amendment process when there are unusually high levels of sustained popular attention to questions of constitutional significance. This Note develops a new empirical strategy to evaluate this claim using the debate over ratification of the Fourteenth Amendment as its test case. The Note applies a statistical process known as unsupervised topic modeling to a dataset containing over 19,000 pages of text from U.S. newspapers published between 1866 and 1884. This innovative methodological technique illuminates the structure of constitutional discourse during this period. The Note finds empirical support for the notion that the salience of constitutional issues was high throughout the ratification debate and then gradually declined as the country returned to a period of normal politics. These findings buttress Ackerman's cyclic theory of constitutional change at one of its more vulnerable points.",2013-05-01,2-s2.0-84878294681,Yale Law Journal,How do you measure a constitutional moment? Using algorithmic topic modeling to evaluate bruce ackerman's theory of constitutional change,"Bruce Ackerman argues that major shifts in constitutional law can occur outside the Article V amendment process when there are unusually high levels of sustained popular attention to questions of constitutional significance. This Note develops a new empirical strategy to evaluate this claim using the debate over ratification of the Fourteenth Amendment as its test case. The Note applies a statistical process known as unsupervised topic modeling to a dataset containing over 19,000 pages of text from U.S. newspapers published between 1866 and 1884. This innovative methodological technique illuminates the structure of constitutional discourse during this period. The Note finds empirical support for the notion that the salience of constitutional issues was high throughout the ratification debate and then gradually declined as the country returned to a period of normal politics. These findings buttress Ackerman's cyclic theory of constitutional change at one of its more vulnerable points."
555,"With the widespread adoption of social media, online learning communities are perceived as a network of knowledge comprised of interconnected individuals with varying roles. Opinion leaders are important in social networks because of their ability to influence the attitudes and behaviours of others via their superior status, education, and social prestige. Many theories have been put forward to explain the formation, characteristics, and durability of social networks, but few address the issue of opinion leader identification. This paper proposes an improved mix framework for opinion leader identification in online learning communities. The framework is validated by an experimental study. By analysing textual content, user behaviour and time, this study ranked opinion leaders based on four distinguishing features: expertise, novelty, influence, and activity. Furthermore, the performances of opinion leaders were further investigated in terms of longevity and centrality. Experimental study on real datasets has shown that our framework effectively identifies opinion leaders in online learning communities. © 2013 Elsevier B.V. All rights reserved.",2013-03-01,2-s2.0-84875275748,Knowledge-Based Systems,An improved mix framework for opinion leader identification in online learning communities,"With the widespread adoption of social media, online learning communities are perceived as a network of knowledge comprised of interconnected individuals with varying roles. Opinion leaders are important in social networks because of their ability to influence the attitudes and behaviours of others via their superior status, education, and social prestige. Many theories have been put forward to explain the formation, characteristics, and durability of social networks, but few address the issue of opinion leader identification. This paper proposes an improved mix framework for opinion leader identification in online learning communities. The framework is validated by an experimental study. By analysing textual content, user behaviour and time, this study ranked opinion leaders based on four distinguishing features: expertise, novelty, influence, and activity. Furthermore, the performances of opinion leaders were further investigated in terms of longevity and centrality. Experimental study on real datasets has shown that our framework effectively identifies opinion leaders in online learning communities. "
556,"In this paper we provide context to Topic Modelling as an Information Warfare technique. Topic modelling is a technique that discovers latent topics in unstructured and unlabelled collection of documents. The topic structure can be searched for interesting and relevant topics. The objectives of this paper is to describe topic modelling, put it in context as a useful IW technique and illustrate its use with two examples. We discuss several applications of topic modelling in the safety and security domain and list several topic model variations that are of special interest to the IW domain. © 2013 IEEE.",2013-01-01,2-s2.0-84896455439,"IEEE International Conference on Adaptive Science and Technology, ICAST",Topic modelling in the information warfare domain,"In this paper we provide context to Topic Modelling as an Information Warfare technique. Topic modelling is a technique that discovers latent topics in unstructured and unlabelled collection of documents. The topic structure can be searched for interesting and relevant topics. The objectives of this paper is to describe topic modelling, put it in context as a useful IW technique and illustrate its use with two examples. We discuss several applications of topic modelling in the safety and security domain and list several topic model variations that are of special interest to the IW domain. "
557,"Topic modeling provides a powerful way to analyze the content of a collection of documents. It has become a popular tool in many research areas, such as text mining, information retrieval, natural language processing, and other related fields. In real-world applications, however, the usefulness of topic modeling is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps, such as vastly reducing input vocabulary. In this article we introduce Regularized Latent Semantic Indexing (RLSI)-including a batch version and an online version, referred to as batch RLSI and online RLSI, respectively-to scale up topic modeling. Batch RLSI and online RLSI are as effective as existing topic modeling techniques and can scale to larger datasets without reducing input vocabulary. Moreover, online RLSI can be applied to stream data and can capture the dynamic evolution of topics. Both versions of RLSI formalize topic modeling as a problem of minimizing a quadratic loss function regularized by L",2013-01-01,2-s2.0-84873891666,ACM Transactions on Information Systems,Regularized latent semantic indexing: A new approach to large-scale topic modeling,"Topic modeling provides a powerful way to analyze the content of a collection of documents. It has become a popular tool in many research areas, such as text mining, information retrieval, natural language processing, and other related fields. In real-world applications, however, the usefulness of topic modeling is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps, such as vastly reducing input vocabulary. In this article we introduce Regularized Latent Semantic Indexing (RLSI)-including a batch version and an online version, referred to as batch RLSI and online RLSI, respectively-to scale up topic modeling. Batch RLSI and online RLSI are as effective as existing topic modeling techniques and can scale to larger datasets without reducing input vocabulary. Moreover, online RLSI can be applied to stream data and can capture the dynamic evolution of topics. Both versions of RLSI formalize topic modeling as a problem of minimizing a quadratic loss function regularized by L"
558,"User-generated reviews on the Web reflect users' sentiment about products, services and social events. Existing researches mostly focus on the sentiment classification of the product and service reviews in document level. Reviews of social events such as economic and political activities, which are called social reviews, have specific characteristics different to the reviews of products and services. In this paper, we propose an unsupervised approach to automatically discover the aspects discussed in Chinese social reviews and also the sentiments expressed in different aspects. The approach is called Multi-aspect Sentiment Analysis for Chinese Online Social Reviews (MSA-COSRs). We first apply the Latent Dirichlet Allocation (LDA) model to discover multi-aspect global topics of social reviews, and then extract the local topic and associated sentiment based on a sliding window context over the review text. The aspect of the local topic is identified by a trained LDA model, and the polarity of the associated sentiment is classified by HowNet lexicon. The experiment results show that MSA-COSR cannot only obtain good topic partitioning results, but also help to improve sentiment analysis accuracy. It helps to simultaneously discover multi-aspect fine-grained topics and associated sentiment. © 2012 Elsevier B.V. All rights reserved.",2013-01-01,2-s2.0-84870067197,Knowledge-Based Systems,Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling and HowNet lexicon,"User-generated reviews on the Web reflect users' sentiment about products, services and social events. Existing researches mostly focus on the sentiment classification of the product and service reviews in document level. Reviews of social events such as economic and political activities, which are called social reviews, have specific characteristics different to the reviews of products and services. In this paper, we propose an unsupervised approach to automatically discover the aspects discussed in Chinese social reviews and also the sentiments expressed in different aspects. The approach is called Multi-aspect Sentiment Analysis for Chinese Online Social Reviews (MSA-COSRs). We first apply the Latent Dirichlet Allocation (LDA) model to discover multi-aspect global topics of social reviews, and then extract the local topic and associated sentiment based on a sliding window context over the review text. The aspect of the local topic is identified by a trained LDA model, and the polarity of the associated sentiment is classified by HowNet lexicon. The experiment results show that MSA-COSR cannot only obtain good topic partitioning results, but also help to improve sentiment analysis accuracy. It helps to simultaneously discover multi-aspect fine-grained topics and associated sentiment. "
559,"Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster. Copyright 2013 by the author(s).",2013-01-01,2-s2.0-84897550363,"30th International Conference on Machine Learning, ICML 2013",A practical algorithm for topic modeling with provable guarantees,"Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster. "
560,"While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96. © 2013 Association for Computational Linguistics.",2013-01-01,2-s2.0-84907315007,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",Automatic term ambiguity detection,"While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96. "
561,"Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin. © 2013 Association for Computational Linguistics.",2013-01-01,2-s2.0-84907326472,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",TopicSpam: A topic-model-based approach for spam detection,"Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin. "
562,"The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval performances tend to be better when using topics with higher semantic coherence. © 2013 Association for Computational Linguistics.",2013-01-01,2-s2.0-84903644004,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",Are semantically coherent topic models useful for ad hoc information retrieval?,"The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval performances tend to be better when using topics with higher semantic coherence. "
563,"In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modeling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process. Copyright 2013 by the author(s).",2013-01-01,2-s2.0-84897505920,"30th International Conference on Machine Learning, ICML 2013",Dependent normalized random measures,"In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modeling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process. "
564,"A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing discourse entities that die out after just one mention (singletons) from those that lead longer lives (coreferent) would benefit NLP applications such as coreference resolution, protagonist identification, topic modeling, and discourse coherence. We build a logistic regression model for predicting the singleton/ coreferent distinction, drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features. The model is effective in its own right (78% accuracy), and incorporating it into a state-of-The-Art coreference resolution system yields a significant improvement.",2013-01-01,2-s2.0-84906515207,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",The life and death of discourse entities: Identifying singleton mentions,"A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing discourse entities that die out after just one mention (singletons) from those that lead longer lives (coreferent) would benefit NLP applications such as coreference resolution, protagonist identification, topic modeling, and discourse coherence. We build a logistic regression model for predicting the singleton/ coreferent distinction, drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features. The model is effective in its own right (78% accuracy), and incorporating it into a state-of-The-Art coreference resolution system yields a significant improvement."
565,"Computing user similarity is key for personalized location-based recommender systems and geographic information retrieval. So far, most existing work has focused on structured or semi-structured data to establish such measures. In this work, we propose topic modeling to exploit sparse, unstructured data, e.g., tips and reviews, as an additional feature to compute user similarity. Our model employs diagnosticity weighting based on the entropy of topics in order to assess the role of commonalities and variabilities between similar users. Finally, we offer a validation technique and results using data from the location-based social network Foursquare.",2013-01-01,2-s2.0-84939620610,Lecture Notes in Geoinformation and Cartography,A thematic approach to user similarity built on geosocial check-ins,"Computing user similarity is key for personalized location-based recommender systems and geographic information retrieval. So far, most existing work has focused on structured or semi-structured data to establish such measures. In this work, we propose topic modeling to exploit sparse, unstructured data, e.g., tips and reviews, as an additional feature to compute user similarity. Our model employs diagnosticity weighting based on the entropy of topics in order to assess the role of commonalities and variabilities between similar users. Finally, we offer a validation technique and results using data from the location-based social network Foursquare."
566,"Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques. © 2013 Association for Computational Linguistics.",2013-01-01,2-s2.0-84907372713,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",Discovering user interactions in ideological discussions,"Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques. "
567,"We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy. We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation. The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For these models we show that there exists a simple variational approximation for probabilistic inference. The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model's hierarchy. We compare our approach to existing implementations of nonparametric HDPs. On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods. Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security - one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy. Copyright 2013 by the author(s).",2013-01-01,2-s2.0-84897555149,"30th International Conference on Machine Learning, ICML 2013",A variational approximation for topic modeling of hierarchical corpora,"We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy. We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation. The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For these models we show that there exists a simple variational approximation for probabilistic inference. The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model's hierarchy. We compare our approach to existing implementations of nonparametric HDPs. On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods. Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security - one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy. "
568,"We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms with provable guarantees based on data-dependent and random projections to identify novel words and associated topics. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme. Copyright 2013 by the author(s).",2013-01-01,2-s2.0-84897526766,"30th International Conference on Machine Learning, ICML 2013",Topic discovery through data dependent and random projections,"We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms with provable guarantees based on data-dependent and random projections to identify novel words and associated topics. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme. "
569,"This article describes agendas as ""packages"" of topics of varying salience, set by the Russian Internet users on Russia's leading blog platform LiveJournal. The research involved modeling LiveJournal's topic structure, viewed as an important component of what is termed here self-generated public opinion. Topic modeling was performed automatically with the LDA algorithm, and complemented with hand labeling of topics. Data were collected by software created by the authors to generate a relational database storing all posts by the top 2,000 LiveJournal users from three one-month periods: two during the Russian parliamentary and presidential elections 2011-2012, and one control period. We find that LiveJournal top users share their attention evenly between ""social/political"" and ""private/recreational"" issues, the proportion being very stable. However, the substitution of diverse public affairs issues by the topics related to national street protests in the politicized periods compared to the control period was found both automatically and manually. The group of topics centered around social issues demonstrates the biggest volatility in terms of its composition and may serve as the foundation for monitoring self-generated public opinion by further application of sentiment/opinion mining methods to these topics. © 2013 Policy Studies Organization.",2013-01-01,2-s2.0-84880716210,Policy and Internet,Mapping the public agenda with topic modeling: The case of the Russian LiveJournal,"This article describes agendas as ""packages"" of topics of varying salience, set by the Russian Internet users on Russia's leading blog platform LiveJournal. The research involved modeling LiveJournal's topic structure, viewed as an important component of what is termed here self-generated public opinion. Topic modeling was performed automatically with the LDA algorithm, and complemented with hand labeling of topics. Data were collected by software created by the authors to generate a relational database storing all posts by the top 2,000 LiveJournal users from three one-month periods: two during the Russian parliamentary and presidential elections 2011-2012, and one control period. We find that LiveJournal top users share their attention evenly between ""social/political"" and ""private/recreational"" issues, the proportion being very stable. However, the substitution of diverse public affairs issues by the topics related to national street protests in the politicized periods compared to the control period was found both automatically and manually. The group of topics centered around social issues demonstrates the biggest volatility in terms of its composition and may serve as the foundation for monitoring self-generated public opinion by further application of sentiment/opinion mining methods to these topics. "
570,"The use of topic models to analyze domain-specific texts often requires manual validation of the latent topics to ensure that they are meaningful. We introduce a framework to support such a large-scale assessment of topical relevance. We measure the correspondence between a set of latent topics and a set of reference concepts to quantify four types of topical misalignment: junk, fused, missing, and repeated topics. Our analysis compares 10,000 topic model variants to 200 expert-provided domain concepts, and demonstrates how our framework can inform choices of model parameters, inference algorithms, and intrinsic measures of topical quality. Copyright 2013 by the author(s).",2013-01-01,2-s2.0-84897492517,"30th International Conference on Machine Learning, ICML 2013",Topic model diagnostics: Assessing domain relevance via topical alignment,"The use of topic models to analyze domain-specific texts often requires manual validation of the latent topics to ensure that they are meaningful. We introduce a framework to support such a large-scale assessment of topical relevance. We measure the correspondence between a set of latent topics and a set of reference concepts to quantify four types of topical misalignment: junk, fused, missing, and repeated topics. Our analysis compares 10,000 topic model variants to 200 expert-provided domain concepts, and demonstrates how our framework can inform choices of model parameters, inference algorithms, and intrinsic measures of topical quality. "
571,"The role of musical influence has long been debated by scholars and critics in the humanities, but never in a data-driven way. In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970's and the mid 1990's. Copyright 2013 by the author(s).",2013-01-01,2-s2.0-84897478211,"30th International Conference on Machine Learning, ICML 2013",Modeling musical influence with topic models,"The role of musical influence has long been debated by scholars and critics in the humanities, but never in a data-driven way. In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970's and the mid 1990's. "
572,"This paper proposes a framework to analyze the interdisciplinary collaboration in a coauthorship network from a meso perspective using topic modeling: (1) a customized topic model is developed to capture and formalize the interdisciplinary feature; and (2) the two algorithms Diversity Subgraph Extraction (DSE) and Constraint-based Diversity Subgraph Extraction (CDSE) are designed and implemented to extract a meso view, i.e. a diversity subgraph of the interdisciplinary collaboration. The proposed framework is demonstrated using a coauthorship network in the field of computer science. A comparison between DSE and Breadth First Search (BSF)-based subgraph extraction favors DSE in capturing the diversity in interdisciplinary collaboration. Potential possibilities for studying various research topics based on the proposed framework of analysis are discussed. © 2012 Elsevier Ltd.",2013-01-01,2-s2.0-84870488103,Journal of Informetrics,Mining diversity subgraph in multidisciplinary scientific collaboration networks: A meso perspective,"This paper proposes a framework to analyze the interdisciplinary collaboration in a coauthorship network from a meso perspective using topic modeling: (1) a customized topic model is developed to capture and formalize the interdisciplinary feature; and (2) the two algorithms Diversity Subgraph Extraction (DSE) and Constraint-based Diversity Subgraph Extraction (CDSE) are designed and implemented to extract a meso view, i.e. a diversity subgraph of the interdisciplinary collaboration. The proposed framework is demonstrated using a coauthorship network in the field of computer science. A comparison between DSE and Breadth First Search (BSF)-based subgraph extraction favors DSE in capturing the diversity in interdisciplinary collaboration. Potential possibilities for studying various research topics based on the proposed framework of analysis are discussed. "
573,"We adapt the popular LDA topic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level. We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions.",2013-01-01,2-s2.0-84926226219,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",A multi-dimensional bayesian approach to lexical style,"We adapt the popular LDA topic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level. We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions."
574,"The LDA topic model is being used to model corpora of documents that can be represented by bags of words. Here we extend the LDA model to deal with documents that are represented by bags of continuous descriptors. Given a finite dictionary of words, our extended LDA model allows for the soft assignment of descriptors to (many) dictionary words. We derive variational inference and parameter estimation procedures for the extended model, which closely resemble those obtained for the original model, with two important differences: First, the histogram of word counts is replaced by a histogram of pseudo word counts, or sums of responsibilities over all descriptors. Second, parameter estimation now depends on the average covariance matrix between these pseudocounts, reflecting the fact that with soft assignment words are not independent. We use this approach to address the detection of novel video events, where we seek to identify video events with low posterior probability. Using a benchmark dataset for novelty detection, we show a very significant improvement in the detection of novel events when using our extended LDA model with soft assignment to words as against hard assignment (the original model), achieving state of the art novelty detection results. Copyright 2013 by the author(s).",2013-01-01,2-s2.0-84897506862,"30th International Conference on Machine Learning, ICML 2013",LDA topic model with soft assignment of descriptors to words,"The LDA topic model is being used to model corpora of documents that can be represented by bags of words. Here we extend the LDA model to deal with documents that are represented by bags of continuous descriptors. Given a finite dictionary of words, our extended LDA model allows for the soft assignment of descriptors to (many) dictionary words. We derive variational inference and parameter estimation procedures for the extended model, which closely resemble those obtained for the original model, with two important differences: First, the histogram of word counts is replaced by a histogram of pseudo word counts, or sums of responsibilities over all descriptors. Second, parameter estimation now depends on the average covariance matrix between these pseudocounts, reflecting the fact that with soft assignment words are not independent. We use this approach to address the detection of novel video events, where we seek to identify video events with low posterior probability. Using a benchmark dataset for novelty detection, we show a very significant improvement in the detection of novel events when using our extended LDA model with soft assignment to words as against hard assignment (the original model), achieving state of the art novelty detection results. "
575,"In this paper, we use a topic model called Labeled Latent Dirichlet Alloca-tion (L-LDA), which is an extension of the LDA model often used for text mining, to analyze the keywords selected by authors in urban planning and urban management papers. We use keywords, sessions and authors related to the Computers in Urban Planning and Urban Management (CUPUM) conference as inputs for the model. We then evaluate the performance by comparing the training and target sets. The results are displayed using Web-based technologies. Our method extracts the characteristics of select-ed keywords, and reveals other relevant topics undetected by the author. Thus, our results can support writers and readers of research papers in the field of urban planning and urban management.",2013-01-01,2-s2.0-84899141382,Proceedings of CUPUM 2013: 13th International Conference on Computers in Urban Planning and Urban Management - Planning Support Systems for Sustainable Urban Development,Analysis of author-selected keywords in urban planning and urban management papers,"In this paper, we use a topic model called Labeled Latent Dirichlet Alloca-tion (L-LDA), which is an extension of the LDA model often used for text mining, to analyze the keywords selected by authors in urban planning and urban management papers. We use keywords, sessions and authors related to the Computers in Urban Planning and Urban Management (CUPUM) conference as inputs for the model. We then evaluate the performance by comparing the training and target sets. The results are displayed using Web-based technologies. Our method extracts the characteristics of select-ed keywords, and reveals other relevant topics undetected by the author. Thus, our results can support writers and readers of research papers in the field of urban planning and urban management."
576,"The proceedings contain 37 papers. The topics discussed include: semi-supervised local feature extraction of hyperspectral images over urban areas; animal identification based on footprint recognition; gender classification using face recognition; supporting rural teachers 21st century skills development through mobile technology use: a case in Cofimvaba, Eastern Cape, South Africa; the relative permittivity of the dielectric composite with a small doping of conductive nanoparticles; topic modeling in the information warfare domain; the case for cyber counter intelligence; identifying vehicle descriptions in microblogging text with the aim of reducing or predicting crime; utilizing cognitive work analysis for the design and evaluation of command and control user interfaces; and a novel cryptographic encryption technique of video images using quantum cryptography for satellite communications.",2013-01-01,2-s2.0-84896469534,"IEEE International Conference on Adaptive Science and Technology, ICAST","ICAST 2013 - 5th International Conference on Adaptive Science and Technology: The Future is Now: Adaptive Science and Technology Unbound, Proceedings","The proceedings contain 37 papers. The topics discussed include: semi-supervised local feature extraction of hyperspectral images over urban areas; animal identification based on footprint recognition; gender classification using face recognition; supporting rural teachers 21st century skills development through mobile technology use: a case in Cofimvaba, Eastern Cape, South Africa; the relative permittivity of the dielectric composite with a small doping of conductive nanoparticles; topic modeling in the information warfare domain; the case for cyber counter intelligence; identifying vehicle descriptions in microblogging text with the aim of reducing or predicting crime; utilizing cognitive work analysis for the design and evaluation of command and control user interfaces; and a novel cryptographic encryption technique of video images using quantum cryptography for satellite communications."
577,"Due to the heterogeneous case-by-case nature of traffic incidents, plenty of relevant information is recorded in free flow text fields instead of constrained value fields. As a result, such text components enclose considerable richness that is invaluable for incident analysis, modeling and prediction. However, the difficulty to formally interpret such data has led to minimal consideration in previous work.In this paper, we focus on the task of incident duration prediction, more specifically on predicting clearance time, the period between incident reporting and road clearance. An accurate prediction will help traffic operators implement appropriate mitigation measures and better inform drivers about expected road blockage time.The key contribution is the introduction of topic modeling, a text analysis technique, as a tool for extracting information from incident reports in real time. We analyze a dataset of 2. years of accident cases and develop a machine learning based duration prediction model that integrates textual with non-textual features. To demonstrate the value of the approach, we compare predictions with and without text analysis using several different prediction models. Models using textual features consistently outperform the others in nearly all circumstances, presenting errors up to 28% lower than models without such information. © 2013 Elsevier Ltd.",2013-01-01,2-s2.0-84887588622,Transportation Research Part C: Emerging Technologies,Text analysis in incident duration prediction,"Due to the heterogeneous case-by-case nature of traffic incidents, plenty of relevant information is recorded in free flow text fields instead of constrained value fields. As a result, such text components enclose considerable richness that is invaluable for incident analysis, modeling and prediction. However, the difficulty to formally interpret such data has led to minimal consideration in previous work.In this paper, we focus on the task of incident duration prediction, more specifically on predicting clearance time, the period between incident reporting and road clearance. An accurate prediction will help traffic operators implement appropriate mitigation measures and better inform drivers about expected road blockage time.The key contribution is the introduction of topic modeling, a text analysis technique, as a tool for extracting information from incident reports in real time. We analyze a dataset of 2. years of accident cases and develop a machine learning based duration prediction model that integrates textual with non-textual features. To demonstrate the value of the approach, we compare predictions with and without text analysis using several different prediction models. Models using textual features consistently outperform the others in nearly all circumstances, presenting errors up to 28% lower than models without such information. "
578,"Bioinformatics is a fast-growing, diverse research field that has recently gained much public attention. Even though there are several attempts to understand the field of bioinformatics by bibliometric analysis, the proposed approach in this paper is the first attempt at applying text mining techniques to a large set of full-text articles to detect the knowledge structure of the field. To this end, we use PubMed Central full-text articles for bibliometric analysis instead of relying on citation data provided in Web of Science. In particular, we develop text mining routines to build a custom-made citation database as a result of mining full-text. We present several interesting findings in this study. First, the majority of the papers published in the field of bioinformatics are not cited by others (63 % of papers received less than two citations). Second, there is a linear, consistent increase in the number of publications. Particularly year 2003 is the turning point in terms of publication growth. Third, most researches of bioinformatics are driven by USA-based institutes followed by European institutes. Fourth, the results of topic modeling and word co-occurrence analysis reveal that major topics focus more on biological aspects than on computational aspects of bioinformatics. However, the top 10 ranked articles identified by PageRank are more related to computational aspects. Fifth, visualization of author co-citation analysis indicates that researchers in molecular biology or genomics play a key role in connecting sub-disciplines of bioinformatics. © 2012 Akadémiai Kiadó, Budapest, Hungary.",2013-01-01,2-s2.0-84879184331,Scientometrics,Detecting the knowledge structure of bioinformatics by mining full-text collections,"Bioinformatics is a fast-growing, diverse research field that has recently gained much public attention. Even though there are several attempts to understand the field of bioinformatics by bibliometric analysis, the proposed approach in this paper is the first attempt at applying text mining techniques to a large set of full-text articles to detect the knowledge structure of the field. To this end, we use PubMed Central full-text articles for bibliometric analysis instead of relying on citation data provided in Web of Science. In particular, we develop text mining routines to build a custom-made citation database as a result of mining full-text. We present several interesting findings in this study. First, the majority of the papers published in the field of bioinformatics are not cited by others (63 % of papers received less than two citations). Second, there is a linear, consistent increase in the number of publications. Particularly year 2003 is the turning point in terms of publication growth. Third, most researches of bioinformatics are driven by USA-based institutes followed by European institutes. Fourth, the results of topic modeling and word co-occurrence analysis reveal that major topics focus more on biological aspects than on computational aspects of bioinformatics. However, the top 10 ranked articles identified by PageRank are more related to computational aspects. Fifth, visualization of author co-citation analysis indicates that researchers in molecular biology or genomics play a key role in connecting sub-disciplines of bioinformatics. "
579,"We present three approaches to lexical chaining based on the LDA topic model and evaluate them intrinsically on a manually annotated set of German documents. After motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains, we describe our new two-level chain annotation scheme, which rooted in the concept of cohesive harmony. Also, we propose a new measure for direct evaluation of lexical chains. Our three LDA-based approaches outperform two knowledge-based state-of-The art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource. Subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within NLP applications.",2013-01-01,2-s2.0-84926154230,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",Three knowledge-free methods for automatic lexical chain extraction,"We present three approaches to lexical chaining based on the LDA topic model and evaluate them intrinsically on a manually annotated set of German documents. After motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains, we describe our new two-level chain annotation scheme, which rooted in the concept of cohesive harmony. Also, we propose a new measure for direct evaluation of lexical chains. Our three LDA-based approaches outperform two knowledge-based state-of-The art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource. Subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within NLP applications."
580,"Contextual factors can greatly influence users' decisions in selecting items, such as songs when listening to music. The goal of a context-aware recommender system is to adapt its recommendations not just to the general preferences of users, but also to the context in which users are seeking those rec- ommendations. In the domain of music recommendation, the explicit contextual factors and their values might not be known to the system, a priori. Moreover, the contextual state of a user can be dynamic and change during an inter- action with the system. In this paper, we present a hybrid context-aware recommender system which infers contextual information from the sequence of songs listened to or specified by a user and uses this information to produce context- aware recommendations. Our system mines popular tags for songs from social media Web sites and uses a topic modeling approach to learn latent topics representing various contexts. We then model each song as a set of latent topics capturing the general characteristics of that song. This representation is used to track and detect changes in user's choice of mu- sic, as reffected in a playlist of song sequence, and adjust the recommendations to better meet the current context of the user. Using our approach, the contextual information can be integrated with any traditional recommendation al- gorithm to produce context-aware recommendations. For our system, we designed and evaluated two hybrid meth- ods. The first hybrid combines collaborative filtering and content-based recommendation techniques, and the second hybrid additionally incorporates information about pairwise song associations. Our evaluation results show that both the hybrid approach and the contextualization can enhance the performance of baseline music recommendation method. Copyright 2012 ACM.",2012-12-10,2-s2.0-84870562922,"International Conference on Information and Knowledge Management, Proceedings",Using social tags to infer context in hybrid music recommendation,"Contextual factors can greatly influence users' decisions in selecting items, such as songs when listening to music. The goal of a context-aware recommender system is to adapt its recommendations not just to the general preferences of users, but also to the context in which users are seeking those rec- ommendations. In the domain of music recommendation, the explicit contextual factors and their values might not be known to the system, a priori. Moreover, the contextual state of a user can be dynamic and change during an inter- action with the system. In this paper, we present a hybrid context-aware recommender system which infers contextual information from the sequence of songs listened to or specified by a user and uses this information to produce context- aware recommendations. Our system mines popular tags for songs from social media Web sites and uses a topic modeling approach to learn latent topics representing various contexts. We then model each song as a set of latent topics capturing the general characteristics of that song. This representation is used to track and detect changes in user's choice of mu- sic, as reffected in a playlist of song sequence, and adjust the recommendations to better meet the current context of the user. Using our approach, the contextual information can be integrated with any traditional recommendation al- gorithm to produce context-aware recommendations. For our system, we designed and evaluated two hybrid meth- ods. The first hybrid combines collaborative filtering and content-based recommendation techniques, and the second hybrid additionally incorporates information about pairwise song associations. Our evaluation results show that both the hybrid approach and the contextualization can enhance the performance of baseline music recommendation method. "
581,"In this paper, a novel approach to topic modeling based on the Higher Order Learning framework, Higher-Order Latent Dirichlet Allocation (HO-LDA), is applied to a critical issue in homeland security, nuclear detection. In addition, this research strives to improve topic models in the 'real time' environment of online learning. In total, seventeen different nuclear radioisotopes are classified, and performance of Higher-Order versus traditional techniques is evaluated. This project employs LDA and HO-LDA on a nuclear detection numeric dataset to gain a topic decomposition of instances. These learned topics are then used as features in a traditional supervised classification algorithm. In essence, the LDA or HO-LDA topic assignments are used as features in supervised learning algorithms that predict the class (isotope), treating LDA or HO-LDA as a feature space transform. Using Topic Modeling on numeric nuclear detection data is cutting edge, as to our knowledge this has never been done before on a nuclear detection dataset. Two methods of feature transformation are evaluated, including Multinomial Feature Creation and Maximum Channel Value Feature Creation. Results demonstrate further evidence that Higher Order Learning techniques can be usefully applied in topic modeling applied to nuclear detection. © 2012 IEEE.",2012-12-01,2-s2.0-84874572256,"2012 IEEE International Conference on Technologies for Homeland Security, HST 2012",Nuclear detection using higher-order topic modeling,"In this paper, a novel approach to topic modeling based on the Higher Order Learning framework, Higher-Order Latent Dirichlet Allocation (HO-LDA), is applied to a critical issue in homeland security, nuclear detection. In addition, this research strives to improve topic models in the 'real time' environment of online learning. In total, seventeen different nuclear radioisotopes are classified, and performance of Higher-Order versus traditional techniques is evaluated. This project employs LDA and HO-LDA on a nuclear detection numeric dataset to gain a topic decomposition of instances. These learned topics are then used as features in a traditional supervised classification algorithm. In essence, the LDA or HO-LDA topic assignments are used as features in supervised learning algorithms that predict the class (isotope), treating LDA or HO-LDA as a feature space transform. Using Topic Modeling on numeric nuclear detection data is cutting edge, as to our knowledge this has never been done before on a nuclear detection dataset. Two methods of feature transformation are evaluated, including Multinomial Feature Creation and Maximum Channel Value Feature Creation. Results demonstrate further evidence that Higher Order Learning techniques can be usefully applied in topic modeling applied to nuclear detection. "
582,"Large amount of electronic clinical data encompass important information in free text format. To be able to help guide medical decision-making, text needs to be efficiently processed and coded. In this research, we investigate techniques to improve classification of Emergency Department computed topography (CT) reports. The proposed system uses Natural Language Processing (NLP) to generate structured output from patient reports and then applies machine learning techniques to code for the presence of clinically important injuries for traumatic orbital fracture victims. Topic modeling of the corpora is also utilized as an alternative representation of the patient reports. Our results show that both NLP and topic modeling improve raw text classification results. Within NLP features, filtering the codes using modifiers produces the best performance. Topic modeling, on the other hand, shows mixed results. Topic vectors provide good dimensionality reduction and get comparable classification results as with NLP features. However, binary topic classification fails to improve upon raw text classification. © 2012 IEEE.",2012-12-01,2-s2.0-84873572611,"Proceedings - 2012 11th International Conference on Machine Learning and Applications, ICMLA 2012",Clinical report classification using natural language processing and topic modeling,"Large amount of electronic clinical data encompass important information in free text format. To be able to help guide medical decision-making, text needs to be efficiently processed and coded. In this research, we investigate techniques to improve classification of Emergency Department computed topography (CT) reports. The proposed system uses Natural Language Processing (NLP) to generate structured output from patient reports and then applies machine learning techniques to code for the presence of clinically important injuries for traumatic orbital fracture victims. Topic modeling of the corpora is also utilized as an alternative representation of the patient reports. Our results show that both NLP and topic modeling improve raw text classification results. Within NLP features, filtering the codes using modifiers produces the best performance. Topic modeling, on the other hand, shows mixed results. Topic vectors provide good dimensionality reduction and get comparable classification results as with NLP features. However, binary topic classification fails to improve upon raw text classification. "
583,"Latent topic modeling has proven to be an effective means for learning the underlying semantic content within document collections. Latent topic modeling has traditionally been applied to bag-of-words representations that ignore word sequence information that can aid in semantic understanding. In this work we introduce a method for efficiently incorporating arbitrarily long word sequences into a topic modeling approach. This method iteratively constructs a constrained set of phrase trees in an unsupervised fashion from a document collection using weighted pointwise mutual information statistics to guide the process. In experiments on the Fisher Corpus of conversational speech, the incorporation of learned phrases into a latent topic model yielded significant improvements in the unsupervised discovery of the known topics present within the data. © 2012 IEEE.",2012-12-01,2-s2.0-84874251784,"2012 IEEE Workshop on Spoken Language Technology, SLT 2012 - Proceedings",Modeling multiword phrases with constrained phrase trees for improved topic modeling of conversational speech,"Latent topic modeling has proven to be an effective means for learning the underlying semantic content within document collections. Latent topic modeling has traditionally been applied to bag-of-words representations that ignore word sequence information that can aid in semantic understanding. In this work we introduce a method for efficiently incorporating arbitrarily long word sequences into a topic modeling approach. This method iteratively constructs a constrained set of phrase trees in an unsupervised fashion from a document collection using weighted pointwise mutual information statistics to guide the process. In experiments on the Fisher Corpus of conversational speech, the incorporation of learned phrases into a latent topic model yielded significant improvements in the unsupervised discovery of the known topics present within the data. "
584,"Probabilistic topic models have been proven very useful for many text mining tasks. Although many variants of topic models have been proposed, most existing works are based on the bag-of-words representation of text in which word combination and order are generally ignored, resulting in inaccurate semantic representation of text. In this paper, we propose a general way to go beyond the bag-of-words representation for topic modeling by applying frequent pattern mining to discover frequent word patterns that can capture semantic associations between words and then using them as additional supplementary semantic units to augment the conventional bag-of-words representation. By viewing a topic model as a generative model for such augmented text data, we can go beyond the bag-of-words assumption to potentially capture more semantic associations between words. Since efficient algorithms for mining frequent word patterns are available, this general strategy for improving topic models can be applied to improve any topic models without substantially increasing the computational complexity of the model. Experiment results show that such a frequent pattern-based data enrichment approach can improve over two representative existing probabilistic topic models for the classification task. We also studied variations of frequent pattern usage in topic modeling and found that using compressed and closed patterns performs best.",2012-12-01,2-s2.0-84878606098,Proceedings of the ASIST Annual Meeting,Enriching text representation with frequent pattern mining for probabilistic topic modeling,"Probabilistic topic models have been proven very useful for many text mining tasks. Although many variants of topic models have been proposed, most existing works are based on the bag-of-words representation of text in which word combination and order are generally ignored, resulting in inaccurate semantic representation of text. In this paper, we propose a general way to go beyond the bag-of-words representation for topic modeling by applying frequent pattern mining to discover frequent word patterns that can capture semantic associations between words and then using them as additional supplementary semantic units to augment the conventional bag-of-words representation. By viewing a topic model as a generative model for such augmented text data, we can go beyond the bag-of-words assumption to potentially capture more semantic associations between words. Since efficient algorithms for mining frequent word patterns are available, this general strategy for improving topic models can be applied to improve any topic models without substantially increasing the computational complexity of the model. Experiment results show that such a frequent pattern-based data enrichment approach can improve over two representative existing probabilistic topic models for the classification task. We also studied variations of frequent pattern usage in topic modeling and found that using compressed and closed patterns performs best."
585,"This paper proposes an improved approach of summarization for spoken multi-party interaction, in which intra-speaker and inter-speaker topics are modeled in a graph constructed with topical relations. Each utterance is represented as a node of the graph, and the edge between two nodes is weighted by the similarity between the two utterances, which is the topical similarity, as evaluated by probabilistic latent semantic analysis (PLSA). We model intra-speaker topics by sharing the topics from the same speaker and inter-speaker topics by partially sharing the topics from the adjacent utterances based on temporal information. For both manual transcripts and ASR output, experiments confirmed the efficacy of combining intra- and inter-speaker topic modeling for summarization.",2012-12-01,2-s2.0-84878563752,"13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012",Integrating intra-speaker topic modeling and temporal-based inter-speaker topic modeling in random walk for improved multi-party meeting summarization,"This paper proposes an improved approach of summarization for spoken multi-party interaction, in which intra-speaker and inter-speaker topics are modeled in a graph constructed with topical relations. Each utterance is represented as a node of the graph, and the edge between two nodes is weighted by the similarity between the two utterances, which is the topical similarity, as evaluated by probabilistic latent semantic analysis (PLSA). We model intra-speaker topics by sharing the topics from the same speaker and inter-speaker topics by partially sharing the topics from the adjacent utterances based on temporal information. For both manual transcripts and ASR output, experiments confirmed the efficacy of combining intra- and inter-speaker topic modeling for summarization."
586,"In this paper, we propose a novel problem of summarizing textual corporate risk factor disclosure, which aims to simultaneously infer the risk types across corpus and assign each risk factor to its most probable risk type. To solve the problem, we develop a variation of LDA topic model called Sent-LDA. The variational EM learning algorithm, which guarantees fast convergence, is derived and implemented for our model. Experiments show that our model is much more efficient and effective than LDA for solving our proposed problem. Specifically, our model is 50 times faster than LDA in the same conditions, and generates better topics for summarization than LDA. Our model is visualized in a publicly available system.",2012-12-01,2-s2.0-84886487906,"International Conference on Information Systems, ICIS 2012",Summarization of corporate risk factor disclosure through topic modeling,"In this paper, we propose a novel problem of summarizing textual corporate risk factor disclosure, which aims to simultaneously infer the risk types across corpus and assign each risk factor to its most probable risk type. To solve the problem, we develop a variation of LDA topic model called Sent-LDA. The variational EM learning algorithm, which guarantees fast convergence, is derived and implemented for our model. Experiments show that our model is much more efficient and effective than LDA for solving our proposed problem. Specifically, our model is 50 times faster than LDA in the same conditions, and generates better topics for summarization than LDA. Our model is visualized in a publicly available system."
587,"This paper describes an automatic topic extraction, categorization, and relevance ranking model for multi-lingual surveys and questions that exploits machine learning algorithms such as topic modeling and fuzzy clustering. Automatically generated question and survey categories are used to build question banks and category-specific survey templates. First, we describe different pre-processing steps we considered for removing noise in the multilingual survey text. Second, we explain our strategy to automatically extract survey categories from surveys based on topic models. Third, we describe different methods to cluster questions under survey categories and group them based on relevance. Last, we describe our experimental results on a large group of unique, real-world survey datasets from the German, Spanish, French, and Portuguese languages and our refining methods to determine meaningful and sensible categories for building question banks. We conclude this document with possible enhancements to the current system and impacts in the business domain. © 2012 IEEE.",2012-12-01,2-s2.0-84873575300,"Proceedings - 2012 11th International Conference on Machine Learning and Applications, ICMLA 2012",A machine learning based topic exploration and categorization on surveys,"This paper describes an automatic topic extraction, categorization, and relevance ranking model for multi-lingual surveys and questions that exploits machine learning algorithms such as topic modeling and fuzzy clustering. Automatically generated question and survey categories are used to build question banks and category-specific survey templates. First, we describe different pre-processing steps we considered for removing noise in the multilingual survey text. Second, we explain our strategy to automatically extract survey categories from surveys based on topic models. Third, we describe different methods to cluster questions under survey categories and group them based on relevance. Last, we describe our experimental results on a large group of unique, real-world survey datasets from the German, Spanish, French, and Portuguese languages and our refining methods to determine meaningful and sensible categories for building question banks. We conclude this document with possible enhancements to the current system and impacts in the business domain. "
588,"Backoff smoothing and topic modeling are crucial issues in n-gram language model. This paper presents a Bayesian non-parametric learning approach to tackle these two issues. We develop a topic-based language model where the numbers of topics and n-grams are automatically determined from data. To cope with this model selection problem, we introduce the nonparametric priors for topics and backoff n-grams. The infinite language models are constructed through the hierarchical Dirichlet process compound Pitman-Yor (PY) process. We develop the topic-based hierarchical PY language model (THPY-LM) with power-law behavior. This model can be simplified to the hierarchical PY (HPY) LM by disregarding the topic information and also the modified Kneser-Ney (MKN) LM by further disregarding the Bayesian treatment. In the experiments, the proposed THPY-LM outperforms state-of-art methods using MKN-LM and HPY-LM. © 2012 IEEE.",2012-12-01,2-s2.0-84874458935,"2012 8th International Symposium on Chinese Spoken Language Processing, ISCSLP 2012",Bayesian nonparametric language models,"Backoff smoothing and topic modeling are crucial issues in n-gram language model. This paper presents a Bayesian non-parametric learning approach to tackle these two issues. We develop a topic-based language model where the numbers of topics and n-grams are automatically determined from data. To cope with this model selection problem, we introduce the nonparametric priors for topics and backoff n-grams. The infinite language models are constructed through the hierarchical Dirichlet process compound Pitman-Yor (PY) process. We develop the topic-based hierarchical PY language model (THPY-LM) with power-law behavior. This model can be simplified to the hierarchical PY (HPY) LM by disregarding the topic information and also the modified Kneser-Ney (MKN) LM by further disregarding the Bayesian treatment. In the experiments, the proposed THPY-LM outperforms state-of-art methods using MKN-LM and HPY-LM. "
589,"Teachers adopting CSCL often face the challenge of handling massive textual information, and finding it difficult to have a clear grasp of the topics being addressed in the discourse. Topic modeling, an emerging field in machine learning, has the potential to solve this problem by automatically extracting from text collections formal representations of latent topics. However, the interpretation of latent topics is still a challenge, which hinders the use of this state-of-the-art technology from wider use in CSCL contexts. In a recent paper, we put forward a novel topic discovery method, the fLDA model, based on Minsky's Frame theory. This method has the advantage of providing outputs that are potentially more easily interpretable for generating the topic of each thematic cluster. In this paper, we show how fLDA can be used in extracting and visualizing the topics of asynchronous online discourse from four classrooms. © ISLS.",2012-12-01,2-s2.0-84878680808,"10th International Conference of the Learning Sciences: The Future of Learning, ICLS 2012 - Proceedings",Automatic extraction of interpretable topics from online discourse,"Teachers adopting CSCL often face the challenge of handling massive textual information, and finding it difficult to have a clear grasp of the topics being addressed in the discourse. Topic modeling, an emerging field in machine learning, has the potential to solve this problem by automatically extracting from text collections formal representations of latent topics. However, the interpretation of latent topics is still a challenge, which hinders the use of this state-of-the-art technology from wider use in CSCL contexts. In a recent paper, we put forward a novel topic discovery method, the fLDA model, based on Minsky's Frame theory. This method has the advantage of providing outputs that are potentially more easily interpretable for generating the topic of each thematic cluster. In this paper, we show how fLDA can be used in extracting and visualizing the topics of asynchronous online discourse from four classrooms. "
590,"We present a novel topic modelling-based methodology to track emerging events in microblogs such as Twitter. Our topic model has an in-built update mechanism based on time slices and implements a dynamic vocabulary. We first show that the method is robust in detecting events using a range of datasets with injected novel events, and then demonstrate its application in identifying trending topics in Twitter. © 2012 The COLING.",2012-12-01,2-s2.0-84876797765,24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers,On-line trend analysis with topic models: Twitter trends detection topic model online,"We present a novel topic modelling-based methodology to track emerging events in microblogs such as Twitter. Our topic model has an in-built update mechanism based on time slices and implements a dynamic vocabulary. We first show that the method is robust in detecting events using a range of datasets with injected novel events, and then demonstrate its application in identifying trending topics in Twitter. "
591,"Online streaming companies such as Netflix have become dominant in the media distribution sector. However, such media delivery services often support very rudimentary search, especially for natural language queries. To provide a more natural search interface, we have developed a conversational movie search system, which parses the recognition hypothesis of a spoken query into semantic classes using conditional random fields (CRFs), and then searches an indexed database with the identified semantics. Topic modeling on user-generated content (e.g., movie reviews) is employed for query expansion. Thirteen searching schemas are supported (such as genre, plot, character and soundtrack search). A crowd-sourcing platform was utilized to automatically collect large-scale annotated data for incremental CRF training.",2012-12-01,2-s2.0-84878536564,"13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012",A conversational movie search system based on conditional random fields,"Online streaming companies such as Netflix have become dominant in the media distribution sector. However, such media delivery services often support very rudimentary search, especially for natural language queries. To provide a more natural search interface, we have developed a conversational movie search system, which parses the recognition hypothesis of a spoken query into semantic classes using conditional random fields (CRFs), and then searches an indexed database with the identified semantics. Topic modeling on user-generated content (e.g., movie reviews) is employed for query expansion. Thirteen searching schemas are supported (such as genre, plot, character and soundtrack search). A crowd-sourcing platform was utilized to automatically collect large-scale annotated data for incremental CRF training."
592,"We present an online story segmentation approach for Broadcast News (BN) that is built upon and integrated into BBN COTS multilingual Broadcast Monitoring System (BMS). We take a discriminative model-based approach, using Support Vector Machines to segment BN transcriptions into thematically coherent stories within the real-time constraints defined by BMS. We extract lexical, topical and story boundary cue features from source languagetranscriptions, machine translated (MT) English and metadata generated by BMS. We leverage BBN's Topic Classification technique to extract topic persistence features, and incorporate topic supporting words and topic clusters to encode thematic transitions. Using the discriminative modelbased approach, we get a relative gain of 27.9% on English BN and 22.0% on Arabic BN over a rule-based system. We also demonstrate a relative improvement of 11.8% in segmentation performance using features extracted from MT English compared to Arabic source features. We highlight the impact of topic model training in our story segmentation approach by varying corpus size to achieve a 13.7% relative gain with increase in number of topics.",2012-12-01,2-s2.0-84878531692,"13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012",Online story segmentation of multilingual streaming Broadcast News,"We present an online story segmentation approach for Broadcast News (BN) that is built upon and integrated into BBN COTS multilingual Broadcast Monitoring System (BMS). We take a discriminative model-based approach, using Support Vector Machines to segment BN transcriptions into thematically coherent stories within the real-time constraints defined by BMS. We extract lexical, topical and story boundary cue features from source languagetranscriptions, machine translated (MT) English and metadata generated by BMS. We leverage BBN's Topic Classification technique to extract topic persistence features, and incorporate topic supporting words and topic clusters to encode thematic transitions. Using the discriminative modelbased approach, we get a relative gain of 27.9% on English BN and 22.0% on Arabic BN over a rule-based system. We also demonstrate a relative improvement of 11.8% in segmentation performance using features extracted from MT English compared to Arabic source features. We highlight the impact of topic model training in our story segmentation approach by varying corpus size to achieve a 13.7% relative gain with increase in number of topics."
593,"We outline a paradigm to preserve results of digital scholarship, whether they are query results, feature values, or topic assignments. This paradigm is characterized by using annotations as multifunctional carriers and making them portable. The testing grounds we have chosen are two significant enterprises, one in the history of science, and one in Hebrew scholarship. The first one (CKCC) focuses on the results of a project where a Dutch consortium of universities, research institutes, and cultural heritage institutions experimented for 4 years with language techniques and topic modeling methods with the aim to analyze the emergence of scholarly debates. The data: a complex set of about 20.000 letters. The second one (DTHB) is a multi-year effort to express the linguistic features of the Hebrew bible in a text database, which is still growing in detail and sophistication. Versions of this database are packaged in commercial bible study software. We state that the results of these forms of scholarship require new knowledge management and archive practices. Only when researchers can build efficiently on each other's (intermediate) results, they can achieve the aggregations of quality data by which new questions can be answered, and hidden patterns visualized. Archives are required to find a balance between preserving authoritative versions of sources and supporting collaborative efforts in digital scholarship. Annotations are promising vehicles for preserving and reusing research results.",2012-12-01,2-s2.0-84878598713,Proceedings of the ASIST Annual Meeting,Annotation as a new paradigm in research archiving : Two case studies: Republic of letters - Hebrew text database,"We outline a paradigm to preserve results of digital scholarship, whether they are query results, feature values, or topic assignments. This paradigm is characterized by using annotations as multifunctional carriers and making them portable. The testing grounds we have chosen are two significant enterprises, one in the history of science, and one in Hebrew scholarship. The first one (CKCC) focuses on the results of a project where a Dutch consortium of universities, research institutes, and cultural heritage institutions experimented for 4 years with language techniques and topic modeling methods with the aim to analyze the emergence of scholarly debates. The data: a complex set of about 20.000 letters. The second one (DTHB) is a multi-year effort to express the linguistic features of the Hebrew bible in a text database, which is still growing in detail and sophistication. Versions of this database are packaged in commercial bible study software. We state that the results of these forms of scholarship require new knowledge management and archive practices. Only when researchers can build efficiently on each other's (intermediate) results, they can achieve the aggregations of quality data by which new questions can be answered, and hidden patterns visualized. Archives are required to find a balance between preserving authoritative versions of sources and supporting collaborative efforts in digital scholarship. Annotations are promising vehicles for preserving and reusing research results."
594,"The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation.",2012-12-01,2-s2.0-84886483941,"International Conference on Information Systems, ICIS 2012","International Conference on Information Systems, ICIS 2012, Volume 2","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation."
595,"The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation.",2012-12-01,2-s2.0-84886483021,"International Conference on Information Systems, ICIS 2012","International Conference on Information Systems, ICIS 2012, Volume 4","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation."
596,"The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation.",2012-12-01,2-s2.0-84886550717,"International Conference on Information Systems, ICIS 2012","International Conference on Information Systems, ICIS 2012, Volume 5","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation."
597,"The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation.",2012-12-01,2-s2.0-84886536369,"International Conference on Information Systems, ICIS 2012","International Conference on Information Systems, ICIS 2012, Volume 1","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation."
598,"The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation.",2012-12-01,2-s2.0-84886470101,"International Conference on Information Systems, ICIS 2012","International Conference on Information Systems, ICIS 2012, Volume 3","The proceedings contain 308 papers. The special focus in this conference is on Information Systems. The topics include: Heterogeneity in IT landscapes and monopoly power of firms; community ecology for innovation concept; peer influence in a very large social network; assessing value in an online network of products; optimal pricing with positive network effects; research on viral marketing propagating oriented to marketing context; repurchase intentions of information technology; a multi-level analysis of the impact of health information technology on hospital performance; institutional logics of adoption and use of health information technology; post-acceptance of electronic medical records; socio-technical attachments and IT change; comparing the predictive ability of PLS and covariance models; a note of caution on covariance-equivalent models in information systems; differential effects of omitting formative indicators; being innovative about service innovation; estimating demand for applications in the new mobile economy; continuance of professional social networking sites; role of online social networks in job search by unemployed individuals; knowledge contribution in online network of practice; the dual role of IS specificity in governing software as a service; relational antecedents of information visibility in value networks; summarization of corporate risk factor disclosure through topic modeling; a hybrid method for cross-domain sentiment classification using multiple sources; designing intelligent expert systems to cope with liars; IS offshore project risk, contracts and team structure; vendor and client project managers; technology desirability; the role of green IS in developing eco-effectiveness; the impact of IT-enabled manufacturing capabilities on plant profitability; inter-industry IT spillovers after the dot-com bust; effects of undesired online video advertising choice on user behavior and attitude; unfulfilled obligations in recommendation agent use; understanding the formation of trust in IT artifacts; user decisions among digital piracy and legal alternatives for film and music; a probabilistic generative model for latent business networks mining; the effects of information disclosure policy on the diffusion of security attacks; interactivity of social media and online consumer behavior; strategies for establishing service oriented design in organizations; alternative genres of is research; ICT standardization strategies and service innovation in health care; technical support and IT capacity demand; online social networks as a source and symbol of stress; the effects of social network usage on organizational identification; using the Kano model to identify attractive user-interface software components; developing a theory of multitasking behavior; an empirical investigation of information systems departments' configurations; exploring the adaptation of enterprise systems implementation methodology; out of the box and onto the stage; an organizing vision perspective on green IS development; the emergence of sustainability as the new dominant logic; social influence and defaults in peer-to-peer lending networks; multi-screen strategy for selling mobile content to customers; the emergent role of IT capabilities; reducing price uncertainty through demand side management; factors affecting perceived persuasiveness of a behavior change support system; surfacing schemas from firms' informational engagements; optimal design of consumer review systems; measuring product type with dynamics of online product review variance; different effects on retailers of online product reviews; measurement of multitasking with focus shift analysis; an exploration of the impact of information privacy invasion; measuring mobile users' concerns for information privacy; examining the rationality of location data disclosure through mobile devices; towards a design theory for software project risk management systems; identifying optimal IT portfolios to promote healthcare quality; designing a web-based application to support peer instruction for very large groups; principles for knowledge creation in collaborative design science research; estimating optimal recommendation set sizes for individual consumers; predicting participation in social media sites by analyzing user participation patterns; a hidden Markov model for conversion rate dynamics in online retail; flow experience and continuance intention toward online learning; design and evaluation of a didactical service blueprinting method for large scale lectures; knowledge sharing and maturation in circles of trust; a model-driven method for the systematic literature review of qualitative empirical research; procedurally transparent design science research; advancing task elicitation systems - an experimental evaluation of design principles; the knowing-doing gap in research methods and what we should do about it; efficient and flexible management of enterprise information systems; granularity metrics for IT services; from observed outcomes to measurable performance; the effect of third party investigation on pay-per-click advertising; identifying factors of e-government acceptance - a literature review; effects of cultural cognitive styles on users' evaluation of website complexity; a sustainability model of green IT initiatives; towards a typology of green IS strategies; digital access, political networks and the diffusion of democracy; the effect of free access on the diffusion of scholarly ideas; an empirical examination of cultural biases in interpersonal economic exchange; effect of business intelligence and IT infrastructure flexibility on organizational agility; news recommender systems with feedback; a multi-theoretical framework for social network-based recommendation; improving coverage of design in information systems education; perceived IT security risks of cloud computing: conceptualization and scale development; capability leapfrogging in the Japanese IT services industry; predatory coercion in social media and protection of children online - a critical discourse analysis approach; encouraging collaborative idea-building in enterprise-wide innovation challenges; analyzing the impact of social media strategies on viewer engagement; supporters in deed - studying online support provision from the perspective of social capital; membership overlap and inter-community collaboration; developing customer agility through information management; keyword search patterns in sponsored link advertisements; the hidden effects of opening bids in online auctions; increasing dynamic capabilities through virtualized grid-in-cloud solutions; design principles for heterogeneity decisions in enterprise architecture management; IT stereotyping and the CEO-CIO headlock; exploring the role of un-enacted projects in IT project portfolio management; the difference of determinants of mobile data services' adoption and continuance - a longitudinal study; a study of effectiveness and satisfaction level of cloud CRM users in Taiwan's enterprises; investigating intelligent agents in a 3D virtual world; an empirical study of a two-sided model of fraudulent exchange; perceptual and conceptual effects of incidental exposure to web ads; self-disclosure on online social networks; critical success factors of location-based services; affect and online privacy concerns; the impact of training and social norms on information security compliance; security management in cross-organizational settings; the effect of customers' emotion on service recovery strategy in IT service failures; the influence of IS affordances on work practices in health care; multi-level knowledge transfer in software development outsourcing projects; organizational performance with environmental knowledge intensity; process visibility - towards a conceptualization and research themes; towards an oil crisis early warning system based on absolute news volume; constructing workflow models from agent profiles; formulating effective coordination strategies in agile global software development teams; the transmission of control in information systems projects; the structuration of task-oriented communication in innovative virtual teams and social media in the entrainment of contention to innovation."
599,"We are developing indicators for the emergence of science and technology (S&T) topics. We are targeting various S&T information resources, including metadata (i.e., bibliographic information) and full text. We explore alternative text analysis approaches - principal components analysis (PCA) and topic modeling - to extract technical topic information. We analyze the topical content to pursue potential applications and innovation pathways. In this presentation we compare alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing (NLP) on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and tf-idf weighting. We compare PCA results to topic modeling results. Our key test set consists of 4104 Web of Science records on Dye-Sensitized Solar Cells (DSSCs). Results suggest good potential to enhance our technical intelligence payoffs from database searches on topics of interest. © 2012 IEEE.",2012-11-01,2-s2.0-84867934136,"2012 Proceedings of Portland International Center for Management of Engineering and Technology: Technology Management for Emerging Technologies, PICMET'12",Comparing methods to extract technical content for technological intelligence,"We are developing indicators for the emergence of science and technology (S&T) topics. We are targeting various S&T information resources, including metadata (i.e., bibliographic information) and full text. We explore alternative text analysis approaches - principal components analysis (PCA) and topic modeling - to extract technical topic information. We analyze the topical content to pursue potential applications and innovation pathways. In this presentation we compare alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing (NLP) on abstracts and titles, together with various keyword sets]. Our process includes combinations of stopword removal, fuzzy term matching, association rules, and tf-idf weighting. We compare PCA results to topic modeling results. Our key test set consists of 4104 Web of Science records on Dye-Sensitized Solar Cells (DSSCs). Results suggest good potential to enhance our technical intelligence payoffs from database searches on topics of interest. "
600,"Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. The advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. By focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. The results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches. © 2012 Elsevier B.V. All rights reserved.",2012-11-01,2-s2.0-84865521691,Decision Support Systems,Feature selection for sentiment analysis based on content and syntax models,"Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. The advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. By focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. The results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches. "
601,"We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. The dependency arises via superposition, subsampling and point transition on the underlying Poisson processes of these measures. The measures used include normalised generalised Gamma processes that demonstrate power law properties, unlike Dirichlet processes used previously in dynamic topic modeling. Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process. Experiments performed on news, blogs, academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models. Copyright 2012 by the author(s)/owner(s).",2012-10-10,2-s2.0-84867136246,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",Dependent hierarchical normalized random measures for dynamic topic modeling,"We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. The dependency arises via superposition, subsampling and point transition on the underlying Poisson processes of these measures. The measures used include normalised generalised Gamma processes that demonstrate power law properties, unlike Dirichlet processes used previously in dynamic topic modeling. Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process. Experiments performed on news, blogs, academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models. "
602,"Social network websites, such as Facebook, YouTube, Lastfm etc, have become a popular platform for users to connect with each other and share content or opinions. They provide rich information for us to study the influence of user's social circle in their decision process. In this paper, we are interested in examining the effectiveness of social network information to predict the user's ratings of items. We propose a novel hierarchical Bayesian model which jointly incorporates topic modeling and probabilistic matrix factorization of social networks. A major advantage of our model is to automatically infer useful latent topics and social information as well as their importance to collaborative filtering from the training data. Empirical experiments on two large-scale datasets show that our algorithm provides a more effective recommendation system than the state-of-the art approaches. Our results reveal interesting insight that the social circles have more influence on people's decisions about the usefulness of information (e.g., bookmarking preference on Delicious) than personal taste (e.g., music preference on Lastfm). We also examine and discuss solutions on potential information leak in many recommendation systems that utilize social information. Copyright 2012 by the author(s)/owner(s).",2012-10-10,2-s2.0-84867125398,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012",Collaborative topic regression with social matrix factorization for recommendation systems,"Social network websites, such as Facebook, YouTube, Lastfm etc, have become a popular platform for users to connect with each other and share content or opinions. They provide rich information for us to study the influence of user's social circle in their decision process. In this paper, we are interested in examining the effectiveness of social network information to predict the user's ratings of items. We propose a novel hierarchical Bayesian model which jointly incorporates topic modeling and probabilistic matrix factorization of social networks. A major advantage of our model is to automatically infer useful latent topics and social information as well as their importance to collaborative filtering from the training data. Empirical experiments on two large-scale datasets show that our algorithm provides a more effective recommendation system than the state-of-the art approaches. Our results reveal interesting insight that the social circles have more influence on people's decisions about the usefulness of information (e.g., bookmarking preference on Delicious) than personal taste (e.g., music preference on Lastfm). We also examine and discuss solutions on potential information leak in many recommendation systems that utilize social information. "
603,"This paper proposes a novel application of topic models to do entity relation detection (ERD). In order to make use of the latent semantics of text, we formulate the task of relation detection as a topic modeling problem. The motivation is to find underlying topics that are indicative of relations between named entities (NEs). Our approach considers pairs of NEs and features associated with them as mini documents, and aims to utilize the underlying topic distributions as indicators for the types of relations that may exist between the NE pair. Our system, ERD-MedLDA, adapts Maximum Entropy Discriminant Latent Dirichlet Allocation (MedLDA) with mixed membership for relation detection. By using supervision, ERD-MedLDA is able to learn topic distributions indicative of relation types. Further, ERD-MedLDA is a topic model that combines the benefits of both, maximum likelihood estimation (MLE) and maximum margin estimation (MME), and the mixed-membership formulation enables the system to incorporate heterogeneous features. We incorporate different features into the system and perform experiments on the ACE 2005 corpus. Our approach achieves better overall performance for precision, recall, and F-measure metrics as compared to baseline SVM-based and LDA-based models. We also find that our system shows better and consistent improvements with the addition of complex informative features as compared to baseline systems. © Copyright Cambridge University Press 2012.",2012-04-01,2-s2.0-84858966026,Natural Language Engineering,ERD-MedLDA: Entity relation detection using supervised topic models with maximum margin learning,"This paper proposes a novel application of topic models to do entity relation detection (ERD). In order to make use of the latent semantics of text, we formulate the task of relation detection as a topic modeling problem. The motivation is to find underlying topics that are indicative of relations between named entities (NEs). Our approach considers pairs of NEs and features associated with them as mini documents, and aims to utilize the underlying topic distributions as indicators for the types of relations that may exist between the NE pair. Our system, ERD-MedLDA, adapts Maximum Entropy Discriminant Latent Dirichlet Allocation (MedLDA) with mixed membership for relation detection. By using supervision, ERD-MedLDA is able to learn topic distributions indicative of relation types. Further, ERD-MedLDA is a topic model that combines the benefits of both, maximum likelihood estimation (MLE) and maximum margin estimation (MME), and the mixed-membership formulation enables the system to incorporate heterogeneous features. We incorporate different features into the system and perform experiments on the ACE 2005 corpus. Our approach achieves better overall performance for precision, recall, and F-measure metrics as compared to baseline SVM-based and LDA-based models. We also find that our system shows better and consistent improvements with the addition of complex informative features as compared to baseline systems. © "
604,"Researchers interests finding has been an active area of investigation for different recommendation tasks. Previous approaches for finding researchers interests exploit writing styles and links connectivity by considering time of documents, while semantics-based intrinsic structure of words is ignored. Consequently, a topic model named Author-Topic model is proposed, which exploits semantics-based intrinsic structure of words present between the authors of research papers. It ignores simultaneous modeling of time factor which results in exchangeability of topics problem, which is, important factor to deal with when finding dynamic research interests. For example, in many real world applications, like finding reviewers for papers and finding taggers in the social tagging systems one needs to consider different time periods. In this paper, we present time topic modeling approach named Temporal-Author-Topic (TAT) which can simultaneously model text, researchers and time of research papers to overcome the exchangeability of topic problem. The mixture distribution over topics is influenced by both co-occurrences of words and timestamps of the research papers. Consequently, topics occurrence and their related researchers change over time, while the meaning of particular topic almost remains unchanged. Proposed approach is used to discover topically related researchers for different time periods. We also show how their interests and relationships change over a time period. Empirical results on large research papers corpus show the effectiveness of our proposed approach and dominance over Author-Topic (AT) model, by handling the exchangeability of topics problem, which enables it to obtain similar meaning of a particular topic overtime. © 2011 Elsevier B.V. All rights reserved.",2012-02-01,2-s2.0-84155189116,Knowledge-Based Systems,Using time topic modeling for semantics-based dynamic research interest finding,"Researchers interests finding has been an active area of investigation for different recommendation tasks. Previous approaches for finding researchers interests exploit writing styles and links connectivity by considering time of documents, while semantics-based intrinsic structure of words is ignored. Consequently, a topic model named Author-Topic model is proposed, which exploits semantics-based intrinsic structure of words present between the authors of research papers. It ignores simultaneous modeling of time factor which results in exchangeability of topics problem, which is, important factor to deal with when finding dynamic research interests. For example, in many real world applications, like finding reviewers for papers and finding taggers in the social tagging systems one needs to consider different time periods. In this paper, we present time topic modeling approach named Temporal-Author-Topic (TAT) which can simultaneously model text, researchers and time of research papers to overcome the exchangeability of topic problem. The mixture distribution over topics is influenced by both co-occurrences of words and timestamps of the research papers. Consequently, topics occurrence and their related researchers change over time, while the meaning of particular topic almost remains unchanged. Proposed approach is used to discover topically related researchers for different time periods. We also show how their interests and relationships change over a time period. Empirical results on large research papers corpus show the effectiveness of our proposed approach and dominance over Author-Topic (AT) model, by handling the exchangeability of topics problem, which enables it to obtain similar meaning of a particular topic overtime. "
605,"Significant progress has been made in information retrieval covering text semantic indexing and multilingual analysis. However, developments in Arabic information retrieval did not follow the extraordinary growth of Arabic usage in the Web during the ten last years. In the tasks relating to semantic analysis, it is preferable to directly deal with texts in their original language. Studies on topic models, which provide a good way to automatically deal with semantic embedded in texts, are not complete enough to assess the effectiveness of the approach on Arabic texts. This paper investigates several text stemming methods for Arabic topic modeling. A new lemma-based stemmer is described and applied to newspaper articles. The Latent Dirichlet Allocation model is used to extract latent topics from three Arabic real-world corpora. For supervised classification in the topics space, experiments show an improvement when comparing to classification in the full words space or with root-based stemming approach. In addition, topic modeling with lemma-based stemming allows us to discover interesting subjects in the press articles published during the 2007-2009 period. © 2011 Springer Science+Business Media, LLC.",2012-02-01,2-s2.0-84856360891,Information Retrieval,Arabic texts analysis for topic modeling evaluation,"Significant progress has been made in information retrieval covering text semantic indexing and multilingual analysis. However, developments in Arabic information retrieval did not follow the extraordinary growth of Arabic usage in the Web during the ten last years. In the tasks relating to semantic analysis, it is preferable to directly deal with texts in their original language. Studies on topic models, which provide a good way to automatically deal with semantic embedded in texts, are not complete enough to assess the effectiveness of the approach on Arabic texts. This paper investigates several text stemming methods for Arabic topic modeling. A new lemma-based stemmer is described and applied to newspaper articles. The Latent Dirichlet Allocation model is used to extract latent topics from three Arabic real-world corpora. For supervised classification in the topics space, experiments show an improvement when comparing to classification in the full words space or with root-based stemming approach. In addition, topic modeling with lemma-based stemming allows us to discover interesting subjects in the press articles published during the 2007-2009 period. "
607,"This paper proposes an improved approach to extractive summarization of spoken multi-party interaction, in which integrated random walk is performed on a graph constructed on topical/ lexical relations. Each utterance is represented as a node of the graph, and the edges' weights are computed from the topical similarity between the utterances, evaluated using probabilistic latent semantic analysis (PLSA), and from word overlap. We model intra-speaker topics by partially sharing the topics from the same speaker in the graph. In this paper, we perform experiments on automatically and manually generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances.",2012-01-01,2-s2.0-84926195075,"NAACL HLT 2012 - 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",Intra-speaker topic modeling for improved multi-party meeting summarization with integrated randomwalk,"This paper proposes an improved approach to extractive summarization of spoken multi-party interaction, in which integrated random walk is performed on a graph constructed on topical/ lexical relations. Each utterance is represented as a node of the graph, and the edges' weights are computed from the topical similarity between the utterances, evaluated using probabilistic latent semantic analysis (PLSA), and from word overlap. We model intra-speaker topics by partially sharing the topics from the same speaker in the graph. In this paper, we perform experiments on automatically and manually generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances."
608,"This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.",2012-01-01,2-s2.0-84923303927,"NAACL HLT 2012 - 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",How text segmentation algorithms gain from topic models,"This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset."
609,"We introduce the social study of bullying to the NLP community. Bullying, in both physical and cyber worlds (the latter known as cyberbullying), has been recognized as a serious national health issue among adolescents. However, previous social studies of bullying are handicapped by data scarcity, while the few computational studies narrowly restrict themselves to cyberbullying which accounts for only a small fraction of all bullying episodes. Our main contribution is to present evidence that social media, with appropriate natural language processing techniques, can be a valuable and abundant data source for the study of bullying in both worlds. We identify several key problems in using such data sources and formulate them as NLP tasks, including text classification, role labeling, sentiment analysis, and topic modeling. Since this is an introductory paper, we present baseline results on these tasks using off-the-shelf NLP solutions, and encourage the NLP community to contribute better models in the future.",2012-01-01,2-s2.0-84875820335,"NAACL HLT 2012 - 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",Learning from bullying traces in social media,"We introduce the social study of bullying to the NLP community. Bullying, in both physical and cyber worlds (the latter known as cyberbullying), has been recognized as a serious national health issue among adolescents. However, previous social studies of bullying are handicapped by data scarcity, while the few computational studies narrowly restrict themselves to cyberbullying which accounts for only a small fraction of all bullying episodes. Our main contribution is to present evidence that social media, with appropriate natural language processing techniques, can be a valuable and abundant data source for the study of bullying in both worlds. We identify several key problems in using such data sources and formulate them as NLP tasks, including text classification, role labeling, sentiment analysis, and topic modeling. Since this is an introductory paper, we present baseline results on these tasks using off-the-shelf NLP solutions, and encourage the NLP community to contribute better models in the future."
610,"This paper proposes a new method of constructing arbitrary class-based related word dictionaries on interactive topic models; we assume that each class is described by a topic. We propose a new semi-supervised method that uses the simplest topic model yielded by the standard EM algorithm; model calculation is very rapid. Furthermore our approach allows a dictionary to be modified interactively and the final dictionary has a hierarchical structure. This paper makes three contributions. First, it proposes a word-based semi-supervised topic model. Second, we apply the semi-supervised topic model to interactive learning; this approach is called the Interactive Topic Model. Third, we propose a score function; it extracts the related words that occupy the middle layer of the hierarchical structure. Experiments show that our method can appropriately retrieve the words belonging to an arbitrary class.",2012-01-01,2-s2.0-84883335566,"Proceedings of the 8th International Conference on Language Resources and Evaluation, LREC 2012",Constructing a class-based lexical dictionary using interactive topic models,"This paper proposes a new method of constructing arbitrary class-based related word dictionaries on interactive topic models; we assume that each class is described by a topic. We propose a new semi-supervised method that uses the simplest topic model yielded by the standard EM algorithm; model calculation is very rapid. Furthermore our approach allows a dictionary to be modified interactively and the final dictionary has a hierarchical structure. This paper makes three contributions. First, it proposes a word-based semi-supervised topic model. Second, we apply the semi-supervised topic model to interactive learning; this approach is called the Interactive Topic Model. Third, we propose a score function; it extracts the related words that occupy the middle layer of the hierarchical structure. Experiments show that our method can appropriately retrieve the words belonging to an arbitrary class."
611,"With a few exceptions, extensions to latent Dirichlet allocation (LDA) have focused on the distribution over topics for each document. Much less attention has been given to the underlying structure of the topics themselves. As a result, most topic models generate topics independently from a single underlying distribution and require millions of parameters, in the form of multinomial distributions over the vocabulary. In this paper, we introduce the Shared Components Topic Model (SCTM), in which each topic is a normalized product of a smaller number of underlying component distributions. Our model learns these component distributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters.",2012-01-01,2-s2.0-84898000701,"NAACL HLT 2012 - 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",Shared components topic models,"With a few exceptions, extensions to latent Dirichlet allocation (LDA) have focused on the distribution over topics for each document. Much less attention has been given to the underlying structure of the topics themselves. As a result, most topic models generate topics independently from a single underlying distribution and require millions of parameters, in the form of multinomial distributions over the vocabulary. In this paper, we introduce the Shared Components Topic Model (SCTM), in which each topic is a normalized product of a smaller number of underlying component distributions. Our model learns these component distributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters."
612,"This paper explores correspondence and mixture topic modeling of documents tagged from two different perspectives. There has been ongoing work in topic modeling of documents with tags (tag-topic models) where words and tags typically reflect a single perspective, namely document content. However, words in documents can also be tagged from different perspectives, for example, syntactic perspective as in part-of-speech tagging or an opinion perspective as in sentiment tagging. The models proposed in this paper are novel in: (i) the consideration of two different tag perspectives - a document level tag perspective that is relevant to the document as a whole and a word level tag perspective pertaining to each word in the document; (ii) the attribution of latent topics with word level tags and labeling latent topics with images in case of multimedia documents; and (iii) discovering the possible correspondence of the words to document level tags. The proposed correspondence tag-topic model shows better predictive power i.e. higher likelihood on heldout test data than all existing tag topic models and even a supervised topic model. To evaluate the models in practical scenarios, quantitative measures between the outputs of the proposed models and the ground truth domain knowledge have been explored. Manually assigned (gold standard) document category labels in Wikipedia pages are used to validate model-generated tag suggestions using a measure of pairwise concept similarity within an ontological hierarchy like WordNet. Using a news corpus, automatic relationship discovery between person names was performed and compared to a robust baseline. © 2011 ACM.",2011-12-13,2-s2.0-83055161660,"International Conference on Information and Knowledge Management, Proceedings",Simultaneous joint and conditional modeling of documents tagged from two perspectives,"This paper explores correspondence and mixture topic modeling of documents tagged from two different perspectives. There has been ongoing work in topic modeling of documents with tags (tag-topic models) where words and tags typically reflect a single perspective, namely document content. However, words in documents can also be tagged from different perspectives, for example, syntactic perspective as in part-of-speech tagging or an opinion perspective as in sentiment tagging. The models proposed in this paper are novel in: (i) the consideration of two different tag perspectives - a document level tag perspective that is relevant to the document as a whole and a word level tag perspective pertaining to each word in the document; (ii) the attribution of latent topics with word level tags and labeling latent topics with images in case of multimedia documents; and (iii) discovering the possible correspondence of the words to document level tags. The proposed correspondence tag-topic model shows better predictive power i.e. higher likelihood on heldout test data than all existing tag topic models and even a supervised topic model. To evaluate the models in practical scenarios, quantitative measures between the outputs of the proposed models and the ground truth domain knowledge have been explored. Manually assigned (gold standard) document category labels in Wikipedia pages are used to validate model-generated tag suggestions using a measure of pairwise concept similarity within an ontological hierarchy like WordNet. Using a news corpus, automatic relationship discovery between person names was performed and compared to a robust baseline. "
613,"Named entities are observed in a large portion of web search queries (named entity queries), where each entity can be associated with many different query terms that refer to various aspects of this entity. Organizing these query terms into topics helps understand major search intents about entities and the discovered topics are useful for applications such as query suggestion. Furthermore, we notice that named entities can often be organized into categories and those from the same category share many generic topics. Therefore, working on a category of named entities instead of individual ones helps avoid the problems caused by the sparsity and noise in the data. In this paper, Named Entity Topic Model (NETM) is proposed to discover generic topics for a category of named entities, where the quality of the generic topics is improved through the model design and the parameter initialization. Experiments based on query log data show that NETM discovers high-quality topics and outperforms the state-of-the-art techniques by 12.8% based on F1 measure. © 2011 ACM.",2011-12-13,2-s2.0-83055165976,"International Conference on Information and Knowledge Management, Proceedings",Topic modeling for named entity queries,"Named entities are observed in a large portion of web search queries (named entity queries), where each entity can be associated with many different query terms that refer to various aspects of this entity. Organizing these query terms into topics helps understand major search intents about entities and the discovered topics are useful for applications such as query suggestion. Furthermore, we notice that named entities can often be organized into categories and those from the same category share many generic topics. Therefore, working on a category of named entities instead of individual ones helps avoid the problems caused by the sparsity and noise in the data. In this paper, Named Entity Topic Model (NETM) is proposed to discover generic topics for a category of named entities, where the quality of the generic topics is improved through the model design and the parameter initialization. Experiments based on query log data show that NETM discovers high-quality topics and outperforms the state-of-the-art techniques by 12.8% based on F1 measure. "
614,"Name ambiguity arises from the polysemy of names and causes uncertainty about the true identity of entities referenced in unstructured text. This is a major problem in areas like information retrieval or knowledge management, for example when searching for a specific entity or updating an existing knowledge base. We approach this problem of named entity disambiguation (NED) using thematic information derived from Latent Dirichlet Allocation (LDA) to compare the entity mention's context with candidate entities in Wikipedia represented by their respective articles. We evaluate various distances over topic distributions in a supervised classification setting to find the best suited candidate entity, which is either covered in Wikipedia or unknown. We compare our approach to a state of the art method and show that it achieves significantly better results in predictive performance, regarding both entities covered in Wikipedia as well as uncovered entities. We show that our approach is in general language independent as we obtain equally good results for named entity disambiguation using the English, the German and the French Wikipedia. © 2011 ACM.",2011-12-13,2-s2.0-83055161746,"International Conference on Information and Knowledge Management, Proceedings",From names to entities using thematic context distance,"Name ambiguity arises from the polysemy of names and causes uncertainty about the true identity of entities referenced in unstructured text. This is a major problem in areas like information retrieval or knowledge management, for example when searching for a specific entity or updating an existing knowledge base. We approach this problem of named entity disambiguation (NED) using thematic information derived from Latent Dirichlet Allocation (LDA) to compare the entity mention's context with candidate entities in Wikipedia represented by their respective articles. We evaluate various distances over topic distributions in a supervised classification setting to find the best suited candidate entity, which is either covered in Wikipedia or unknown. We compare our approach to a state of the art method and show that it achieves significantly better results in predictive performance, regarding both entities covered in Wikipedia as well as uncovered entities. We show that our approach is in general language independent as we obtain equally good results for named entity disambiguation using the English, the German and the French Wikipedia. "
615,"With an increasingly amount of information in web forums, quick comprehension of threads in web forums has become a challenging research problem. To handle this issue, this paper investigates the task of Web Forum Thread Summarization (WFTS), aiming to give a brief statement of each thread that involving multiple dynamic topics. When applied to the task of WFTS, traditional summarization methods are cramped by topic dependencies, topic drifting and text sparseness. Consequently, we explore an unsupervised topic propagation model in this paper, the Post Propagation Model (PPM), to burst through these problems by simultaneously modeling the semantics and the reply relationship existing in each thread. Each post in PPM is considered as a mixture of topics, and a product of Dirichlet distributions in previous posts is employed to model each topic dependencies during the asynchronous discussion. Based on this model, the task of WFTS is accomplished by extracting most significant sentences in a thread. The experimental results on two different forum data sets show that WFTS based on the PPM outperforms several state-of-the-art summarization methods in terms of ROUGE metrics. © 2011 ACM.",2011-12-13,2-s2.0-83055179512,"International Conference on Information and Knowledge Management, Proceedings",Summarizing web forum threads based on a latent topic propagation process,"With an increasingly amount of information in web forums, quick comprehension of threads in web forums has become a challenging research problem. To handle this issue, this paper investigates the task of Web Forum Thread Summarization (WFTS), aiming to give a brief statement of each thread that involving multiple dynamic topics. When applied to the task of WFTS, traditional summarization methods are cramped by topic dependencies, topic drifting and text sparseness. Consequently, we explore an unsupervised topic propagation model in this paper, the Post Propagation Model (PPM), to burst through these problems by simultaneously modeling the semantics and the reply relationship existing in each thread. Each post in PPM is considered as a mixture of topics, and a product of Dirichlet distributions in previous posts is employed to model each topic dependencies during the asynchronous discussion. Based on this model, the task of WFTS is accomplished by extracting most significant sentences in a thread. The experimental results on two different forum data sets show that WFTS based on the PPM outperforms several state-of-the-art summarization methods in terms of ROUGE metrics. "
616,"We propose a hierarchical nonparametric topic model, based on the hierarchical Dirichlet process (HDP), that accounts for dependencies among the data. The HDP mixture models are useful for discovering an unknown semantic structure (i.e., topics) from a set of unstructured data such as a corpus of documents. For simplicity, HDP makes an exchangeability assumption that any permutation of the data points would result in the same joint probability of the data being generated. This exchangeability assumption poses a problem for some domains where there are clear and strong dependencies among the data. A model that allows for non-exchangeability of data can capture these dependencies and assign higher probabilities to clusters that account for data dependencies, for example, inferring topics that reflect the temporal patterns of the data. Our model incorporates the distance dependent Chinese restaurant process (ddCRP), which clusters data with an inherent bias toward clusters of data points that are near to one another, into a hierarchical construction analogous to the HDP, and we call this new prior the distance dependent Chinese restaurant franchise (ddCRF). When tested with temporal datasets, the ddCRF mixture model shows clear improvements in data fit compared to the HDP in terms of heldout likelihood and complexity. The resulting set of topics shows the sequential emergence and disappearance patterns of topics. © 2011 ACM.",2011-12-13,2-s2.0-83055161763,"International Conference on Information and Knowledge Management, Proceedings",Accounting for data dependencies within a hierarchical dirichlet process mixture model,"We propose a hierarchical nonparametric topic model, based on the hierarchical Dirichlet process (HDP), that accounts for dependencies among the data. The HDP mixture models are useful for discovering an unknown semantic structure (i.e., topics) from a set of unstructured data such as a corpus of documents. For simplicity, HDP makes an exchangeability assumption that any permutation of the data points would result in the same joint probability of the data being generated. This exchangeability assumption poses a problem for some domains where there are clear and strong dependencies among the data. A model that allows for non-exchangeability of data can capture these dependencies and assign higher probabilities to clusters that account for data dependencies, for example, inferring topics that reflect the temporal patterns of the data. Our model incorporates the distance dependent Chinese restaurant process (ddCRP), which clusters data with an inherent bias toward clusters of data points that are near to one another, into a hierarchical construction analogous to the HDP, and we call this new prior the distance dependent Chinese restaurant franchise (ddCRF). When tested with temporal datasets, the ddCRF mixture model shows clear improvements in data fit compared to the HDP in terms of heldout likelihood and complexity. The resulting set of topics shows the sequential emergence and disappearance patterns of topics. "
617,"Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. In this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. © 2011 Association for Computational Linguistics.",2011-12-01,2-s2.0-84859058588,ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,Interactive topic modeling,"Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. In this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. "
618,"In this paper we present findings from a project that used topic modeling and associated techniques to chart the emergence and growth of research topics in engineering education research over 9 years, from 2000-2008. As a field engineering education research has undergone significant changes over the past decade. There has been an increase in the number of scholars and practitioners involved in the field, particularly those that are applying rigorous research principles to advance understanding of engineering education. In such a circumstance, it is important to understand the topics, approaches, and ideas that have caught the imagination of people in the community. Since this nature of work has not been done in relation to engineering education research, a significant part of the effort described here is innovative and exploratory in nature where different techniques were tested with the goal to collect a diversity of topics that are of interest to the community. We identify major categories of topics and primary topics of interest to the community. We also identify a lack of engagement with theoretical and analytical ideas as an area of concern. © 2011 IEEE.",2011-12-01,2-s2.0-84863260498,"Proceedings - Frontiers in Education Conference, FIE",Utilizing topic modeling techniques to identify the emergence and growth of research topics in engineering education,"In this paper we present findings from a project that used topic modeling and associated techniques to chart the emergence and growth of research topics in engineering education research over 9 years, from 2000-2008. As a field engineering education research has undergone significant changes over the past decade. There has been an increase in the number of scholars and practitioners involved in the field, particularly those that are applying rigorous research principles to advance understanding of engineering education. In such a circumstance, it is important to understand the topics, approaches, and ideas that have caught the imagination of people in the community. Since this nature of work has not been done in relation to engineering education research, a significant part of the effort described here is innovative and exploratory in nature where different techniques were tested with the goal to collect a diversity of topics that are of interest to the community. We identify major categories of topics and primary topics of interest to the community. We also identify a lack of engagement with theoretical and analytical ideas as an area of concern. "
619,The proceedings contain 440 papers. The topics discussed include: innovation for a crowded curriculum: learning modules for tomorrow's energy engineers; assessing cognitive skills in asynchronous online discussion: a case study of student centered e-learning environment in Indonesia; improving energy literacy among middle school youth with project-based learning pedagogies; enhancing communications among courses linked with prerequisites; a comparison of mentoring functions in capstone courses across engineering disciplines; human side of engineering: dealing with complex and ethical challenges; developing case modules for teaching software engineering and computer science concepts; a grounded theory approach to effects of virtual facilitation on team communication and the development of professional skills; a pass/fail option for first-semester engineering students: a critical EVA; and utilizing topic modeling techniques to identify the emergence and growth of research topics in engineering education.,2011-12-01,2-s2.0-84858274566,"Proceedings - Frontiers in Education Conference, FIE","41st Annual Frontiers in Education Conference: Celebrating 41 Years of Monumental Innovations from Around the World, FIE 2011",The proceedings contain 440 papers. The topics discussed include: innovation for a crowded curriculum: learning modules for tomorrow's energy engineers; assessing cognitive skills in asynchronous online discussion: a case study of student centered e-learning environment in Indonesia; improving energy literacy among middle school youth with project-based learning pedagogies; enhancing communications among courses linked with prerequisites; a comparison of mentoring functions in capstone courses across engineering disciplines; human side of engineering: dealing with complex and ethical challenges; developing case modules for teaching software engineering and computer science concepts; a grounded theory approach to effects of virtual facilitation on team communication and the development of professional skills; a pass/fail option for first-semester engineering students: a critical EVA; and utilizing topic modeling techniques to identify the emergence and growth of research topics in engineering education.
620,"Building topic models in federated digital collections presents numerous challenges due to metadata inconsistencies. The quality of topical metadata is difficult to ascertain and is interspersed with often irrelevant administrative metadata. In this study, we propose a way to improve topic modeling in large collections by identifying documents that convey only weak topical information. These documents are ignored when training topic models. Their topical associations are instead inferred model training. A method is outlined for identifying weakly topical documents by defining runs of similar documents in a collection. In preliminary evaluation using a corpus from the Institute of Museum and Library Services Digital Collections and Content aggregation, results show an increase in coherence among words in topics. In showing this, we demonstrate that it may be beneficial to induce topic models using less, higher-quality data.",2011-12-01,2-s2.0-84861440392,Proceedings of the ASIST Annual Meeting,Building topic models in a federated digital library through selective document exclusion,"Building topic models in federated digital collections presents numerous challenges due to metadata inconsistencies. The quality of topical metadata is difficult to ascertain and is interspersed with often irrelevant administrative metadata. In this study, we propose a way to improve topic modeling in large collections by identifying documents that convey only weak topical information. These documents are ignored when training topic models. Their topical associations are instead inferred model training. A method is outlined for identifying weakly topical documents by defining runs of similar documents in a collection. In preliminary evaluation using a corpus from the Institute of Museum and Library Services Digital Collections and Content aggregation, results show an increase in coherence among words in topics. In showing this, we demonstrate that it may be beneficial to induce topic models using less, higher-quality data."
621,"In the Social Commerce customers evolve to an impodant information source for companies. The customers use communication platforms of the Web 2.0, for example Twitter, in order to express their opinions about products or discuss their experiences with them. These opinions can be veIy impodant for the development of products or the product range of a company. Our approach enables a company viewing opinions about its products whch are published using the microblogging service Twitter. A first step in our research progress is detecting topics in a specific context. In a futher step the entries correspondmg to these topics has to be analyzed for opinions. For topic detection we use topic modeling with the Latent Dirichlet Allocation. In our paper we found event-based topics in the context of Sony's 3D TV sets. In future work we are able to implement Opinion Mining algorithms to determine sentiments in the enhies correspondmg to the detected topics.",2011-12-01,2-s2.0-84870226006,"17th Americas Conference on Information Systems 2011, AMCIS 2011",Analyzing customer sentiments in microblogs - a topicmodel- based approach for Twitter datasets,"In the Social Commerce customers evolve to an impodant information source for companies. The customers use communication platforms of the Web 2.0, for example Twitter, in order to express their opinions about products or discuss their experiences with them. These opinions can be veIy impodant for the development of products or the product range of a company. Our approach enables a company viewing opinions about its products whch are published using the microblogging service Twitter. A first step in our research progress is detecting topics in a specific context. In a futher step the entries correspondmg to these topics has to be analyzed for opinions. For topic detection we use topic modeling with the Latent Dirichlet Allocation. In our paper we found event-based topics in the context of Sony's 3D TV sets. In future work we are able to implement Opinion Mining algorithms to determine sentiments in the enhies correspondmg to the detected topics."
622,"A new nonparametric Bayesian model is developed to integrate dictionary learning and topic model into a unified framework. The model is employed to analyze partially annotated images, with the dictionary learning performed directly on image patches. Efficient inference is performed with a Gibbs-slice sampler, and encouraging results are reported on widely used datasets. Copyright 2011 by the author(s)/owner(s).",2011-10-07,2-s2.0-80053450173,"Proceedings of the 28th International Conference on Machine Learning, ICML 2011",On the integration of topic modeling and dictionary learning,"A new nonparametric Bayesian model is developed to integrate dictionary learning and topic model into a unified framework. The model is employed to analyze partially annotated images, with the dictionary learning performed directly on image patches. Efficient inference is performed with a Gibbs-slice sampler, and encouraging results are reported on widely used datasets. "
623,"A new hierarchical tree-based topic model is developed, based on nonparametric Bayesian techniques. The model has two unique attributes: (i) a child node in the tree may have more than one parent, with the goal of eliminating redundant sub-topics deep in the tree; and (ii) parsimonious sub-topics are manifested, by removing redundant usage of words at multiple scales. The depth and width of the tree are unbounded within the prior, with a retrospective sampler employed to adaptively infer the appropriate tree size based upon the corpus under study. Excellent quantitative results are manifested on five standard data sets, and the inferred tree structure is also found to be highly interpretable. Copyright 2011 by the author(s)/owner(s).",2011-10-07,2-s2.0-80053435307,"Proceedings of the 28th International Conference on Machine Learning, ICML 2011",Topic modeling with nonparametric Markov tree,"A new hierarchical tree-based topic model is developed, based on nonparametric Bayesian techniques. The model has two unique attributes: (i) a child node in the tree may have more than one parent, with the goal of eliminating redundant sub-topics deep in the tree; and (ii) parsimonious sub-topics are manifested, by removing redundant usage of words at multiple scales. The depth and width of the tree are unbounded within the prior, with a retrospective sampler employed to adaptively infer the appropriate tree size based upon the corpus under study. Excellent quantitative results are manifested on five standard data sets, and the inferred tree structure is also found to be highly interpretable. "
624,"Generative models of text typically associate a multinomial with every class label or topic. Even in simple models this requires the estimation of thousands of parameters; in multifaceted latent variable models, standard approaches require additional latent ""switching"" variables for every token, complicating inference. In this paper, we propose an alternative generative model for text. The central idea is that each class label or latent topic is endowed with a model of the deviation in log-frequency from a constant background distribution. This approach has two key advantages: we can enforce sparsity to prevent overfitting, and we can combine generative facets through simple addition in log space, avoiding the need for latent switching variables. We demonstrate the applicability of this idea to a range of scenarios: classification, topic modeling, and more complex multifaceted generative models. Copyright 2011 by the author(s)/owner(s).",2011-10-07,2-s2.0-80053452684,"Proceedings of the 28th International Conference on Machine Learning, ICML 2011",Sparse additive generative models of text,"Generative models of text typically associate a multinomial with every class label or topic. Even in simple models this requires the estimation of thousands of parameters; in multifaceted latent variable models, standard approaches require additional latent ""switching"" variables for every token, complicating inference. In this paper, we propose an alternative generative model for text. The central idea is that each class label or latent topic is endowed with a model of the deviation in log-frequency from a constant background distribution. This approach has two key advantages: we can enforce sparsity to prevent overfitting, and we can combine generative facets through simple addition in log space, avoiding the need for latent switching variables. We demonstrate the applicability of this idea to a range of scenarios: classification, topic modeling, and more complex multifaceted generative models. "
625,"We propose a method which uses high-level learner feedback to recommend learning materials that match the knowledge level of a specific learner. Machine learning and topic inference techniques will be applied to documents that were rated by the learner to infer information on the learner's conceptual development. The inferred topics will be linked to a domain ontology, allowing us to offer the learner knowledge-rich feedback regarding his level of understanding. In addition, appropriate learning materials can be recommended on the basis of the learner's computational model. The proposed method is especially useful in lifelong learning contexts, in which tutor support is often not available.",2011-09-26,2-s2.0-80053036625,CSEDU 2011 - Proceedings of the 3rd International Conference on Computer Supported Education,Hidden patterns in learner feedback: Generalizing from noisy self-assessment during self-directed learning,"We propose a method which uses high-level learner feedback to recommend learning materials that match the knowledge level of a specific learner. Machine learning and topic inference techniques will be applied to documents that were rated by the learner to infer information on the learner's conceptual development. The inferred topics will be linked to a domain ontology, allowing us to offer the learner knowledge-rich feedback regarding his level of understanding. In addition, appropriate learning materials can be recommended on the basis of the learner's computational model. The proposed method is especially useful in lifelong learning contexts, in which tutor support is often not available."
626,"With the advent of the Web and various specialized digital libraries, the automatic extraction of useful information from text has become an increasingly important research in Data mining. In this paper we present a new MH based algorithm that extracts both the topics expressed in large text document collections and also models how the authors of documents use those topics. The methodology is illustrated using a sample of 1740 documents and 2037 authors of NIPS conference papers. A novel feature of our model is the inclusion of MH sampling for author topic models, in which authors are modeled as probability distributions over topics. The author-topic models can be used to support a variety of interactive and exploratory queries on the dataset. Algorithm proposed in this paper is the implementation of enhanced author topic modeling in text collection for extraction of topics from documents which will be useful for efficient search and retrieval. This paper presents an unsupervised learning technique for extracting information from the real world large text collections. This involves clustering which is used for extracting a representation from a collection of documents. Each cluster is associated with a topic and a single document is associated in only one cluster. Traditional Author Topic Model encounters problem in case of multi topic documents. Experimental results using proposed algorithm achieved the same classification accuracy with reduced time (50%) to extract the topics. © 2011 IEEE.",2011-09-05,2-s2.0-80052213571,"International Conference on Recent Trends in Information Technology, ICRTIT 2011",A author topic model based unsupervised algorithm for learning topics from large text collections,"With the advent of the Web and various specialized digital libraries, the automatic extraction of useful information from text has become an increasingly important research in Data mining. In this paper we present a new MH based algorithm that extracts both the topics expressed in large text document collections and also models how the authors of documents use those topics. The methodology is illustrated using a sample of 1740 documents and 2037 authors of NIPS conference papers. A novel feature of our model is the inclusion of MH sampling for author topic models, in which authors are modeled as probability distributions over topics. The author-topic models can be used to support a variety of interactive and exploratory queries on the dataset. Algorithm proposed in this paper is the implementation of enhanced author topic modeling in text collection for extraction of topics from documents which will be useful for efficient search and retrieval. This paper presents an unsupervised learning technique for extracting information from the real world large text collections. This involves clustering which is used for extracting a representation from a collection of documents. Each cluster is associated with a topic and a single document is associated in only one cluster. Traditional Author Topic Model encounters problem in case of multi topic documents. Experimental results using proposed algorithm achieved the same classification accuracy with reduced time (50%) to extract the topics. "
627,"Using a topic modeling algorithm to find relevant materials in a large corpus of textual items is not new; however, to date there has been little investigation into its usefulness to end-users. This article describes two methods we used to research this issue. In both methods, we used an instance of HathiTrust containing a snapshot of art, architecture and art history records from early 2010, that was populated with navigable terms generated using the topic modeling algorithm. In the first method, we created an unmoderated environment in which people navigated this instance on their own without supervision. In the second method, we talked to expert users as they navigated this same HathiTrust instance. Our unmoderated testing environment resulted in some conflicting results (use of topic facets was high, but satisfaction rating was somewhat low), while our one-on-one sessions with expert users give us reason to believe that topics and other subject terms (LCSH) are best used in conjunction with each other. This is a possibility we are interested in researching further. © 2011 Kat Hagedorn, Michael Kargela, Youn Noh, and David Newman.",2011-09-01,2-s2.0-84856507097,D-Lib Magazine,A new way to find: Testing the use of clustering topics in digital libraries,"Using a topic modeling algorithm to find relevant materials in a large corpus of textual items is not new; however, to date there has been little investigation into its usefulness to end-users. This article describes two methods we used to research this issue. In both methods, we used an instance of HathiTrust containing a snapshot of art, architecture and art history records from early 2010, that was populated with navigable terms generated using the topic modeling algorithm. In the first method, we created an unmoderated environment in which people navigated this instance on their own without supervision. In the second method, we talked to expert users as they navigated this same HathiTrust instance. Our unmoderated testing environment resulted in some conflicting results (use of topic facets was high, but satisfaction rating was somewhat low), while our one-on-one sessions with expert users give us reason to believe that topics and other subject terms (LCSH) are best used in conjunction with each other. This is a possibility we are interested in researching further. "
628,"In this paper, the task of text segmentation is approached from a topic modeling perspective. We investigate the use of two unsupervised topic models, latent Dirichlet allocation (LDA) and multinomial mixture (MM), to segment a text into semantically coherent parts. The proposed topic model based approaches consistently outperform a standard baseline method on several datasets. A major benefit of the proposed LDA based approach is that along with the segment boundaries, it outputs the topic distribution associated with each segment. This information is of potential use in applications such as segment retrieval and discourse analysis. However, the proposed approaches, especially the LDA based method, have high computational requirements. Based on an analysis of the dynamic programming (DP) algorithm typically used for segmentation, we suggest a modification to DP that dramatically speeds up the process with no loss in performance. The proposed modification to the DP algorithm is not specific to the topic models only; it is applicable to all the algorithms that use DP for the task of text segmentation. © 2010 Elsevier Ltd. All rights reserved.",2011-07-01,2-s2.0-79957621525,Information Processing and Management,Text segmentation: A topic modeling perspective,"In this paper, the task of text segmentation is approached from a topic modeling perspective. We investigate the use of two unsupervised topic models, latent Dirichlet allocation (LDA) and multinomial mixture (MM), to segment a text into semantically coherent parts. The proposed topic model based approaches consistently outperform a standard baseline method on several datasets. A major benefit of the proposed LDA based approach is that along with the segment boundaries, it outputs the topic distribution associated with each segment. This information is of potential use in applications such as segment retrieval and discourse analysis. However, the proposed approaches, especially the LDA based method, have high computational requirements. Based on an analysis of the dynamic programming (DP) algorithm typically used for segmentation, we suggest a modification to DP that dramatically speeds up the process with no loss in performance. The proposed modification to the DP algorithm is not specific to the topic models only; it is applicable to all the algorithms that use DP for the task of text segmentation. "
629,"Context-based communication services analyze user data and offer new and novel services that enhance end user unified communication experience. These services rely on data analysis and machine learning techniques to predict user behavior. In this paper we look at topic modeling as an unsupervised learning tool to categorize user communication data for retrieval. However, modeling topics based on user communication data, such as emails, meetings, invites, etc, poses several interesting challenges. One challenge is that user communication, even for a single topic, varies with the current context of the participating users. Other challenges include low lexical content and high contextual data in communication corpus. Hence, relying primarily on lexical analysis could result in inferior topic models. In this paper, we look at this problem of modeling topics for documents based on user communication. First, we use Latent Dirichlet Allocation (LDA) for extracting topics. LDA models documents as a mixture of latent topics where each topic consists of a probabilistic distribution over words. Then we use a technique that overlays a user-relational model over the lexical topic model generated by LDA. In this paper, we present our work and discuss our results. © 2011 IEEE.",2011-03-17,2-s2.0-79952556690,"2011 3rd International Conference on Communication Systems and Networks, COMSNETS 2011",Augmenting topic models with user relations in context based communication services,"Context-based communication services analyze user data and offer new and novel services that enhance end user unified communication experience. These services rely on data analysis and machine learning techniques to predict user behavior. In this paper we look at topic modeling as an unsupervised learning tool to categorize user communication data for retrieval. However, modeling topics based on user communication data, such as emails, meetings, invites, etc, poses several interesting challenges. One challenge is that user communication, even for a single topic, varies with the current context of the participating users. Other challenges include low lexical content and high contextual data in communication corpus. Hence, relying primarily on lexical analysis could result in inferior topic models. In this paper, we look at this problem of modeling topics for documents based on user communication. First, we use Latent Dirichlet Allocation (LDA) for extracting topics. LDA models documents as a mixture of latent topics where each topic consists of a probabilistic distribution over words. Then we use a technique that overlays a user-relational model over the lexical topic model generated by LDA. In this paper, we present our work and discuss our results. "
630,"Scientific collaboration and endorsement are well-established research topics which utilize three kinds of methods: survey/questionnaire, bibliometrics, and complex network analysis. This paper combines topic modeling and path-finding algorithms to determine whether productive authors tend to collaborate with or cite researchers with the same or different interests, and whether highly cited authors tend to collaborate with or cite each other. Taking information retrieval as a test field, the results show that productive authors tend to directly coauthor with and closely cite colleagues sharing the same research interests; they do not generally collaborate directly with colleagues having different research topics, but instead directly or indirectly cite them; and highly cited authors do not generally coauthor with each other, but closely cite each other. © 2010.",2011-01-01,2-s2.0-78650524080,Journal of Informetrics,Scientific collaboration and endorsement: Network analysis of coauthorship and citation networks,"Scientific collaboration and endorsement are well-established research topics which utilize three kinds of methods: survey/questionnaire, bibliometrics, and complex network analysis. This paper combines topic modeling and path-finding algorithms to determine whether productive authors tend to collaborate with or cite researchers with the same or different interests, and whether highly cited authors tend to collaborate with or cite each other. Taking information retrieval as a test field, the results show that productive authors tend to directly coauthor with and closely cite colleagues sharing the same research interests; they do not generally collaborate directly with colleagues having different research topics, but instead directly or indirectly cite them; and highly cited authors do not generally coauthor with each other, but closely cite each other. "
631,"Community Question Answering (CQA) services have evolved into a popular way of information seeking and providing. User-posted questions in CQA are generally organized into hierarchical categories. In this paper, we define and study a novel problem which is referred to as New Category Identification (NCI) in CQA question archives. New Category Identification is primarily concerned with detecting and characterizing new or emerging categories which are not included in the existing category hierarchy. We define this problem formally, and propose both unsupervised and semi-supervised topic modeling methods to solve it. Experiments with a ground-truth set built from Yahoo! Answers show that our methods identify and interpret new categories effectively. © 2010 ACM.",2010-12-01,2-s2.0-78651318340,"International Conference on Information and Knowledge Management, Proceedings",Identifying new categories in Community Question Answering archives: A topic modeling approach,"Community Question Answering (CQA) services have evolved into a popular way of information seeking and providing. User-posted questions in CQA are generally organized into hierarchical categories. In this paper, we define and study a novel problem which is referred to as New Category Identification (NCI) in CQA question archives. New Category Identification is primarily concerned with detecting and characterizing new or emerging categories which are not included in the existing category hierarchy. We define this problem formally, and propose both unsupervised and semi-supervised topic modeling methods to solve it. Experiments with a ground-truth set built from Yahoo! Answers show that our methods identify and interpret new categories effectively. "
632,"Latent Dirichlet allocation (LDA) has been widely used for analyzing large text corpora. In this paper we propose the topic-weak-correlated LDA (TWC-LDA) for topic modeling, which constrains different topics to be weak-correlated. This is technically achieved by placing a special prior over the topic-word distributions. Reducing the overlapping between the topic-word distributions makes the learned topics more interpretable in the sense that each topic word-distribution can be clearly associated to a distinctive semantic meaning. Experimental results on both synthetic and real-world corpus show the superiority of the TWC-LDA over the basic LDA for semantically meaningful topic discovery and document classification. ©2010 IEEE.",2010-12-01,2-s2.0-79851478468,"2010 7th International Symposium on Chinese Spoken Language Processing, ISCSLP 2010 - Proceedings",Topic-weak-correlated latent dirichlet allocation,"Latent Dirichlet allocation (LDA) has been widely used for analyzing large text corpora. In this paper we propose the topic-weak-correlated LDA (TWC-LDA) for topic modeling, which constrains different topics to be weak-correlated. This is technically achieved by placing a special prior over the topic-word distributions. Reducing the overlapping between the topic-word distributions makes the learned topics more interpretable in the sense that each topic word-distribution can be clearly associated to a distinctive semantic meaning. Experimental results on both synthetic and real-world corpus show the superiority of the TWC-LDA over the basic LDA for semantically meaningful topic discovery and document classification. "
633,"We present a topic mixture language modeling approach making use of the soft classification notion of topic models. Given a text document set, we first perform document soft classification by applying a topic modeling process such as probabilistic latent semantic analyses (PLSA) or latent Dirichlet allocation (LDA) on the dataset. Then we can derive topic-specific n-gram counts from the classified texts. Finally we build topic-specific n-gram language models (LM) from the n-gram counts using traditional n-gram modeling approach. In decoding we perform topic inference from the processing context, and we use unsupervised topic adaptation approach to combine the topic-specific models. Experimental results show that the suggested method outperforms the state-of-the-art topic-model-based unsupervised adaptation approaches. ©2010 IEEE.",2010-12-01,2-s2.0-79851480448,"2010 7th International Symposium on Chinese Spoken Language Processing, ISCSLP 2010 - Proceedings",Building topic mixture language models using the document soft classification notion of topic models,"We present a topic mixture language modeling approach making use of the soft classification notion of topic models. Given a text document set, we first perform document soft classification by applying a topic modeling process such as probabilistic latent semantic analyses (PLSA) or latent Dirichlet allocation (LDA) on the dataset. Then we can derive topic-specific n-gram counts from the classified texts. Finally we build topic-specific n-gram language models (LM) from the n-gram counts using traditional n-gram modeling approach. In decoding we perform topic inference from the processing context, and we use unsupervised topic adaptation approach to combine the topic-specific models. Experimental results show that the suggested method outperforms the state-of-the-art topic-model-based unsupervised adaptation approaches. "
634,"Exploring community is fundamental for uncovering the connections between structure and function of complex networks and for practical applications in many disciplines such as biology and sociology. In this paper, we propose a TTR-LDA-Community model which combines the Latent Dirichlet Allocation model (LDA) and the Girvan-Newman community detection algorithm with an inference mechanism. The model is then applied to data from Delicious, a popular social tagging system, over the time period of 2005-2008. Our results show that 1) users in the same community tend to be interested in similar set of topics in all time periods; and 2) topics may divide into several sub-topics and scatter into different communities over time. We evaluate the effectiveness of our model and show that the TTR-LDA-Community model is meaningful for understanding communities and outperforms TTR-LDA and LDA models in tag prediction. © 2010 ACM.",2010-12-01,2-s2.0-78651324362,"International Conference on Information and Knowledge Management, Proceedings",Community-based topic modeling for social tagging,"Exploring community is fundamental for uncovering the connections between structure and function of complex networks and for practical applications in many disciplines such as biology and sociology. In this paper, we propose a TTR-LDA-Community model which combines the Latent Dirichlet Allocation model (LDA) and the Girvan-Newman community detection algorithm with an inference mechanism. The model is then applied to data from Delicious, a popular social tagging system, over the time period of 2005-2008. Our results show that 1) users in the same community tend to be interested in similar set of topics in all time periods; and 2) topics may divide into several sub-topics and scatter into different communities over time. We evaluate the effectiveness of our model and show that the TTR-LDA-Community model is meaningful for understanding communities and outperforms TTR-LDA and LDA models in tag prediction. "
635,"This paper presents a hierarchical generative model that captures the latent relation of cause and effect underlying user behavioral-originated data such as papers, twitter, and purchase history. Our proposal, the Latent Interest Topic model (LIT), introduces a latent variable into each document and each author layer in a coherent generative model. We call the former variable the document class, and the latter variable the author class, where these classes are indicator variables that allow the inclusion of different types of probability, and can be shared over documents with similar content and authors with similar interests, respectively. Significantly, unlike other works, LIT differentiates, respectively, document topics and user interests by using these classes. Consequently, LIT is superior to previous models in explaining the causal relationships behind the data by merging similar distributions; it also makes the computation process easier. Experiments on a research paper corpus show that the proposed model can well capture document and author classes, and reduce the dimensionality of documents to a low-dimensional author-document space, making it useful as a generative model. © 2010 ACM.",2010-12-01,2-s2.0-78651285299,"International Conference on Information and Knowledge Management, Proceedings",Latent interest-topic model: Finding the causal relationships behind dyadic data,"This paper presents a hierarchical generative model that captures the latent relation of cause and effect underlying user behavioral-originated data such as papers, twitter, and purchase history. Our proposal, the Latent Interest Topic model (LIT), introduces a latent variable into each document and each author layer in a coherent generative model. We call the former variable the document class, and the latter variable the author class, where these classes are indicator variables that allow the inclusion of different types of probability, and can be shared over documents with similar content and authors with similar interests, respectively. Significantly, unlike other works, LIT differentiates, respectively, document topics and user interests by using these classes. Consequently, LIT is superior to previous models in explaining the causal relationships behind the data by merging similar distributions; it also makes the computation process easier. Experiments on a research paper corpus show that the proposed model can well capture document and author classes, and reduce the dimensionality of documents to a low-dimensional author-document space, making it useful as a generative model. "
636,"In the field of multi-document summarization, the Pyramid method has become an important approach for evaluating machine-generated summaries. The method is based on the manual annotation of text spans with the same meaning in a set of human model summaries. In this paper, we present an unsupervised, probabilistic topic modeling approach for automatically identifying such semantically similar text spans. Our approach reveals some of the structure of model summaries and identifies topics that are good approximations of the Summary Content Units (SCU) used in the Pyramid method. Our results show that the topic model identifies topic-sentence associations that correspond to the contributors of SCUs, suggesting that the topic modeling approach can generate a viable set of candidate SCUs for facilitating the creation of Pyramids.",2010-12-01,2-s2.0-80053392390,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference",Learning summary content units with topic modeling,"In the field of multi-document summarization, the Pyramid method has become an important approach for evaluating machine-generated summaries. The method is based on the manual annotation of text spans with the same meaning in a set of human model summaries. In this paper, we present an unsupervised, probabilistic topic modeling approach for automatically identifying such semantically similar text spans. Our approach reveals some of the structure of model summaries and identifies topics that are good approximations of the Summary Content Units (SCU) used in the Pyramid method. Our results show that the topic model identifies topic-sentence associations that correspond to the contributors of SCUs, suggesting that the topic modeling approach can generate a viable set of candidate SCUs for facilitating the creation of Pyramids."
637,This work presents a study to bridge topic modeling and personalized search. A probabilistic topic model is used to extract topics from user search history. These topics can be seen as a roughly summary of user preferences and further treated as feedback within the KL-Divergence retrieval model to estimate a more accurate query model. The topics more relevant to current query contribute more in updating the query model which helps to distinguish between relevant and irrelevant parts and filter out noise in user search history. We designed task oriented user study and the results show that: (1) The extracted topics can be used to cluster queries according to topics. (2) The proposed approach improves ranking quality consistently for queries matching user past interests and is robust for queries not matching past interests.,2010-12-01,2-s2.0-80053427148,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference",Bridging topic modeling and personalized search,This work presents a study to bridge topic modeling and personalized search. A probabilistic topic model is used to extract topics from user search history. These topics can be seen as a roughly summary of user preferences and further treated as feedback within the KL-Divergence retrieval model to estimate a more accurate query model. The topics more relevant to current query contribute more in updating the query model which helps to distinguish between relevant and irrelevant parts and filter out noise in user search history. We designed task oriented user study and the results show that: (1) The extracted topics can be used to cluster queries according to topics. (2) The proposed approach improves ranking quality consistently for queries matching user past interests and is robust for queries not matching past interests.
638,"This paper presents a topic-driven framework for generating a generic summary from multi-documents. Our approach is based on the intuition that, from the statistical point of view, the summary's probability distribution over the topics should be consistent with the multi-documents'probability distribution over the inherent topics. Here, the topics are defined as weighted ""bag-of-words"" and derived by Latent Dirichlet Allocation from a collection of documents, either the given multi-documents or a related large-scale corpus. In this sense, we could represent various kinds of text units, such as word, sentence, summary, document and multi-documents, using a single vector space model via their corresponding probability distributions over the derived topics. Therefore, we are able to extract a sentence or summary by calculating the similarity between a sentence/summary and the given multi-documents via their topic probability distributions. In particular, we propose two methods in similarity measurement: the static method and the dynamic method. While the former is employed to detect the salience of information in a static way, the later further controls redundancy in a dynamic way. In addition, we integrate various popular features to improve the performance. Evaluation on the TAC 2008 update summarization task shows encouraging results. © 2010 IEEE.",2010-12-01,2-s2.0-79551514793,"Proceedings - 2010 International Conference on Asian Language Processing, IALP 2010",Topic-driven multi-document summarization,"This paper presents a topic-driven framework for generating a generic summary from multi-documents. Our approach is based on the intuition that, from the statistical point of view, the summary's probability distribution over the topics should be consistent with the multi-documents'probability distribution over the inherent topics. Here, the topics are defined as weighted ""bag-of-words"" and derived by Latent Dirichlet Allocation from a collection of documents, either the given multi-documents or a related large-scale corpus. In this sense, we could represent various kinds of text units, such as word, sentence, summary, document and multi-documents, using a single vector space model via their corresponding probability distributions over the derived topics. Therefore, we are able to extract a sentence or summary by calculating the similarity between a sentence/summary and the given multi-documents via their topic probability distributions. In particular, we propose two methods in similarity measurement: the static method and the dynamic method. While the former is employed to detect the salience of information in a static way, the later further controls redundancy in a dynamic way. In addition, we integrate various popular features to improve the performance. Evaluation on the TAC 2008 update summarization task shows encouraging results. "
639,"The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric mixed membership model-each data point is modeled with a collection of components of different proportions. Though powerful, the HDP makes an assumption that the probability of a component being exhibited by a data point is positively correlated with its proportion within that data point. This might be an undesirable assumption. For example, in topic modeling, a topic (component) might be rare throughout the corpus but dominant within those documents (data points) where it occurs. We develop the IBP compound Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data prevalence and within-data proportion in a mixed membership model. The ICD combines properties from the HDP and the Indian buffet process (IBP), a Bayesian nonparametric prior on binary matrices. The ICD assigns a subset of the shared mixture components to each data point. This subset, the data point's ""focus"", is determined independently from the amount that each of its components contribute. We develop an ICD mixture model for text, the focused topic model (FTM), and show superior performance over the HDP-based topic model. Copyright 2010 by the author(s)/owner(s).",2010-09-17,2-s2.0-77956537774,"ICML 2010 - Proceedings, 27th International Conference on Machine Learning",The IBP compound Dirichlet process and its application to focused topic modeling,"The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric mixed membership model-each data point is modeled with a collection of components of different proportions. Though powerful, the HDP makes an assumption that the probability of a component being exhibited by a data point is positively correlated with its proportion within that data point. This might be an undesirable assumption. For example, in topic modeling, a topic (component) might be rare throughout the corpus but dominant within those documents (data points) where it occurs. We develop the IBP compound Dirichlet process (ICD), a Bayesian nonparametric prior that decouples across-data prevalence and within-data proportion in a mixed membership model. The ICD combines properties from the HDP and the Indian buffet process (IBP), a Bayesian nonparametric prior on binary matrices. The ICD assigns a subset of the shared mixture components to each data point. This subset, the data point's ""focus"", is determined independently from the amount that each of its components contribute. We develop an ICD mixture model for text, the focused topic model (FTM), and show superior performance over the HDP-based topic model. "
640,"Dimension reduction is popular for learning predictive models in high-dimensional spaces. It can highlight the relevant part of the feature space and avoid the curse of dimensionality. However, it can also be harmful because any reduction loses information. In this paper, we propose the projection penalty framework to make use of dimension reduction without losing valuable information. Reducing the feature space before learning predictive models can be viewed as restricting the model search to some parameter sub-space. The idea of projection penalties is that instead of restricting the search to a parameter subspace, we can search in the full space but penalize the projection distance to this subspace. Dimension reduction is used to guide the search, rather than to restrict it. We propose projection penalties for linear dimension reduction, and then generalize to kernel-based reduction and other nonlinear methods. We test projection penalties with various dimension reduction techniques in different prediction tasks, including principal component regression and partial least squares in regression tasks, kernel dimension reduction in face recognition, and latent topic modeling in text classification. Experimental results show that projection penalties are a more effective and reliable way to make use of dimension reduction techniques. Copyright 2010 by the author(s)/owner(s).",2010-09-17,2-s2.0-77956535625,"ICML 2010 - Proceedings, 27th International Conference on Machine Learning",Projection penalties: Dimension reduction without loss,"Dimension reduction is popular for learning predictive models in high-dimensional spaces. It can highlight the relevant part of the feature space and avoid the curse of dimensionality. However, it can also be harmful because any reduction loses information. In this paper, we propose the projection penalty framework to make use of dimension reduction without losing valuable information. Reducing the feature space before learning predictive models can be viewed as restricting the model search to some parameter sub-space. The idea of projection penalties is that instead of restricting the search to a parameter subspace, we can search in the full space but penalize the projection distance to this subspace. Dimension reduction is used to guide the search, rather than to restrict it. We propose projection penalties for linear dimension reduction, and then generalize to kernel-based reduction and other nonlinear methods. We test projection penalties with various dimension reduction techniques in different prediction tasks, including principal component regression and partial least squares in regression tasks, kernel dimension reduction in face recognition, and latent topic modeling in text classification. Experimental results show that projection penalties are a more effective and reliable way to make use of dimension reduction techniques. "
641,"The amount of textual data that is available for researchers and businesses to analyze is increasing at a dramatic rate. This reality has led IS researchers to investigate various text mining techniques. This essay examines four text mining methods that are frequently used in order to identify their characteristics and limitations. The four methods that we examine are (1) latent semantic analysis, (2) probabilistic latent semantic analysis, (3) latent Dirichlet allocation, and (4) correlated topic model. We review these four methods and compare them with topic detection and spam filtering to reveal their peculiarity. Our paper sheds light on the theory that underlies text mining methods and provides guidance for researchers who seek to apply these methods.",2010-09-01,2-s2.0-78649819232,Journal of Computer Information Systems,An empirical comparison of four text mining methods,"The amount of textual data that is available for researchers and businesses to analyze is increasing at a dramatic rate. This reality has led IS researchers to investigate various text mining techniques. This essay examines four text mining methods that are frequently used in order to identify their characteristics and limitations. The four methods that we examine are (1) latent semantic analysis, (2) probabilistic latent semantic analysis, (3) latent Dirichlet allocation, and (4) correlated topic model. We review these four methods and compare them with topic detection and spam filtering to reveal their peculiarity. Our paper sheds light on the theory that underlies text mining methods and provides guidance for researchers who seek to apply these methods."
642,"Topic models could have a huge impact on improving the ways users find and discover content in digital libraries and search interfaces, through their ability to automatically learn and apply subject tags to each and every item in a collection, and their ability to dynamically create virtual collections on the fly. However, much remains to be done to tap this potential, and empirically evaluate the true value of a given topic model to humans. In this work, we sketch out some sub-tasks that we suggest pave the way towards this goal, and present methods for assessing the coherence and inter-pretability of topics learned by topic models. Our large-scale user study includes over 70 human subjects evaluating and scoring almost 500 topics learned from collections from a wide range of genres and domains. We show how a scoring model - based on pointwise mutual information of word-pairs using Wikipedia, Google and MEDLINE as external data sources - performs well at predicting human scores. This automated scoring of topics is an important first step to integrating topic modeling into digital libraries. © 2010 ACM.",2010-08-05,2-s2.0-77955114158,Proceedings of the ACM International Conference on Digital Libraries,Evaluating topic models for digital libraries,"Topic models could have a huge impact on improving the ways users find and discover content in digital libraries and search interfaces, through their ability to automatically learn and apply subject tags to each and every item in a collection, and their ability to dynamically create virtual collections on the fly. However, much remains to be done to tap this potential, and empirically evaluate the true value of a given topic model to humans. In this work, we sketch out some sub-tasks that we suggest pave the way towards this goal, and present methods for assessing the coherence and inter-pretability of topics learned by topic models. Our large-scale user study includes over 70 human subjects evaluating and scoring almost 500 topics learned from collections from a wide range of genres and domains. We show how a scoring model - based on pointwise mutual information of word-pairs using Wikipedia, Google and MEDLINE as external data sources - performs well at predicting human scores. This automated scoring of topics is an important first step to integrating topic modeling into digital libraries. "
643,"This paper addresses the problem of semantics-based temporal expert finding, which means identifying a person with given expertise for different time periods. For example, many real world applications like reviewer matching for papers and finding hot topics in newswire articles need to consider time dynamics. Intuitively there will be different reviewers and reporters for different topics during different time periods. Traditional approaches used graph-based link structure by using keywords based matching and ignored semantic information, while topic modeling considered semantics-based information without conferences influence (richer text semantics and relationships between authors) and time information simultaneously. Consequently they result in not finding appropriate experts for different time periods. We propose a novel Temporal-Expert-Topic (TET) approach based on Semantics and Temporal Information based Expert Search (STMS) for temporal expert finding, which simultaneously models conferences influence and time information. Consequently, topics (semantically related probabilistic clusters of words) occurrence and correlations change over time, while the meaning of a particular topic almost remains unchanged. By using Bayes Theorem we can obtain topically related experts for different time periods and show how experts' interests and relationships change over time. Experimental results on scientific literature dataset show that the proposed generalized time topic modeling approach significantly outperformed the non-generalized time topic modeling approaches, due to simultaneously capturing conferences influence with time information. © 2010 Elsevier B.V. All rights reserved.",2010-08-01,2-s2.0-77955671812,Knowledge-Based Systems,Temporal expert finding through generalized time topic modeling,"This paper addresses the problem of semantics-based temporal expert finding, which means identifying a person with given expertise for different time periods. For example, many real world applications like reviewer matching for papers and finding hot topics in newswire articles need to consider time dynamics. Intuitively there will be different reviewers and reporters for different topics during different time periods. Traditional approaches used graph-based link structure by using keywords based matching and ignored semantic information, while topic modeling considered semantics-based information without conferences influence (richer text semantics and relationships between authors) and time information simultaneously. Consequently they result in not finding appropriate experts for different time periods. We propose a novel Temporal-Expert-Topic (TET) approach based on Semantics and Temporal Information based Expert Search (STMS) for temporal expert finding, which simultaneously models conferences influence and time information. Consequently, topics (semantically related probabilistic clusters of words) occurrence and correlations change over time, while the meaning of a particular topic almost remains unchanged. By using Bayes Theorem we can obtain topically related experts for different time periods and show how experts' interests and relationships change over time. Experimental results on scientific literature dataset show that the proposed generalized time topic modeling approach significantly outperformed the non-generalized time topic modeling approaches, due to simultaneously capturing conferences influence with time information. "
644,"In this paper, the task of text segmentation is approached from a topic modeling perspective. We investigate the use of latent Dirichlet allocation (LDA) topic model to segment a text into semantically coherent segments. A major benefit of the proposed approach is that along with the segment boundaries, it outputs the topic distribution associated with each segment. This information is of potential use in applications like segment retrieval and discourse analysis. The new approach outperforms a standard baseline method and yields significantly better performance than most of the available unsupervised methods on a benchmark dataset. Copyright 2009 ACM.",2009-12-01,2-s2.0-74549168970,"International Conference on Information and Knowledge Management, Proceedings",Text segmentation via topic modeling: An analytical study,"In this paper, the task of text segmentation is approached from a topic modeling perspective. We investigate the use of latent Dirichlet allocation (LDA) topic model to segment a text into semantically coherent segments. A major benefit of the proposed approach is that along with the segment boundaries, it outputs the topic distribution associated with each segment. This information is of potential use in applications like segment retrieval and discourse analysis. The new approach outperforms a standard baseline method and yields significantly better performance than most of the available unsupervised methods on a benchmark dataset. "
645,"Topic-based text summaries promise to help average users quickly understand a text collection and derive insights. Recent research has shown that the Latent Dirichlet Allocation (LDA) model is one of the most effective approaches to topic analysis. However, the LDA-based results may not be ideal for human understanding and consumption. In this paper, we present several topic and keyword re-ranking approaches that can help users better understand and consume the LDA-derived topics in their text analysis. Our methods process the LDA output based on a set of criteria that model a user's information needs. Our evaluation demonstrates the usefulness of the methods in summarizing several large-scale, real world data sets. Copyright 2009 ACM.",2009-12-01,2-s2.0-74549195580,"International Conference on Information and Knowledge Management, Proceedings",Topic and keyword re-ranking for LDA-based topic modeling,"Topic-based text summaries promise to help average users quickly understand a text collection and derive insights. Recent research has shown that the Latent Dirichlet Allocation (LDA) model is one of the most effective approaches to topic analysis. However, the LDA-based results may not be ideal for human understanding and consumption. In this paper, we present several topic and keyword re-ranking approaches that can help users better understand and consume the LDA-derived topics in their text analysis. Our methods process the LDA output based on a set of criteria that model a user's information needs. Our evaluation demonstrates the usefulness of the methods in summarizing several large-scale, real world data sets. "
646,"The proceedings contain 9 papers. The topics discussed include: integrating web-based intelligence retrieval and decision-making from the twitter trends knowledge base; a tag recommendation system for folksonomy; annotating Wikipedia articles with semantic tags for structured retrieval; automobile, car and BMW: horizontal and hierarchical approach in social tagging systems; characterizing the evolution of collaboration network; privacy-enhanced public view for social graphs; commentary-based video categorization and concept discovery; cross-language linking of news stories on the web using interlingual topic modeling; and detecting opinion leaders and trends in online social networks.",2009-12-01,2-s2.0-74249122055,"International Conference on Information and Knowledge Management, Proceedings","2nd ACM Workshop on Social Web Search and Mining, SWSM'09, Co-located with the 18th ACM International Conference on Information and Knowledge Management, CIKM 2009","The proceedings contain 9 papers. The topics discussed include: integrating web-based intelligence retrieval and decision-making from the twitter trends knowledge base; a tag recommendation system for folksonomy; annotating Wikipedia articles with semantic tags for structured retrieval; automobile, car and BMW: horizontal and hierarchical approach in social tagging systems; characterizing the evolution of collaboration network; privacy-enhanced public view for social graphs; commentary-based video categorization and concept discovery; cross-language linking of news stories on the web using interlingual topic modeling; and detecting opinion leaders and trends in online social networks."
647,"We have studied the problem of linking event information across different languages without the use of translation systems or dictionaries. The linking is based on interlingua information obtained through probabilistic topic models trained on comparable corpora written in two languages (in our case English and Dutch). The achieve this, we expand the Latent Dirichlet Allocation model to process documents in two languages. We demonstrate the validity of the learned interlingual topics in a document clustering task, where the evaluation is performed on Google News. Copyright 2009 ACM.",2009-12-01,2-s2.0-74049123165,"International Conference on Information and Knowledge Management, Proceedings",Cross-language linking of news stories on the web using interlingual topic modelling,"We have studied the problem of linking event information across different languages without the use of translation systems or dictionaries. The linking is based on interlingua information obtained through probabilistic topic models trained on comparable corpora written in two languages (in our case English and Dutch). The achieve this, we expand the Latent Dirichlet Allocation model to process documents in two languages. We demonstrate the validity of the learned interlingual topics in a document clustering task, where the evaluation is performed on Google News. "
648,"This paper presents a new Bayesian topical trend analysis. We regard the parameters of topic Dirichlet priors in latent Dirichlet allocation as a function of document timestamps and optimize the parameters by a gradient-based algorithm. Since our method gives similar hyperparameters to the documents having similar timestamps, topic assignment in collapsed Gibbs sampling is affected by timestamp similarities. We compute TFIDF-based document similarities by using a result of collapsed Gibbs sampling and evaluate our proposal by link detection task of Topic Detection and Tracking. Copyright 2009 ACM.",2009-12-01,2-s2.0-74549123327,"International Conference on Information and Knowledge Management, Proceedings",Dynamic hyperparameter optimization for bayesian topical trend analysis,"This paper presents a new Bayesian topical trend analysis. We regard the parameters of topic Dirichlet priors in latent Dirichlet allocation as a function of document timestamps and optimize the parameters by a gradient-based algorithm. Since our method gives similar hyperparameters to the documents having similar timestamps, topic assignment in collapsed Gibbs sampling is affected by timestamp similarities. We compute TFIDF-based document similarities by using a result of collapsed Gibbs sampling and evaluate our proposal by link detection task of Topic Detection and Tracking. "
649,Most approaches to topic modeling assume an independence between documents that is frequently violated. We present an topic model that makes use of one or more user-specified graphs describing relationships between documents. These graph are encoded in the form of a Markov random field over topics and serve to encourage related documents to have similar topic structures. Experiments on show upwards of a 10% improvement in modeling performance. © 2009 ACL and AFNLP.,2009-12-01,2-s2.0-80052655956,"ACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational Linguistics and 4th Int. Joint Conf. on Natural Language Processing of the AFNLP, Proceedings of the Conf.",Markov random topic fields,Most approaches to topic modeling assume an independence between documents that is frequently violated. We present an topic model that makes use of one or more user-specified graphs describing relationships between documents. These graph are encoded in the form of a Markov random field over topics and serve to encourage related documents to have similar topic structures. Experiments on show upwards of a 10% improvement in modeling performance. 
650,"We present a semi-supervised learning method for building domain-specific language models (LM) from general-domain data. This method is aimed to use small amount of domain-specific data as seeds to tap domain-specific resources residing in larger amount of general-domain data with the help of topic modeling technologies. The proposed algorithm first performs topic decomposition (TD) on the combined dataset of domain-specific and general-domain data using probabilistic latent semantic analysis (PLSA). Then it derives domain-specific word n-gram counts with mixture modeling scheme of PLSA. Finally, it uses traditional n-gram modeling approach to construct domain-specific LMs from the domain-specific word n-gram counts. Experimental results show that this approach can outperform both stat-of-the-art methods and the simulated supervised learning method with our data sets. In particular, the semi-supervised learning method can achieve better performance even with very small amount of domain-specific data. © 2009 IEEE.",2009-12-01,2-s2.0-77950896666,"2009 International Conference on Asian Language Processing: Recent Advances in Asian Language Processing, IALP 2009",Semi-supervised learning of domain-specific language models from general domain data,"We present a semi-supervised learning method for building domain-specific language models (LM) from general-domain data. This method is aimed to use small amount of domain-specific data as seeds to tap domain-specific resources residing in larger amount of general-domain data with the help of topic modeling technologies. The proposed algorithm first performs topic decomposition (TD) on the combined dataset of domain-specific and general-domain data using probabilistic latent semantic analysis (PLSA). Then it derives domain-specific word n-gram counts with mixture modeling scheme of PLSA. Finally, it uses traditional n-gram modeling approach to construct domain-specific LMs from the domain-specific word n-gram counts. Experimental results show that this approach can outperform both stat-of-the-art methods and the simulated supervised learning method with our data sets. In particular, the semi-supervised learning method can achieve better performance even with very small amount of domain-specific data. "
651,"Topic modeling has been a key problem for document analysis. One of the canonical approaches for topic modeling is Probabilistic Latent Semantic Indexing, which maximizes the joint probability of documents and terms in the corpus. The major disadvantage of PLSI is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting. Latent Dirichlet Allocation (LDA) is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random variable. Both of these two methods discover the hidden topics in the Euclidean space. However, there is no convincing evidence that the document space is Euclidean, or flat. Therefore, it is more natural and reasonable to assume that the document space is a manifold, either linear or nonlinear. In this paper, we consider the problem of topic modeling on intrinsic document manifold. Specifically, we propose a novel algorithm called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI) for topic modeling. LapPLSI models the document space as a submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question. We compare the proposed LapPLSI approach with PLSI and LDA on three text data sets. Experimental results show that LapPLSI provides better representation in the sense of semantic structure. Copyright 2008 ACM.",2008-12-01,2-s2.0-70349247055,"International Conference on Information and Knowledge Management, Proceedings",Modeling hidden topics on document manifold,"Topic modeling has been a key problem for document analysis. One of the canonical approaches for topic modeling is Probabilistic Latent Semantic Indexing, which maximizes the joint probability of documents and terms in the corpus. The major disadvantage of PLSI is that it estimates the probability distribution of each document on the hidden topics independently and the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting. Latent Dirichlet Allocation (LDA) is proposed to overcome this problem by treating the probability distribution of each document over topics as a hidden random variable. Both of these two methods discover the hidden topics in the Euclidean space. However, there is no convincing evidence that the document space is Euclidean, or flat. Therefore, it is more natural and reasonable to assume that the document space is a manifold, either linear or nonlinear. In this paper, we consider the problem of topic modeling on intrinsic document manifold. Specifically, we propose a novel algorithm called Laplacian Probabilistic Latent Semantic Indexing (LapPLSI) for topic modeling. LapPLSI models the document space as a submanifold embedded in the ambient space and directly performs the topic modeling on this document manifold in question. We compare the proposed LapPLSI approach with PLSI and LDA on three text data sets. Experimental results show that LapPLSI provides better representation in the sense of semantic structure. "
652,"The rapidly increasing popularity of community-based Question Answering (cQA) services, e.g. Yahoo! Answers, Baidu Zhidao, etc. have attracted great attention from both academia and indus-try. Besides the basic problems, like question searching and an-swer finding, it should be noted that the low participation rate of users in cQA service is the crucial problem which limits its de-velopment potential. In this paper, we focus on addressing this problem by recommending answer providers, in which a question is given as a query and a ranked list of users is returned according to the likelihood of answering the question. Based on the intuitive idea for recommendation, we try to introduce topic-level model to improve heuristic term-level methods, which are treated as the baselines. The proposed approach consists of two steps: (1) dis-covering latent topics in the content of questions and answers as well as latent interests of users to build user profiles; (2) recom-mending question answerers for new arrival questions based on latent topics and term-level model. Specifically, we develop a general generative model for questions and answers in cQA, which is then altered to obtain a novel computationally tractable Bayesian network model. Experiments are carried out on a real-world data crawled from Yahoo! Answers during Jun 12 2007 to Aug 04 2007, which consists of 118510 questions, 772962 answers and 150324 users. The experimental results reveal signif-icant improvements over the baseline methods and validate the positive influence of topic-level information. Copyright 2008 ACM.",2008-12-01,2-s2.0-70349236044,"International Conference on Information and Knowledge Management, Proceedings",Tapping on the potential of Q&A community by recommending answer providers,"The rapidly increasing popularity of community-based Question Answering (cQA) services, e.g. Yahoo! Answers, Baidu Zhidao, etc. have attracted great attention from both academia and indus-try. Besides the basic problems, like question searching and an-swer finding, it should be noted that the low participation rate of users in cQA service is the crucial problem which limits its de-velopment potential. In this paper, we focus on addressing this problem by recommending answer providers, in which a question is given as a query and a ranked list of users is returned according to the likelihood of answering the question. Based on the intuitive idea for recommendation, we try to introduce topic-level model to improve heuristic term-level methods, which are treated as the baselines. The proposed approach consists of two steps: (1) dis-covering latent topics in the content of questions and answers as well as latent interests of users to build user profiles; (2) recom-mending question answerers for new arrival questions based on latent topics and term-level model. Specifically, we develop a general generative model for questions and answers in cQA, which is then altered to obtain a novel computationally tractable Bayesian network model. Experiments are carried out on a real-world data crawled from Yahoo! Answers during Jun 12 2007 to Aug 04 2007, which consists of 118510 questions, 772962 answers and 150324 users. The experimental results reveal signif-icant improvements over the baseline methods and validate the positive influence of topic-level information. "
653,"Unstructured information in the form of natural language text is abundant in various kinds of organisations. To increase information sharing, organisational learning, decision-making and productivity, large amounts of unstructured text need to be analysed on a daily basis. Full text searching alone is not sufficient as a first approach to help users understand what a collection of electronic documents is about, since it does not provide the user with an overview of the underlying concepts in the document collection. A topic model is a useful mechanism for identifying and characterising various concepts embedded in a document collection allowing the user to navigate the collection in a topic-guided manner. Topics, made up of significant words, provide the user with an overview of the content of the document collection. Each document is represented as a mixture of automatically constructed topics and the user may select documents related to a specific topic of interest and vice versa. Similarities between documents may be found by looking at what documents are assigned to a specific topic enabling the user to find other documents related to a given document. This methodology enables users to digest a larger number of documents, assisting them in spending more of their time in actually reading than finding relevant information. © 2008 PICMET.",2008-09-30,2-s2.0-52449110280,"PICMET: Portland International Center for Management of Engineering and Technology, Proceedings",Leveraging unstructured information using topic modelling,"Unstructured information in the form of natural language text is abundant in various kinds of organisations. To increase information sharing, organisational learning, decision-making and productivity, large amounts of unstructured text need to be analysed on a daily basis. Full text searching alone is not sufficient as a first approach to help users understand what a collection of electronic documents is about, since it does not provide the user with an overview of the underlying concepts in the document collection. A topic model is a useful mechanism for identifying and characterising various concepts embedded in a document collection allowing the user to navigate the collection in a topic-guided manner. Topics, made up of significant words, provide the user with an overview of the content of the document collection. Each document is represented as a mixture of automatically constructed topics and the user may select documents related to a specific topic of interest and vice versa. Similarities between documents may be found by looking at what documents are assigned to a specific topic enabling the user to find other documents related to a given document. This methodology enables users to digest a larger number of documents, assisting them in spending more of their time in actually reading than finding relevant information. "
